{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "author: Abhishek Kumar Dubey\n",
    "badges: false\n",
    "categories:\n",
    "- Deep Learning\n",
    "date: '2023-03-04'\n",
    "description: Convolutional Neural Networks\n",
    "image: CS5480_images/Acrobat_nlkYEK3c49.png\n",
    "title: Deep Learning 5\n",
    "toc: true\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the problem in using feed-forward networks for image data?\n",
    "\n",
    "- We do rasterization  of image, so we lose special context in case of feed-forward image.\n",
    "\n",
    "- When we flatten the image in case of feed forward network, we will have humongous  number of inputs data, so we will have much more parameter. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks\n",
    "\n",
    "- Must deal with very high dimensional inputs\n",
    "    - 150 x 150 pixels = 22500 inputs, or 3 x 22500 if RGB pixels\n",
    "- Need to look at the 2D topology of pixels (or 3D for video data)\n",
    "\n",
    "- Need invariance to certain variations we can expect\n",
    "    - Translations, illumination, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks\n",
    "\n",
    "- Object can appear either in the left image or in the right image\n",
    "- Output indicates presence of object regardless of position\n",
    "- What do we know about the weights?\n",
    "- Each possible location the object can appear in has its own set of hidden units <br><br>\n",
    "  ![](CS5480_images/Acrobat_EHnrYA8aU9.png)<br><br>\n",
    "- Each set detects the same features except in a different location.\n",
    "- Locations can overlap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally\n",
    "\n",
    "- Local Connectivity\n",
    "- Parameter Sharing\n",
    "- Pooling\n",
    "\n",
    "More recently\n",
    "\n",
    "- Normalization\n",
    "- Last layer customization\n",
    "- Loss functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the cat is there always only to left at the time of training, and cat is it the time of testing at down it will not work,\n",
    "\n",
    "- so what we can do is that we can divide the network in small patches, and we can have different set of weights \n",
    "\n",
    "- But we also want the weight to be shared so can we have just one kernal for all the patches \n",
    "\n",
    "- we want to give focus on high activation on one local area so we can use pooling\n",
    "\n",
    "- using all these we get the notion of convolution network "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local Connectivity\n",
    "\n",
    "- Each hidden unit is connected only to a subregion (patch) of the input image\n",
    "- It is connected to all channels\n",
    "    -1 if greyscale image\n",
    "    - 3 (R, G, B) for color image\n",
    "\n",
    "- Solves the following problems:\n",
    "    - Fully connected hidden layer would have an unmanageable number of parameters\n",
    "    - Computing the linear activations of the hidden units would be very expensive"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Units are connected to all channels:\n",
    "    - 1 channel if grayscale image, 3 channels (R, G, B) if color image <br><br>\n",
    "      ![](CS5480_images/Acrobat_Opxw7db3xf.png)\n",
    "    - Units organized into the same ‘‘feature map’’ share parameters\n",
    "    - Hidden units within a feature map cover different positions in the image<br><br>\n",
    "      ![](CS5480_images/Acrobat_pPoZEY3LbH.png)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter Sharing\n",
    "\n",
    "- Units organized into the same ‘‘feature map’’ share parameters\n",
    "- Hidden units within a feature map cover different positions in the image\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reduces the number of parameters further\n",
    "- Will extract the same features at every position\n",
    " - features are equivariant or invariant\n",
    "\n",
    "we want invariance here\n",
    "\n",
    "equivariant means if we change  the input by 10, output  will change by 10, but in our chase we want even if position change the output should be the same"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local Connectivity and Parameter Sharing\n",
    "\n",
    "- Fully connected layer\n",
    "\n",
    "    ![](CS5480_images/Acrobat_GWKAwfBZh7.png)\n",
    "\n",
    "    - Hidden Units: 120,000\n",
    "    - Params : 14.4 billion\n",
    "    - 200x200x3\n",
    "\n",
    "- Locally connected layer\n",
    "\n",
    "    ![](CS5480_images/Acrobat_zOYTpBJnbo.png)\n",
    "\n",
    "    - Hidden Units: 120,000\n",
    "    - Params : 3.2 Million\n",
    "\n",
    "- The convolution of an image $X$ with a kernel $k$ is computed as:\n",
    "  $$(x*k)_{ij}=\\,\\sum_{p q}\\,x_{i+p,j+q}\\,\\,\\,k_{r-p,r-q}$$\n",
    "  \n",
    "  ![](CS5480_images/Acrobat_mwFEX4uV0m.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional layer with single feature map.\n",
    "\n",
    "- Sharing parameters\n",
    "- Exploiting the stationarity property and preserves locality of pixel dependencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Layer (with Stride)\n",
    "\n",
    "![](CS5480_images/Acrobat_cCg2muzMGF.png)\n",
    "\n",
    "- Image size: $W_1\\times H_1\\times D_1$\n",
    "- Receptive field size: $F\\times F$\n",
    "- #Feature maps: $K$\n",
    "\n",
    "- It is also better to do zero padding to preserve input size spatially.\n",
    "\n",
    "- $W_2=(W_1-F)/S+1$\n",
    "- $H_2=(H_1-F)/S+1$\n",
    "- $D_2=K$\n",
    "\n",
    "- stride reduces the computation\n",
    "- aggregating could help in getting better solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-tip}\n",
    "\n",
    "while looking back\n",
    "- To find receptive field size $r_{l-1}$ of a layer $l-1$ :\n",
    "  $$r_{l-1}=r_l*s_l+(f_l-s_l)$$\n",
    "  here, $l_l$ is the receptive field size of the current layer<br>\n",
    "  $s_l$ is the stride which brings to the current layer, <br>\n",
    "  $f_l$ is the kernel size which brings to the current layer.<br>\n",
    "  \n",
    "while looking front\n",
    "- To find receptive filed size at next layer\n",
    "  $$r_{l+1}=f_l+s_l(r_l-1)$$\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Convolutional Layer\n",
    "\n",
    "![](CS5480_images/Acrobat_szlFh8M2e6.png)\n",
    "\n",
    "$$y_{j}^{n}\\,=\\,f(\\sum_{k=1}^{F}\\,x_{k}^{n-1}\\,*\\,w_{k j}^{n})$$\n",
    "\n",
    "- Here $f$ is a non linear activation function.\n",
    "- $F =$ no. of feature maps\n",
    "- $n=$ layer index \n",
    "- $*$ represents element by element multiplication"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A typical deep convolutional network\n",
    "\n",
    "![](CS5480_images/Acrobat_6ajOZqhuPk.png)\n",
    "\n",
    "Other layers\n",
    "\n",
    "- Pooling ( Next key idea)\n",
    "- Normalization\n",
    "- Fully connected\n",
    "\n",
    "Pooling Layer\n",
    "\n",
    "- Role of an aggregator.\n",
    "- Invariance to image transformation and increases compactness to representation.\n",
    "- Pooling types: Max, Average, L2 etc.\n",
    "- Every patch is non-overlapping.\n",
    "\n",
    "- Take max value from a non-overlapping patch\n",
    "- pooling is done on each channel individually \n",
    "\n",
    "- It serves the purpose of aggregator \n",
    "- It also servers the purpose of little bit of local in-variance.\n",
    "- Because we are taking best in a local patch.\n",
    "- we can do max agv etc, but max works the best.\n",
    "- There are no weight on the pooling layer, nothing to learn\n",
    "- The subsequent layer gets even lesser work to do due to max pool"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization Layer\n",
    "\n",
    "- Local contrast normalization (Jarrett\n",
    "    - Improves invariances\n",
    "    - Improves sparsity\n",
    "- Local response normalization \n",
    "    - Kind of “lateral inhibition” and performed across the channels\n",
    "- Batch normalization\n",
    "    - Activation of the mini batch is centered to zero mean and unit variance to prevent internal covariate shifts."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially CNN had no normalization layer \n",
    "\n",
    "- Now we have batch norm\n",
    "- we can control the varience using batch norm\n",
    "\n",
    "- there are other norm\n",
    "- local contrast norm, normalize the contrast in a patch, locally for every patch\n",
    "\n",
    "- local response  normalization\n",
    "\n",
    "    - take one top left pixel across all the feature map, ie across channel and normalize it, \n",
    " kind of lateral inhibition and performed across the channels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "two successive 3x3 filter is same as 5x5 filter "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully Connected Layer\n",
    "\n",
    "- Multi layer perceptron\n",
    "- Role of a classifier\n",
    "- Generally used in final layers to classify the object represented in terms of discriminative parts and higher semantic entities.\n",
    "- SoftMax\n",
    "    - Normalizes the output.\n",
    "      $$z_{n}=\\frac{e^{x_{n}}}{\\sum_{i=1}^{K}e^{x_{i}}}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case Study: Alex Net\n",
    "\n",
    "- Winner of ImageNet LSVRC 2012.\n",
    "- Trained over 1.2M images using SGD with regularization.\n",
    "- Deep architecture (60M parameters)\n",
    "- Optimized GPU implementation (cuda convnet)\n",
    "- input to Alex net is fromm image net, image size 224 x 224, filter size is  11x11x3\n",
    "and stride  of 4\n",
    "- image itself was padded with 3 pix to make 227 x 227\n",
    "(227-11)/4+1= 55"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet Architecture\n",
    "\n",
    "- 8 Layers in total (5 convolutional layers, 3 fully connected layers)\n",
    "- Trained on ImageNet Dataset \n",
    "- Response normalization layers follow the first and second convolutional layers.\n",
    "- Max pooling follow first, second and the fifth convolutional layers.\n",
    "- The ReLU non linearity is applied to the output of every layer\n",
    "\n",
    "![](CS5480_images/Acrobat_tOFxUd9LdT.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter Calculation\n",
    "\n",
    "- $W_2=[\\ (\\left.W_1-F+2P\\right)/S]+1 \\text{ and } D_2=K$\n",
    "- $S = 4, W_1 = 227, F =11, P = 0, D_2 = 96$\n",
    "- $W_2 = (227 - 11 )/4 + 1 = 55$\n",
    "- Output Size: $55 \\times 55 \\times 96$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convolutional layers cumulatively contain about 90-95% of computation, only about 5% of the parameters\n",
    "- Fully connected layers contain about 95% of parameters.\n",
    "  ![](CS5480_images/Acrobat_Y27pyAtToa.png)\n",
    "- Trained with stochastic gradient descent\n",
    "  - on two NVIDIA GTX 580 3GB GPUs, for about a week\n",
    "- 650,000 neurons, 60 M parameters, 630 M connections, Final feature layer: 4096 - dimensional\n",
    "- Learning: Minimizing the loss function (incl. regularization) w.r.t . parameters of the network\n",
    "  $$\\theta^{\\ast}=a r g m i n_{\\theta}\\sum_{n=1}^{N}L(x^{n},y^{n},\\theta)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation in CNNs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know that we have modular designs, so we will focus on a backward pass on just one module.\n",
    "Let the points on $X,$ $w$ and $y$ be $X(r,c)$, $w(a,b)$ and $y(r,c)$.\n",
    "Consider the below diagram.\n",
    "\n",
    "![](CS5480_images/Acrobat_VDdtVH22PT.png)\n",
    "\n",
    "$X\\left\\lbrack r+a,c+b\\right\\rbrack$ Can be considered as a cropped part of the image where filter was applied in a particular stride. So now we can write the below equation.\n",
    "\n",
    "$$y\\left\\lbrack r,c\\right\\rbrack =\\sum_{a=0}^{k_1 -1} \\sum_{b=0}^{k_2 -1} X\\left\\lbrack r+a,c+b\\right\\rbrack w\\left\\lbrack a,b\\right\\rbrack$$\n",
    "\n",
    "This equation denotes the convolution of the cropped part of the image.\n",
    "\n",
    "Now we want to calculate partial derivative of $L$ with respect to $W$ at the point $a', b'$. Since the filter $w[a',b']$ effects every parts of  $y$ so we need to take a summation over $y$. Since y has a dimension of $r\\times c$. So we can write the equation as shown below.\n",
    "$$\\frac{\\partial L}{\\partial w\\left\\lbrack a^{\\prime } ,b^{\\prime } \\right\\rbrack }=\\sum_{r=0}^{N_1 -1} \\sum_{c=0}^{N_2 -1} \\frac{\\partial L}{\\partial y\\left\\lbrack r,c\\right\\rbrack }\\frac{\\partial y\\left\\lbrack r,c\\right\\rbrack }{\\partial w\\left\\lbrack a^{\\prime } ,b^{\\prime } \\right\\rbrack }$$\n",
    "\n",
    "The quantity $\\frac{\\partial L}{\\partial y\\left\\lbrack r,c\\right\\rbrack }$  is known as it is incoming gradient from the upper layer in general, so we need to calculate only $\\frac{\\partial y\\left\\lbrack r,c\\right\\rbrack }{\\partial w\\left\\lbrack a^{\\prime } ,b^{\\prime } \\right\\rbrack }$, This equation denotes the partial derivative of $y$ with respect to a particular point of $w$,Which depends on only the same particular  points in corresponding to $X$. So we can write.\n",
    "$$\\frac{\\partial y\\left\\lbrack r,c\\right\\rbrack }{\\partial w\\left\\lbrack a^{\\prime } ,b^{\\prime } \\right\\rbrack }=X\\left\\lbrack r+a^{\\prime } ,c+b^{\\prime } \\right\\rbrack$$\n",
    "So finally we can write as below.\n",
    "$$\\frac{\\partial L}{\\partial w\\left\\lbrack a^{\\prime } ,b^{\\prime } \\right\\rbrack }=\\sum_{r=0}^{N_1 -1} \\sum_{c=0}^{N_2 -1} \\frac{\\partial L}{\\partial y\\left\\lbrack r,c\\right\\rbrack }X\\left\\lbrack r+a^{\\prime } ,c+b^{\\prime } \\right\\rbrack$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to find the gradient with respect to $X$.Here we need to understand that a particular point in $X$ affects an area (patch) in $y$ Let's call this patch as $P_Y$.\n",
    "$$\\frac{\\partial L}{\\partial X\\left\\lbrack r^{\\prime } ,c^{\\prime } \\right\\rbrack }=\\sum_{P_y }^{\\;} \\frac{\\partial L}{\\partial y_{P_y } }\\frac{\\partial y_{P_y } }{\\partial X\\left\\lbrack r^{\\prime } ,c^{\\prime } \\right\\rbrack }$$\n",
    "\n",
    "Which can be written as :\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial X\\left\\lbrack r^{\\prime } ,c^{\\prime } \\right\\rbrack }=\\sum_{a=0}^{k_1 -1} \\sum_{b=0}^{k_2 -1} \\frac{\\partial L}{\\partial y\\left\\lbrack r^{\\prime } -a,c^{\\prime } -b\\right\\rbrack }\\frac{\\partial y\\left\\lbrack r^{\\prime } -a,c^{\\prime } -b\\right\\rbrack }{\\partial X\\left\\lbrack r^{\\prime } ,c^{\\prime } \\right\\rbrack }$$\n",
    "\n",
    "same as earlier we already know $\\frac{\\partial L}{\\partial y\\left\\lbrack r^{\\prime } -a,c^{\\prime } -b\\right\\rbrack }$ we only need to compute $\\frac{\\partial y\\left\\lbrack r^{\\prime } -a,c^{\\prime } -b\\right\\rbrack }{\\partial X\\left\\lbrack r^{\\prime } ,c^{\\prime } \\right\\rbrack }$\n",
    "\n",
    "To find this now we first look at the conv operation itself:\n",
    "\n",
    "$$y\\left\\lbrack r^{\\prime } ,c^{\\prime } \\right\\rbrack =\\sum_{a^{\\prime } =0}^{k_1 -1} \\sum_{b^{\\prime } =0}^{k_2 -1} X\\left\\lbrack r^{\\prime } +a^{\\prime } ,c^{\\prime } +b^{\\prime } \\right\\rbrack w\\left\\lbrack a^{\\prime } ,b^{\\prime } \\right\\rbrack$$\n",
    "\n",
    "Hence \n",
    "\n",
    "$$y\\left\\lbrack r^{\\prime } -a,c^{\\prime } -b\\right\\rbrack =\\sum_{a^{\\prime } =0}^{k_1 -1} \\sum_{b^{\\prime } =0}^{k_2 -1} X\\left\\lbrack r^{\\prime } -a+a^{\\prime } ,c^{\\prime } -b+b^{\\prime } \\right\\rbrack w\\left\\lbrack a^{\\prime } ,b^{\\prime } \\right\\rbrack$$\n",
    "\n",
    "In the above equation which $w$ correspond  to $\\partial X\\left\\lbrack r^{\\prime } ,c^{\\prime } \\right\\rbrack$, the answer is $w[a,b]$ because in the above equation when $a^\\prime=a$ and $b^\\prime=b$ it becomes $X\\left\\lbrack r^{\\prime } ,c^{\\prime } \\right\\rbrack$<br>\n",
    "In short we just want to know the coefficient of $X\\left\\lbrack r^{\\prime } ,c^{\\prime } \\right\\rbrack$ which is $w\\left\\lbrack a,b\\right\\rbrack$\n",
    "\n",
    "Hence \n",
    "\n",
    "$$\\frac{\\partial y\\left\\lbrack r^{\\prime } -a,c^{\\prime } -b\\right\\rbrack }{\\partial X\\left\\lbrack r^{\\prime } ,c^{\\prime } \\right\\rbrack }=w\\left\\lbrack a,b\\right\\rbrack$$\n",
    "\n",
    "Finally we get \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial X\\left\\lbrack r^{\\prime } ,c^{\\prime } \\right\\rbrack }=\\sum_{a=0}^{k_1 -1} \\sum_{b=0}^{k_2 -1} \\frac{\\partial L}{\\partial y\\left\\lbrack r^{\\prime } -a,c^{\\prime } -b\\right\\rbrack }w\\left\\lbrack a,b\\right\\rbrack$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling Layer:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider Max pooling.Now consider 2 cross 2 grid, It has total 4 value.Among all these 4 value only one value goes in forward direction.It means that only one has contributed.So the, one who contributes gets rewarded.It means that while doing back propagation, only one will receive the gradient. Other 3 will not receive any gradient.\n",
    "\n",
    "$$x = \\begin{cases}\n",
    "    {\\frac{\\sum_{k=1}x_{k}}{m}},\\,{\\frac{\\partial g}{\\partial x}}={\\frac{1}{m}} &\\text{mean pooling }  \\\\\n",
    "    {\\mathrm{max}}(x),{\\frac{\\partial g}{\\partial x_i}} = \\begin{cases}\n",
    "        1 &\\text{if } x_i=\\max(x) \\\\\n",
    "        c &\\text{if } 0, \\text{otherwise}\n",
    "    \\end{cases} &\\text{Max pooling } \\\\\n",
    "    \\|x\\|_{p}=\\left(\\sum_{k=1}|x_{k}|^{p}\\right)^{1/p}\\ \\ ,{\\frac{\\partial g}{\\partial x_i}}=\\left(\\sum_{k=1}|x_{k}|^{p}\\right)^{1/p-1}|x_{i}|^{p-1}& L^P \\text{ pooling } \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "In max pooling, the unit that contributed (won) gets all the gradient."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectures of CNNs over time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet\n",
    "\n",
    "![](CS5480_images/Acrobat_DoxcC8rHEw.png)\n",
    "\n",
    "- Conv filters were 5 cross 5 and were applied with stride one.\n",
    "- Pooling layer where 2 x 2 and were applied with with stride two.\n",
    "- Show the architecture was Conv Pool Conv Pool FC FC.\n",
    "\n",
    "ZFNet\n",
    "\n",
    "![](CS5480_images/Acrobat_HqqmCEoD1g.png)\n",
    "\n",
    "![](CS5480_images/Acrobat_CoqJ1kTWMm.png)\n",
    "\n",
    "- CONV1 : Change form (11 x 11 stride 4) to (7 x 7 stride 2)\n",
    "- CONV3,4,5: instead of 384, 384, 256 filters use 512, 1024, 512\n",
    "-  By doing this they brought ImageNet top 5 error from 16.4% to 11.7% "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG Net\n",
    "\n",
    "![](CS5480_images/Acrobat_HJaB3F0w53.png)\n",
    "\n",
    "- More layers lead to more nonlinearities\n",
    "- Only 3 x 3 CONV stride 1, pad 1 and 2 x 2 MAXPOOL stride 2\n",
    "- Smaller receptive fields:\n",
    "    - less parameters; faster\n",
    "    - two 3 X 3 leads to 5 X 5; three 3 x 3 leads to 7 x 7, so we can have only 3 x 3 receptive field and should be enough\n",
    "    - Fewer parameters\n",
    "        - Three $3 ^2C^2$ (VS) $7 ^2C^2$\n",
    "\n",
    "- There was a performance improvement from VGG 11 to 13 and also from 13 to 16. But there was no significant performance improvement from VGG 16 to 19. In fact, the error increased. This can be attributed to vanishing gradient problem.\n",
    "\n",
    "  ![](CS5480_images/Acrobat_OPnnN7lsfL.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogLeNet\n",
    "\n",
    "This was a deeper network with more computational efficiency.\n",
    "\n",
    "- It has total 22 layers.\n",
    "- It has efficient inception modules.\n",
    "- It doesn't have any fully connected layers.\n",
    "- It has only 5,000,000 parameters, which is 12 times lesser than the alexnet.\n",
    "- ILSVRC's 14 classification winner with  6.7% top error 5 error.\n",
    "\n",
    "What is an inception module?\n",
    "\n",
    "- The idea here is to design a good local network topology and then stack these modules on top of each other.\n",
    "- Apply combination of many parallel kernel operations like 1 cross 1 convolution 3 cross 3.convolution operations.\n",
    "- Can also apply the polling operation in parallel.\n",
    "- Instead of choosing just one kernel size, we can choose many kernel size and use all together in parallel.\n",
    "- But now there are 2 problems. The first problem is the computational complexity and the second problem is the size of the output. So to address this issue we use one cross one convolution before others convolutions.\n",
    "- The notion of one cross one convolution is also called as the bottleneck layer.\n",
    "  ![](CS5480_images/Acrobat_Z7LpyGWRWD.png)\n",
    "  ![](CS5480_images/Acrobat_4Mckbj3F7Y.png)\n",
    "- This network doesn't use any fully connected layer at the output.\n",
    "- Before stacking all the inception module it usages Stem network.\n",
    "- It also uses auxiliary classification output to inject additional gradients at lower layer.It helps improving the gradient to flow at the lower layers.\n",
    "  ![](CS5480_images/Acrobat_nlkYEK3c49.png)\n",
    "- It has 22 total layer with weights including each parallel layer in the inception module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "$\\tiny  {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0e5e69b8442e8f020791fad6bfcef3777f0e89e0f1a2517b6b628b2eaf0fe66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
