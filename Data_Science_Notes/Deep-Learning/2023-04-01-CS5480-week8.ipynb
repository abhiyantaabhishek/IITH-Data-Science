{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "author: Abhishek Kumar Dubey\n",
    "badges: false\n",
    "categories:\n",
    "- Deep Learning\n",
    "date: '2023-04-01'\n",
    "description: Transformer. Attention is all you need, Self-Attention, Multi-Head Attention, Positional Encoding, Deep Generative Models\n",
    "image: CS5480_images/VoiceAccess_z1E32fzQvA.png\n",
    "title: Deep Learning 8\n",
    "toc: true\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention and transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transformers are primarily encoder decoder model.\n",
    "- Encoder takes input sentence and creates a bottleneck representation.\n",
    "- Bottleneck is smaller in size compared to input\n",
    "- In general, Decoder takes bottleneck as an input.And reconstructs the input back.\n",
    "- It is a translation task. The quality of the translation depends on the summary provided by the encoder.\n",
    "- RNN or LSTM Doesn't provide a good Summary."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key component of the transformer:\n",
    "\n",
    "- Self attention.\n",
    "- Multi head attention.\n",
    "- Positional encoding.\n",
    "- Encoder Decoder architecture.\n",
    "<br><br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transformers in a Nutshell:<br><br>\n",
    "    ![](CS5480_images/VoiceAccess_V61qdBU65b.png)<br><br>\n",
    "    ![](CS5480_images/VoiceAccess_zJJCB4CiGy.png)<br><br>\n",
    "    ![](CS5480_images/VoiceAccess_Pb8m3aFp28.png)<br><br>\n",
    "    ![](CS5480_images/VoiceAccess_9R273DW1o1.png)<br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-Attention\n",
    "\n",
    "- Consider two input sentences we want to translate:\n",
    "    - The animal didn’t cross the street because it was too\n",
    "    tired.\n",
    "    - The animal didn’t cross the street because it was too\n",
    "    wide\n",
    "- \"it\" refers to \"animal\" in first case, but to \"street\" in\n",
    "second case; this is hard for traditional Seq2Seq models\n",
    "to model\n",
    "- As the model processes each word, self-attention allows\n",
    "it to look at other positions in input sequence to help\n",
    "get a better encoding<br><br>\n",
    "  ![](CS5480_images/VoiceAccess_0cqvlyBB66.png) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention Explained\n",
    "\n",
    "- STEP 1<br><br>\n",
    "    ![](CS5480_images/VoiceAccess_y7rqLCQ9T5.png)<br><br>\n",
    "    - Create three vectors from encoder’s input vector $(x_i)$\n",
    "        - Query vector $(q_i)$\n",
    "        - Key vector $(k_i)$\n",
    "        - Value vector $(v_i)$\n",
    "    - These are created by multiplying input with weight matrices $W^Q,W^K,W^V$ learned during training.\n",
    "    - In the paper $q,k,v \\in \\mathbb{R}^{64}$ and $x \\in \\mathbb{R}^{512}$\n",
    "    - $q, k, v$ doesn't have to be smaller then $x$, It was tend to make computation of multi head attention constant.\n",
    "    - Dimensions of $W^Q,W^K,W^V$ are $512\\times 64$\n",
    "\n",
    "- STEP 2<br><br>\n",
    "    ![](CS5480_images/VoiceAccess_6FUXClgCYJ.png)<br><br>\n",
    "    - Calculate self-attention scores :  score all words of input sentence against themselves;\n",
    "    - Take dot product of query vector with key vector of respective words.\n",
    "    - E.g. for input “Thinking”, first score would be $q_1 \\cdot k_1$ (with itself); second score would be dot product of $q_1 \\cdot k_2$ (with ”Machines”), and so on ...\n",
    "    - Scores then divided by $\\sqrt{\\text{length}(k)}$\n",
    "    - This is Scaled Dot-Product Attention, this design choice leads to more stable gradients.<br><br>\n",
    "\n",
    "- STEP 3<br><br>\n",
    "    ![](CS5480_images/VoiceAccess_8m1UoW4e4o.png)\n",
    "    - Softmax used to get normalized probability scores; determines how much each word will be expressed at this position.\n",
    "    - Clearly, word at this position will have highest softmax score, but sometimes it’s useful to attend to another word that is relevant.\n",
    "\n",
    "- STEP 4\n",
    "    - Multiply each value vector by softmax score; It  Keeps values of word(s) we want to focus on intact, and drown out irrelevant words.\n",
    "\n",
    "- STEP 5\n",
    "    - Sum up weighted value vectors :  Produces output of self-attention layer at this position (for first word)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention: Illustration\n",
    "\n",
    "![](CS5480_images/VoiceAccess_sUvKVQ9Awg.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "- Improves performance of the attention layer in two ways:<br><br>\n",
    "    ![](CS5480_images/VoiceAccess_omGGIQzzOe.png)<br><br>\n",
    "    - Expands model’s ability to focus on different positions. In example above, $z_1$ contains a bit of every other encoding, but dominated by actual word itself\n",
    "    - Gives attention layer multiple “representation subspaces”; we have not one, but multiple sets of Query/Key/Value weight matrices; after training, each set is used to project input embeddings into different representation subspaces."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention: Illustration\n",
    "\n",
    "![](CS5480_images/VoiceAccess_x7a00yNbeH.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "- Unlike RNN and CNN encoders, attention encoder outputs do not depend on order of inputs.\n",
    "- But order of sequence conveys important information for machine translation tasks and language modeling.\n",
    "- The idea: Add positional information of input token in the sequence into input embedding vectors.\n",
    "  $$PE_{pos,2i}=\\sin\\left(\\frac{pos}{1000^{\\frac{2i}{d_{emb}}}}\\right)$$\n",
    "  $$PE_{pos,2i+1}=\\cos\\left(\\frac{pos}{1000^{\\frac{2i}{d_{emb}}}}\\right)$$  \n",
    "- Final input embeddings are concatenation of learnable embedding and positional encoding."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "- Stack of N=6 identical layers <br><br>\n",
    "  ![](CS5480_images/VoiceAccess_z1E32fzQvA.png)<br><br>\n",
    "  ![](CS5480_images/VoiceAccess_m1nmG5Pcx5.png)<br><br>\n",
    "- Each layer has a multi-head self-attention layer and a simple position-wise fully connected feed forward network.\n",
    "- Each sub-layer has a residual connection and layer-normalization; all sub-layers output data of same dimension $d_{\\text{model}} = 512$<br><br>\n",
    "  ![](CS5480_images/VoiceAccess_83AW5w6jyq.png) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "- Stack of N=6 identical layers<br><br>\n",
    "  ![](CS5480_images/VoiceAccess_NQxHvCdS9h.png)<br><br>\n",
    "- Each layer has two sub-layers of multi-head attention mechanisms and one sub-layer of fully-connected feedforward network.\n",
    "- Similar to encoder, each sub-layer adopts a residual connection and a layer-normalization.\n",
    "- First multi-head attention sub-layer is modified to prevent positions from attending to subsequent positions, as we don’t want to look into future of target sequence when predicting current position."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers: Full Architecture\n",
    "\n",
    "![](CS5480_images/VoiceAccess_FzItVRymZa.png)<br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers in NLP: Language Understanding\n",
    "\n",
    "- Pre-training language models showed impressive results on multiple natural language processing tasks like natural language inference, paraphrasing, named entity recognition and question answering.\n",
    "- To apply the learnt language representations to downstream tasks, two strategies are followed: feature − based and fine − tuning.\n",
    "    - Feature-based methods develop task specific architectures and utilize the pre-trained knowledge in solving the task\n",
    "    - Fine-tuning approaches try to minimize task-specific parameters and solve the task by fine-tuning pre-trained parameters.\n",
    "- Both these strategies used unidirectional language models for pre-training which restricted the power of the language representations for fine-tuning\n",
    "- Jacob et al solved this problem by using a 'masked language model' (MLM) pre-training objective in their paper, BERT.<br><br>\n",
    "  ![](CS5480_images/VoiceAccess_avTvLXIlzV.png)<br><br>\n",
    "- Recent works achieved performance gains on many NLP tasks by pre-training on large data and fine-tuning on specific tasks\n",
    "- Although task-agnostic, these methods still require large task-specific data for fine-tuning. In contrary, humans can learn a new language task by simple instructions only.\n",
    "- In an attempt to bridge this gap, Brown et al show that we can improve task-agnostic few-shot performance by scaling up language models.\n",
    "- Their model, named GPT-3 is trained with 175 billion parameters and it shows significant performance gain when tested in few-shot setting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers in Computer Vision: Object Detection\n",
    "\n",
    "![](CS5480_images/VoiceAccess_nV23kN7fUE.png)<br><br>\n",
    "![](CS5480_images/VoiceAccess_jCynWy6UYw.png)<br><br>\n",
    "![](CS5480_images/VoiceAccess_5fZQ0fEPKO.png)<br><br>\n",
    "\n",
    "- Image split into fixed-size patches\n",
    "- Each of them linearly embedded\n",
    "- Position embeddings added to resulting sequence of vectors\n",
    "- Patches fed to standard Transformer encoder\n",
    "- In order to perform classification, standard approach of adding an extra learnable “classification token” added to sequence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers in Computer Vision: General Purpose Backbone\n",
    "\n",
    "- Problems in the previous transformer architectures:\n",
    "    - Tokens are of fixed scale in NLP problems, which is not suitable in vision tasks.\n",
    "    - Computational complexity is quadratic to image size in ViT model and using high resolution images for dense pixel level prediction tasks would make the computation intractable.\n",
    "- To solve these problems, Liu et al proposed Swin Transformer with hierarchical feature maps and linear computational complexity.\n",
    "- An image is split into non-overlapping patches and to produce hierarchical feature maps, these patches are merged as we go deep into the network.\n",
    "- The original transformer block is replaced by a Swin transformer block that uses shifted windows and generates feature maps with a linear computational complexity.<br><br>\n",
    "\n",
    "![](CS5480_images/VoiceAccess_ce3SgnjODp.png)<br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers in other applications\n",
    "\n",
    "- Transformers now used in various applications like image classification, object detection/tracking, speech recognition and graph representation\n",
    "- Dong et al proposed Speech-Transformer that proposes a 2D-Attention mechanism that attends to both time and frequency axes jointly\n",
    "- Liu et al introduced a self-supervised speech pre-training method called TERA which uses alteration along three orthogonal axes to pre-train Transformer Encoders on a large amount of unlabeled speech\n",
    "- Graph Neural Networks (GNNs) used to learn node representations on fixed and homogeneous graphs.Yun et al proposed Graph Transformer Networks (GTNs) that are capable of learning from mis-specified and heterogeneous graphs.\n",
    "- Ying et al proposed Graphormer, which attained excellent results on a broad range of graph representation learning tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Generative Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminative vs Generative Models\n",
    "\n",
    "- Discriminative models: Learn conditional distribution $p( y|x)$\n",
    "- Generative models: Learn joint distribution $p( x,y )$ and infer conditional through Bayes’ rule\n",
    "- Advantage of generative models\n",
    "    - Potential to understand and explain the underlying structure of input data even when there are no labels\n",
    "    - Simulate environments\n",
    "    - Multimodal learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applications of Deep Generative Models \n",
    "\n",
    "- (High resolution/Interactive) Image generation\n",
    "- Debiasing\n",
    "- Outlier detection\n",
    "- Image inpainting\n",
    "- Text to image generation\n",
    "- Image to image translation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taxonomy of Generative Models \n",
    "\n",
    "![](CS5480_images/VoiceAccess_q1yPqYvXtv.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Networks\n",
    "\n",
    "- Generator<br><br>\n",
    "  ![](CS5480_images/VoiceAccess_HcTdckzmKU.png)<br><br>\n",
    "- Discriminator <br><br>\n",
    "  ![](CS5480_images/VoiceAccess_TfqiVmy47A.png)<br><br>\n",
    "- Generative Adversarial Networks<br><br>\n",
    "  ![](CS5480_images/VoiceAccess_otPTVqGFaV.png)<br><br>\n",
    "- Generator $G$ picks a point $z$ from noise distribution, and produces a sample.\n",
    "- $G(z)$ represents a mapping from noise distribution to the input space.\n",
    "- $G(\\cdot)$ is a network\n",
    "- Discriminator $D$ takes input from both real world images and generated image $G(z)$, and assigns the correct label to both real world image data and generated image.\n",
    "- We simultaneously train $G$ and $D$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN: 2 Player Game\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"50%\"}\n",
    "Generator G tries to:\n",
    "\n",
    "- maximize $D(G(z))$ or,<br> minimize $(1- D(G(z)))$\n",
    "- minimize $D(x)$\n",
    "- So, we can combine both and get:\n",
    "  $$\\boxed{\\text{minimize } D(x) + (1-D(G(z)))}$$\n",
    ":::\n",
    "\n",
    "::: {.column width=\"50%\"}\n",
    "Discriminator D tries to:\n",
    "\n",
    "- maximize $D(x)$\n",
    "- minimize $D(G(z))$ or,<br> maximize $(1 -D(G(z)))$\n",
    "- So, we can combine both and get:\n",
    "  $$\\boxed{\\text{maximize } D(x) + (1-D(G(z)))}$$\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "So finally both plays mix max game:\n",
    " $$\\boxed{\\min_G \\max_D D(x) + (1-D(G(z)))}$$\n",
    "To analyze average behaviour, we will take expectation:\n",
    "$$\\boxed{\\min_G \\max_D \\mathbb{E}[D(x)] + \\mathbb{E}[(1-D(G(z)))]}$$\n",
    "\n",
    "Now we know that $x$ is sampled from real data , $z$ is sampled from normal distribution $\\mathcal{N}(0,1)$, we can write the above equation in log form as below\n",
    "\n",
    "$$\\boxed{\\min_G \\max_D V(D,G)= \\mathbb{E}_{x\\sim p_{\\text{data}}(x)}[\\log D(x)] + \\mathbb{E}_{z\\sim p_z{(z)}}[\\log(1-D(G(z)))]}$$\n",
    "\n",
    "This tries to find NASH equilibrium.\n",
    "\n",
    "Below is the snap from the paper which describes the algorithm:<br><br>\n",
    "![](CS5480_images/VoiceAccess_Ajm0NIi1rd.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pitfalls of GANs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No guarantee to equilibrium\n",
    "    - Mode collapsing\n",
    "    - Oscillation\n",
    "    - No indicator when to finish"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN Hacks\n",
    "\n",
    "- Normalize the inputs\n",
    "    - Normalize the images between -1 and 1\n",
    "    - Tanh as the last layer of the generator output\n",
    "- A modified loss function\n",
    "    - In GAN papers, the loss function to optimize $G$ is $\\min (\\log (1- D))$, but in practice we  practically use $\\max \\log D$\n",
    "    - Flip labels when training generator: real = fake, fake = real\n",
    "- Don’t sample from a uniform distribution, sample from a Gaussian distribution (spherical)\n",
    "- BatchNorm\n",
    "    - Construct different mini batches for real and fake, i.e. each mini batch needs to contain only all real images or all generated images.\n",
    "    - When batch norm is not an option, use instance normalization (for each sample, subtract mean and divide by standard deviation)\n",
    "- Avoid Sparse Gradients\n",
    "    - Stability of the GAN game suffers if we have sparse gradients\n",
    "    - Leaky ReLU = good ( in both $G$ and $D$ )\n",
    "    - For down sampling , use: Average Pooling, Conv2d + stride\n",
    "    - For Upsampling, use: [PixelShuffle](https://arxiv.org/abs/1609.05158), ConvTranspose2d + stride\n",
    "- Use soft and noisy labels\n",
    "    - Label Smoothing, i.e. if we have two target labels: Real=1 and Fake=0, then for each incoming sample, if it is real, then replace the label with a random number between 0.7 and 1.2, and if it is a fake sample, replace it with 0.0 and 0.3 (this is an example)\n",
    "    - Make the labels noisy for the discriminator: occasionally flip the labels when training the discriminator."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variations of GAN\n",
    "\n",
    "![](CS5480_images/VoiceAccess_K0V3UZNAyo.png)<br><br>\n",
    "![](CS5480_images/VoiceAccess_TWvqQ7A2Ep.png)<br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popular Variants: Wasserstein GAN\n",
    "\n",
    "- Weight clamping to maintain k Lipschitz continuity\n",
    "- JSD to earth mover distance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popular Variants: Cycle GANs\n",
    "\n",
    "- It is Unpaired Image-to-Image Translation<br><br>\n",
    "  ![](CS5480_images/VoiceAccess_stTjgWeAi8.png)<br><br>\n",
    "  ![](CS5480_images/VoiceAccess_cHmZXBJkC5.png)<br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popular Variants: Progressive GANs\n",
    "\n",
    "- It generates high resolution image progressively as explained below:<br><br>\n",
    "  ![](CS5480_images/VoiceAccess_XUDqFvcszC.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popular Variants: Big GANs GANs\n",
    "\n",
    "- State-of-the-art at this time for large-scale image generation\n",
    "- Puts together multiple steps to obtain the improvements\n",
    "- Model design: Self-attention GAN\n",
    "- Class-conditional batch normalization\n",
    "- Update discriminator more\n",
    "- Moving average of model weights, etc…."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GANs: Takeaways\n",
    "\n",
    "- GANs can generate sharp samples from high dimensional output space\n",
    "- Finding Nash point is almost impossible\n",
    "- No stopping criteria\n",
    "- Generally produce sharp images with artifacts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following resources were referred in this notes:\n",
    "\n",
    "- [Illustrated transformer](http://jalammar.github.io/illustrated-transformer/)<br>\n",
    "- [Transformer architecture positional encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)\n",
    "- [Ganhacks](https://github.com/soumith/ganhacks)\n",
    "- [The gan zoo](https://github.com/hindupuravinash/the-gan-zoo)\n",
    "- [Really awesome gan](https://github.com/nightrome/really-awesome-gan)\n",
    "- [Goodfellow talk](https://www.youtube.com/watch?v=AJVyzd0rqdc&t=1184s) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "$\\tiny  {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0e5e69b8442e8f020791fad6bfcef3777f0e89e0f1a2517b6b628b2eaf0fe66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
