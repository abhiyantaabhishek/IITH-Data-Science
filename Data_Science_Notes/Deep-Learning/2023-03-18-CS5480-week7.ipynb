{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "author: Abhishek Kumar Dubey\n",
    "badges: false\n",
    "categories:\n",
    "- Deep Learning\n",
    "date: '2023-03-25'\n",
    "description: Recurrent Neural Networks, Attention and Transformers\n",
    "image: CS5480_images/VoiceAccess_56w2Jughz5.png\n",
    "title: Deep Learning 7\n",
    "toc: true\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Traditional feed-forward network assume that all inputs and outputs are independent of each other."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Counterexample language/speech modeling\n",
    "    - Predicting the next word in a sentence depends on the entire sequence of words before the current word\n",
    "    - Example: “The man who wore a wig on his head went inside”\n",
    "        - Who went inside? Man or wig?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the same weights/layers for every time step (hence, recurrent ), and pass on the output to the next time step Courtesy: <br><br>\n",
    "  ![](CS5480_images/Acrobat_iduxTVwnBA.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A recurrent neural network and the unfolding in time of the computation involved in its forward computation <br><br>\n",
    "  ![](CS5480_images/VoiceAccess_FDHN2gQ7fh.png)\n",
    "- We can notice that <br>\n",
    "\n",
    "  $$\\begin{align*}\n",
    "    s_{t}&=\\operatorname{tanh}(U x_{t}+W s_{t-1})\\\\\n",
    "    \\hat{y}_{t}&=\\mathrm{softrnax}(Vs_{t})\\\\\n",
    "    E_{t}(y_{t},\\hat{y_{t}})&=-y_{t}\\log\\hat{y_{t}} \\\\\n",
    "    E(y,{\\hat{y}})&=\\sum_{t}E_{t}(y_{t},{\\hat{y}}_{t})\\\\\n",
    "    &=-\\sum_{t}y_{t}\\log\\;{\\hat{y}}_{t}\n",
    "  \\end{align*}$$\n",
    "\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kinds of RNNs\n",
    "\n",
    "![](CS5480_images/VoiceAccess_J6FmUvHCo5.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop Through Time (BPTT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unrolled RNN <br><br>\n",
    "  ![](CS5480_images/VoiceAccess_DQEZ5vcZAU.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Goal\n",
    "    - Calculate error gradients w.r.t U, V and W\n",
    "    - Learn weights using SGD\n",
    "- Just like we sum up errors, we also sum up gradients at each time step for one training example\n",
    "  $${\\frac{\\partial E}{\\partial W}}=\\sum_{t}{\\frac{\\partial E_{t}}{\\partial W}}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\n",
    "    \\frac{\\partial E_{3}}{\\partial V}&=\\frac{\\partial E_{3}}{\\partial\\hat{y}_{3}}\\frac{\\partial\\hat{y}_{3}}{\\partial V}\\\\\n",
    "    &=\\frac{\\partial E_{3}}{\\partial\\hat{y}_{3}}\\frac{\\partial\\hat{y}_{3}}{\\partial z_{3}}\\frac{\\partial z_{3}}{\\partial V}\\\\\n",
    "    &=(\\hat y_{3}-y_{3})\\otimes s_{3}\n",
    "\\end{align*}$$\n",
    "\n",
    "Here \n",
    "- $\\otimes$ is outer product\n",
    "- $\\frac{\\partial E_{3}}{\\partial\\hat{y}_{3}} = \\hat y_{3}-y_{3}$ (if we use mean square type of error)\n",
    "- $\\frac{\\partial\\hat{y}_{3}}{\\partial z_{3}}$ is ignored, assuming we are not using the activation function\n",
    "- $\\frac{\\partial z_{3}}{\\partial V}=s_{3}$ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](CS5480_images/VoiceAccess_uBKGn1RSWv.png)\n",
    "![](CS5480_images/VoiceAccess_UY69EgVNKH.png)\n",
    "\n",
    "$${\\frac{\\partial E_{3}}{\\partial W}}={\\frac{\\partial E_{3}}{\\partial{\\hat{y}}_{3}}}{\\frac{\\partial{\\hat{y}}_{3}}{\\partial s_{3}}}\\left[{\\frac{\\partial s_{3}}{\\partial W}}+{\\frac{\\partial s_{3}}{\\partial s_2}}{\\frac{\\partial s_{2}}{\\partial W}}+{\\frac{\\partial s_{3}}{\\partial s_2}}{\\frac{\\partial s_{2}}{\\partial s_1}}{\\frac{\\partial s_{1}}{\\partial W}}\\right]$$\n",
    "\n",
    "It can be generalized days:\n",
    "\n",
    "$${\\cfrac{\\partial E_{3}}{\\partial W}}=\\sum_{k=0}^{3}{\\cfrac{\\partial E_{3}}{\\partial{\\hat{y}}_{3}}}{\\cfrac{\\partial{\\hat{y}}_{3}}{\\partial s_{3}}}{\\cfrac{\\partial s_{3}}{\\partial s_{k}}}{\\cfrac{\\partial s_{k}}{\\partial W}}$$\n",
    "\n",
    "$\\cfrac{\\partial E_{3}}{\\partial U}$ can be calculate in same was  $\\cfrac{\\partial E_{3}}{\\partial W}$\n",
    "\n",
    "$$s_{3}=\\operatorname{tanh}(U x_{t}+W s_{2})$$\n",
    "\n",
    "Similar to backprop , we can define:\n",
    "$$\\delta_{2}^{(3)}\\,=\\,\\frac{\\partial E_{3}}{\\partial z_{2}}\\,=\\frac{\\partial E_{3}}{\\partial s_{3}}\\frac{\\partial s_{3}}{\\partial s_{2}}\\frac{\\partial s_{2}}{\\partial z_{2}}$$\n",
    "here we assume\n",
    "$$z_{2}=U x_{2}+W s_{1}$$\n",
    "and $s_2$ is obtained by applying some activation function on $z_2$\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W}=\\sum_{i=0}^T \\frac{\\partial E_i }{\\partial W}\\propto\\sum_{i=0}^T \\left(\\prod_{i=k+1}^y \\frac{\\partial s_i }{\\partial s_{i-1} }\\right)\\frac{\\partial s_k }{\\partial W}$$\n",
    "\n",
    "- Problems \n",
    "    - Vanishing gradient $\\left\\|{\\frac{\\partial s_{i}}{\\partial s_{i-1}}}\\right\\|_{2}<1$\n",
    "    - Exploding gradient $\\left\\|{\\frac{\\partial s_{i}}{\\partial s_{i-1}}}\\right\\|_{2}>1$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vanishing gradient problem:\n",
    "\n",
    "$${\\frac{\\partial E_{3}}{\\partial W}}=\\sum_{k=0}^{3}{\\frac{\\partial E_{3}}{\\partial{\\hat{y}}_{3}}}{\\frac{\\partial{\\hat{y}}_{3}}{\\partial s_{3}}}\\left(\\prod_{j=k+1}^{3}{\\frac{\\partial s_{j}}{\\partial s_{j-1}}}\\right){\\frac{\\partial s_{k}}{\\partial W}}$$\n",
    "- For sigmoid activation function gradient age upper bounded by 1.\n",
    "- it means that gradient will vanish overtime if there is a long range dependency\n",
    "- it can be solved using truncated gradient descent but this is inefficient. And this issue is solved by LSTM up to some extent\n",
    "\n",
    "exploding gradient problem\n",
    "\n",
    "- if the weights are high the gradient will explode.\n",
    "- but this problem can easily be solved by clipping the gradient.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short Term Memory (LSTM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](CS5480_images/VoiceAccess_cekucnJian.png)\n",
    "\n",
    "- Input gate: scales input to cell (write) \n",
    "- Output gate: Scales output from cell (read) \n",
    "- Forget gate: Scales old cell value (reset)\n",
    "\n",
    "\n",
    "$$\\begin{array}{l}\n",
    "i=\\sigma (x_t U^i +s_{t-1} W^i )\\\\\n",
    "f=\\sigma (x_t U^f +s_{t-1} W^f )\\\\\n",
    "o=\\sigma (x_t U^o +s_{t-1} W^g )\\\\\n",
    "g=\\tanh (x_t U^g +s_{t-1} W^g )\\\\\n",
    "c_t =c_{t-1} \\circ f+g\\circ i\\\\\n",
    "s_t =\\tanh \\left(c_t \\right)\\circ o\n",
    "\\end{array}$$\n",
    "\n",
    "- here bias is ignored.\n",
    "- for all the garing function sigmoid activation fucntion is used, and for others tanh is used. sigmoid is used for gating, becasue for gate we need values between 0 and 1.\n",
    "\n",
    "::: {.alert .alert-dismissible .alert-primary}\n",
    "-  if he picks the input gate to always one and the forget gate to always zero, then we get the fucntion of  almost RNN.\n",
    "- almost because tanh is added.\n",
    ":::\n",
    "\n",
    "::: {.alert .alert-dismissible .alert-secondary}\n",
    "- now if we have a closer look at $c_t =c_{t-1} \\circ f+g\\circ i$, we can see that the sales state doesnt only depend on the gated inputbut it also depends on the previous cell state. it means there is a skip connection between previous state to current state. it helps the gradient flow for long run\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM variant\n",
    "\n",
    "peepholes LSTM\n",
    "\n",
    "![](CS5480_images/VoiceAccess_XC2slb8Wbk.png)\n",
    "\n",
    "- LSTM with peephole connections\n",
    "  $$\\begin{array}{l}{{f_{t}=\\sigma\\left(W_{f}\\cdot[C_{t-1},h_{t-1},x_{t}]\\right)}}\\\\ \n",
    "  {{{i}_{t}=\\sigma\\left(W_{i}\\cdot[C_{t-1},h_{t-1},x_{t}]\\ +\\ b_{i}\\right)}}\\\\  \n",
    "  {{o_{t}=\\sigma\\left(W_{o}\\cdot[C_{t},h_{t-1},x_{t}]\\ +\\ b_{\\sigma}\\right)}}\\end{array}$$\n",
    "\n",
    "- Coupled forget and input gates\n",
    "  $$C_{t}=f_{t}\\star C_{t-1}+(1-f_{t})\\ast{\\bar{C}}_{t}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRUs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](CS5480_images/VoiceAccess_WmWfbx5H8i.png)\n",
    "\n",
    "$$\\begin{align*}\n",
    "    z&=\\sigma(x_{t}U^{z}+s_{t-1}W^{z})\\\\\n",
    "    r&=\\sigma(x_{t}U^{r}+s_{t-1}W^{r})\\\\\n",
    "    h&=\\tanh(x_{t}U^{h}+\\left(s_{t-1}\\circ r)W^{h}\\right)\\\\\n",
    "    s_{t}&=(1-z)\\circ h+z\\circ s_{t-1}\n",
    "\\end{align*}$$\n",
    "\n",
    "- Reset gate: defines how much of the previous memory to keep around.\n",
    "- Update gate: determines how to combine the new input with the previous memory.\n",
    "- A GRU has two gates, an LSTM has three gates.\n",
    "- In GRUs\n",
    "    - No internal memory ($c_t$) different from the exposed hidden state.\n",
    "    - No output gate as in LSTMs.\n",
    "    - The input and forget gates of LSTMs are coupled by an update gate in GRUs, and the reset gate (GRUs) is applied directly to the previous hidden state.\n",
    "    \n",
    "::: {.alert .alert-dismissible .alert-info}\n",
    "- if the reset rate is set to all 1 and update get is set to all 0, then it behaves like RNN\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Applications"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language Modeling and Generating Text\n",
    "\n",
    "- Given a sequence of words we want to predict the probability of each word given the previous words \n",
    "- Can also be looked at as a generative model = fun! \n",
    "- Typical input is a sequence of words; output = sequence of predicted words\n",
    "    - During training, $o_t=x_{t-1}$\n",
    "\n",
    "Machine Translation\n",
    "\n",
    "- Sequence of words in source language $\\Rightarrow$ Sequence of words in target language\n",
    "\n",
    "Speech Recognition\n",
    "\n",
    "- Given an input sequence of acoustic signals from a sound wave, predict a sequence of phonetic segments together with their probabilities."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining CNNs and RNNs: Image Captioning\n",
    "\n",
    "- the Image can be passed to a pre-trained network and the output of that network can be passed to RNN. <br><br>\n",
    "  ![](CS5480_images/VoiceAccess_921MmMvpfk.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention and Transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "History:\n",
    "\n",
    "Visual attention computation model \n",
    "  \n",
    "- Attention modeled as a way to focus on parts of images <br><br>\n",
    "  ![](CS5480_images/VoiceAccess_X1QjjudVA4.png) <br>\n",
    "- Notions of top-down and bottom-up attention\n",
    "    - bottom up attention: Suppose we are going somewhere, and something attractive passes in front of us, we pay attention to that, so here we pay attention to any content which is attractive we are not selective here.\n",
    "    - top down attention: Now suppose we are searching for something very passionately, now we will not be attracted if other things comes in front of us, we will pay attention only if we see what we are looking for.\n",
    "\n",
    "Spatial Transformer Networks\n",
    "\n",
    "- here author introduced a kind of  localization network and grid generator, which creates the attention mechanism and adds this to the original net of path.<br><br>\n",
    "  ![](CS5480_images/VoiceAccess_mXWhBdjNAO.png)\n",
    "\n",
    "Attention in Image Captioning\n",
    "\n",
    "- Generate captions from an input image using the attention mechanism.\n",
    "- VGGnet as encoder for extracting features from input image\n",
    "- LSTM decoder generates captions by producing one word at each time step by using the context vector, previous hidden state and previously generated words.<br><br>\n",
    "  ![](CS5480_images/VoiceAccess_M6vYx4czp7.png)\n",
    "- Here the network predicts attention and the distribution over vocab, the attention is then multiplied with the features as shown in the epoch figure.\n",
    "- $z_2$ can get either $a_1$ attended feature or $a_1$  and $a_2$  attended feature. ( shown in the above figure )\n",
    "- Context Vector : The context vector $z_t$ is computed by a function $\\phi$ using the attention vectors $a_i$ and the\n",
    "positive weights $\\alpha_i, i = 1, 2, \\dots,L$ correspond to different locations of the input image.\n",
    "  $$\\begin{align*}\n",
    "    e_{t i}&=\\,f_{a t t}(\\mathrm{a}_{i},\\mathrm{h}_{t-1})\\\\\n",
    "    \\alpha_{t i}&={\\frac{e x p(e_{t i})}{\\sum_{k=1}^{L}e x p(e_{t k})}}\\\\\n",
    "    \\hat{z}_{t}&=\\phi(\\{\\mathbf{a}_{i}\\},\\{\\alpha_{i}\\})\n",
    "    \\end{align*}$$\n",
    "- At each location $i$, the weight $\\alpha_i$ for the annotation vector $a_i$ is calculated by an attention mechanism $f_{att}$.\n",
    "- $\\alpha_i$ can be interpreted in two ways based on the two versions of this attention mechanism, namely the stochastic _Hard_ and the deterministic _Soft_\n",
    "- Hard attention:\n",
    "  - In hard attention, to generate the word $y_i$, the weight $\\alpha_i$ is seen as the probability that location $i$ is the correct place to focus on.\n",
    "  - $s_t$ is defined as the location variable that determines where the attention should be focused while producing the $t^{th}$ word.\n",
    "  - They assign a multinoulli distribution with weights $\\alpha_i$ as the parameters and view $\\hat z_t$ as a random variable.\n",
    "    $$p(s_{t i}=1|s_{j<t},\\mathbf{a})=\\alpha_{t,i}$$\n",
    "    $$\\hat{\\mathbf{z}}_{t}=\\sum_{t}s_{t,i}\\mathbf{a}_{i}$$\n",
    "  - We can’t simply train the model with back propagation while using hard attention as we probabilistically sample from one of the attention vectors $a_i$ and use that as the context vector $\\hat z_t$.\n",
    "- Soft Attention: \n",
    "  - In hard attention, to generate the word $y_i$, the weight $\\alpha_i$ is seen as as the relative importance given to location $i$ in combining the attention vectors $a^′_i$ s.\n",
    "  - Instead of sampling the location vector st every time as in the hard attention, an expectation of the context vector $\\hat z_t$ is taken and a deterministic attention model is formulated as:\n",
    "    $$\\mathbb{E}_{p(s_{t}|a)}[{\\hat{\\mathbf{E}}}_{t}]=\\sum_{i=1}^{L}\\alpha_{t,i}\\mathbf{a}_{i}$$\n",
    "    $$\\phi(\\{\\mathbf{a}_{i}\\},\\{\\alpha_{i}\\})=\\sum_{i}^{L}\\alpha_{i}\\mathbf{a}_{i}$$\n",
    "  - As this model is differentiable under the deterministic attention, it can be trained end-to-end using back propagation.\n",
    "- Hard vs Soft Attention <br><br>\n",
    "  ![](CS5480_images/VoiceAccess_6CkbIf5HaT.png)\n",
    "\n",
    "DRAW: Deep Recurrent Attentive Writer\n",
    "\n",
    "- here they used variational autoencoder, it is a generative model.\n",
    "- they shown that image can be generated in recurrent way.\n",
    "- attention was implemented using read and write module as shown in the figure.<br><br>\n",
    "  ![](CS5480_images/VoiceAccess_r5pqtsdAHJ.png)<br>\n",
    "- the method was learning the grid, where  to focus on the image <br><br>\n",
    "  ![](CS5480_images/VoiceAccess_QlGWRpoSx3.png)\n",
    "-  The grid has central coordinate as $(g_x,g_y)$ and $\\delta$ is the gap between the grid locations,\n",
    "- At each grid location they implement a gaussian with the variance $\\sigma$\n",
    "- Now the network can learn which area of the image to focus at, it can learn  $(g_x,g_y)$ adjust the $\\delta$ and can manage to focus on small part or large part the image, and $\\sigma$ tells how much of the local neighbor to take into account.\n",
    "  $$(\\tilde{g}_{X},\\tilde{g}_{Y},\\log\\sigma^{2},\\log\\tilde{\\delta},\\log\\gamma)=\\,W(h^{d e c})$$\n",
    "\n",
    "self-attention\n",
    "\n",
    "- Largely inspired by the need in machine translation\n",
    "- Encoder-becoder DNN models (RNNs, LSTMs, GRUs) were used for neural machine translation then\n",
    "  - Encoder takes input sentence and creates a summary (last hidden layer also called Context Vector ).\n",
    "  - Decoder translates input sentence sequentially by processing the summary.\n",
    "- Problems with this approach:\n",
    "  - Translation quality depends on quality of summary.\n",
    "  - RNNs/LSTMs create bad summaries for longer sentences (long-range dependency problem).\n",
    "  - We can’t give more importance to a set of words compared to others in the input sentence\n",
    "\n",
    "Origin of Self-Attention and Transformers\n",
    "\n",
    "- Introduced by Bahdanau et al to improve the encoder-decoder based neural machine translation in NLP<br><br>\n",
    "  ![](CS5480_images/VoiceAccess_HMEoB1Oq6Q.png)<br>\n",
    "- They use a BiRNN as encoder and generate annotation $h_j$ by concatenating forward and backward hidden states.\n",
    "- Each annotation $h_j$ contains information about the whole\n",
    "input sequence with more emphasis around the $j^{th}$ word.\n",
    "- To focus on all input word embeddings while creating a context vector, they use a weighted sum of the hidden states rather than the final hidden state.\n",
    "- For output word $y_i$, context vector $c_i$ is the weighted sum of\n",
    "annotations $h_j$\n",
    "  $$c_{i}=\\sum_{j=1}^{T_{x}}\\alpha_{i j}h_{j}$$\n",
    "- The weight $\\alpha_{ij}$ for a given annotation  $h_j$ is:\n",
    "  $$\\alpha_{i j}=\\frac{\\exp(e_{i j})}{\\sum_{k=1}^{T_{x}}\\exp(e_{i k})}$$\n",
    "  where \n",
    "  $$e_{i j}=a(s_{i-1},h_{j})$$\n",
    "  is an alignment model that finds how well the $j^{th}$ input matches with the $i^{th}$ output."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation for Transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sequential computation prevents parallelization <br><br>\n",
    "  ![](CS5480_images/VoiceAccess_HUw4SOz2C7.png)\n",
    "- Despite GRUs and LSTMs, RNNs still need attention mechanism to deal with long-range dependencies – path length for co-dependent computation between states grows with sequence length.\n",
    "- But if attention gives us access to any state, maybe we don’t need the RNN?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "$\\tiny  {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0e5e69b8442e8f020791fad6bfcef3777f0e89e0f1a2517b6b628b2eaf0fe66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
