{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "author: Abhishek Kumar Dubey\n",
    "badges: false\n",
    "categories:\n",
    "- Deep Learning\n",
    "date: '2023-02-11'\n",
    "description: Filtering, correlation and convolution.\n",
    "image: CS5480_images//Acrobat_fJKIgb7Ffx.png\n",
    "title: Deep Learning 4\n",
    "toc: true\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Filtering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the pixels in an image based on some function of a local neighborhood of each pixel\n",
    "\n",
    "- One simple version: linear filtering\n",
    "    - Replace each pixel by a linear combination (a weighted sum) of its neighbors\n",
    "- The prescription for the linear combination is called the “kernel” (or “mask”, “"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Filtering: Cross correlation\n",
    "- simple averge \n",
    "  $$G[i,j]=\\underbrace{\\frac{1}{(2k+1)^{2}}}_{\\text{Uniform weight to each pixel} } \\times \\overbrace{ \\sum_{u=-k}^{k}\\sum_{v=-k}^{k}F[i+u,j+v]}^{\\text{loop over all pixels in neighborhood}} $$\n",
    "- weighted average, \n",
    "  $$G[i,j]=\\sum_{u=-k}^{k}\\sum_{v=-k}^{k}\\underbrace{H[u,v]}_{\\text{Non-uniform weight}} F[i+u,j+v]$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- what we get in $G$ depends on how we choose $H$\n",
    "- different $H$ is required for different features\n",
    "- THis kind of linear filter is called cross corelation\n",
    "- it is a measure of correlation between $H$ and $F$, it can be also viewed as local operation\n",
    "- $H$ is called kernel as well"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smoothing by Averaging Computer Vision\n",
    "\n",
    "- take $5 \\times 5$ or $3 \\times 3$ kernal and apply on image \n",
    "- $5 \\times 5$ will blur more as compared to  $3 \\times 3$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Filter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want nearest neighboring pixels to have the most influence on the output?\n",
    "- Use Gaussian filter <br><br>\n",
    "  ![](CS5480_images//Acrobat_fJKIgb7Ffx.png)<br><br>\n",
    "- Removes high frequency components from the image (“low pass filter”). More on this later.\n",
    "- With Gaussian filter, we can preserve more structure, in mean (average) filter there is blockyness, but in Gaussian filter there is a fading kind of affect, no blockyness.\n",
    "- Smoothing with a Gaussian <br><br>\n",
    "  ![](CS5480_images//Acrobat_R9L6puubpD.png)<br><br>\n",
    "- Mean vs Gaussian<br>\n",
    "  In below pic left is mean, right is Gaussian:<br><br>\n",
    "  ![](CS5480_images//Acrobat_EaiHqIHeFd.png)<br><br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of kernel or mask: \n",
    "\n",
    "- Gaussian function has infinite support, but discrete filters use finite kernels.<br><br>\n",
    "  ![](CS5480_images//Acrobat_Bv8uLOlUeV.png)<br>\n",
    "\n",
    "What is the result of filtering the impulse signal (image) F with the arbitrary kernel H?\n",
    "\n",
    "- it double filpps the image\n",
    "\n",
    "- for convolution we dobuble fipp the kernal and then do dot product"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of simple kernels on image\n",
    "\n",
    "- Does nothing\n",
    "  $$\\left\\lbrack \\begin{array}{ccc}\n",
    "  0 & 0 & 0\\\\\n",
    "  0 & 1 & 0\\\\\n",
    "  0 & 0 & 0\n",
    "  \\end{array}\\right\\rbrack$$\n",
    "- Blurs\n",
    "  $$\\frac{1}{9}\\left\\lbrack \\begin{array}{ccc}\n",
    "  1 & 1 & 1\\\\\n",
    "  1 & 1 & 1\\\\\n",
    "  1 & 1 & 1\n",
    "  \\end{array}\\right\\rbrack$$\n",
    "- Shifts left by one pixel with correlation\n",
    "  $$\\left\\lbrack \\begin{array}{ccc}\n",
    "  0 & 0 & 0\\\\\n",
    "  0 & 0 & 1\\\\\n",
    "  0 & 0 & 0\n",
    "  \\end{array}\\right\\rbrack$$  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Filtering: Correlation Example\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the result of filtering the impulse signal (image) $F$ with the\n",
    "arbitrary kernel $H$?\n",
    "\n",
    "- It double flips the kernel as shown below <br><br>\n",
    "  ![](CS5480_images//Acrobat_qqLzTio3r5.png)<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convolution operator\n",
    "  $$G[i,j]=\\sum_{u=-k}^{k}\\sum_{v=-k}^{k}H[u,v]F[i-u,j-v]$$\n",
    "  and $H$ is then called the impulse response function.\n",
    "- Equivalent to flip the filter in both dimensions (bottom to top, right to left) and apply cross correlation. Notice minus sign in the above equation $(F[i-u,j-v])$<br><br>\n",
    "  ![](CS5480_images//Acrobat_wiFH6WY42a.png)<br>\n",
    "- Denoted by<br>\n",
    "  $$G=H*F$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation vs Convolution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Correlation :<br>\n",
    "  $G=H\\otimes{\\cal F}$<br>\n",
    "  $G[i,j]=\\sum_{u=-k}^{k}\\sum_{v=-k}^{k}H[u,v]F[i+u,j+v]$\n",
    "- Convolution :<br>\n",
    "  $G=H*F$<br>\n",
    "  $G[i,j]=\\sum_{u=-k}^{k}\\sum_{v=-k}^{k}H[u,v]F[i-u,j-v]$\n",
    "\n",
    "- For a Gaussian or box filter, how will the outputs differ (among correlation and convolution )?\n",
    "    - it will not as filter is symmetric\n",
    "\n",
    "- For impulse image\n",
    "    - we saw above\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boundary Effects"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- valid conv, we agree ouput will be smaller, we don't touch the image\n",
    "- padded conv, we pad the boundary and the conv,\n",
    "    - zero, wrap, clamp. mirror\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation vs Convolution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Both correlation and convolution are linear shift invariant (LSI) operators , which obey both the superposition principle (Linearity)\n",
    "  $$h\\circ(f_{0}+f_{1})=h\\circ f_{o}+h\\circ f_{1}$$\n",
    "  and the shift invariance principle <br>\n",
    "  $$\\text{If  \\;\\;}g(i,j)=f(i+k,j+l)\\leftrightarrow(h\\circ g)(i,j)=(h\\circ f)(i+k,j+l)$$\n",
    "\n",
    "  which means that shifting a signal commutes with applying the operator.\n",
    "- Is the same as saying that the effect of the operator is the same\n",
    "everywhere.\n",
    "\n",
    "\n",
    "- What’s the difference?\n",
    "\n",
    "    - Commutativity?\n",
    "        - Convolution is commutative, but correlation is not.\n",
    "\n",
    "    - Associativity?\n",
    "        - Convolution is associative, but correlation is not.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution: A Linear Operator\n",
    "\n",
    "- Commutative:  $a * b = b * a$\n",
    "    - Conceptually no difference between filter and signal\n",
    "- Associative: $a * (b * c) = (a * b) * c$\n",
    "    - Often apply several filters one after another: $(((a * b1) * b2) * b3)$\n",
    "    - This is equivalent to applying one filter: $a * (b1 * b2 * b3)$\n",
    "- Distributes over addition: $a * (b + c) = (a * b) + (a * c)$\n",
    "- Scalars factor out: $ka * b = a * kb = k (a * b)$\n",
    "- Identity: unit impulse $e = [\\dots, 0, 0, 1, 0, 0, \\dots], a * e = a$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separable Filters\n",
    "\n",
    "- The process of performing a convolution requires $K\n",
    "^2$ operations per pixel, where $K$ is the size (width or height) of the convolution kernel.\n",
    "- In many cases, this operation can be speed up by first performing a $1D$ horizontal convolution followed by a $1D$ vertical convolution, requiring $2K$ operations.\n",
    "- If this is possible, then the convolution kernel is called __separable__\n",
    "- And it is the outer product of two kernels $K = vh^T$\n",
    "- How can we tell if a given kernel K is indeed separable?\n",
    "    - Look at the singular value decomposition (SVD) , and if only one\n",
    "singular value is non zero, then it is separable\n",
    "      $$K=\\mathbf{U}\\Sigma\\mathbf{V}^{T}=\\sum_{i}\\sigma_{i}u_{i}v_{i}^{T}$$\n",
    "      with $\\Sigma=\\mathrm{diag}(\\sigma_{i})$<br>\n",
    "      ${\\sqrt{\\sigma_{1}}}\\mathbf{u}_{1}$ and ${\\sqrt{\\sigma_{1}}}V_{1}^{T}$  are the vertical and horizontal kernels."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Filtering: Edge Detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Map image from 2d array of pixels to a set of curves or line segments or contours .\n",
    "- More compact than pixels.\n",
    "- Look for strong gradients, post process.\n",
    "- Edges look like steep cliffs\n",
    "- An edge is a place of rapid change in the image intensity function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An edge is a place of rapid change in the image intensity function<br><br>\n",
    "![](CS5480_images//Acrobat_iS6cO4gT2u.png)<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives with convolution\n",
    "\n",
    "- For 2D function, f(x,y), the partial derivative is:\n",
    "  $$\\frac{\\partial f(x,y)}{\\partial x}=\\operatorname*{lim}_{\\varepsilon\\to0}\\frac{f(x+\\varepsilon,y)-f(x,y)}{\\varepsilon}$$\n",
    "\n",
    "- For discrete data, we can approximate using finite differences:\n",
    "  $${\\frac{\\partial f(x,y)}{\\partial x}}\\approx{\\frac{f(x+1,y)-f(x,y)}{1}}$$\n",
    "- To implement above as convolution, what would be the associated filter?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobel Edge Detection Filter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image gradient"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The gradient of an image:\n",
    "  $${\\nabla}f=\\left[{\\frac{\\partial f}{\\partial x}},{\\frac{\\partial f}{\\partial y}}\\right]$$\n",
    "\n",
    "- The gradient points in the direction of most rapid change in intensity <br><br>\n",
    "  ![](CS5480_images/Acrobat_xRYjpwe3Sa.png) <br>\n",
    "\n",
    "- The gradient direction orientation of edge normal) is given by:\n",
    "  $$\\theta=\\tan^{-1}\\left(\\frac{\\partial f}{\\partial y}\\bigg/\\frac{\\partial f}{\\partial x}\\right)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effects of Noise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a single row or column of a noisy image\n",
    "\n",
    "- Plotting intensity as a function of position gives a signal <br><br>\n",
    "  ![](CS5480_images/Acrobat_Oim6GeG6f1.png)<br><br>\n",
    "- We are not able to find the edge.So we need to smoothen it first.We can use blurring here for proper smoothing.Once smoothing is done, we can apply, gradient operation. Mathematically $\\displaystyle\\frac{\\partial }{\\partial x}( h*f)$ <br><br>\n",
    "  ![](CS5480_images/Acrobat_OL0SowqWcz.png)<br><br>\n",
    "- Notice.Nothing is in convolution operation here.So instead of doing convolution on the image we can First, find the gradient of the convolution filter.And keep it and now, convolve this on the image.As convolution follows associative property, so it will produce the same result as earlier.Mathematically, it can be express as:  $\\displaystyle\\left(\\frac{\\partial }{\\partial x} h\\right)*f$ <br><br>\n",
    "  ![](CS5480_images/Acrobat_irSVddifsE.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going Beyond Edges\n",
    "\n",
    "- How to combine these two images to form a panorama?<br><br>\n",
    "  ![](CS5480_images/Acrobat_eiGQvLoH3z.png)<br>\n",
    "- The answer is feature extraction and matching, Image alignment\n",
    "\n",
    "\n",
    "- Detection : Identify the interest\n",
    "- Description : Extract vector feature descriptor around each interest point\n",
    "- Matching :  Determine correspondence between descriptors in two<br><br>\n",
    "  ![](CS5480_images/Acrobat_Tfz420a13i.png)<br>\n",
    "- to find 6 values of rotaition matrix we nee only 3 pair points.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More motivation\n",
    "\n",
    "\n",
    "- Feature points are used for:\n",
    "    - Image alignment (e.g., mosaics)\n",
    "    - 3D reconstruction\n",
    "    - Motion tracking\n",
    "    - Object recognition\n",
    "    - Indexing and database retrieval\n",
    "    - Robot navigation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What point to choose for feature extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want uniqueness\n",
    "\n",
    "\n",
    "- Look for image regions that are unusual: lead to unambiguous matches in other images\n",
    "\n",
    "- How to define \"unusual\"?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local measure of uniqueness\n",
    "\n",
    "- Suppose we only consider a small window of pixels\n",
    "\n",
    "  - What defines whether a feature is a good or bad - candidate? <br><br>\n",
    "  ![](CS5480_images/Acrobat_T7hkKKM1xh.png)<br>\n",
    "  - How does the window change when we shift it?\n",
    "  - Shifting the window in any direction causes a big change<br><br>\n",
    "    ![](CS5480_images/Acrobat_maeHAmLNEl.png)<br>\n",
    "\n",
    "A simple matching criteria\n",
    "\n",
    "- Consider the below image:<br><br>\n",
    "  ![](CS5480_images/Acrobat_2ZSntn4wcL.png)<br><br>\n",
    "  - Consider shifting the window $W$ by $(u,v)$\n",
    "  - compare each pixel before and after by summing up the squared differences (SSD)\n",
    "  - We can notice that if the window moves on the edge, there will not be any significant squared differences (SSD) here.But if the window moves on the corner then there will be some squared differences (SSD).\n",
    "  - The squared distance is given as below:\n",
    "    $$E(u,v)=\\sum_{(x,y)\\in W}\\left[I(x+v,y+v)-I(x,y)\\right]^{2}$$\n",
    "  - Compare two image patches using (weighted) summed square difference (also, called auto correlation function\n",
    "    $$E_{A C}(\\Delta\\mathbf{u})=\\sum_{\\cdot}{\\mathbf{}}w(\\mathbf{p}_{i})[I_{0}(\\mathbf{p}_{i}+\\Delta u)-I_{0}(\\mathbf{p}_{i})]^{2}$$\n",
    "    ![](CS5480_images/Acrobat_tE3olrWBXr.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to select an interest point"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Small motion assumption\n",
    "- Using a Taylor Series expansion\n",
    "  $$I_{0}({\\bf p}_{i}+\\Delta{\\bf u})\\approx I_{0}({\\bf p}_{i})+\\nabla I_{0}({\\bf p}_{i})\\Delta{\\bf u}$$\n",
    "  where\n",
    "  $$\\nabla I_{0}(\\mathbf{p}_{i})=\\left({\\frac{\\partial I_{0}}{\\partial x}},{\\frac{\\partial I_{0}}{\\partial y}}\\right)(\\mathbf{p}_{i})$$\n",
    "- the image gradient. We can approximate the autocorrelation as\n",
    "  $$\n",
    "  \\begin{align*}\n",
    "    E_{A C}(\\Delta\\mathbf{u})&=\\sum_{\\cdot}{\\mathbf{}}w(\\mathbf{p}_{i})[I_{0}(\\mathbf{p}_{i}+\\Delta u)-I_{0}(\\mathbf{p}_{i})]^{2}\\\\\n",
    "    &\\approx\\sum_{\\cdot}{\\mathbf{}}w(\\mathbf{p}_{i})[I_{0}({\\bf p}_{i})+\\nabla I_{0}({\\bf p}_{i})\\Delta{\\bf u}-I_{0}(\\mathbf{p}_{i})]^{2}\\\\\n",
    "    &=\\sum_{\\cdot}{\\mathbf{}}w(\\mathbf{p}_{i})[\\nabla I_{0}({\\bf p}_{i})\\Delta{\\bf u}]^{2}\\\\\n",
    "    &=\\Delta \\mathbf u^T \\mathbf A\\Delta \\mathbf u\n",
    "  \\end{align*}\n",
    "  $$\n",
    "- Gradient can be computed with the filtering techniques we saw, e.g., derivatives of Gaussians.\n",
    "- The autocorrelation is\n",
    "  $$E_{A C}(\\Delta\\mathbf{u}) = \\Delta \\mathbf u^T \\mathbf A\\Delta \\mathbf u$$\n",
    "  with \n",
    "  $$\\mathbf{A}=\\sum_{u}\\sum_{\\nu}w(u,v)\\left[{\\begin{array}{l l}{I_{x}^{2}}&{I_{x}J_{y}}\\\\ {I_{y}I_{x}}&{I_{y}^{2}}\\end{array}}\\right]=w*\\left[{\\begin{array}{l l}{I_{x}^{2}}&{I_{x}I_{y}}\\\\ {I_{y}I_{x}}&{I_{y}^{2}}\\end{array}}\\right]$$\n",
    "  where we have replaced the weighted summations with discrete convolutions with the weighting kernel $w$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using eigenvalues"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The eigenvalues of $\\mathbf A$ reveal the amount of intensity change in the two principal\n",
    "orthogonal gradient directions in the window.\n",
    "  $$\\mathbf{A}=\\mathbf{U}\\left[\\begin{array}{c c}{{\\lambda_{0}}}&{{0}}\\\\ {{0}}&{{\\lambda_{1}}}\\end{array}\\right]\\mathbf{U}^T$$\n",
    "  with \n",
    "  $$\\mathbf{A}\\mathbf{u}_{i}=\\lambda_{i}\\mathbf{u}_{i}$$\n",
    "- In case of matrix:\n",
    "\n",
    "  ![](CS5480_images/Acrobat_kAgoElQQY7.png)\n",
    "- An example, here $\\lambda$ is an eigen value\n",
    "\n",
    "  ![](CS5480_images/Acrobat_yCe1sdTyUz.png)\n",
    "\n",
    "- Classification of image points using eigenvalues of $\\mathbf A$\n",
    "\n",
    "  ![](CS5480_images/Acrobat_ZY9uM5q7ES.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Type of responses\n",
    "\n",
    "  ![](CS5480_images/Acrobat_QajpWmL1qv.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- But here we still need to calculate the eigenvalues. There is a method where we can compare the eigenvalues without even calculating it.\n",
    "- We know that the trace of a matrix is the summation of the eigenvalues and the determinant of the matrix is the multiplication of the eigenvalues. We will use the same property.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harris corner detector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First compute the gradient at each point of the image.\n",
    "- Compute $\\mathbf A$ for each image window to get its cornerness scores\n",
    "- Compute the eigenvalues/compute the following function $M_c$\n",
    "  $$M_{c}=\\lambda_{1}\\lambda_{2}-\\kappa\\left(\\lambda_{1}+\\lambda_{2}\\right)^{2}=\\operatorname*{det}(A)-\\kappa\\ \\mathrm{trace}^{2}(A)$$\n",
    "- Find points whose surrounding window gave large corner response ($M_c >$ threshold).\n",
    "- Take the points of local maxima, i.e., perform non maximum suppression."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A lot of other interest point detectors\n",
    "\n",
    "- Hessian\n",
    "- Lowe:DoG\n",
    "- Lindeberg: scale selection\n",
    "- Mikolajczyk& Schmid : Hessian/Harris Laplacian/Affine\n",
    "- Tuyttelaars & Van Gool: EBR and IBR\n",
    "- Matas: MSER\n",
    "- Kadir& Brady: Salient Regions\n",
    "- SpeededUp Robust Features (SURF) of Bay et al."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIFT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 1: Scales-pace extrema Detection:  Detect interesting points (invariant to scale and orientation) using DOG.\n",
    "- Step 2:Keypoint Localization: Determine location and scale at each candidate location, and select them based on stability.\n",
    "- Step 3: Orientation Estimation: Use local image gradients to assigned orientation to each localized key point . Preserve theta, scale and location for each feature.\n",
    "- Step 4:Keypoint Descriptor: Extract local image gradients at selected scale around keypoint and form a representation invariant to local shape distortion and representation invariant to local shape distortion and illumination them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SIFT: Scale space Extrema Detection\n",
    "\n",
    "  ![](CS5480_images/Acrobat_WJCt9NdOwQ.png)\n",
    "\n",
    "- Constructing scale space\n",
    "\n",
    "  ![](CS5480_images/Acrobat_GUXndO3PJF.png)\n",
    "\n",
    "  $$D(x,y,\\sigma)=L(x,y,k\\sigma)-L(x,y,\\sigma)$$\n",
    "  where \n",
    "  $$L(x,y,\\sigma)=G(x,y,\\sigma)*I(x,y)$$\n",
    "\n",
    "  ![](CS5480_images/Acrobat_InyGPDX2qG.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Determine the location and scale of keypoints to sub-pixel and sub-scale accuracy by fitting a 3D quadratic polynomial:\n",
    "    - Key point location: $X_i = (x_i,y_i,\\sigma_i)$<br>\n",
    "      Note: We are finding key points for each and every scale. If there is a corner at one scale, tt can look like an edge, at another scale.\n",
    "    - offset : $\\Delta X = (x-x_i,y-y_i,\\sigma-\\sigma_i)$<br>\n",
    "      Map the image back to the original location.\n",
    "    - Sub-Pixel, Sub-scale, estimated location: $X_i=X_i+\\Delta X$<br><br>\n",
    "      ![](CS5480_images/Acrobat_R3Bz8DKp9t.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIFT: Keypoint Localization\n",
    "\n",
    "- Use Taylor expansion to locally approximate $D(x,y, \\sigma )$ (i.e.,\n",
    "DoG function) and estimate $\\Delta X$\n",
    "  $$D(\\Delta X)=D(X_{i})+{\\frac{\\partial D^{T}(X_{i})}{\\partial X}}\\Delta X+{\\frac{1}{2}}\\Delta X^{T}\\,{\\frac{\\partial^2 D(X_{i})}{\\partial X^2}}\\Delta X$$\n",
    "- Find the extrema of $D( \\Delta X)$:\n",
    "  $$\\frac{\\partial D(X_{i})}{\\partial X}+\\frac{\\partial ^{2}D(X_{i})}{\\partial X^{2}}\\Delta X=0$$\n",
    "  From above we get \n",
    "  $$\\Delta X=-\\frac{\\displaystyle{\\partial}^{2}D^{-1}(X_{i})}{\\displaystyle{\\partial}X^{2}}\\frac{\\partial D(X_{i})}{\\displaystyle{\\partial}X}$$\n",
    "- $\\Delta X$ can be computed by solving a 3x3 linear system\n",
    "  $$\\displaystyle\n",
    "  \\begin{bmatrix}\n",
    "    \\displaystyle\\frac{\\partial^2 D}{\\partial \\sigma^2} &\\displaystyle \\frac{\\partial^2 D}{\\partial \\sigma y} &\\displaystyle \\frac{\\partial^2 D}{\\partial \\sigma x}\\\\\n",
    "    \\displaystyle\\frac{\\partial^2 D}{\\partial \\sigma y} &\\displaystyle \\frac{\\partial^2 D}{\\partial  y^2} &\\displaystyle \\frac{\\partial^2 D}{y x} \\\\\n",
    "    \\displaystyle\\frac{\\partial^2 D}{\\partial \\sigma x} &\\displaystyle \\frac{\\partial^2 D}{\\partial  yx} &\\displaystyle \\frac{\\partial^2 D}{ \\partial x^2}\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix}\n",
    "    \\Delta \\sigma\\\\ \n",
    "    \\Delta y\\\\ \n",
    "    \\Delta x\\\\ \n",
    "  \\end{bmatrix}=-\n",
    "  \\begin{bmatrix}\n",
    "   \\displaystyle \\frac{\\partial D}{ \\partial \\sigma} \\\\\n",
    "   \\displaystyle \\frac{\\partial D}{ \\partial y}  \\\\\n",
    "   \\displaystyle \\frac{\\partial D}{ \\partial x} \n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "  where \n",
    "  $$\\begin{align*}\n",
    "    {\\frac{\\partial D}{\\partial\\sigma}}&={\\frac{D_{k+1}^{i,j}-D_{k-1}^{i,j}}{2}}\\\\\n",
    "    \\frac{{\\partial}^{2}D}{{\\partial}\\sigma^{2}}&=\\frac{D_{k-1}^{i,j}-2D_{k}^{i,j}+D_{k+1}^{i,j}}{1}\\\\\n",
    "    \\frac{\\partial^{2}D}{\\partial\\sigma y}\\!&=\\!\\frac{({D}_{k+1}^{i+1,j}-{D}_{k-1}^{i+1,j})\\!-\\!({D}_{k+1}^{i-1,j}-{D}_{k-1}^{i-1,j})}{4}\\\\\n",
    "  \\end{align*}$$\n",
    "  if $\\Delta X>0.5$ in any dimension, repeat.\n",
    "- Next, reject the low contrast points and the points that lie on the edge\n",
    "- Low contrast points elimination:\n",
    "  - if $|D(X_{i}+\\Delta X)|<0.03$ reject keypoint.\n",
    "    - assumes that image values have been normalized in $[0,1]$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Edge elimination\n",
    "    - Similar to Harris corner detector!\n",
    "    - SIFT instead uses Hessian\n",
    "      $$\\mathbf{H}={\\left[\\begin{array}{l l}{D_{x x}}&{D_{x y}}\\\\ {D_{x y}}&{D_{y y}}\\end{array}\\right]}$$\n",
    "      $$\\begin{align*}\n",
    "        \\operatorname{Tr}(\\mathbf{H})&=D_{x x}+D_{y y}=\\alpha+\\beta,\\\\\n",
    "        \\operatorname{Det}({\\bf H})&=D_{x x}D_{y y}-(D_{x y})^{2}=\\alpha\\beta.\n",
    "      \\end{align*}$$\n",
    "      Hence\n",
    "      $${\\frac{\\operatorname{Tr}(\\mathbf{H})^{2}}{\\operatorname{Det}(\\mathbf{H})}}={\\frac{(\\alpha+\\beta)^{2}}{\\alpha\\beta}}={\\frac{(r\\beta+\\beta)^{2}}{r\\beta^{2}}}={\\frac{(r+1)^{2}}{r}},$$\n",
    "      Reject key point if\n",
    "      $$\\frac{\\mathrm{Tr}({\\bf H})^{2}}{\\mathrm{Det}({\\bf H})}<\\frac{(r+1)^{2}}{r}$$\n",
    "      (SIFT uses $r = 10$) ($r=\\frac{\\alpha}{\\beta}$)\n",
    "      \n",
    "      ![](CS5480_images/Acrobat_xIzbcVfEtJ.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIFT: Orientation Assignment\n",
    "\n",
    "- Use scale of point to choose correct image:\n",
    "  $$L(x,y)=G(x,y,\\sigma)*I(x,y)$$\n",
    "- Compute gradient magnitude and orientation using finite differences:\n",
    "  $$\\begin{align*}\n",
    "    m\\bigl(x,y\\bigr)&=\\sqrt{\\bigl(L(x+1,y)-L\\bigl(x-1,y\\bigr)\\bigr)^{2}+\\bigl(L(x,y+1)-L\\bigl(x,y-1\\bigr)\\bigr)^{2}}\\\\\n",
    "    \\theta(x,y)&=\\tan^{-1}\\left(\\frac{\\left(L(x,y+1)-L(x,y-1)\\right)}{\\left(L(x+1,_{,}y)-L(x-1,_{,}y)\\right)}\\right)\n",
    "  \\end{align*}$$\n",
    "- Use this to compute final descriptor.\n",
    "- Create histogram of gradient directions, within a region around the keypoint, at selected scale:\n",
    "  $$L(x,y,\\sigma)=G(x,y,\\sigma){*}\\,J(x,y)$$\n",
    "  and \n",
    "  $$m(x,y)={\\sqrt{(L(x+1,y)-L(x-1,y))^{2}+(L(x,y+1)-L(x,y-1))^{2}}}$$\n",
    "  and \n",
    "  $$\\theta(x,y)=a\\tan2{\\big(}(L(x,y+1)-L(x,y-1){\\big)}/(L(x+1,y)-L(x-1,y){\\big)})$$\n",
    "\n",
    "  ![](CS5480_images/Acrobat_MThJ0iJaj2.png)\n",
    "\n",
    "- Histogram entries are weighted by $( i )$ gradient magnitude and $(ii)$ a Gaussian function with $\\sigma$ equal to $1.5$ times the scale of the keypoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIFT: Feature descriptor\n",
    "\n",
    "- Compute the gradient at each pixel in a $16 \\times 16$ window around the detected keypoint, using the appropriate level of the Gaussian pyramid at which the keypoint was detected.\n",
    "- Downweight gradients by a Gaussian fall off function (blue circle) to reduce the influence of gradients far from the center.\n",
    "- In each 4 x 4 quadrant, compute a gradient orientation histogram using 8 orientation histogram bins.\n",
    "\n",
    "  ![](CS5480_images/Acrobat_LVo3Y4bw6c.png)\n",
    "\n",
    "- The resulting 128 nonnegative values form a raw version of the SIFT descriptor vector.\n",
    "- To reduce the effects of contrast or gain (additive variations arealready removed by the gradient), the 128 D vector is normalized tounit length.\n",
    "- Great engineering effort!\n",
    "\n",
    "- Extraordinarily robust matching technique\n",
    "    - Changes in viewpoint: up to about 60 degree out of plane rotation\n",
    "    - Changes in illumination: sometimes even day vs. night\n",
    "    - Fast and efficientcan run in real time\n",
    "    - Lots of code available<br><br>\n",
    "\n",
    "Review\n",
    "\n",
    "![](CS5480_images/Acrobat_ts4bKZdd8E.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SURF = Speeded Up Robust Features\n",
    "\n",
    "- Uses box filters instead of Gaussians to approximate Laplacians\n",
    "- Uses wavelets to get keypoint orientations\n",
    "- Few more changes (including indescriptor generation)\n",
    "- Leads to 3x speedup over SIFT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on image features: LBP Computer Vision\n",
    "\n",
    "- LBP = Local Binary Patterns\n",
    "\n",
    "  ![](CS5480_images/Acrobat_gVW7Ns7tmi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "$\\tiny  {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0e5e69b8442e8f020791fad6bfcef3777f0e89e0f1a2517b6b628b2eaf0fe66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
