<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Abhishek Kumar Dubey">
<meta name="dcterms.date" content="2022-11-05">
<meta name="description" content="Support Vector Regression,Linear Regression">

<title>Machine Learning 9</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../Data_Science_Notes/Mathematics/2022-08-06-CS6660-week1.html" rel="next">
<link href="../../Data_Science_Notes/Machine-Learning/2022-10-29-CS5590-week8.html" rel="prev">
<link href="../../logo.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-5ZQX02R26E"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-5ZQX02R26E', { 'anonymize_ip': true});
</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Machine Learning 9">
<meta property="og:description" content="Support Vector Regression,Linear Regression">
<meta property="og:image" content="http://localhost:4200/Data_Science_Notes/Machine-Learning/CS5590_images/chrome_xLZgcRJMUG.png">
<meta property="og:image:height" content="192">
<meta property="og:image:width" content="260">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../logo.png" alt="" class="navbar-logo">
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html" rel="" target="">
 <span class="menu-text"><i class="fa-solid fa-house" aria-label="house"></i> Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="../../Data_Science_Notes/index.html" rel="" target="" aria-current="page">
 <span class="menu-text"><i class="fa-solid fa-book-open-reader" aria-label="book-open-reader"></i> Data Science Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../Paper_Study/index.html" rel="" target="">
 <span class="menu-text"><i class="fa-solid fa-user-ninja" aria-label="user-ninja"></i> Paper Study</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../Data_Science_Projects/index.html" rel="" target="">
 <span class="menu-text"><i class="fa-solid fa-file-powerpoint" aria-label="file-powerpoint"></i> Data Science Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text"><i class="fa-solid fa-address-card" aria-label="address-card"></i> About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/abhiyantaabhishek" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/abhishek-kumar-dubey-585a86179/" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../Data_Science_Notes/index.html">Data Science Notes</a></li><li class="breadcrumb-item"><a href="../../Data_Science_Notes/Machine-Learning/2022-08-06-CS5590-week1.html">Machine Learning</a></li><li class="breadcrumb-item"><a href="../../Data_Science_Notes/Machine-Learning/2022-11-05-CS5590-week9.html">Machine Learning 9</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Machine Learning 9</h1>
                  <div>
        <div class="description">
          Support Vector Regression,Linear Regression
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Machine Learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Abhishek Kumar Dubey </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 5, 2022</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../Data_Science_Notes/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Science Notes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Deep-Learning/2023-01-07-CS5480-week1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deep Learning 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deep Learning 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Deep-Learning/2023-02-04-CS5480-week3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deep Learning 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deep Learning 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Deep-Learning/2023-03-04-CS5480-week5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deep Learning 5</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deep Learning 6</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deep Learning 7</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Deep-Learning/2023-04-01-CS5480-week8.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deep Learning 8</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Deep-Learning/2023-04-15-CS5480-week9.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deep Learning 9</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Machine Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth2 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Machine-Learning/2022-08-06-CS5590-week1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Machine Learning 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Machine-Learning/2022-08-20-CS5590-week2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Machine Learning 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Machine-Learning/2022-08-27-CS5590-week3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Machine Learning 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Machine-Learning/2022-09-10-CS5590-week4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Machine Learning 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Machine-Learning/2022-09-24-CS5590-week5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Machine Learning 5</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Machine-Learning/2022-10-08-CS5590-week6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Machine Learning 6</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Machine-Learning/2022-10-15-CS5590-week7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Machine Learning 7</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Machine-Learning/2022-10-29-CS5590-week8.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Machine Learning 8</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Machine-Learning/2022-11-05-CS5590-week9.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Machine Learning 9</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false">
 <span class="menu-text">Mathematics</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Mathematics/2022-08-06-CS6660-week1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probability Theory 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Mathematics/2022-08-13-CS6660-week2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probability Theory 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Mathematics/2022-09-03-CS6660-week3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Linear Algebra 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Mathematics/2022-09-17-CS6660-week4_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probability Theory 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Mathematics/2022-09-17-CS6660-week4_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Linear Algebra 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Mathematics/2022-09-24-CS6660-week5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probability Theory 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Mathematics/2022-10-01-CS6660-week6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Linear Algebra 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Mathematics/2022-10-15-CS6660-week7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probability Theory 5</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false">
 <span class="menu-text">Reinforcement Learning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Reinforcement-Learning/2023-08-05-CS6140-week1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Reinforcement Learning 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Reinforcement-Learning/2023-08-12-CS6140-week2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Reinforcement Learning 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Reinforcement-Learning/2023-08-19-CS6140-week3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Reinforcement Learning 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Reinforcement-Learning/2023-08-26-CS6140-week4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Reinforcement Learning 4</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#support-vector-regression" id="toc-support-vector-regression" class="nav-link active" data-scroll-target="#support-vector-regression"><span class="header-section-number">1</span> Support Vector Regression</a>
  <ul>
  <li><a href="#non-linear-kernel-svr" id="toc-non-linear-kernel-svr" class="nav-link" data-scroll-target="#non-linear-kernel-svr"><span class="header-section-number">1.1</span> Non-linear (Kernel) SVR</a></li>
  <li><a href="#svr-derivation" id="toc-svr-derivation" class="nav-link" data-scroll-target="#svr-derivation"><span class="header-section-number">1.2</span> SVR: Derivation</a></li>
  <li><a href="#svr-summary" id="toc-svr-summary" class="nav-link" data-scroll-target="#svr-summary"><span class="header-section-number">1.3</span> SVR: Summary</a>
  <ul>
  <li><a href="#strengths-of-svr" id="toc-strengths-of-svr" class="nav-link" data-scroll-target="#strengths-of-svr"><span class="header-section-number">1.3.1</span> Strengths of SVR</a></li>
  <li><a href="#weaknesses-of-svr" id="toc-weaknesses-of-svr" class="nav-link" data-scroll-target="#weaknesses-of-svr"><span class="header-section-number">1.3.2</span> Weaknesses of SVR</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression"><span class="header-section-number">2</span> Logistic Regression</a>
  <ul>
  <li><a href="#training" id="toc-training" class="nav-link" data-scroll-target="#training"><span class="header-section-number">2.1</span> Training</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">2.2</span> Summary</a></li>
  </ul></li>
  <li><a href="#clustering" id="toc-clustering" class="nav-link" data-scroll-target="#clustering"><span class="header-section-number">3</span> Clustering</a>
  <ul>
  <li><a href="#k-means-clustering" id="toc-k-means-clustering" class="nav-link" data-scroll-target="#k-means-clustering"><span class="header-section-number">3.1</span> k-Means Clustering</a>
  <ul>
  <li><a href="#selecting-initial-centroids" id="toc-selecting-initial-centroids" class="nav-link" data-scroll-target="#selecting-initial-centroids"><span class="header-section-number">3.1.1</span> Selecting Initial Centroids</a></li>
  <li><a href="#possible-solutions" id="toc-possible-solutions" class="nav-link" data-scroll-target="#possible-solutions"><span class="header-section-number">3.1.2</span> Possible Solutions</a></li>
  <li><a href="#evaluating-k-means-clusters" id="toc-evaluating-k-means-clusters" class="nav-link" data-scroll-target="#evaluating-k-means-clusters"><span class="header-section-number">3.1.3</span> Evaluating k-Means Clusters</a></li>
  <li><a href="#limitations" id="toc-limitations" class="nav-link" data-scroll-target="#limitations"><span class="header-section-number">3.1.4</span> Limitations</a></li>
  <li><a href="#extensions" id="toc-extensions" class="nav-link" data-scroll-target="#extensions"><span class="header-section-number">3.1.5</span> Extensions</a></li>
  </ul></li>
  <li><a href="#hierarchical-clustering" id="toc-hierarchical-clustering" class="nav-link" data-scroll-target="#hierarchical-clustering"><span class="header-section-number">3.2</span> Hierarchical Clustering</a>
  <ul>
  <li><a href="#strengths" id="toc-strengths" class="nav-link" data-scroll-target="#strengths"><span class="header-section-number">3.2.1</span> Strengths</a></li>
  <li><a href="#two-main-types-of-hierarchical-clustering" id="toc-two-main-types-of-hierarchical-clustering" class="nav-link" data-scroll-target="#two-main-types-of-hierarchical-clustering"><span class="header-section-number">3.2.2</span> Two main types of hierarchical clustering</a></li>
  <li><a href="#agglomerative-clustering-algorithm" id="toc-agglomerative-clustering-algorithm" class="nav-link" data-scroll-target="#agglomerative-clustering-algorithm"><span class="header-section-number">3.2.3</span> Agglomerative Clustering Algorithm</a></li>
  <li><a href="#methodology" id="toc-methodology" class="nav-link" data-scroll-target="#methodology"><span class="header-section-number">3.2.4</span> Methodology</a></li>
  <li><a href="#inter-cluster-similarity" id="toc-inter-cluster-similarity" class="nav-link" data-scroll-target="#inter-cluster-similarity"><span class="header-section-number">3.2.5</span> Inter-cluster Similarity</a></li>
  <li><a href="#limitations-1" id="toc-limitations-1" class="nav-link" data-scroll-target="#limitations-1"><span class="header-section-number">3.2.6</span> Limitations</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#graph-basedspectral-clustering" id="toc-graph-basedspectral-clustering" class="nav-link" data-scroll-target="#graph-basedspectral-clustering"><span class="header-section-number">4</span> Graph-based/Spectral Clustering</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="support-vector-regression" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="support-vector-regression"><span class="header-section-number">1</span> Support Vector Regression</h2>
<ul>
<li><p>Given training data <span class="math inline">\{x_i,y_i\}_{i=1}^n</span></p></li>
<li><p>Find <span class="math inline">w_1</span> and <span class="math inline">b</span> <br> such that <span class="math inline">y=w_1+b</span> optimally describes the data:<br><br> <img src="CS5590_images/Acrobat_xEaCsnna8F.png" class="img-fluid"><br><br></p></li>
<li><p>The thinner the tube more complex the model.</p>
<ul>
<li>Lazy case (underfitting):<br><br> <img src="CS5590_images/Acrobat_e4ICDp5NzH.png" class="img-fluid"></li>
<li>Suspiciously smart case (overfitting):<br><br> <img src="CS5590_images/Acrobat_e3nfNIlEnj.png" class="img-fluid"><br><br></li>
<li>Compromising case (good generalizability)<br><br> <img src="CS5590_images/Acrobat_Zumi4MZWQa.png" class="img-fluid"><br><br></li>
</ul></li>
<li><p><strong>Formulation</strong>:<br><br></p>
<p><span class="math display">\min_{w_1 ,b,\xi_i ,{\xi_i }^* } \frac{1}{2}w_1^2 +C\sum_i \left(\xi_i +\xi_i^* \right)</span> subject to :<br><br> <span class="math display">\begin{align*}
y_i-(w_ix_{i1}) -b &amp;\leq \epsilon +\xi_i \;\;\text{green line area below}\\
(w_ix_{i1}) +b -y_i&amp;\leq \epsilon +\xi_i^* \;\;\text{red line area below}\\
\xi_i,\xi_i^*\ge 0 \quad &amp; \quad i=1,2,\dots,n
\end{align*}</span> <img src="CS5590_images/Acrobat_HHfPmbZv6S.png" class="img-fluid"> <img src="CS5590_images/Acrobat_L0ecctxzVs.png" class="img-fluid"> <img src="CS5590_images/Acrobat_WgHPzjerND.png" class="img-fluid"><br><br> <span class="math inline">\epsilon</span> controls width of the tube.</p></li>
<li><p>Role of <span class="math inline">C</span></p>
<ul>
<li>Small <span class="math inline">C</span><br><br> <img src="CS5590_images/Acrobat_HvMiVCGSxG.png" class="img-fluid"><br><br></li>
<li>Big <span class="math inline">C</span><br><br> <img src="CS5590_images/Acrobat_cHx9nWZEEl.png" class="img-fluid"><br><br></li>
</ul></li>
</ul>
<section id="non-linear-kernel-svr" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="non-linear-kernel-svr"><span class="header-section-number">1.1</span> Non-linear (Kernel) SVR</h3>
<p><span class="math display">\min_{w_1 ,b,\xi_i ,{\xi_i }^* } \frac{1}{2}(w_1^2 +w_2^2 )+C\sum_i \left(\xi_i +\xi_i^* \right)</span> subject to :<br><br> <span class="math display">\begin{align*}
  y_i-(\mathbf{w}'\phi( x_{i1})) -b &amp;\leq \epsilon +\xi_i \\
  (\mathbf{w}'\phi( x_{i1})) +b -y_i&amp;\leq \epsilon +\xi_i^* \\
  \xi_i,\xi_i^*\ge 0 \;&amp;\; i=1,2,\dots,n
  \end{align*}</span></p>
</section>
<section id="svr-derivation" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="svr-derivation"><span class="header-section-number">1.2</span> SVR: Derivation</h3>
<p><span class="math display">\min_{w_1 ,b,\xi_i ,{\xi_i }^* } \frac{1}{2}(\left\lVert \mathbf{w} \right\rVert)+C\sum_i \left(\xi_i +\xi_i^* \right)</span> subject to :<br><br> <span class="math display">\begin{align*}
y_i-(\mathbf{w}'\phi( x_{i})) -b &amp;\leq \epsilon +\xi_i \\
(\mathbf{w}'\phi( x_{i})) +b -y_i&amp;\leq \epsilon +\xi_i^* \\
\xi_i,\xi_i^*\ge 0 \;&amp;\; i=1,2,\dots,n
\end{align*}</span> Lagrangian: <span class="math display">\begin{align*}
L:=\;&amp;\frac{1}{2}(\left\lVert \mathbf{w} \right\rVert)+C\sum_i \left(\xi_i +\xi_i^* \right) \\
&amp;-\sum_i(\eta_i\xi_i+\eta_i^*\xi_i^*)\\
&amp;-\sum_i \alpha_i(\epsilon +\xi_i -y_i+(\mathbf{w}'\phi( x_{i})) +b  )\\
&amp;-\sum_i \alpha_i^*(\epsilon +\xi_i^* +y_i -(\mathbf{w}'\phi( x_{i})) -b )
\end{align*}</span> Minimize with respect to <span class="math inline">\mathbf{w},b,\xi_i,\xi_i^*</span><br> Maximize with respect to <span class="math inline">\alpha_i,\alpha_i^*,\eta_i,\eta_i^*</span></p>
</section>
<section id="svr-summary" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="svr-summary"><span class="header-section-number">1.3</span> SVR: Summary</h3>
<section id="strengths-of-svr" class="level4" data-number="1.3.1">
<h4 data-number="1.3.1" class="anchored" data-anchor-id="strengths-of-svr"><span class="header-section-number">1.3.1</span> Strengths of SVR</h4>
<ul>
<li>No local minima</li>
<li>Scales relatively well to high-dimensional data</li>
<li>Trade-off between classifier complexity and error can be controlled explicitly via <span class="math inline">C</span> and <span class="math inline">\epsilon</span></li>
<li>Overfitting is avoided (for any fixed <span class="math inline">C</span> and <span class="math inline">\epsilon</span>)</li>
<li>The “curse of dimensionality” is avoided through kernel functions</li>
</ul>
</section>
<section id="weaknesses-of-svr" class="level4" data-number="1.3.2">
<h4 data-number="1.3.2" class="anchored" data-anchor-id="weaknesses-of-svr"><span class="header-section-number">1.3.2</span> Weaknesses of SVR</h4>
<ul>
<li>What is the best trade-off parameter <span class="math inline">C</span> and best <span class="math inline">\epsilon</span> ?</li>
<li>What is a good transformation of the original space ?</li>
</ul>
</section>
</section>
</section>
<section id="logistic-regression" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="logistic-regression"><span class="header-section-number">2</span> Logistic Regression</h2>
<ul>
<li><p>To predict an outcome variable that is categorical from one or more categorical or continuous predictor variables.</p></li>
<li><p>Used because having a categorical outcome variable violates the assumption of linearity in normal regression.</p></li>
<li><p>Let <span class="math inline">X</span> be the data instance, and <span class="math inline">Y</span> be the class label: Learn <span class="math inline">P(Y\mid X)</span> directly</p>
<ul>
<li>Let <span class="math inline">W = (W_1, W_2,\dots, W_n), X=(X_1, X_2, \dots , X_n), \mathbf W\cdot \mathbf X</span> is the dot product</li>
<li>Sigmoid function:<br> <span class="math display">P(Y=1\mid \mathbf{X})=\frac{1}{1+e^{-\mathbf{WX}}}</span></li>
</ul></li>
<li><p>Generative models, e.g., Naïve Bayes: <br> If we estimate <span class="math inline">P(X\mid Y),P(Y)</span> from the data and use bayesian rule to find <span class="math inline">P(Y\mid X=x)</span> it can be considered as generative modeling.<br> It can also generate the data <span class="math inline">P(X)=\sum_yP(y)P(X\mid y)</span></p></li>
<li><p>Discriminative models, e.g., Logistic Regression:<br> If we estimate <span class="math inline">P(Y\mid X)</span> directly from the data then it can be considered as discriminative modeling.</p></li>
<li><p>In logistic regression, we learn the conditional distribution <span class="math inline">P(Y\mid X)</span></p></li>
<li><p>Let <span class="math inline">P_y(X;W)</span> be our estimate of <span class="math inline">P(Y\mid X)</span>, where <span class="math inline">W</span> is a vector of adjustable parameters.</p></li>
<li><p>Assume there are two classes, <span class="math inline">y = 0</span> and <span class="math inline">y = 1</span> and<br> <span class="math display">P_1( \mathbf{X};\mathbf{W})=\frac{1}{1+e^{-\mathbf{WX}}}</span> and <span class="math display">P_0( \mathbf{X};\mathbf{W})=1-\frac{1}{1+e^{-\mathbf{WX}}}</span> log odd <span class="math display">\begin{align*}
\log\frac{P_1( \mathbf{X};\mathbf{W})}{P_0( \mathbf{X};\mathbf{W})}&amp;=\log\frac{\frac{1}{1+e^{-\mathbf{WX}}}}{1-\frac{1}{1+e^{-\mathbf{WX}}}}\\
&amp;=\log\frac{\frac{1}{1+e^{-\mathbf{WX}}}}{\frac{1+e^{-\mathbf{WX}}-1}{1+e^{-\mathbf{WX}}}}\\
&amp;=\log e^\mathbf{WX}\\
&amp;=\mathbf{WX}
\end{align*}</span> That is, the log odds of class <span class="math inline">1</span> is a linear function of <span class="math inline">\mathbf{X}</span></p></li>
<li><p>We find <span class="math inline">\mathbf{W}</span> using Conditional data likelihood — Probability of observed <span class="math inline">Y</span> values in the training data, conditioned on corresponding <span class="math inline">X</span> values.</p></li>
<li><p>We choose parameters <span class="math inline">\mathbf{W}</span> that satisfy. <span class="math display">\mathbf{W}=\argmax_\mathbf{W}\prod_lP(y^l \mid \mathbf{X}^l,\mathbf{W})</span> where</p>
<ul>
<li>w = <span class="math inline">&lt;w_0,w_1,\dots,w_n&gt;</span> is the vector of parameters to be estimated,</li>
<li><span class="math inline">y^l</span> denotes the observed value of <span class="math inline">Y</span> in the <span class="math inline">l^{\text{th}}</span> training example, and</li>
<li><span class="math inline">\mathbf{X}^l</span> denotes the observed value of <span class="math inline">X</span> in the <span class="math inline">l^{\text{th}}</span> training example</li>
</ul></li>
<li><p>Equivalently, we can work with log of conditional likelihood: <span class="math display">\mathbf{W}=\argmax_\mathbf{W}\sum_l \ln P(y^l \mid \mathbf{X}^l,\mathbf{W})</span></p></li>
<li><p>Conditional data log likelihood, <span class="math inline">l(W)</span>, can be written as <span class="math display">l(W)=\sum_l y^l \ln P(y^l=1 \mid \mathbf{X}^l,\mathbf{W})+(1-y^l) \ln P(y^l=0 \mid \mathbf{X}^l,\mathbf{W})</span></p></li>
<li><p>Note here that <span class="math inline">Y</span> can take only values <span class="math inline">0</span> or <span class="math inline">1</span>, so only one of the two terms in the expression will be non-zero for any given <span class="math inline">y^l</span></p></li>
</ul>
<section id="training" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="training"><span class="header-section-number">2.1</span> Training</h3>
<ul>
<li>We need to estimate: <span class="math display">\mathbf{W}=\argmax_\mathbf{W}\sum_l \ln P(y^l \mid \mathbf{X}^l,\mathbf{W})</span></li>
<li>Equivalently, we can minimize negative log likelihood</li>
<li>This is convex – so, unique global minimum</li>
<li><strong>No closed-form solution though</strong>. Iterative method required.</li>
<li>Use gradient ascent (descent) for the maximization (min) problem</li>
<li>The <span class="math inline">i^{\text{th}}</span> component of the vector gradient has the form <span class="math display">\frac{\partial }{\partial w_i }l\left(\mathbf{W}\right)=\sum_l x_i^l \left(y^l -\underbrace{\hat{P} \left(y^l  =1\mid {\mathit{\mathbf{X}}}^l ,\mathit{\mathbf{W}}\right)}_{\text{Logistic regression prediction}}\right)</span></li>
<li>Beginning with initial weights, we repeatedly update the weights in the direction of the gradient, changing the <span class="math inline">i^{\text{th}}</span> weight according to <span class="math display">w_i \leftarrow w_i+\eta \sum_l x_i^l \left(y^l -\hat{P} \left(y^l =1\mid {\mathit{\mathbf{X}}}^l ,\mathit{\mathbf{W}}\right)\right)</span></li>
<li>Overfitting can arise especially when data has very high dimensions and is sparse</li>
<li>One approach -&gt; modified “penalized log likelihood function,” which penalizes large values of <span class="math inline">\mathbf{W}</span>, as before. <span class="math display">\mathbf{W}=\argmax_\mathbf{W}\sum_l \ln P(y^l \mid \mathbf{X}^l,\mathbf{W})-\frac{\lambda}{2}\left\lVert \mathbf{W} \right\rVert^2</span></li>
<li>Derivative then becomes: <span class="math display">\frac{\partial }{\partial w_i }l\left(\mathit{\mathbf{W}}\right)=\sum_l x_i^l \left(y^l -\hat{P} \left(y^l =1\mid {\mathit{\mathbf{X}}}^l ,\mathit{\mathbf{W}}\right)\right)-\lambda w_i</span></li>
</ul>
</section>
<section id="summary" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="summary"><span class="header-section-number">2.2</span> Summary</h3>
<ul>
<li>In general, NB and LR make different assumptions
<ul>
<li>NB: Features independent given class -&gt; assumption on <span class="math inline">P(X\mid Y)</span></li>
<li>LR: Functional form of <span class="math inline">P(Y\mid X)</span>, no assumption on <span class="math inline">P(X\mid Y)</span></li>
</ul></li>
<li>LR is a linear classifier
<ul>
<li>decision rule is a hyperplane</li>
</ul></li>
<li>LR optimized by conditional likelihood
<ul>
<li>no closed-form solution</li>
<li>Concave (convex) -&gt; global optimum with gradient ascent (descent)</li>
</ul></li>
<li>LR can be extended to multiple class using softmax.</li>
</ul>
</section>
</section>
<section id="clustering" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="clustering"><span class="header-section-number">3</span> Clustering</h2>
<p>Types of Clustering Methods</p>
<ul>
<li>In terms of objective:
<ul>
<li>Monothetic: cluster members have some common property
<ul>
<li>E.g. All are males aged 20-35, or all have X% response to test B</li>
</ul></li>
<li>Polythetic: cluster members are similar to each other
<ul>
<li>Distance between elements defines membership</li>
</ul></li>
</ul></li>
<li>In terms of overlap of clusters
<ul>
<li>Hard clustering: clusters do not overlap</li>
<li>Soft clustering: clusters may overlap
<ul>
<li>“Strength of association” between element and cluster</li>
</ul></li>
</ul></li>
<li>In terms of methodology
<ul>
<li>Flat/partitioning (vs) hierarchical: Set of groups (vs) taxonomy</li>
<li>Density-based (vs) Model/Distribution-based: DBSCAN vs GMMs</li>
<li>Connectionist (vs) Centroid-based: k-means vs Hierarchical clustering</li>
</ul></li>
</ul>
<p>Outline</p>
<ul>
<li>K-Means</li>
<li>Hierarchical Clustering</li>
<li>Graph-based/Spectral Clustering</li>
<li>DBSCAN</li>
<li>Model-based Clustering (GMM and Expectation Maximization)</li>
<li>Evaluation of Clustering Algorithms</li>
</ul>
<section id="k-means-clustering" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="k-means-clustering"><span class="header-section-number">3.1</span> k-Means Clustering</h3>
<ul>
<li>Partitional clustering approach</li>
<li>Each cluster is associated with a centroid (center point)</li>
<li>Each point is assigned to the cluster with the closest centroid</li>
<li>Number of clusters, K, must be specified</li>
<li>The basic algorithm is very simple
<ol type="1">
<li>Select <span class="math inline">K</span> points as initial centroids</li>
<li><strong>Repeat</strong>:
<ul>
<li>Form <span class="math inline">K</span> clusters by assigning all point to the closest centroid</li>
<li>Recompute centroid for each cluster.</li>
</ul></li>
<li><strong>until</strong> The centroid doesn’t change.</li>
</ol></li>
<li>Initial centroids are often chosen randomly.
<ul>
<li>Clusters produced can vary from one run to another.</li>
<li>The centroid is (typically) the mean of the points in the cluster.</li>
</ul></li>
<li>‘Closeness’ is measured by Euclidean distance, cosine similarity,correlation, etc.</li>
<li>K-means will converge for common similarity measures mentioned above (local minimum though)</li>
<li>Most of the convergence happens in the first few iterations.
<ul>
<li>Often the stopping condition is changed to ‘Until relatively few points change clusters’</li>
</ul></li>
<li>Nearby points may not end up in the same cluster</li>
</ul>
<section id="selecting-initial-centroids" class="level4" data-number="3.1.1">
<h4 data-number="3.1.1" class="anchored" data-anchor-id="selecting-initial-centroids"><span class="header-section-number">3.1.1</span> Selecting Initial Centroids</h4>
<p>How difficult is this?</p>
<ul>
<li>If there are <span class="math inline">K</span> ‘real’ clusters then the chance of selecting one centroid from each cluster is small
<ul>
<li>Chance is relatively small when <span class="math inline">K</span> is large</li>
<li>If clusters are the same size, <span class="math inline">n</span>, then<br> <span class="math display">\begin{align*}
p&amp;=\frac{\text{number of ways to select one centroid from each cluster}}{\text{Number of ways to select }K \text{ centroid}}\\
&amp;=\frac{K!n^K}{(Kn)^K}\\
&amp;=\frac{K!}{K^K}
\end{align*}</span></li>
<li>For example, if <span class="math inline">K = 10</span>, then probability <span class="math inline">= 10!/10^{10} = 0.00036</span></li>
</ul></li>
</ul>
</section>
<section id="possible-solutions" class="level4" data-number="3.1.2">
<h4 data-number="3.1.2" class="anchored" data-anchor-id="possible-solutions"><span class="header-section-number">3.1.2</span> Possible Solutions</h4>
<ul>
<li>Multiple runs
<ul>
<li>Helps, but probability is not on our side</li>
</ul></li>
<li>Sample and use hierarchical clustering to determine initial centroids</li>
<li>Select more than <span class="math inline">k</span> initial centroids and then select among these initial centroids
<ul>
<li>Select most widely separated</li>
</ul></li>
<li>Bisecting $K-means
<ul>
<li>Not as susceptible to initialization issues</li>
</ul></li>
</ul>
</section>
<section id="evaluating-k-means-clusters" class="level4" data-number="3.1.3">
<h4 data-number="3.1.3" class="anchored" data-anchor-id="evaluating-k-means-clusters"><span class="header-section-number">3.1.3</span> Evaluating k-Means Clusters</h4>
<ul>
<li>Most common measure is Sum of Squared Error (SSE)
<ul>
<li>For each point, the error is the distance to the nearest cluster</li>
<li>To get SSE, we square these errors and sum them. <span class="math display">\mathrm{SSE}=\sum_{i=1}^K\sum_{i\in C_i}\mathrm{dist}^2(m_i,x)</span></li>
</ul></li>
<li><span class="math inline">x</span> is a data point in cluster <span class="math inline">C_i</span> and <span class="math inline">m_i</span> is the representative point for cluster <span class="math inline">C_i</span></li>
<li>Can show that <span class="math inline">m_i</span> corresponds to the center (mean) of the cluster</li>
<li>Given two clusterings, we can choose the one with the smaller error</li>
<li>One easy way to reduce SSE is to increase <span class="math inline">K</span>, the number of clusters</li>
<li>A good clustering with smaller <span class="math inline">K</span> can have a lower SSE than a poor clustering with higher <span class="math inline">K</span></li>
<li>Relatively faster than other clustering methods: <span class="math inline">O(</span> # iterations * # clusters * # instances * # dimensions <span class="math inline">)</span></li>
</ul>
</section>
<section id="limitations" class="level4" data-number="3.1.4">
<h4 data-number="3.1.4" class="anchored" data-anchor-id="limitations"><span class="header-section-number">3.1.4</span> Limitations</h4>
<ul>
<li>k-Means has problems when clusters are of differing
<ul>
<li>Sizes, Densities, Non-globular shapes</li>
</ul></li>
<li>Sensitive to outliers</li>
<li>The number of clusters <span class="math inline">(K)</span> is difficult to determine<br><br> <img src="CS5590_images/Acrobat_6cXliqPWl5.png" class="img-fluid"><br><br></li>
</ul>
</section>
<section id="extensions" class="level4" data-number="3.1.5">
<h4 data-number="3.1.5" class="anchored" data-anchor-id="extensions"><span class="header-section-number">3.1.5</span> Extensions</h4>
<ul>
<li>Use of various distance metrics
<ul>
<li>Euclidean distance <br> <span class="math display">d(x,y)=\sqrt{\sum_{i=1}^n \left\lvert x_i-y_i \right\rvert^2}</span></li>
<li>Manhattan (city-block) distance <span class="math display">d(x,y)=\sum_{i=1}^n \left\lvert x_i-y_i \right\rvert</span></li>
<li>Cosine distance <span class="math display">\begin{align*}
  &amp;\cos(x,y)=\frac{\sum_{i=1}^nx_iy_i}{\sqrt{\sum_{i=1}^nx_i^2}\sqrt{\sum_{i=1}^ny_i^2}}\\
  &amp;d(x,y)=1-\cos(x,y)
  \end{align*}</span></li>
<li>Chebyshev distance <span class="math display">\mathrm{dist}(x_i,x_j)=\max(\left\lvert x_{i1}-x_{j1} \right\rvert ,\left\lvert x_{i2}-x_{j2} \right\rvert ,\dots ,\left\lvert x_{ir}-x_{jr} \right\rvert) </span></li>
</ul></li>
<li><span class="math inline">k</span>-Medioids</li>
<li>Bisecting <span class="math inline">k</span>-Means</li>
<li><span class="math inline">k</span>-Means <span class="math inline">++</span></li>
</ul>
</section>
</section>
<section id="hierarchical-clustering" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="hierarchical-clustering"><span class="header-section-number">3.2</span> Hierarchical Clustering</h3>
<p>As we do not know how many clusters to use, so the idea of hierarchical clustering is to use all <span class="math inline">k</span> clusters hierarchically.</p>
<ul>
<li><p>Types of Clustering Methods</p>
<p><img src="CS5590_images/Acrobat_jvMmR6OD1l.png" class="img-fluid"><br><br></p></li>
</ul>
<p>Hierarchical Clustering</p>
<ul>
<li>Produces a set of nested clusters organized as a hierarchical tree</li>
<li>Can be visualized as a <strong>dendrogram</strong>
<ul>
<li>A tree like diagram that records the sequences of merges or splits<br><br> <img src="CS5590_images/Acrobat_XlYsFF9aTO.png" class="img-fluid"><br><br></li>
</ul></li>
</ul>
<section id="strengths" class="level4" data-number="3.2.1">
<h4 data-number="3.2.1" class="anchored" data-anchor-id="strengths"><span class="header-section-number">3.2.1</span> Strengths</h4>
<ul>
<li>Do not have to assume any particular number of clusters
<ul>
<li>Any desired number of clusters can be obtained by ‘cutting’ the dendrogram at the proper level</li>
</ul></li>
<li>They may correspond to meaningful taxonomies
<ul>
<li>Example in biological sciences (e.g., animal kingdom, phylogeny reconstruction, …)</li>
</ul></li>
</ul>
</section>
<section id="two-main-types-of-hierarchical-clustering" class="level4" data-number="3.2.2">
<h4 data-number="3.2.2" class="anchored" data-anchor-id="two-main-types-of-hierarchical-clustering"><span class="header-section-number">3.2.2</span> Two main types of hierarchical clustering</h4>
<ol type="1">
<li>Agglomerative:
<ul>
<li>Start with the points as individual clusters</li>
<li>At each step, merge the closest pair of clusters until only one cluster (or <span class="math inline">k</span> clusters) left</li>
</ul></li>
<li>Divisive:
<ul>
<li>Start with one, all-inclusive cluster</li>
<li>At each step, split a cluster until each cluster contains a point (or there are <span class="math inline">k</span> clusters)</li>
</ul></li>
</ol>
<ul>
<li>Traditional hierarchical algorithms use a similarity or distance matrix
<ul>
<li>Merge or split one cluster at a time</li>
</ul></li>
</ul>
</section>
<section id="agglomerative-clustering-algorithm" class="level4" data-number="3.2.3">
<h4 data-number="3.2.3" class="anchored" data-anchor-id="agglomerative-clustering-algorithm"><span class="header-section-number">3.2.3</span> Agglomerative Clustering Algorithm</h4>
<ul>
<li>More popular hierarchical clustering technique</li>
<li>Basic algorithm is straightforward
<ol type="1">
<li>Compute the proximity matrix</li>
<li>Let each data point be a cluster</li>
<li><strong>Repeat</strong>
<ol type="1">
<li>Merge the two closest clusters</li>
<li>Update the proximity matrix</li>
</ol></li>
<li><strong>Until</strong> only a single cluster remains</li>
</ol></li>
<li>Key operation is the computation of the proximity of two clusters
<ul>
<li>Different approaches to defining the distance between clusters distinguish the different algorithms</li>
</ul></li>
</ul>
</section>
<section id="methodology" class="level4" data-number="3.2.4">
<h4 data-number="3.2.4" class="anchored" data-anchor-id="methodology"><span class="header-section-number">3.2.4</span> Methodology</h4>
<ul>
<li>Start with clusters of individual points and a proximity matrix (pairwise distance between each data point)<br><br> <img src="CS5590_images/mspaint_4PJPGBUzQ2.png" class="img-fluid"><br><br></li>
<li>After some merging steps, we have some clusters<br><br> <img src="CS5590_images/mspaint_11lcWyxY5z.png" class="img-fluid"><br><br></li>
<li>We want to merge the two closest clusters (C2 and C5) and update the proximity matrix.<br><br> <img src="CS5590_images/mspaint_McTTx9mF84.png" class="img-fluid"><br><br></li>
<li>The question is “How do we update the proximity matrix?”<br><br> <img src="CS5590_images/mspaint_EgIpeiigHW.png" class="img-fluid"><br><br></li>
</ul>
<p>We can update the proximity matrix using Inter-cluster Similarity</p>
</section>
<section id="inter-cluster-similarity" class="level4" data-number="3.2.5">
<h4 data-number="3.2.5" class="anchored" data-anchor-id="inter-cluster-similarity"><span class="header-section-number">3.2.5</span> Inter-cluster Similarity</h4>
<ul>
<li>MIN (Single-link)<br><br> <img src="CS5590_images/Acrobat_DgthH0kVLa.png" class="img-fluid"><br><br></li>
<li>MAX (Complete-link)<br><br> <img src="CS5590_images/Acrobat_XG7dtguqQd.png" class="img-fluid"><br><br></li>
<li>Group Average (Average-link)<br><br> <img src="CS5590_images/Acrobat_V87JygBnBI.png" class="img-fluid"><br><br></li>
<li>Distance Between Centroids<br><br> <img src="CS5590_images/Acrobat_XTf6gyPsxn.png" class="img-fluid"><br><br></li>
</ul>
</section>
<section id="limitations-1" class="level4" data-number="3.2.6">
<h4 data-number="3.2.6" class="anchored" data-anchor-id="limitations-1"><span class="header-section-number">3.2.6</span> Limitations</h4>
<ul>
<li>Once a decision is made to combine two clusters, it cannot be undone</li>
<li>No objective function is directly minimized</li>
<li>Different schemes have problems with one or more of the following:
<ul>
<li>Sensitivity to noise and outliers (MIN)</li>
<li>Difficulty handling different sized clusters and non-convex shapes (Group average, MAX)</li>
<li>Breaking large clusters (MAX)</li>
</ul></li>
</ul>
</section>
</section>
</section>
<section id="graph-basedspectral-clustering" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="graph-basedspectral-clustering"><span class="header-section-number">4</span> Graph-based/Spectral Clustering</h2>
<ul>
<li>Associate each data item with a vertex in a weighted graph
<ul>
<li>weights on the edges between elements are large if the elements are similar and small if they are not.</li>
</ul></li>
<li>Cut the graph into connected components with relatively large interior weights by cutting edges with relatively low weights.</li>
<li>Clustering becomes a graph cut problem.<br><br> <img src="CS5590_images/mspaint_MeMOc3QDXr.png" class="img-fluid"><br><br></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="abhiyantaabhishek/IITH-Data-Science" data-repo-id="R_kgDOILoB8A" data-category="Announcements" data-category-id="DIC_kwDOILoB8M4CSJcL" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../Data_Science_Notes/Machine-Learning/2022-10-29-CS5590-week8.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Machine Learning 8</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../Data_Science_Notes/Mathematics/2022-08-06-CS6660-week1.html" class="pagination-link">
        <span class="nav-page-text">Probability Theory 1</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Copyright 2022, Abhishek Kumar Dubey</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/abhiyantaabhishek">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/abhishek-kumar-dubey-585a86179/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>



</body></html>