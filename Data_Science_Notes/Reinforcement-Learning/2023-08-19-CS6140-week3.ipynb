{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "author: Abhishek Kumar Dubey\n",
    "badges: false\n",
    "categories:\n",
    "- Reinforcement Learning\n",
    "date: '2023-08-18'\n",
    "description: Value Functions with Policy, Decomposition of State Value Function, MDP with Policy, Optimal Policy, Solution to an MDP, Greedy Policy\n",
    "image: CS6140_images/Acrobat_9thkiyFyb5.png\n",
    "title: Reinforcement Learning 3\n",
    "toc: true\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy \n",
    "\n",
    "Let $\\pi$ denote a policy that maps state space $\\mathcal S$ to action space $\\mathcal A$.\n",
    "\n",
    "Types of policy:\n",
    "\n",
    "- Deterministic policy : $a=\\pi(s),s \\in \\mathcal  S, a \\in \\mathcal A$\n",
    "- Stochastic Policy : $\\pi(a|s)=P[a_{t}=a|s_{t}=s]$\n",
    "\n",
    "Example: Consider the Grid world : \n",
    "\n",
    "![](CS6140_images/Acrobat_Y5FL60FCtN.png)\n",
    "\n",
    "Here states are from 1 to 14 and two shaded terminal states. And actions are {Right, Left, Up, Down}\n",
    "\n",
    " \n",
    " - Deterministic Policy : $\\pi(s) = \\begin{cases}\n",
    "    \\text{Down} &\\text{if } s=\\{3,7,11\\} \\\\\n",
    "    \\text{Right} &\\text{Otherwise } \n",
    " \\end{cases}$\n",
    "\n",
    "- Stochastic Policy: $\\pi(a|s)$ could be a uniform action between all available actions at state $s$. 25% chance of going in any direction\n",
    "\n",
    "::: {.callout-note }\n",
    "-  A deterministic policy is easily exploited.\n",
    "- A uniform random policy is optimal ( i.e. Nash equilibrium)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Functions with Policy\n",
    "\n",
    "Given  MDP with policy $\\pi$ we define the value of a policy as follows:\n",
    "\n",
    "::: {.alert .alert-dismissible .alert-info}\n",
    "\n",
    "__State-Value Function__\n",
    "\n",
    "The value function $V^\\pi(s)$ in state $s$ is expected (discounted) total return starting from state $s$ and then following the policy $\\pi$\n",
    "\n",
    "$$V^{\\pi}(s)=\\mathbb{E}_{\\pi}\\left(\\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k+1}|s_{t}=s\\right)$$\n",
    "\n",
    ":::\n",
    "\n",
    "Example: \n",
    "\n",
    "![](CS6140_images/Acrobat_JiESL0y2O7.png)\n",
    "\n",
    "- sates are $S=\\{A,B,G_{1},G_{2}\\};$ states $G_1$ and $G_2$ are terminal sates.\n",
    "- Two actions $\\mathcal A = \\{a_1,a_2\\}$\n",
    "- Value fo states $\\{A,B\\}$ using forward policy $\\pi_f$ is given by, $V_{\\pi f}(A) = 11,V_{\\pi f}(B) = 10$ \n",
    "- Value fo states $\\{A,B\\}$ using forward policy $\\pi_b$ is given by, $V_{\\pi b}(A) = 5,V_{\\pi b}(B) = 6$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposition of State Value Function\n",
    "\n",
    "The state-value function can be decomposed into immediate reward plus discounted value\n",
    "of successor state\n",
    "\n",
    "$$V^{\\pi}(s)=\\mathbb{E}_{\\pi}(r_{t+1}+\\gamma V^{\\pi}(s_{t+1})|s_{t}=s)$$\n",
    "\n",
    "Expanding the expectation, with ${\\cal R}_{s s^{\\prime}}^{a}={\\cal R}(s,a,s^{\\prime})$ we get:\n",
    "\n",
    "$$\\mathbb{E}_{\\pi}[r_{t+1}|s_{t}=s]=\\sum_{a}\\pi(a|s)\\sum_{s^{\\prime}}{\\mathcal{P}}_{s s^{\\prime}}^{a}{\\mathcal{R}}_{s s^{\\prime}}^{a} \\tag{1}$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\mathbb{E}_{\\pi}[\\gamma V^{\\pi}(s_{t+1})|s_{t}=s]=\\sum_{a}\\pi(a|s)\\sum_{s^{\\prime}}p_{s s^{\\prime}}^{a}\\gamma V^{\\pi}(s^{\\prime}) \\tag{2}$$\n",
    "\n",
    "using equation $(1)$ and $(2)$ we get \n",
    "\n",
    "$$\\boxed{V^{\\pi}(s)=\\sum_{a}\\pi(a|s)\\sum_{s^{\\prime}}\\mathcal{P}_{s s^{\\prime}}^{a}\\left[\\mathcal{R}_{s s^{\\prime}}^{a}+\\gamma V^{\\pi}(s^{\\prime})\\right]}$$\n",
    "\n",
    "\n",
    "\n",
    "::: {.alert .alert-dismissible .alert-success}\n",
    "::: {.callout-important }\n",
    "::: {.alert .alert-dismissible .alert-danger}\n",
    "$$\\boxed{V^{\\pi}(s)=\\sum_{a}\\pi(a|s)\\sum_{s^{\\prime}}\\mathcal{P}_{s s^{\\prime}}^{a}\\left[\\mathcal{R}_{s s^{\\prime}}^{a}+\\gamma V^{\\pi}(s^{\\prime})\\right]}$$\n",
    ":::\n",
    "This equation is called Bellman Evaluation Operator\n",
    ":::\n",
    ":::\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Formulation of Bellman Evaluation Equation\n",
    "\n",
    "$$V^{\\pi}(s)=\\sum_{a}\\pi(a|s)\\sum_{s^{\\prime}}\\mathcal{P}_{s s^{\\prime}}^{a}\\left[\\mathcal{R}_{s s^{\\prime}}^{a}+\\gamma V^{\\pi}(s^{\\prime})\\right]$$\n",
    "\n",
    "Denote, \n",
    "\n",
    "$${\\mathcal{P}}^{\\pi}(s^{\\prime}|s)=\\sum_{a\\in\\mathcal{A}}\\pi\\bigl(a|s\\bigr){\\mathcal{P}}_{s s^{\\prime}}^{a}$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\mathcal R^{\\pi}(s)= \\sum_{a\\in\\mathcal{A}}\\pi(a|s)\\sum_{s^{\\prime}}\\mathcal{P}_{s s^{\\prime}}^{a}\\mathcal{R}_{s s^{\\prime}}^{a}=\\mathbb{E}(r_{t+1}|s_{t}=s)$$\n",
    "\n",
    "Using $\\mathcal P^\\pi$ and $\\mathcal R^ \\pi$, fot finite state MDP, Bellman evaluation equation can be written as \n",
    "\n",
    "$$V^{\\pi}={\\cal R}^{\\pi}+\\gamma{\\cal P}^{\\pi}V^{\\pi}\\implies V^{\\pi}=(I-\\gamma{\\cal P}^{\\pi})^{-1}{\\cal R}^{\\pi}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function Computation : Example\n",
    "\n",
    "Consider below diagram :\n",
    "\n",
    "![](CS6140_images/Acrobat_9thkiyFyb5.png)\n",
    "\n",
    "- States $S=\\{A,B,C,D\\};$ State $D$ is terminal state.\n",
    "- Two action  $\\mathcal A = \\{a_1,a_2\\}$\n",
    "- Stochastic Environment with action chosen succeeding 90% and failing 10%.\n",
    "- Upon failure, agent moves in the direction suggested by the other action.\n",
    "- Consider deterministic policy $\\pi_1$ that choses action $a_1$ in all states.\n",
    "- Transition matrix corresponding to policy $\\pi_1$ is given by\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "     &A&B&C&D \\\\\n",
    "    A&0&0.9&0.1&0\\\\\n",
    "    B&0.1&0&0&0.9\\\\\n",
    "    C&0.9&0&0&0.1\\\\\n",
    "    D&0&0&0&1 \\\\ \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "- Values of the states under the policy $\\pi_1$ is given by:\n",
    "  - $V^{\\pi_1}(D) = 100$\n",
    "  -  $V^{\\pi_1}(C) = 0.9\\times [-10+V^{\\pi_1}(A)]+0.1\\times [-10 +V^{\\pi_1}(D)]$\n",
    "  -  $V^{\\pi_1}(B) = 0.9\\times [-10+V^{\\pi_1}(D)]+0.1\\times [-10 +V^{\\pi_1}(A)]$\n",
    "  -  $V^{\\pi_1}(A) = 0.9\\times [-10+V^{\\pi_1}(B)]+0.1\\times [-10 +V^{\\pi_1}(C)]$\n",
    "\n",
    "Fom above we get three equations :\n",
    "\n",
    "$$\\begin{align*}\n",
    "    0.9\\times V^{\\pi_1}(A) - V^{\\pi_1}(C) &= 0\\\\\n",
    "    -0.1 \\times V^{\\pi_1}(A)+V^{\\pi_1}(B)&=80\\\\\n",
    "    V^{\\pi_1}(A)-0.9\\times V^{\\pi_1}(B) - 0.1 \\times V^{\\pi_1}(C)&=-10\n",
    "\\end{align*}$$\n",
    "\n",
    "Solving these we get:\n",
    "\n",
    "$V^{\\pi_1}=\\{75.610,87.561,68.049,100\\}$\n",
    "\n",
    "Similarly if we choose $\\pi_2$ in all the states, then $V^{\\pi_2}$ can be calculated following same approach:\n",
    "\n",
    "$V^{\\pi_2}=\\{75.610,68.049,87.561,100\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP with Policy\n",
    "\n",
    "::: {.callout-important }\n",
    "MDP + POLICY = MRP\n",
    ":::\n",
    "\n",
    "- MDP + policy = Markov Reward Process\n",
    "- Given a MDP $\\langle \\mathcal S, \\mathcal A, \\mathcal P,\\mathcal R, \\gamma \\rangle$ and a policy $\\pi$\n",
    "- The MRP is given by $\\langle \\mathcal S,  \\mathcal P^{\\pi},\\mathcal R^{\\pi}, \\gamma \\rangle$ and a policy $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Value Function\n",
    "\n",
    "\n",
    "\n",
    "::: {.alert .alert-dismissible .alert-info}\n",
    "The action-value function  $Q(s, a)$ under policy $\\pi$ is the expected return starting from state\n",
    "$s$ and __taking action $a$__ and then following the policy $\\pi$\n",
    "\n",
    "$$Q^{\\pi}(s,a)=\\mathbb{E}_{\\pi}\\left(\\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k+1}|s_{t}=s,a_{t}=a\\right)$$\n",
    "\n",
    "The action-value function can similarly be decomposed as\n",
    "\n",
    "$$Q^{\\pi}(s,a)=\\mathbb{E}_{\\pi}(r_{t+1}+\\gamma Q^{\\pi}(s_{t+1},a_{t+1})|s_{t}=s,a_{t}=a)$$\n",
    ":::\n",
    "\n",
    "Expanding the expectation we have $Q^{\\pi}(s,a)$ to be:\n",
    " \n",
    " $$Q^{\\pi}(s,a)=\\sum_{s^{\\prime}}\\mathcal{P}_{s s^{\\prime}}^{a}\\biggl[R_{s s^{\\prime}}^{a}+\\gamma\\sum_{a^{\\prime}}\\pi(a^{\\prime}|s^{\\prime})Q^{\\pi}(s^{\\prime},a^{\\prime})\\biggr]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship between $V^\\pi(\\cdot)$ and $Q^\\pi(\\cdot)$\n",
    "\n",
    "Using definition of $V^\\pi(s)$ and $Q^\\pi(s,a)$ we can arrive at the following relationship :\n",
    "\n",
    "$$V^{\\pi}(s)=\\sum_{a\\in A}\\pi(a|s)Q^{\\pi}(s,a)$$\n",
    "$$Q^{\\pi}(s,a)=\\sum_{s^{\\prime}\\in{\\mathcal{S}}}{\\mathcal{P}}_{s s^{\\prime}}^{a}\\left[{\\mathcal{R}}_{s s^{\\prime}}^{a}+\\gamma V^{\\pi}(s^{\\prime})\\right]$$\n",
    "\n",
    "Explanation : $$V^{\\pi}(s)=\\underbrace{\\sum_{a\\in A}\\pi(a|s)}_{\\substack{\\text{take into account all the actions} \\\\ \\text{that can be taken as per policy}}} Q^{\\pi}(s,a)$$\n",
    "\n",
    "$$Q^{\\pi}(s,a)=\\sum_{s^{\\prime}\\in{\\mathcal{S}}}{\\mathcal{P}}_{s s^{\\prime}}^{a}\\left[\\underbrace{{\\mathcal{R}}_{s s^{\\prime}}^{a}}_{\\substack{\\text{reward for taking an}\\\\ \\text{action in current state}}} +\\underbrace{\\gamma V^{\\pi}(s^{\\prime})}_{\\substack{\\text{follow the policy}\\\\ \\text{from next state}}} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Policy\n",
    "\n",
    "Define a partial ordering over policies\n",
    "\n",
    "$\\pi \\ge \\pi', \\text{ if } V^\\pi(s)\\ge V^{\\pi'}(s), \\forall s \\in S$\n",
    "\n",
    "::: {.alert .alert-dismissible .alert-info}\n",
    "__Theorem__\n",
    "\n",
    "- There exists an optimal policy $\\pi_*$ that is better than or equal to all other policies.\n",
    "- All optimal policies achieve the optimal value function, $V_*(s)=V^{\\pi_*}(s)$\n",
    "- All optimal policies achieve the optimal action-value function, $Q_*(s,a)=Q^{\\pi_*}(s,a)$\n",
    ":::\n",
    "\n",
    "Consider the Grid world, actions are left, right, up, Down , reward on all transitions : $R_t=-1$\n",
    "\n",
    "![](CS6140_images/Acrobat_Y5FL60FCtN.png)\n",
    "\n",
    "Goal : Reach any of the goal state in as minimum plays as possible\n",
    "\n",
    "Question : What could be an optimal policy to achieve the above objective ?\n",
    "\n",
    "![](CS6140_images/Acrobat_YgodlkBegH.png)\n",
    "\n",
    "Question : How many optimal policies are there ?\n",
    "\n",
    "Answer : There are infinite optimal policies (including some deterministic ones)\n",
    "\n",
    "::: {.callout-note }\n",
    "There wil be at least one policy that is deterministic\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to an MDP\n",
    "\n",
    "Solving an MDP means finding a policy $\\pi_*$ as follows\n",
    "\n",
    "$$\\pi_{*}=\\argmax_\\pi \\left[\\mathbb{E}_{\\pi}\\left(\\sum_{t=0}^{\\infty}\\gamma^{t}r_{t+1}\\right)\\right]$$\n",
    "\n",
    "- Denote optimal value function, $V_*(s)=V^{\\pi_*}(s)$\n",
    "- Denote optimal action value function, $Q_*(s,a)=Q^{\\pi_*}(s,a)$\n",
    "- The main goal in RL or solving an MDP means finding an optimal value function $V_*$ or optimal action value function $Q_*$ or optimal policy $\\pi_*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding an Optimal Policy\n",
    "\n",
    "Suppose we are given $Q_*(s,a)$. Can we find an optimal policy ?\n",
    "\n",
    "An optimal policy can be found by maximizing  over $Q_*(s,a)$\n",
    "\n",
    "$$\\pi_*(a\\mid s) = \\begin{cases}\n",
    "    1 &\\text{if } a=\\argmax_{a\\in \\mathcal A} Q_*(s,a) \\\\\n",
    "    0 &\\text{if } \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "- If we know $Q_*(s,a)$, we immediately have an optimal policy \n",
    "- ::: {.callout-note }\n",
    "There is always a deterministic optimal policy for any MDP\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy (Optimal) Policy\n",
    "\n",
    "::: {.alert .alert-dismissible .alert-warning}\n",
    "_Greedy policy with respect to optimal (action) value function is an optimal policy._\n",
    ":::\n",
    "\n",
    "An optimal policy can be found by maximizing  over $Q_*(s,a)$\n",
    "\n",
    "$$\\pi_*(s) = \\begin{cases}\n",
    "    1 &\\text{if } a=\\argmax_{a\\in \\mathcal A} Q_*(s,a) \\\\\n",
    "    0 &\\text{if } \\text{otherwise}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy Policy\n",
    "\n",
    "For a given  $Q^\\pi(\\cdot,\\cdot)$, define $\\pi'(s)$ as follows\n",
    "\n",
    "$$\\pi'(s) = \\operatorname{greedy}(Q)= \\begin{cases}\n",
    "    1 &\\text{if } a=\\argmax_{a\\in \\mathcal A} Q^\\pi(s,a) \\\\\n",
    "    0 &\\text{if } \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "For a given $V^\\pi(\\cdot)$ define $\\pi'(s)$ as follows \n",
    "\n",
    "$$\\pi'(s) = \\operatorname{greedy}(V)= \\begin{cases}\n",
    "    1 &\\text{if } a=\\arg\\operatorname*{max}_{a\\in A}\\left[\\sum_{s^{\\prime}\\in S}\\mathcal{P}_{s s^{\\prime}}^{a}\\left(\\mathcal{R}_{s s^{\\prime}}^{a}+\\gamma V^{\\pi}(s^{\\prime})\\right)\\right] \\\\\n",
    "    0 &\\text{if } \\text{otherwise}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship between $V_*(\\cdot)$ and $Q_*(\\cdot)$\n",
    "\n",
    "Using definition of $V_*(s)$ and $Q_*(s,a)$ we can arrive at the following relationship :\n",
    "\n",
    "$$V_*(s)=\\max_a Q_*(s,a)$$\n",
    "$$Q_*(s,a)=\\sum_{s^{\\prime}\\in{\\mathcal{S}}}{\\mathcal{P}}_{s s^{\\prime}}^{a}\\left[{\\mathcal{R}}_{s s^{\\prime}}^{a}+\\gamma V_*(s^{\\prime})\\right]$$\n",
    "\n",
    "Explanation : $$V_*(s)=\\underbrace{\\max_a}_{\\substack{\\text{no need to take into account all the actions} \\\\ \\text{just take max as max is optimal}}} Q_*(s,a)$$\n",
    "\n",
    "$$Q_*(s,a)=\\sum_{s^{\\prime}\\in{\\mathcal{S}}}{\\mathcal{P}}_{s s^{\\prime}}^{a}\\left[\\underbrace{{\\mathcal{R}}_{s s^{\\prime}}^{a}}_{\\substack{\\text{reward for taking an}\\\\ \\text{action in current state}}} +\\underbrace{\\gamma V_*(s^{\\prime})}_{\\substack{\\text{follow the optimum  policy}\\\\ \\text{from next state}}} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "Is there a way to arrive at $\\pi_*$ starting from an arbitrary policy $\\pi$, The answer is policy iteration.\n",
    "\n",
    "- Evaluate the policy $\\pi$ \n",
    "  - Compute $V^{\\pi}(s)=\\mathbb{E}_{\\pi}(r_{t+1}+\\gamma r_{t+2}+\\gamma^{2}r_{t+3}+\\cdot\\cdot\\cdot\\cdot\\vert s_{t}=s)$\n",
    "- Improve the policy $\\pi$\n",
    "  - $\\pi^{\\prime}(s)=\\operatorname{greedy}(V^{\\pi}(s))$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "$\\tiny  {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0e5e69b8442e8f020791fad6bfcef3777f0e89e0f1a2517b6b628b2eaf0fe66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
