{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "author: Abhishek Kumar Dubey\n",
    "badges: false\n",
    "categories:\n",
    "- Reinforcement Learning\n",
    "date: '2023-08-26'\n",
    "description: Value Functions with Policy, Decomposition of State Value Function, MDP with Policy, Optimal Policy, Solution to an MDP, Greedy Policy\n",
    "image: CS6140_images/Acrobat_9thkiyFyb5.png\n",
    "title: Reinforcement Learning 4\n",
    "toc: true\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact method: Policy Iteration\n",
    "\n",
    "Is there a way to arrive at $\\pi_*$ starting from an arbitrary policy $\\pi$, The answer is policy iteration.\n",
    "\n",
    "- Evaluate the policy $\\pi$ \n",
    "  - Compute $V^{\\pi}(s)=\\mathbb{E}_{\\pi}(r_{t+1}+\\gamma r_{t+2}+\\gamma^{2}r_{t+3}+\\cdot\\cdot\\cdot\\cdot\\vert s_{t}=s)$\n",
    "- Improve the policy $\\pi$\n",
    "  - $\\pi^{\\prime}(s)=\\operatorname{greedy}(V^{\\pi}(s))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact method: Value Iteration\n",
    "\n",
    "Is there a way to arrive at $V_*$ starting from an arbitrary value $V_*$, The answer is value iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bellman Evaluation Equation\n",
    "\n",
    "$$V^{\\pi}(s)=\\sum_{a}\\pi(a|s)\\sum_{s^{\\prime}}\\mathcal{P}_{s s^{\\prime}}^{a}\\left[\\mathcal{R}_{s s^{\\prime}}^{a}+\\gamma V^{\\pi}(s^{\\prime})\\right]$$\n",
    "\n",
    "For a MDP with $S=n$ , Bellman Evaluation Equation for $V^\\pi (S)$ is a system of $n=|S|$ (linear) equation with  $n$ vriable and can be solved if the model is known\n",
    "\n",
    "Denote, \n",
    "\n",
    "$${\\mathcal{P}}^{\\pi}(s^{\\prime}|s)=\\sum_{a\\in\\mathcal{A}}\\pi\\bigl(a|s\\bigr){\\mathcal{P}}_{s s^{\\prime}}^{a}$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\mathcal R^{\\pi}(s)= \\sum_{a\\in\\mathcal{A}}\\pi(a|s)\\sum_{s^{\\prime}}\\mathcal{P}_{s s^{\\prime}}^{a}\\mathcal{R}_{s s^{\\prime}}^{a}=\\mathbb{E}(r_{t+1}|s_{t}=s)$$\n",
    "\n",
    "Using $\\mathcal P^\\pi$ and $\\mathcal R^ \\pi$, fot finite state MDP, Bellman evaluation equation can be written as \n",
    "\n",
    "$$V^{\\pi}={\\cal R}^{\\pi}+\\gamma{\\cal P}^{\\pi}V^{\\pi}\\implies V^{\\pi}=(I-\\gamma{\\cal P}^{\\pi})^{-1}{\\cal R}^{\\pi}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimality Equation for State Value Function\n",
    "\n",
    "Recursive formulation for $V_*(s)$ :\n",
    "\n",
    "$$V_{*}(s)=\\operatorname*{max}_{a}Q_{*}(s,a)=\\operatorname*{max}_{a}\\left[\\sum_{s^{\\prime}\\in{\\cal S}}p_{s s^{\\prime}}^{a}(\\mathcal{R}_{s s^{\\prime}}^{a}+\\gamma V_{*}(s^{\\prime}))\\right]$$\n",
    "\n",
    "::: {.callout-note}\n",
    "Optimality equations are non-linear system of equations with $n$ unknowns and $n$ non-linear constraints (the max operator is non-linear)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the Bellman Optimality Equation\n",
    "\n",
    "- Bellman optimality equations are non-linear\n",
    "- In general, there are no closed form solutions\n",
    "- Iterative methods are typically used\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman’s Optimality Principle\n",
    "\n",
    "::: {.callout-caution title=\"Principle of Optimality\"}\n",
    "The tail of an optimal must be optimal\n",
    "\n",
    "- Any optimal policy can be subdivided into two components; an optimal first action, followed by an optimal policy from successor state $s′$ .\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Methodology : Dynamic Programming\n",
    "\n",
    "Bellman optimality equation :\n",
    "\n",
    "$$V_{*}(s)=\\operatorname*{max}_{a}\\left[\\sum_{s^{\\prime}\\in{\\cal S}}p_{s s^{\\prime}}^{a}(\\mathcal{R}_{s s^{\\prime}}^{a}+\\gamma V_{*}(s^{\\prime}))\\right]$$\n",
    "\n",
    "- Optimal Substructure : Optimal solution can be constructed from optimal solutions to subproblems\n",
    "- Overlapping Subproblems : Problem can be broken down into subproblems and can be\n",
    "reused several times.\n",
    "- Markov Decision Processes, generally, satisfy both these characteristics.\n",
    "- Dynamic Programming is a popular solution method for problems having such properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration : Idea\n",
    "\n",
    "- Suppose we know the value $V_∗ (s′)$\n",
    "- Then the solution $V_∗ (s)$ can be found by one step look ahead\n",
    "  $$V_{*}(s)\\leftarrow \\operatorname*{max}_{a}\\left[\\sum_{s^{\\prime}\\in{\\cal S}}p_{s s^{\\prime}}^{a}(\\mathcal{R}_{s s^{\\prime}}^{a}+\\gamma V_{*}(s^{\\prime}))\\right]$$\n",
    "- Idea of value iteration is to perform the above updates iteratively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration : Algorithm\n",
    "\n",
    "::: {.alert .alert-dismissible .alert-success}\n",
    "\n",
    "-----\n",
    "Algorithm value Iteration\n",
    "\n",
    "-----\n",
    "\n",
    "1. Start with an initial value function $V_1(\\cdot)$;\n",
    "2. for $k=1,2,\\cdots,K$ do\n",
    "3. for $s\\in S$ do\n",
    "4. Calculate $$V_{k+1}(s)\\leftarrow \\operatorname*{max}_{a}\\left[\\sum_{s^{\\prime}\\in{\\cal S}}p_{s s^{\\prime}}^{a}(\\mathcal{R}_{s s^{\\prime}}^{a}+\\gamma V_{k}(s^{\\prime}))\\right]$$\n",
    "5. end for\n",
    "6. end for\n",
    "\n",
    "-----\n",
    "\n",
    ":::\n",
    "\n",
    "![](CS6140_images/2023-09-02_19-45.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration : Remarks\n",
    "\n",
    "- The sequence of value functions $\\{V_1 , V_2 , \\cdots , \\}$ converge to $V_*$\n",
    "- Convergence is independent of the choice of $V_0$\n",
    "- Intermediate value functions need not correspond to a policy in the sense of satisfying the Bellman Evaluation Equation\n",
    "- However, for any $k$, one can come up with a greedy policy as follows: \n",
    "  $$\\pi _{k+1}(s)=\\leftarrow \\text{greedy}V_k(s)$$\n",
    "- The crux of proving the above statement lie in Banach Fixed Point Theorem/ Contraction Mapping Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimality Equation for Action-Value Function\n",
    "\n",
    "There is a recursive formulation for $\\mathcal Q_*(\\cdot,\\cdot)$\n",
    "\n",
    "$${\\cal Q}_{*}(s,a)=\\left[\\sum_{s^{\\prime}\\in S}{\\cal P}_{s s^{\\prime}}^{a}\\left({\\cal R}_{s s^{\\prime}}^{a}+\\gamma\\,\\operatorname*{max}_{a^{\\prime}}(s^{\\prime},a^{\\prime})\\right)\\right]$$\n",
    "\n",
    "One could similarly conceive an iterative algorithm to compute optimal $Q_*$ using the above recursive formulation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof  of  Value Iteration Convergence \n",
    "\n",
    "### Notion of Convergence\n",
    "\n",
    "Let $\\mathcal V$ be a vector space, A sequence of vectors $\\{v_n \\in \\mathcal V\\}$ (with $n \\in \\mathbb N$) is said to converge to $v$ if and only if \n",
    "$$\\lim_{n \\to \\infty}   \\Vert v_n -v \\Vert=0$$\n",
    "\n",
    "![](CS6140_images/2023-09-02_20-06.png)\n",
    "\n",
    "### Cauchy Sequence\n",
    "\n",
    "A sequence of vectors $\\{v_n\\}\\in \\mathcal V$ (with $n \\in \\mathbb N$) is said to be Cauchy sequence if and only if, for each $\\epsilon > 0$, there exists and $N_\\epsilon$ such that $\\Vert v_n-v_m \\Vert \\le \\epsilon$ for any $n,m > N_\\epsilon$\n",
    "\n",
    "![](CS6140_images/2023-09-02_20-12.png)\n",
    "\n",
    "::: {.callout-important }\n",
    "- It is not necessary that every Cauchy sequence will converge, but if it has completeness then a cauchy sequence will converge \n",
    ":::\n",
    "\n",
    "\n",
    "### Notion of Completeness\n",
    "\n",
    "A normed vector space $(\\mathcal V, \\Vert \\cdot \\Vert)$ is complete, if and only if, every Cauchy sequence in $\\mathcal V$ converges to a point in $\\mathcal V$ \n",
    "\n",
    "### Contractions\n",
    "\n",
    "Let $(\\mathcal V, \\Vert \\cdot \\Vert)$ be a normed vector space and let $L : \\mathcal V \\rightarrow \\mathcal  V$. we say that $L$ is a contraction, or a contraction mapping, if there is a real number $\\gamma \\in [0,1)$, such that $$\\Vert L(v)-L(u)\\Vert \\le \\gamma \\Vert v-u \\Vert$$ for all $u$ and $v$ in $\\mathcal V$, where the term $\\gamma$ is called a Lipschitz coefficient for $L$\n",
    "\n",
    "![](CS6140_images/2023-09-02_21-04.png)\n",
    "\n",
    "\n",
    "### Notion of Fixed point \n",
    "\n",
    "A vector $v \\in \\mathcal V$ is  a fixed point of the map $L : \\mathcal V \\rightarrow \\mathcal V$ if $L(v)=v$\n",
    "\n",
    "![](CS6140_images/2023-09-02_21-07.png)\n",
    "\n",
    "### Banach Fixed point theorem \n",
    "\n",
    "Let $\\langle \\mathcal V, \\Vert \\cdot \\Vert \\rangle$ be a complete normed vector space and let $L : \\mathcal V \\rightarrow \\mathcal V$ be a $\\gamma$ - contraction mapping, Then iterative application of $L$ converges to a unique fixed point in $\\mathcal V$ independent of the starting point.\n",
    "\n",
    "![](CS6140_images/2023-09-02_21-13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Function Space\n",
    "\n",
    "- $\\mathcal S$ is a discrete state space with $\\vert \\mathcal S \\vert=n$\n",
    "- $\\mathcal A_s \\subseteq \\mathcal A$ be the non-empty subset of actions allowed from state $s$\n",
    "- $\\mathcal V$ be a vector space of set of all bounded real valued functions from $\\mathcal S$ to $\\mathbb R$\n",
    "- Measure the distance between state value functions $u,v \\in \\mathcal V$ using the max-norm defined as follows\n",
    "  $$\\|u-v\\|=\\|u-v\\|_{\\infty}=\\operatorname*{max}_{s\\in S}|u(s)-v(s)|\\quad s\\in S;u,v\\in{\\mathcal{V}}$$\n",
    "  - Largest distance between state values\n",
    "- The space $\\mathcal V$ is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Evaluation Operator\n",
    "\n",
    "\n",
    "$$V^{\\pi}_{k+1}(s)=\\sum_{a}\\pi(a|s)\\sum_{s^{\\prime}}\\mathcal{P}_{s s^{\\prime}}^{a}\\left[\\mathcal{R}_{s s^{\\prime}}^{a}+\\gamma V^{\\pi}_k(s^{\\prime})\\right]$$\n",
    "\n",
    "\n",
    "Denote, \n",
    "\n",
    "$${\\mathcal{P}}^{\\pi}(s^{\\prime}|s)=\\sum_{a\\in\\mathcal{A}}\\pi\\bigl(a|s\\bigr){\\mathcal{P}}_{s s^{\\prime}}^{a}$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\mathcal R^{\\pi}(s)= \\sum_{a\\in\\mathcal{A}}\\pi(a|s)\\sum_{s^{\\prime}}\\mathcal{P}_{s s^{\\prime}}^{a}\\mathcal{R}_{s s^{\\prime}}^{a}=\\mathbb{E}(r_{t+1}|s_{t}=s)$$\n",
    "\n",
    "Then , we can write, \n",
    "\n",
    "$$V^{\\pi}={\\cal R}^{\\pi}+\\gamma{\\cal P}^{\\pi}V^{\\pi}$$\n",
    "OR $$V_{k+1}={\\cal R}^{\\pi}+\\gamma{\\cal P}^{\\pi}V_k$$\n",
    "\n",
    "Define Bellman Evaluation Operator $(\\mathcal L^\\pi: \\mathcal V \\rightarrow \\mathcal V)$ as, \n",
    "$$L^{\\pi}(v)={\\cal R}^{\\pi}+\\gamma{\\cal P}^{\\pi}v$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Optimality Operator\n",
    "\n",
    "$$V_{k+1}(s)=\\max_a\\left[\\sum_{s^{\\prime}\\in{\\cal S}}{\\cal P}_{s s^{\\prime}}^{a}\\left({\\cal R}_{s s^{\\prime}}^{a}+\\gamma V_{k}(s^{\\prime})\\right)\\right]$$\n",
    "\n",
    "Denote, \n",
    "\n",
    "$${\\mathcal{P}}^{a}(s)=\\sum_{s'\\in\\mathcal{S}}{\\mathcal{P}}_{s s^{\\prime}}^{a}$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\mathcal R^{a}(s)= \\sum_{s'\\in\\mathcal{S}}\\mathcal{P}_{s s^{\\prime}}^{a}\\mathcal{R}_{s s^{\\prime}}^{a}$$\n",
    "\n",
    "Then , we can write, \n",
    "\n",
    "$$V_{k+1}=\\max_{a \\in \\mathcal A}\\left[ {\\cal R}^{a}+\\gamma{\\cal P}^{a}V_k \\right]$$\n",
    "\n",
    "Define Bellman Optimality Operator : $(\\mathcal L: \\mathcal V \\rightarrow \\mathcal V)$ as, \n",
    "\n",
    "$$L(v)=\\max_{a\\in \\mathcal A}[{\\cal R}^{a}+\\gamma{\\cal P}^{a}v]$$\n",
    "\n",
    "- Note that since value functions are a mapping from state space to real numbers one can also think of $\\mathcal L^\\pi$ and $\\mathcal L$ as mappings from $\\mathbb R_d \\to \\mathbb R_d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Points of Maps $\\mathcal L^\\pi$ and $\\mathcal L$\n",
    "\n",
    "We can see that $V^\\pi$ is a fixed point of function $\\mathcal L^\\pi$\n",
    "\n",
    "$$\\mathcal L^\\pi V^\\pi=V^\\pi$$\n",
    "\n",
    "and $V_*$ is fixed point of operator $\\mathcal L$\n",
    "\n",
    "$$\\mathcal L V_*=V_*$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Evaluation Operator is a Contraction\n",
    "\n",
    "\n",
    "Recall that Bellman evaluation operator is given by $(\\mathcal L^\\pi: \\mathcal V \\rightarrow \\mathcal V)$\n",
    "$$L^{\\pi}(v)={\\cal R}^{\\pi}+\\gamma{\\cal P}^{\\pi}v$$\n",
    "\n",
    "- This operator is $\\lambda$ contraction. i.e., it makes value fuctions closer by at least $\\lambda$.\n",
    "\n",
    "Proof \n",
    "\n",
    "- For any two value functions $u$ and $v$ in the space $V$, we have,\n",
    "\n",
    "$$\\begin{align*}\n",
    "    \\|L^\\pi(u)-L^\\pi(v)\\|_\\infty &= \\|(\\mathcal R^\\pi + \\lambda \\mathcal P^\\pi u)-(\\mathcal R^\\pi + \\lambda \\mathcal P^\\pi v)\\|_\\infty \\\\\n",
    "    &= \\|\\lambda \\mathcal P^\\pi(u-v)\\|_\\infty\\\\\n",
    "    &\\le \\gamma \\| \\mathcal P ^\\pi \\|_\\infty \\|(u-v)\\|_\\infty = \\gamma \\|u-v\\|_\\infty\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "we used the property that for every $x \\in \\mathbb R^n,$ and $A$ , a $m \\times n$ matrix, $\\|Ax\\|_\\infty \\le \\|A\\|_\\infty \\|x\\|_\\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence of bellman Updates\n",
    "\n",
    "- Banach fixed-point theorem guarantees that iteratively applying evaluation operator $\\mathcal L^\\pi$ to any function $V \\in \\mathcal V$ will converge to a unique function $V^\\pi \\in V$\n",
    "- Similarly, the Bellman optimality operator $(\\mathcal L :  \\mathcal V \\to \\mathcal V)$ \n",
    "  $$L(v)=\\max_{a\\in \\mathcal A}[{\\cal R}^{a}+\\gamma{\\cal P}^{a}v]$$\n",
    "  is also (A similar argument as $\\mathcal L^\\pi$) a $\\gamma$ contraction and hence iteratively applying optimality operator $\\mathcal L$ to any function $V \\in \\mathcal V$ will converge to a unique function $V_* \\in V$\n",
    "-  Also, $V_* = \\max_\\pi V^\\pi(\\cdot)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration \n",
    "\n",
    "We can arrive at $\\pi_*$ starting form an arbitrary policy $\\pi$ \n",
    "\n",
    "- Evaluate the policy $\\pi$ \n",
    "  - Compute $V^{\\pi}(s)={\\mathbb{E}}_{\\pi}(r_{t+1}+{\\gamma}{r_{t+2}}+{\\gamma}^{2}r_{t+3}+\\cdot\\cdot\\cdot\\cdot\\cdot\\vert s_{t}=s)$\n",
    "- Improve policy $\\pi$\n",
    "  - $\\pi'(s)=\\text{greedy}(V^\\pi(s) )$\n",
    "\n",
    "### Policy Evaluation\n",
    "\n",
    "- Problem : Evaluate a given policy $\\pi$\n",
    "- Compute $V^{\\pi}(s)={\\mathbb{E}}_{\\pi}(r_{t+1}+{\\gamma}{r_{t+2}}+{\\gamma}^{2}r_{t+3}+\\cdot\\cdot\\cdot\\cdot\\cdot\\vert s_{t}=s)$\n",
    "\n",
    "- Solution 1: solve a system of linear equation using any solver\n",
    "- Solution 2: Iterative application of Bellman Evaluation Equation\n",
    "  - Iterative update rule :\n",
    "    $$V_{k+1}^{\\pi}(s)\\leftarrow\\sum_{a}\\pi(a|s)\\sum_{s^{\\prime}}\\mathcal{P}_{s s^{\\prime}}^{a}\\left[\\mathcal{R}_{s s^{\\prime}}^{a}+\\gamma\\mathcal{V}_{k}^{\\pi}(s^{\\prime})\\right]$$\n",
    "  - The sequence of value function $\\{V_1^\\pi,V_2^\\pi, \\cdots ,\\}$ converges to $V^\\pi$\n",
    "\n",
    "### Policy Improvement\n",
    "\n",
    "Suppose we know $V^\\pi$ . How to improve policy $\\pi$\n",
    "\n",
    "The answer lies in the definition of action value function $Q^\\pi (s,a)$, Recall that,\n",
    "\n",
    "\n",
    "$$\\begin{align*}\n",
    "    Q^{\\pi}(s,a)&=\\mathbb{E}_{\\pi}\\left(\\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k+1}|s_{t}=s,a_{t}=a\\right)\\\\\n",
    "    &= \\mathbb{E}(r_{t+1}+\\gamma V^{\\pi}(s_{t+1})|s_{t}=s,a_{t}=a)\\\\\n",
    "    &=\\sum_{s^{\\prime}\\in{\\mathcal{S}}}{\\mathcal{P}}_{s s^{\\prime}}^{a}\\left[{\\mathcal{R}}_{s s^{\\prime}}^{a}+\\gamma\\mathcal{V}^{\\pi}\\left(s^{\\prime}\\right)\\right]\n",
    "\\end{align*}$$\n",
    "\n",
    "- If $\\mathcal{Q}^{\\pi}(s,a)>V^{\\pi}(s) \\implies$ Better to select action $a$ in state $s$ and thereafter follow the policy $\\pi$\n",
    "- This is a special case of the policy improvement theorem. \n",
    "\n",
    "__Policy Improvement Theorem__\n",
    "\n",
    "::: {.callout-note title=\"Theorem\"}\n",
    "Let $\\pi$ and $\\pi^*$ be any pair of deterministic policy such that, for all $s \\in \\mathcal  S$, \n",
    "\n",
    "$$Q^{\\pi}(s,\\pi^{\\prime}(s))\\ge V^{\\pi}(s).$$\n",
    "Then $V^{\\pi^{\\prime}}(s)\\geq V^{\\pi}(s) \\quad \\forall s \\in \\mathcal S$ \n",
    ":::\n",
    "\n",
    "Proof :\n",
    "\n",
    "$$\\begin{align*}\n",
    "    V^\\pi(s) &\\le {\\cal{Q}}^{\\pi}(s,\\pi^{\\prime}(s))=\\mathbb{E}_{\\pi^{\\prime}}(r_{t+1}+\\gamma V^{\\pi}(s_{t+1})|s_{t}=s)\\\\\n",
    "    &\\le \\mathbb{E}_{\\pi^{\\prime}}(r_{t+1}+\\gamma Q^{\\pi}(s_{t+1},\\pi^{\\prime}(s_{t+1}))|s_{t}=s)\\\\\n",
    "    &=\\mathbb{E}_{\\pi^{\\prime}}(r_{t+1}+\\gamma r_{t+2}+\\gamma^{2}V^{\\pi}(s_{t+2})|s_{t}=s)\\\\\n",
    "    &\\le \\mathbb{E}_{\\pi^{\\prime}}(r_{t+1}+\\gamma r_{t+2}+\\gamma^{2}Q^{\\pi}(s_{t+2},\\pi^{\\prime}(s_{t+2}))|s_{t}=s)\\\\\n",
    "    &\\le \\mathbb{E}_{\\pi^{\\prime}}(r_{t+1}+\\gamma r_{t+2}+\\gamma^{2}r_{t+3}+\\cdot\\cdot\\cdot\\cdot\\left|s_{t}=s\\right)=V^{\\pi^{'}}(s)\n",
    "\\end{align*}$$\n",
    "\n",
    "- Now consider the greedy policy $\\pi' = \\text{greedy}(V^\\pi)$\n",
    "- Then, $\\pi' \\ge \\pi$, That is $V^{\\pi'}(S) \\ge V^{\\pi}(S) \\quad \\forall s \\in \\mathcal S$\n",
    "  - By definition of $\\pi'$, at state s, the action chosen by policy $\\pi'$ is given by the greedy operator \n",
    "    $$\\pi'(s) = \\argmax_a Q^\\pi (s, a)$$\n",
    "  - This improves the value from any state $s$ over one step\n",
    "    $$Q^{\\pi}(s,\\pi^{\\prime}(s))=\\operatorname*{max}_{a}Q^{\\pi}(s,a)\\geq Q^{\\pi}(s,\\pi(s))=V^{\\pi}(s)$$\n",
    "  - It therefore improves the value function, $V^{\\pi^{\\prime}}(s)\\ge V^{\\pi}\\left(s\\right)$\n",
    "- Policy $\\pi'$ is at least as good as policy $\\pi$\n",
    "- If improvement stops,\n",
    "  $$Q^{\\pi}(s,\\pi^{\\prime}(s))=\\operatorname*{max}_{a}Q^{\\pi}(s,a)=Q^{\\pi}(s,\\pi(s))=V^{\\pi}(s)$$\n",
    "- Bellman Optimality equation is satisfied as, \n",
    "  $$V^\\pi(s)=\\max_a Q^\\pi (s,a)$$\n",
    "- The policy $\\pi$ for which the improvement stops is the optimal policy.\n",
    "  $$V^\\pi(s)=V_*(s)\\quad \\forall s \\in \\mathcal S$$\n",
    "\n",
    "\n",
    "### Policy Iteration : Algorithm\n",
    "\n",
    "\n",
    "::: {.alert .alert-dismissible .alert-success}\n",
    "\n",
    "-----\n",
    "Algorithm Policy Iteration\n",
    "\n",
    "-----\n",
    "\n",
    "1. Start with an initial policy $\\pi_1$\n",
    "2. For $i=1,2,\\cdots,N$ do\n",
    "3. Evaluate $V^{\\pi_i}(s)\\quad \\forall s \\in \\mathcal  S$. That is,\n",
    "4. For $k=1,2,\\cdots K$ do\n",
    "5. For all $s \\in \\mathcal S$ calculate\n",
    "   $$V_{k+1}^{\\pi_{i}}(s)\\leftarrow\\sum_{a}\\pi(a|s)\\sum_{s^{\\prime}}\\mathcal{P}_{s s^{\\prime}}^{a}\\ \\left[{\\mathcal{R}}_{s s^{\\prime}}^{a}\\ +\\gamma V_{k}^{\\pi_{i}}(s^{\\prime})\\right]$$\n",
    "6. end for\n",
    "7. Perform policy Improvement\n",
    "   $$\\pi_{i+1}=\\text{greedy}(V^{\\pi_i})$$\n",
    "8. End for\n",
    "\n",
    "-----\n",
    "\n",
    ":::\n",
    "\n",
    "![](CS6140_images/2023-09-03_05-50.png)\n",
    "![](CS6140_images/2023-09-03_05-24.png)\n",
    "\n",
    "\n",
    "### Policy Iteration : Schematic Representation\n",
    "\n",
    "<br>\n",
    "![](CS6140_images/2023-09-03_05-53.png)\n",
    "\n",
    "- The sequence $\\{\\pi_1,\\pi_2,\\cdots\\}$ is guaranteed to converge.\n",
    "- At convergence, both current policy and the value function associated with the policy are optimal.\n",
    "\n",
    "\n",
    "### Modified Policy Iteration \n",
    "\n",
    "Can we computationally simplify policy iteration process\n",
    "\n",
    "- We need not wait for policy evaluation to converge to $V^\\pi$\n",
    "- We can have a stopping criterion like $\\epsilon$- convergence of value function evaluation or $K$ iteration of policy evaluation\n",
    "- Extreme case of $K=1$ is __value iteration__, we update the policy every iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Extension\n",
    "\n",
    "\n",
    "### Asynchronous Dynamic Programming\n",
    "\n",
    "- Update to states are done individually, in any order\n",
    "- For each selected state, apply the appropriate backup\n",
    "- Can significantly reduce computation \n",
    "- Convergence guaranteed exist, if all states are selected sufficient number of times.\n",
    "\n",
    "### Real Time Dynamic Programming\n",
    "\n",
    "- Idea : Update only states that are relevant to agent\n",
    "- After each time step, we get $s_t,a_t,r_{t+1}$\n",
    "- perform the following update\n",
    "  $$V{\\big(}s_{t}{\\big)}\\leftarrow{\\mathrm{max}}\\left[\\sum_{s^{\\prime}\\in{\\cal S}}{\\mathcal{P}}_{s_{t}s^{\\prime}}^{a}\\left({\\mathcal{R}}_{s_{t}s^{\\prime}}^{a}+\\gamma V(s^{\\prime})\\right)\\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks\n",
    "\n",
    "- MDP Setting : The agent has knowledge of the state transition matrices $\\mathcal P_{ss'}$ and the reward function $\\mathcal R$\n",
    "- RL Setting : The agent does not have knowledge of the state transition matrices $\\mathcal P^a_{ss'}$ and the reward function $\\mathcal R$\n",
    "  - The goal in both cases are same; Determine optimal sequence of actions such that the total discounted future reward is maximum.\n",
    "  - Although, this course would assume Markovian structure to state transitions, in many (sequential) decision making problems we may have to consider the history as well.\n",
    "\n",
    "Prediction and Control using Dynamic Programming : \n",
    "\n",
    "- Dynamic Programming assumes full knowledge of MDP\n",
    "- Used for both prediction and control in an MDP\n",
    "- Prediction\n",
    "  - Input MDP $\\langle \\mathcal S, \\mathcal A, \\mathcal P, \\mathcal R, \\gamma \\rangle$ and policy $\\pi$\n",
    "  - output $v^\\pi(\\cdot)$\n",
    "- Control\n",
    "  - Input MDP $\\langle \\mathcal S, \\mathcal A, \\mathcal P, \\mathcal R, \\gamma \\rangle$ \n",
    "  - Output Optimal value function $V_*(\\cdot)$ or optimal policy $\\pi_*$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "$\\tiny  {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0e5e69b8442e8f020791fad6bfcef3777f0e89e0f1a2517b6b628b2eaf0fe66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
