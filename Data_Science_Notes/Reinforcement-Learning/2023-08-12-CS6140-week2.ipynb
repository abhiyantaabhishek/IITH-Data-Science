{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "author: Abhishek Kumar Dubey\n",
    "badges: false\n",
    "categories:\n",
    "- Reinforcement Learning\n",
    "date: '2023-08-12'\n",
    "description: Markov Reward Process,Morkov Decision Process, Transition Probability, Markov Chain,Value Function, Bellman Equation\n",
    "image: CS6140_images/Acrobat_TLEDVUQcXo.png\n",
    "title: Reinforcement Learning 2\n",
    "toc: true\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "- State $s_t$ : abstraction of observation, \n",
    "- reward $r_t$ :\n",
    "- action $a_t$ : \n",
    "\n",
    "for an action at $t$ the reward is $r_{t+1}$, at the state $s_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morkov Decision Process\n",
    "\n",
    "- Markov Decision Process (MDP) provides a mathematical framework for modeling\n",
    "decision making process\n",
    "- Can formally describe the working of the environment and agent in the RL setting\n",
    "- Can handle huge variety of interesting settings\n",
    " - Multi-arm Bandits - Single state MDPs\n",
    " - Optimal Control - Continuous MDPs\n",
    "- Core problem in solving an MDP is to find an 'optimal' policy (or behaviour) for the\n",
    "decision maker (agent) in order to maximize the total future reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morkov chain\n",
    "\n",
    "### Random Variable and Stochastic process\n",
    "\n",
    "- Random Variable (Non-mathematical definition) : A random variable is a variable whose value depend on the outcome of a random\n",
    "phenomenon\n",
    " - Outcome of a coin toss\n",
    " - Outcome of roll of a dice\n",
    "\n",
    "### Stochastic Process\n",
    "\n",
    "A stochastic or random process, denoted by $\\{s_t\\}_t \\in T$ , can be defined as a collection of\n",
    "random variables that is indexed by some mathematical set $T$\n",
    "\n",
    "- Index set has the interpretation of time\n",
    "- The set $T$ is, typically, $\\mathbb N$ or $\\mathbb R$\n",
    "- Typically, in optimal control problems, the index set is continuous (say $\\mathbb R$)\n",
    "- Throughout this course (RL), the index set is always discrete (say $\\mathbb N$)\n",
    "- Let $\\{s_t\\}_t \\in T$ be a stochastic process\n",
    "- Let $s_t$ be the state at time $t$ of the stochastic process $\\{s_t\\}_t \\in T$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Property\n",
    "\n",
    "::: {.alert .alert-dismissible .alert-info}\n",
    "A state $s_t$ of a stochastic process $\\{s_t\\}_t \\in T$  is said to have Markov property if :\n",
    "\n",
    "$$P(s_{t+1}|s_{t})=P(s_{t+1}|s_{1},\\cdot\\cdot\\cdot,s_{t})$$\n",
    "\n",
    "The state $s_t$ at time $t$ captures all relevant information from history and is a sufficient\n",
    "statistic of the future.\n",
    ":::\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition Probability\n",
    "\n",
    "State Transition Probability:\n",
    "\n",
    "For a Markov state $s$ and a successor state $s'$, the state transition probability is defined by\n",
    "\n",
    "$$\\mathcal P_{s s^{\\prime}}=P(s_{t+1}=s^{\\prime}|s_{t}=s)$$\n",
    "\n",
    "State transition matrix $\\mathcal P$ then denotes the transition probabilities from all states $s$ to all\n",
    "successor states $s'$ (with each row summing to 1)\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "   \\mathcal P_{11} & \\mathcal P_{12} & \\cdots & \\mathcal P_{1n} \\\\\n",
    "   \\vdots \\\\\n",
    "   \\mathcal P_{n1} & \\mathcal P_{n2} & \\cdots & \\mathcal P_{nn} \\\\\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chain\n",
    "\n",
    "A stochastic process $\\{s_t\\}_t \\in T$  is a Markov process or Markov Chain if the sequence of\n",
    "random states satisfy the Markov property. It is represented by tuple $\\langle \\mathcal S,\\mathcal P \\rangle$ where $\\mathcal S$\n",
    "denote the set of states and $\\mathcal P$ denote the state transition probablity\n",
    "\n",
    "### Markov Chain : Example 1\n",
    "\n",
    "![](CS6140_images/Acrobat_TLEDVUQcXo.png)\n",
    "\n",
    "State $\\mathcal S$ = Sunny; Rainy\n",
    "\n",
    "Transition Probability \n",
    "$$\\mathcal P= \\begin{bmatrix}\n",
    "    .8 & .2\\\\\n",
    "    .7 & .3\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "- Probability that tomorrow will be 'Rainy' given today is 'Sunny' = 0.2\n",
    "\n",
    "- Probability that day-after-tomorrow will be 'Rainy' given today is 'Sunny' is given\n",
    "by  (tomorrow is sunny and day after tomorrow is rainy ) or ( tomorrow is rainy and day after tomorrow is rainy)\n",
    " 0.8 * 0.2 + 0.2 * 0.3  = 0.22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, if one step transition matrix is given by,\n",
    "\n",
    "$$\\mathcal P = \\begin{bmatrix}\n",
    "    \\mathcal P_{ss} &  \\mathcal P_{sr}\\\\\n",
    "    \\mathcal P_{rs} &   \\mathcal P_{rr}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "then the two step transition matrix is given by\n",
    "\n",
    "$$\\mathcal P_{(2)} = \\mathcal P^{2}$$\n",
    "\n",
    "::: {.callout-important}\n",
    "In general, n-step transition matrix is given by\n",
    "$$\\mathcal P_{(n)} = \\mathcal P^{n}$$\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption\n",
    "\n",
    "- We made an important assumption in arriving at the above expression. That the one-step\n",
    "transition matrix stays constant through time or independent of time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov chains generated using such transition matrices are called homogeneous\n",
    "Markov chains (transition probabilities depend on the length of time interval $[t_1, t_2]$ but not on the\n",
    "exact time instants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Chains : Examples 2\n",
    "\n",
    "One dimensional random walk\n",
    "\n",
    "- A walker flips a coin every time slot to decide which 'way' to go.\n",
    "- $s_{t+1} = \\begin{cases}\n",
    "    s_t +1 &\\text{with probability } p \\\\\n",
    "    s_t -1 &\\text{with probability } 1-p\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notion of Absorbing State\n",
    "\n",
    "A state $s \\in S$ is called absorbing state if it is impossible to leave the state. That is,\n",
    "\n",
    "$$\\mathcal P_{ss'} = \\begin{cases}\n",
    "    1 &\\text{if } s=s' \\\\\n",
    "    0 &\\text{otherwise } \n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Reward Process\n",
    "\n",
    "::: {.alert .alert-dismissible .alert-info}\n",
    "A Markov reward process is a tuple $\\langle \\mathcal S, \\mathcal P, \\textcolor{red}{\\mathcal R},\\textcolor{red} {\\gamma} \\rangle$  a Markov chain with values\n",
    "\n",
    "- $\\mathcal S$ : (Finite) set of states\n",
    "- $\\mathcal P$  : State transition probablity\n",
    "- $\\textcolor{red}{\\mathcal R}$ : Reward for being in state $s_t$ is given by a deterministic function $\\textcolor{red}{\\mathcal R}$\n",
    "$$\\textcolor{red}{r_{t+1}}= \\textcolor{red}{\\mathcal R(s_t)} $$\n",
    "- $\\textcolor{red}{\\gamma}$: Discount factor such that $\\textcolor{red}{\\gamma} \\in [0,1]$\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Return\n",
    "\n",
    "- At each time step $t$, there is a reward $r_{t+1}$ associated with being in state $s_t$ \n",
    "- Ideally, we would like the agent to pick such trajectories in which the cumulative\n",
    "reward accumulated by traversing such a path is high\n",
    "\n",
    "If the reward sequence is given by $\\left\\{r_{t+1},\\,r_{t+2},\\,r_{t+3},\\,\\cdot\\,\\cdot\\,\\right\\},$  then, we want to\n",
    "maximize the sum\n",
    "$$r_{t+1}+r_{t+2}+r_{t+3}+\\cdot\\cdot\\cdot$$\n",
    "\n",
    "Define $G_t$ to be \n",
    "\n",
    "$$G_{t}=r_{t+1}+r_{t+2}+r_{t+3}+\\cdot\\cdot\\cdot\\cdot=\\sum_{k=0}^{\\infty}r_{t+k+1}$$\n",
    "\n",
    "The goal of the agent is to pick such paths that maximize $G_t$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total (Discounted) Return\n",
    "\n",
    "In the case that the underlying stochastic process has infinite terms the above\n",
    "summation (total return) could be divergent\n",
    "\n",
    "Therefore, we introduce discount factor $\\gamma \\in [0,1]$ and redefine $G_t$ as\n",
    "\n",
    "$$G_{t}=r_{t+1}+\\gamma r_{t+2}+\\gamma^{2}r_{t+3}+\\cdot\\cdot\\cdot\\cdot=\\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k+1}$$\n",
    "\n",
    "- $G_t$ is the total discounted return starting from time $t$\n",
    "- If  $\\gamma < 1$ then the infinite sum has a finite value if the reward sequence is bounded.\n",
    "-  $\\gamma$ close to $0$ the agent considers only with immediate reward(s) (myopic)\n",
    "- $\\gamma$ close to $1$ the agent considers future reward more strongly (far-sighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Remarks on Discounting\n",
    "\n",
    "- Mathematically convenient to discount rewards\n",
    "- Avoids infinite returns in cyclic and infinite horizon setting\n",
    "- Discount rate determines the present value of future reward\n",
    "- Offers trade-off between being 'myopic' and 'far-sighted' reward\n",
    "- In finite MDPs, it is sometimes possible to use discounted reward (i.e. $\\gamma= 1$), for example, if all sequences terminate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function\n",
    "\n",
    "::: {.alert .alert-dismissible .alert-success}\n",
    "The value function $V (s)$ gives the long-term value of state $s \\in S$\n",
    "$$V(s)=\\mathbb{E}\\left(G_{t}|s_{t}=s\\right)=\\mathbb{E}\\left(\\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k+1}|s_{t}=s\\right)$$\n",
    "\n",
    "- Value function $V (s)$ determines the value of being in state $s$\n",
    "- $V (s)$ measures the potential future rewards we may get from being in state $s$\n",
    "- ::: {.alert .alert-dismissible .alert-danger}\n",
    "  $V (s)$ is independent of $t$\n",
    "  ::: \n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function Computation : Example\n",
    "\n",
    "Consider the following MRP. Assume $\\gamma = 1$\n",
    "\n",
    "![](CS6140_images/Acrobat_AEklDhPQq7.png)\n",
    "\n",
    "- $V(S_4)=6$\n",
    "- $V(S_3) = 3+ V(S_4)=9$\n",
    "- $V(S_2) = 1+ V(S_4)=7$\n",
    "- $V(S_1) = -1+0.4 \\times V(S_3)+0.6 \\times V(S_2)=6.8$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposition of Value Function\n",
    "\n",
    "Let $s$ be the state at time step $t$ and $s'$ be the state at time steps $t + 1$, the value function can be\n",
    "decomposed into sum of two parts\n",
    "\n",
    "- Immediate reward $r_{t+1}$ \n",
    "- Discounted value of next state $S'$ i.e.  $\\gamma V(S')$\n",
    "\n",
    "  ::: {.callout-important}\n",
    "  $$\\begin{align*}\n",
    "      V(s)=\\mathbb{E}\\,(G_{t}|s_{t}=s) &= \\mathbb{E}\\left(\\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k+1}|s_{t}=s\\right)\\\\\n",
    "      & =\\mathbb{E}\\left(r_{t+1}+\\gamma V(s_{t+1})|s_{t}=s\\right)\n",
    "  \\end{align*}$$\n",
    "  :::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof\n",
    "\n",
    "We know that,\n",
    "\n",
    "$$G_{t}=\\left(r_{t+1}+\\gamma r_{t+2}+\\gamma^{2}r_{t+3}+\\cdot\\cdot\\cdot\\right)=\\sum_{k=0}^{\\infty}\\left(\\gamma^{k}r_{t+k+1}\\right)$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "    V(S) &= \\mathbb{E}\\left(G_{t}|s_{t}=s\\right)=\\mathbb{E}\\left(\\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k+1}|s_{t}=s\\right)\\\\\n",
    "    & =\\mathbb{E}\\left(r_{t+1}+\\gamma r_{t+2}+\\gamma^{2}r_{t+3}+\\cdots|s_{t}=s\\right)\\\\\n",
    "    & =\\underbrace{\\mathbb{E}(r_{t+1}|s_{t}=s)+\\sum_{k=1}^{\\infty}\\gamma^{k}\\operatorname{E}(r_{t+k+1}|s_{t}=s)}_{\\text{By linearity of conditional expectation }}  \\\\\n",
    "    & = \\mathbb{E}(r_{t+1}|s_{t}=s)+\\overbrace{\\gamma\\sum_{s^{\\prime}\\in{\\mathcal{S}}}\\underbrace{P(s^{\\prime}|s)}_{\\text{transition  probability}}}^{\\text{take all transition in account }}  \\underbrace{\\sum_{k=0}^{\\infty}\\gamma^{k}\\mathbb{E}\\left(r_{t+k+1}|s_{t}=s,s_{t+1}=s^{\\prime}\\right)}_{\\text{Count rewards again on reaching to s' form s}} \\\\\n",
    "    & = \\mathbb{E}(r_{t+1}|s_{t}=s)+\\gamma\\sum_{s^{\\prime}\\in{\\mathcal{S}}}P(s^{\\prime}|s)\\underbrace{\\sum_{k=0}^{\\infty}\\gamma^{k}\\mathbb{E}(r_{t+k+1}|s_{t+1}=s^{\\prime})}_{\\text{Using Markov property}}  \\\\\n",
    "    & = \\underbrace{\\mathbb{E}(r_{t+1}+\\gamma V(s_{t+1})|s_{t}=s)}_{\\text{by the definition of value function }} \n",
    "\\end{align*}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function : Evaluation\n",
    "\n",
    "We have\n",
    "$$V(s)=\\mathbb{E}(r_{t+1}+\\gamma V(s_{t+1})|s_{t}=s)$$\n",
    "\n",
    "![](CS6140_images/Acrobat_RichghiIgC.png)\n",
    "\n",
    "$$V(s)={\\mathcal{R}}(s)+\\gamma\\left[{\\mathcal{P}}_{s s_{a}^{\\prime}}V(s_{a}^{'})+{\\mathcal{P}}_{s s_{b}^{\\prime}}V(s_{b}^{'})+{\\mathcal{P}}_{s s_{c}^{\\prime}}V(s_{c}^{'})+{\\mathcal{P}}_{s s_{d}^{\\prime}}V(s_{d}^{'})\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Equation for Markov Reward Process\n",
    "\n",
    "$$V(s)=\\mathbb{E}(r_{t+1}+\\gamma V(s_{t+1})|s_{t}=s)$$\n",
    "\n",
    "For any $s' \\in S$ a successor state of $s$ with transition probability $\\mathcal P_{ss'}$ , we can rewrite the\n",
    "above equation as (using definition of Expectation)\n",
    "\n",
    "::: {.callout-important}\n",
    "$$V(s)=\\mathbb{E}(r_{t+1}|s_{t}=s)+\\gamma\\sum_{s^{\\prime}\\in{\\mathcal{S}}}{\\mathcal{P}}_{s s^{\\prime}}V(s^{\\prime})$$\n",
    "\n",
    "This is the Bellman Equation for value functions\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Equation in Matrix Form\n",
    "\n",
    "Let $S=\\{1,2,\\cdot\\cdot\\cdot,n\\}$ and $\\mathcal P$ be known. Then one can write the Bellman equation can as,\n",
    "\n",
    "$${ V}=\\mathcal{R}+\\gamma\\mathcal{P}{ V}$$\n",
    "\n",
    "$$\\left\\lbrack \\begin{array}{c}\n",
    "V\\left(1\\right)\\\\\n",
    "V\\left(2\\right)\\\\\n",
    "\\vdots \\\\\n",
    "V\\left(n\\right)\n",
    "\\end{array}\\right\\rbrack =\\left\\lbrack \\begin{array}{c}\n",
    "\\mathcal{R}\\left(1\\right)\\\\\n",
    "\\mathcal{R}\\left(2\\right)\\\\\n",
    "\\vdots \\\\\n",
    "\\mathcal{R}\\left(n\\right)\n",
    "\\end{array}\\right\\rbrack +\\gamma \\left\\lbrack \\begin{array}{cccc}\n",
    "{\\mathcal{P}}_{11}  & {\\mathcal{P}}_{12}  & \\cdots  & {\\mathcal{P}}_{1n} \\\\\n",
    "{\\mathcal{P}}_{21}  & {\\mathcal{P}}_{22}  & \\cdots  & {\\mathcal{P}}_{2n} \\\\\n",
    "\\vdots  &  &  & \\\\\n",
    "{\\mathcal{P}}_{n1}  & {\\mathcal{P}}_{\\mathrm{n2}}  & \\cdots  & {\\mathcal{P}}_{\\mathrm{nn}} \n",
    "\\end{array}\\right\\rbrack \\times \\left\\lbrack \\begin{array}{c}\n",
    "V\\left(1\\right)\\\\\n",
    "V\\left(2\\right)\\\\\n",
    "\\vdots \\\\\n",
    "V\\left(n\\right)\n",
    "\\end{array}\\right\\rbrack$$\n",
    "\n",
    "Solving for $V$ we get\n",
    "\n",
    "$$\\boxed{V=(I-\\gamma P)^{-1}R}$$\n",
    "The discount factor should be $\\gamma < 1$ for the inverse to exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "Value function computed for a particular state provides the expected number of\n",
    "plays to reach the goal state $s_{100}$ from that state, in snake ladder game.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process\n",
    "\n",
    "::: {.alert .alert-dismissible .alert-info}\n",
    "A Markov reward process is a tuple $\\langle \\mathcal S,\\textcolor{red}{ \\mathcal A}, \\mathcal P, \\mathcal R, \\gamma \\rangle$  a Markov chain with values\n",
    "\n",
    "- $\\mathcal S$ : (Finite) set of states\n",
    "- <div class=\"text-danger\"> $\\mathcal A$ : (Finite) set of actions</div>\n",
    "- $\\mathcal P$  : State transition probability\n",
    "  $${\\mathcal{P}}_{s s^{\\prime}}^{\\textcolor{red}{a}}=\\mathbb{P}(s_{t+1}=s^{\\prime}|s_{t}=s,\\textcolor{red}{a_{t}=a}),\\textcolor{red}{a_{t}\\in{\\mathcal{A}}}$$\n",
    "- $\\mathcal R$ : Reward for taking action at $\\color{red}a_t$ state $s_t$ and transitioning to state $s_{t+1}$ is given by\n",
    "the deterministic function $\\mathcal R$\n",
    "$$r_{t+1}= \\mathcal R(s_t,\\textcolor{red}{a_t},s_{t+1}) $$\n",
    "- $\\gamma$: Discount factor such that $\\gamma \\in [0,1]$\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow Diagram\n",
    "\n",
    "![](CS6140_images/Acrobat_DP5w6tUgbb.png)\n",
    "\n",
    "The goal is to choose a sequence of actions such that the expected total discounted\n",
    "future reward $\\mathbb{E}(G_{t}|s_{t}=s)$ is maximized where\n",
    "\n",
    "$$G_{t}=\\sum_{k=0}^{\\infty}\\left(\\gamma^{k}r_{t+k+1}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite and Infinite Horizon MDPs\n",
    "\n",
    "![](CS6140_images/Acrobat_DP5w6tUgbb.png)\n",
    "\n",
    "- If T is fixed and finite, the resultant MDP is a finite horizon MDP\n",
    "  - Wealth management problem\n",
    "- If T is infinite, the resultant MDP is infinite horizon MDP\n",
    "  - Certain Atari games\n",
    "- When state $S$ is finite, the MDP is called finite state MDPs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "$\\tiny  {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0e5e69b8442e8f020791fad6bfcef3777f0e89e0f1a2517b6b628b2eaf0fe66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
