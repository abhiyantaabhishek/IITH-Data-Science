{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "author: Abhishek Kumar Dubey\n",
    "badges: false\n",
    "categories:\n",
    "- Reinforcement Learning\n",
    "date: '2023-08-05'\n",
    "description: Introduction\n",
    "image: CS6140_images/Acrobat_KZXurX42ch.png\n",
    "title: Reinforcement Learning 1\n",
    "toc: true\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IN RL we need to make sequence of decision, it is not isolated \n",
    "decision as in case of classification and regression.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IN RL\n",
    "\n",
    "- there is a process for decision making\n",
    "- reward system\n",
    "- learn series of actions  (optimal one)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](CS6140_images/Acrobat_KZXurX42ch.png)\n",
    "\n",
    "- Observations are non i.i.d and are sequential in nature\n",
    "- Agent's action (may) affect the subsequent observations seen\n",
    "- There is no supervisor; Only reward signal (feedback)\n",
    "- Reward or feedback can be delayed\n",
    "- environment  gives reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "tic toe game\n",
    "solving rubic cube\n",
    "inventory control"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "challenges\n",
    "\n",
    "- delayed feedback\n",
    "- credit assignment problem: we do not know which move \n",
    "lead to win of a fame\n",
    "- stochastic environment: due to some noise we may not be able to take all the move\n",
    "- Definition of reward function: How do we decide a reward function\n",
    "- Data collection problem : Agent is the one who needs to \n",
    "collect the data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning by Trail and Error\n",
    "\n",
    "- Random movement by agent for exploration\n",
    "- Em"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thondrike's Cat: \n",
    "\n",
    "- cat in a grid, fish outside, cat is hugary \n",
    "- cat need to come out and eat the fish\n",
    "- cat doesn't know how to come out\n",
    "- but there is a lever, pulling which it can come out\n",
    "- cat will do a lot of things, and randomly it will press the lever\n",
    "- experiment was performed many times on the same cat\n",
    "- cat can come out now from it's exprience \n",
    "\n",
    "Law of Effect (1898):\n",
    "Any behavior that is followed by pleasant consequences is likely to be repeated, and any\n",
    "behavior followed by unpleasant consequences is likely to be stopped"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pavlov's DOG\n",
    "\n",
    "- there is dog\n",
    "- food for the dog\n",
    "- bring food in front of the dog\n",
    "- dog salvation started\n",
    "- now bring tuning fork, no salivation\n",
    "- now bring tuning fork and then food, dog salivate once it sees food\n",
    "- repeat the process\n",
    "- now only after looking at tuning fork dog salivate \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connections to Temporal Difference\n",
    "\n",
    "- Ivav Pavlov laid the ground for classical conditioning (1901)\n",
    "- First theory that incorporated time into the learning procedure\n",
    "- Rescorla-Wagner (RW) (1972) model is a formal model to explain Pavlovian conditioning\n",
    "- Temporal-Difference (TD) learning, that extends RW model, is an approach to\n",
    "learning how to predict a quantity that depends on future values of a given signal\n",
    "(Sutton, 1984)\n",
    "- TD learning forms the basis of almost all RL algorithms that we see today\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connections to Optimal Control\n",
    "\n",
    "- Outcomes are partly random and partly under the control of the decision maker\n",
    "- Markov Decision Process (MDP) (Bellman, 1957) is used as a framework to model\n",
    "and solve sequential decision problem\n",
    "- People working in control theory have contributed to optimal sequential decision\n",
    "making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Modern Reinforcement Learning\n",
    "\n",
    "- The temporal difference (TD) thread and the optimal control thread were bought\n",
    "together by Watkins (1989) when he proposed the famous Q-learning algorithm\n",
    "- Gerald Tesauro (1992) employed TD learning to play backgammon; The developed\n",
    "software agent was able to beat experts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "$\\tiny  {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0e5e69b8442e8f020791fad6bfcef3777f0e89e0f1a2517b6b628b2eaf0fe66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
