<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Abhishek Kumar Dubey">
<meta name="dcterms.date" content="2023-08-26">
<meta name="description" content="Value Functions with Policy, Decomposition of State Value Function, MDP with Policy, Optimal Policy, Solution to an MDP, Greedy Policy">

<title>Reinforcement Learning 4</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../Data_Science_Notes/Reinforcement-Learning/2023-09-02-CS6140-week5.html" rel="next">
<link href="../../Data_Science_Notes/Reinforcement-Learning/2023-08-19-CS6140-week3.html" rel="prev">
<link href="../../logo.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-5ZQX02R26E"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-5ZQX02R26E', { 'anonymize_ip': true});
</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Reinforcement Learning 4">
<meta property="og:description" content="Value Functions with Policy, Decomposition of State Value Function, MDP with Policy, Optimal Policy, Solution to an MDP, Greedy Policy">
<meta property="og:image" content="http://localhost:4200/Data_Science_Notes/Reinforcement-Learning/CS6140_images/Acrobat_9thkiyFyb5.png">
<meta property="og:image:height" content="203">
<meta property="og:image:width" content="271">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../logo.png" alt="" class="navbar-logo">
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html" rel="" target="">
 <span class="menu-text"><i class="fa-solid fa-house" aria-label="house"></i> Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="../../Data_Science_Notes/index.html" rel="" target="" aria-current="page">
 <span class="menu-text"><i class="fa-solid fa-book-open-reader" aria-label="book-open-reader"></i> Data Science Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../Paper_Study/index.html" rel="" target="">
 <span class="menu-text"><i class="fa-solid fa-user-ninja" aria-label="user-ninja"></i> Paper Study</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../Data_Science_Projects/index.html" rel="" target="">
 <span class="menu-text"><i class="fa-solid fa-file-powerpoint" aria-label="file-powerpoint"></i> Data Science Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text"><i class="fa-solid fa-address-card" aria-label="address-card"></i> About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/abhiyantaabhishek" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/abhishek-kumar-dubey-585a86179/" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../Data_Science_Notes/index.html">Data Science Notes</a></li><li class="breadcrumb-item"><a href="../../Data_Science_Notes/Reinforcement-Learning/2023-08-05-CS6140-week1.html">Reinforcement Learning</a></li><li class="breadcrumb-item"><a href="../../Data_Science_Notes/Reinforcement-Learning/2023-08-26-CS6140-week4.html">Reinforcement Learning 4</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Reinforcement Learning 4</h1>
                  <div>
        <div class="description">
          Value Functions with Policy, Decomposition of State Value Function, MDP with Policy, Optimal Policy, Solution to an MDP, Greedy Policy
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Reinforcement Learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Abhishek Kumar Dubey </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">August 26, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../Data_Science_Notes/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Science Notes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Deep-Learning/2023-01-07-CS5480-week1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deep Learning 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deep Learning 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Deep-Learning/2023-02-04-CS5480-week3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deep Learning 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deep Learning 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Deep-Learning/2023-03-04-CS5480-week5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deep Learning 5</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deep Learning 6</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deep Learning 7</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Deep-Learning/2023-04-01-CS5480-week8.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deep Learning 8</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Deep-Learning/2023-04-15-CS5480-week9.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deep Learning 9</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">
 <span class="menu-text">Machine Learning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Machine-Learning/2022-08-06-CS5590-week1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Machine Learning 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Machine-Learning/2022-08-20-CS5590-week2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Machine Learning 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Machine-Learning/2022-08-27-CS5590-week3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Machine Learning 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Machine-Learning/2022-09-10-CS5590-week4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Machine Learning 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Machine-Learning/2022-09-24-CS5590-week5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Machine Learning 5</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Machine-Learning/2022-10-08-CS5590-week6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Machine Learning 6</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Machine-Learning/2022-10-15-CS5590-week7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Machine Learning 7</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Machine-Learning/2022-10-29-CS5590-week8.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Machine Learning 8</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Machine-Learning/2022-11-05-CS5590-week9.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Machine Learning 9</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false">
 <span class="menu-text">Mathematics</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Mathematics/2022-08-06-CS6660-week1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probability Theory 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Mathematics/2022-08-13-CS6660-week2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probability Theory 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Mathematics/2022-09-03-CS6660-week3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Linear Algebra 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Mathematics/2022-09-17-CS6660-week4_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probability Theory 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Mathematics/2022-09-17-CS6660-week4_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Linear Algebra 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Mathematics/2022-09-24-CS6660-week5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probability Theory 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Mathematics/2022-10-01-CS6660-week6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Linear Algebra 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Mathematics/2022-10-15-CS6660-week7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probability Theory 5</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Reinforcement Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth2 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Reinforcement-Learning/2023-08-05-CS6140-week1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Reinforcement Learning 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Reinforcement-Learning/2023-08-12-CS6140-week2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Reinforcement Learning 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Reinforcement-Learning/2023-08-19-CS6140-week3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Reinforcement Learning 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Reinforcement-Learning/2023-08-26-CS6140-week4.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Reinforcement Learning 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Reinforcement-Learning/2023-09-02-CS6140-week5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Reinforcement Learning 5</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Notes/Reinforcement-Learning/2023-09-09-CS6140-week6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Reinforcement Learning 6</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#exact-method-policy-iteration" id="toc-exact-method-policy-iteration" class="nav-link active" data-scroll-target="#exact-method-policy-iteration"><span class="header-section-number">1</span> Exact method: Policy Iteration</a></li>
  <li><a href="#exact-method-value-iteration" id="toc-exact-method-value-iteration" class="nav-link" data-scroll-target="#exact-method-value-iteration"><span class="header-section-number">2</span> Exact method: Value Iteration</a></li>
  <li><a href="#bellman-evaluation-equation" id="toc-bellman-evaluation-equation" class="nav-link" data-scroll-target="#bellman-evaluation-equation"><span class="header-section-number">3</span> Bellman Evaluation Equation</a></li>
  <li><a href="#optimality-equation-for-state-value-function" id="toc-optimality-equation-for-state-value-function" class="nav-link" data-scroll-target="#optimality-equation-for-state-value-function"><span class="header-section-number">4</span> Optimality Equation for State Value Function</a></li>
  <li><a href="#solving-the-bellman-optimality-equation" id="toc-solving-the-bellman-optimality-equation" class="nav-link" data-scroll-target="#solving-the-bellman-optimality-equation"><span class="header-section-number">5</span> Solving the Bellman Optimality Equation</a></li>
  <li><a href="#bellmans-optimality-principle" id="toc-bellmans-optimality-principle" class="nav-link" data-scroll-target="#bellmans-optimality-principle"><span class="header-section-number">6</span> Bellman’s Optimality Principle</a></li>
  <li><a href="#solution-methodology-dynamic-programming" id="toc-solution-methodology-dynamic-programming" class="nav-link" data-scroll-target="#solution-methodology-dynamic-programming"><span class="header-section-number">7</span> Solution Methodology : Dynamic Programming</a></li>
  <li><a href="#value-iteration-idea" id="toc-value-iteration-idea" class="nav-link" data-scroll-target="#value-iteration-idea"><span class="header-section-number">8</span> Value Iteration : Idea</a></li>
  <li><a href="#value-iteration-algorithm" id="toc-value-iteration-algorithm" class="nav-link" data-scroll-target="#value-iteration-algorithm"><span class="header-section-number">9</span> Value Iteration : Algorithm</a></li>
  <li><a href="#value-iteration-remarks" id="toc-value-iteration-remarks" class="nav-link" data-scroll-target="#value-iteration-remarks"><span class="header-section-number">10</span> Value Iteration : Remarks</a></li>
  <li><a href="#optimality-equation-for-action-value-function" id="toc-optimality-equation-for-action-value-function" class="nav-link" data-scroll-target="#optimality-equation-for-action-value-function"><span class="header-section-number">11</span> Optimality Equation for Action-Value Function</a></li>
  <li><a href="#proof-of-value-iteration-convergence" id="toc-proof-of-value-iteration-convergence" class="nav-link" data-scroll-target="#proof-of-value-iteration-convergence"><span class="header-section-number">12</span> Proof of Value Iteration Convergence</a>
  <ul>
  <li><a href="#notion-of-convergence" id="toc-notion-of-convergence" class="nav-link" data-scroll-target="#notion-of-convergence"><span class="header-section-number">12.1</span> Notion of Convergence</a></li>
  <li><a href="#cauchy-sequence" id="toc-cauchy-sequence" class="nav-link" data-scroll-target="#cauchy-sequence"><span class="header-section-number">12.2</span> Cauchy Sequence</a></li>
  <li><a href="#notion-of-completeness" id="toc-notion-of-completeness" class="nav-link" data-scroll-target="#notion-of-completeness"><span class="header-section-number">12.3</span> Notion of Completeness</a></li>
  <li><a href="#contractions" id="toc-contractions" class="nav-link" data-scroll-target="#contractions"><span class="header-section-number">12.4</span> Contractions</a></li>
  <li><a href="#notion-of-fixed-point" id="toc-notion-of-fixed-point" class="nav-link" data-scroll-target="#notion-of-fixed-point"><span class="header-section-number">12.5</span> Notion of Fixed point</a></li>
  <li><a href="#banach-fixed-point-theorem" id="toc-banach-fixed-point-theorem" class="nav-link" data-scroll-target="#banach-fixed-point-theorem"><span class="header-section-number">12.6</span> Banach Fixed point theorem</a></li>
  <li><a href="#value-function-space" id="toc-value-function-space" class="nav-link" data-scroll-target="#value-function-space"><span class="header-section-number">12.7</span> Value Function Space</a></li>
  <li><a href="#bellman-evaluation-operator" id="toc-bellman-evaluation-operator" class="nav-link" data-scroll-target="#bellman-evaluation-operator"><span class="header-section-number">12.8</span> Bellman Evaluation Operator</a></li>
  <li><a href="#bellman-optimality-operator" id="toc-bellman-optimality-operator" class="nav-link" data-scroll-target="#bellman-optimality-operator"><span class="header-section-number">12.9</span> Bellman Optimality Operator</a></li>
  <li><a href="#fixed-points-of-maps-mathcal-lpi-and-mathcal-l" id="toc-fixed-points-of-maps-mathcal-lpi-and-mathcal-l" class="nav-link" data-scroll-target="#fixed-points-of-maps-mathcal-lpi-and-mathcal-l"><span class="header-section-number">12.10</span> Fixed Points of Maps <span class="math inline">\mathcal L^\pi</span> and <span class="math inline">\mathcal L</span></a></li>
  <li><a href="#bellman-evaluation-operator-is-a-contraction" id="toc-bellman-evaluation-operator-is-a-contraction" class="nav-link" data-scroll-target="#bellman-evaluation-operator-is-a-contraction"><span class="header-section-number">12.11</span> Bellman Evaluation Operator is a Contraction</a></li>
  <li><a href="#convergence-of-bellman-updates" id="toc-convergence-of-bellman-updates" class="nav-link" data-scroll-target="#convergence-of-bellman-updates"><span class="header-section-number">12.12</span> Convergence of bellman Updates</a></li>
  </ul></li>
  <li><a href="#policy-iteration" id="toc-policy-iteration" class="nav-link" data-scroll-target="#policy-iteration"><span class="header-section-number">13</span> Policy Iteration</a>
  <ul>
  <li><a href="#policy-evaluation" id="toc-policy-evaluation" class="nav-link" data-scroll-target="#policy-evaluation"><span class="header-section-number">13.1</span> Policy Evaluation</a></li>
  <li><a href="#policy-improvement" id="toc-policy-improvement" class="nav-link" data-scroll-target="#policy-improvement"><span class="header-section-number">13.2</span> Policy Improvement</a></li>
  <li><a href="#policy-iteration-algorithm" id="toc-policy-iteration-algorithm" class="nav-link" data-scroll-target="#policy-iteration-algorithm"><span class="header-section-number">13.3</span> Policy Iteration : Algorithm</a></li>
  <li><a href="#policy-iteration-schematic-representation" id="toc-policy-iteration-schematic-representation" class="nav-link" data-scroll-target="#policy-iteration-schematic-representation"><span class="header-section-number">13.4</span> Policy Iteration : Schematic Representation</a></li>
  <li><a href="#modified-policy-iteration" id="toc-modified-policy-iteration" class="nav-link" data-scroll-target="#modified-policy-iteration"><span class="header-section-number">13.5</span> Modified Policy Iteration</a></li>
  </ul></li>
  <li><a href="#possible-extension" id="toc-possible-extension" class="nav-link" data-scroll-target="#possible-extension"><span class="header-section-number">14</span> Possible Extension</a>
  <ul>
  <li><a href="#asynchronous-dynamic-programming" id="toc-asynchronous-dynamic-programming" class="nav-link" data-scroll-target="#asynchronous-dynamic-programming"><span class="header-section-number">14.1</span> Asynchronous Dynamic Programming</a></li>
  <li><a href="#real-time-dynamic-programming" id="toc-real-time-dynamic-programming" class="nav-link" data-scroll-target="#real-time-dynamic-programming"><span class="header-section-number">14.2</span> Real Time Dynamic Programming</a></li>
  </ul></li>
  <li><a href="#remarks" id="toc-remarks" class="nav-link" data-scroll-target="#remarks"><span class="header-section-number">15</span> Remarks</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="exact-method-policy-iteration" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="exact-method-policy-iteration"><span class="header-section-number">1</span> Exact method: Policy Iteration</h2>
<p>Is there a way to arrive at <span class="math inline">\pi_*</span> starting from an arbitrary policy <span class="math inline">\pi</span>, The answer is policy iteration.</p>
<ul>
<li>Evaluate the policy <span class="math inline">\pi</span>
<ul>
<li>Compute <span class="math inline">V^{\pi}(s)=\mathbb{E}_{\pi}(r_{t+1}+\gamma r_{t+2}+\gamma^{2}r_{t+3}+\cdot\cdot\cdot\cdot\vert s_{t}=s)</span></li>
</ul></li>
<li>Improve the policy <span class="math inline">\pi</span>
<ul>
<li><span class="math inline">\pi^{\prime}(s)=\operatorname{greedy}(V^{\pi}(s))</span></li>
</ul></li>
</ul>
</section>
<section id="exact-method-value-iteration" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="exact-method-value-iteration"><span class="header-section-number">2</span> Exact method: Value Iteration</h2>
<p>Is there a way to arrive at <span class="math inline">V_*</span> starting from an arbitrary value <span class="math inline">V_*</span>, The answer is value iteration.</p>
</section>
<section id="bellman-evaluation-equation" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="bellman-evaluation-equation"><span class="header-section-number">3</span> Bellman Evaluation Equation</h2>
<p><span class="math display">V^{\pi}(s)=\sum_{a}\pi(a|s)\sum_{s^{\prime}}\mathcal{P}_{s s^{\prime}}^{a}\left[\mathcal{R}_{s s^{\prime}}^{a}+\gamma V^{\pi}(s^{\prime})\right]</span></p>
<p>For a MDP with <span class="math inline">S=n</span> , Bellman Evaluation Equation for <span class="math inline">V^\pi (S)</span> is a system of <span class="math inline">n=|S|</span> (linear) equation with <span class="math inline">n</span> vriable and can be solved if the model is known</p>
<p>Denote,</p>
<p><span class="math display">{\mathcal{P}}^{\pi}(s^{\prime}|s)=\sum_{a\in\mathcal{A}}\pi\bigl(a|s\bigr){\mathcal{P}}_{s s^{\prime}}^{a}</span></p>
<p>and</p>
<p><span class="math display">\mathcal R^{\pi}(s)= \sum_{a\in\mathcal{A}}\pi(a|s)\sum_{s^{\prime}}\mathcal{P}_{s s^{\prime}}^{a}\mathcal{R}_{s s^{\prime}}^{a}=\mathbb{E}(r_{t+1}|s_{t}=s)</span></p>
<p>Using <span class="math inline">\mathcal P^\pi</span> and <span class="math inline">\mathcal R^ \pi</span>, fot finite state MDP, Bellman evaluation equation can be written as</p>
<p><span class="math display">V^{\pi}={\cal R}^{\pi}+\gamma{\cal P}^{\pi}V^{\pi}\implies V^{\pi}=(I-\gamma{\cal P}^{\pi})^{-1}{\cal R}^{\pi}</span></p>
</section>
<section id="optimality-equation-for-state-value-function" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="optimality-equation-for-state-value-function"><span class="header-section-number">4</span> Optimality Equation for State Value Function</h2>
<p>Recursive formulation for <span class="math inline">V_*(s)</span> :</p>
<p><span class="math display">V_{*}(s)=\operatorname*{max}_{a}Q_{*}(s,a)=\operatorname*{max}_{a}\left[\sum_{s^{\prime}\in{\cal S}}p_{s s^{\prime}}^{a}(\mathcal{R}_{s s^{\prime}}^{a}+\gamma V_{*}(s^{\prime}))\right]</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Optimality equations are non-linear system of equations with <span class="math inline">n</span> unknowns and <span class="math inline">n</span> non-linear constraints (the max operator is non-linear)</p>
</div>
</div>
</section>
<section id="solving-the-bellman-optimality-equation" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="solving-the-bellman-optimality-equation"><span class="header-section-number">5</span> Solving the Bellman Optimality Equation</h2>
<ul>
<li>Bellman optimality equations are non-linear</li>
<li>In general, there are no closed form solutions</li>
<li>Iterative methods are typically used</li>
</ul>
</section>
<section id="bellmans-optimality-principle" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="bellmans-optimality-principle"><span class="header-section-number">6</span> Bellman’s Optimality Principle</h2>
<div class="callout callout-style-default callout-caution callout-titled" title="Principle of Optimality">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Principle of Optimality
</div>
</div>
<div class="callout-body-container callout-body">
<p>The tail of an optimal must be optimal</p>
<ul>
<li>Any optimal policy can be subdivided into two components; an optimal first action, followed by an optimal policy from successor state <span class="math inline">s′</span> .</li>
</ul>
</div>
</div>
</section>
<section id="solution-methodology-dynamic-programming" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="solution-methodology-dynamic-programming"><span class="header-section-number">7</span> Solution Methodology : Dynamic Programming</h2>
<p>Bellman optimality equation :</p>
<p><span class="math display">V_{*}(s)=\operatorname*{max}_{a}\left[\sum_{s^{\prime}\in{\cal S}}p_{s s^{\prime}}^{a}(\mathcal{R}_{s s^{\prime}}^{a}+\gamma V_{*}(s^{\prime}))\right]</span></p>
<ul>
<li>Optimal Substructure : Optimal solution can be constructed from optimal solutions to subproblems</li>
<li>Overlapping Subproblems : Problem can be broken down into subproblems and can be reused several times.</li>
<li>Markov Decision Processes, generally, satisfy both these characteristics.</li>
<li>Dynamic Programming is a popular solution method for problems having such properties</li>
</ul>
</section>
<section id="value-iteration-idea" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="value-iteration-idea"><span class="header-section-number">8</span> Value Iteration : Idea</h2>
<ul>
<li>Suppose we know the value <span class="math inline">V_∗ (s′)</span></li>
<li>Then the solution <span class="math inline">V_∗ (s)</span> can be found by one step look ahead <span class="math display">V_{*}(s)\leftarrow \operatorname*{max}_{a}\left[\sum_{s^{\prime}\in{\cal S}}p_{s s^{\prime}}^{a}(\mathcal{R}_{s s^{\prime}}^{a}+\gamma V_{*}(s^{\prime}))\right]</span></li>
<li>Idea of value iteration is to perform the above updates iteratively</li>
</ul>
</section>
<section id="value-iteration-algorithm" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="value-iteration-algorithm"><span class="header-section-number">9</span> Value Iteration : Algorithm</h2>
<div class="alert alert-dismissible alert-success">
<table class="table">
<colgroup>
<col style="width: 8%">
</colgroup>
<tbody>
<tr class="odd">
<td>Algorithm value Iteration</td>
</tr>
</tbody>
</table>
<ol type="1">
<li>Start with an initial value function <span class="math inline">V_1(\cdot)</span>;</li>
<li>for <span class="math inline">k=1,2,\cdots,K</span> do</li>
<li>for <span class="math inline">s\in S</span> do</li>
<li>Calculate <span class="math display">V_{k+1}(s)\leftarrow \operatorname*{max}_{a}\left[\sum_{s^{\prime}\in{\cal S}}p_{s s^{\prime}}^{a}(\mathcal{R}_{s s^{\prime}}^{a}+\gamma V_{k}(s^{\prime}))\right]</span></li>
<li>end for</li>
<li>end for</li>
</ol>
<hr>
</div>
<p><img src="CS6140_images/2023-09-02_19-45.png" class="img-fluid"></p>
</section>
<section id="value-iteration-remarks" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="value-iteration-remarks"><span class="header-section-number">10</span> Value Iteration : Remarks</h2>
<ul>
<li>The sequence of value functions <span class="math inline">\{V_1 , V_2 , \cdots , \}</span> converge to <span class="math inline">V_*</span></li>
<li>Convergence is independent of the choice of <span class="math inline">V_0</span></li>
<li>Intermediate value functions need not correspond to a policy in the sense of satisfying the Bellman Evaluation Equation</li>
<li>However, for any <span class="math inline">k</span>, one can come up with a greedy policy as follows: <span class="math display">\pi _{k+1}(s)=\leftarrow \text{greedy}V_k(s)</span></li>
<li>The crux of proving the above statement lie in Banach Fixed Point Theorem/ Contraction Mapping Theorem</li>
</ul>
</section>
<section id="optimality-equation-for-action-value-function" class="level2" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="optimality-equation-for-action-value-function"><span class="header-section-number">11</span> Optimality Equation for Action-Value Function</h2>
<p>There is a recursive formulation for <span class="math inline">\mathcal Q_*(\cdot,\cdot)</span></p>
<p><span class="math display">{\cal Q}_{*}(s,a)=\left[\sum_{s^{\prime}\in S}{\cal P}_{s s^{\prime}}^{a}\left({\cal R}_{s s^{\prime}}^{a}+\gamma\,\operatorname*{max}_{a^{\prime}}(s^{\prime},a^{\prime})\right)\right]</span></p>
<p>One could similarly conceive an iterative algorithm to compute optimal <span class="math inline">Q_*</span> using the above recursive formulation</p>
</section>
<section id="proof-of-value-iteration-convergence" class="level2" data-number="12">
<h2 data-number="12" class="anchored" data-anchor-id="proof-of-value-iteration-convergence"><span class="header-section-number">12</span> Proof of Value Iteration Convergence</h2>
<section id="notion-of-convergence" class="level3" data-number="12.1">
<h3 data-number="12.1" class="anchored" data-anchor-id="notion-of-convergence"><span class="header-section-number">12.1</span> Notion of Convergence</h3>
<p>Let <span class="math inline">\mathcal V</span> be a vector space, A sequence of vectors <span class="math inline">\{v_n \in \mathcal V\}</span> (with <span class="math inline">n \in \mathbb N</span>) is said to converge to <span class="math inline">v</span> if and only if <span class="math display">\lim_{n \to \infty}   \Vert v_n -v \Vert=0</span></p>
<p><img src="CS6140_images/2023-09-02_20-06.png" class="img-fluid"></p>
</section>
<section id="cauchy-sequence" class="level3" data-number="12.2">
<h3 data-number="12.2" class="anchored" data-anchor-id="cauchy-sequence"><span class="header-section-number">12.2</span> Cauchy Sequence</h3>
<p>A sequence of vectors <span class="math inline">\{v_n\}\in \mathcal V</span> (with <span class="math inline">n \in \mathbb N</span>) is said to be Cauchy sequence if and only if, for each <span class="math inline">\epsilon &gt; 0</span>, there exists and <span class="math inline">N_\epsilon</span> such that <span class="math inline">\Vert v_n-v_m \Vert \le \epsilon</span> for any <span class="math inline">n,m &gt; N_\epsilon</span></p>
<p><img src="CS6140_images/2023-09-02_20-12.png" class="img-fluid"></p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>It is not necessary that every Cauchy sequence will converge, but if it has completeness then a cauchy sequence will converge</li>
</ul>
</div>
</div>
</section>
<section id="notion-of-completeness" class="level3" data-number="12.3">
<h3 data-number="12.3" class="anchored" data-anchor-id="notion-of-completeness"><span class="header-section-number">12.3</span> Notion of Completeness</h3>
<p>A normed vector space <span class="math inline">(\mathcal V, \Vert \cdot \Vert)</span> is complete, if and only if, every Cauchy sequence in <span class="math inline">\mathcal V</span> converges to a point in <span class="math inline">\mathcal V</span></p>
</section>
<section id="contractions" class="level3" data-number="12.4">
<h3 data-number="12.4" class="anchored" data-anchor-id="contractions"><span class="header-section-number">12.4</span> Contractions</h3>
<p>Let <span class="math inline">(\mathcal V, \Vert \cdot \Vert)</span> be a normed vector space and let <span class="math inline">L : \mathcal V \rightarrow \mathcal V</span>. we say that <span class="math inline">L</span> is a contraction, or a contraction mapping, if there is a real number <span class="math inline">\gamma \in [0,1)</span>, such that <span class="math display">\Vert L(v)-L(u)\Vert \le \gamma \Vert v-u \Vert</span> for all <span class="math inline">u</span> and <span class="math inline">v</span> in <span class="math inline">\mathcal V</span>, where the term <span class="math inline">\gamma</span> is called a Lipschitz coefficient for <span class="math inline">L</span></p>
<p><img src="CS6140_images/2023-09-02_21-04.png" class="img-fluid"></p>
</section>
<section id="notion-of-fixed-point" class="level3" data-number="12.5">
<h3 data-number="12.5" class="anchored" data-anchor-id="notion-of-fixed-point"><span class="header-section-number">12.5</span> Notion of Fixed point</h3>
<p>A vector <span class="math inline">v \in \mathcal V</span> is a fixed point of the map <span class="math inline">L : \mathcal V \rightarrow \mathcal V</span> if <span class="math inline">L(v)=v</span></p>
<p><img src="CS6140_images/2023-09-02_21-07.png" class="img-fluid"></p>
</section>
<section id="banach-fixed-point-theorem" class="level3" data-number="12.6">
<h3 data-number="12.6" class="anchored" data-anchor-id="banach-fixed-point-theorem"><span class="header-section-number">12.6</span> Banach Fixed point theorem</h3>
<p>Let <span class="math inline">\langle \mathcal V, \Vert \cdot \Vert \rangle</span> be a complete normed vector space and let <span class="math inline">L : \mathcal V \rightarrow \mathcal V</span> be a <span class="math inline">\gamma</span> - contraction mapping, Then iterative application of <span class="math inline">L</span> converges to a unique fixed point in <span class="math inline">\mathcal V</span> independent of the starting point.</p>
<p><img src="CS6140_images/2023-09-02_21-13.png" class="img-fluid"></p>
</section>
<section id="value-function-space" class="level3" data-number="12.7">
<h3 data-number="12.7" class="anchored" data-anchor-id="value-function-space"><span class="header-section-number">12.7</span> Value Function Space</h3>
<ul>
<li><span class="math inline">\mathcal S</span> is a discrete state space with <span class="math inline">\vert \mathcal S \vert=n</span></li>
<li><span class="math inline">\mathcal A_s \subseteq \mathcal A</span> be the non-empty subset of actions allowed from state <span class="math inline">s</span></li>
<li><span class="math inline">\mathcal V</span> be a vector space of set of all bounded real valued functions from <span class="math inline">\mathcal S</span> to <span class="math inline">\mathbb R</span></li>
<li>Measure the distance between state value functions <span class="math inline">u,v \in \mathcal V</span> using the max-norm defined as follows <span class="math display">\|u-v\|=\|u-v\|_{\infty}=\operatorname*{max}_{s\in S}|u(s)-v(s)|\quad s\in S;u,v\in{\mathcal{V}}</span>
<ul>
<li>Largest distance between state values</li>
</ul></li>
<li>The space <span class="math inline">\mathcal V</span> is complete.</li>
</ul>
</section>
<section id="bellman-evaluation-operator" class="level3" data-number="12.8">
<h3 data-number="12.8" class="anchored" data-anchor-id="bellman-evaluation-operator"><span class="header-section-number">12.8</span> Bellman Evaluation Operator</h3>
<p><span class="math display">V^{\pi}_{k+1}(s)=\sum_{a}\pi(a|s)\sum_{s^{\prime}}\mathcal{P}_{s s^{\prime}}^{a}\left[\mathcal{R}_{s s^{\prime}}^{a}+\gamma V^{\pi}_k(s^{\prime})\right]</span></p>
<p>Denote,</p>
<p><span class="math display">{\mathcal{P}}^{\pi}(s^{\prime}|s)=\sum_{a\in\mathcal{A}}\pi\bigl(a|s\bigr){\mathcal{P}}_{s s^{\prime}}^{a}</span></p>
<p>and</p>
<p><span class="math display">\mathcal R^{\pi}(s)= \sum_{a\in\mathcal{A}}\pi(a|s)\sum_{s^{\prime}}\mathcal{P}_{s s^{\prime}}^{a}\mathcal{R}_{s s^{\prime}}^{a}=\mathbb{E}(r_{t+1}|s_{t}=s)</span></p>
<p>Then , we can write,</p>
<p><span class="math display">V^{\pi}={\cal R}^{\pi}+\gamma{\cal P}^{\pi}V^{\pi}</span> OR <span class="math display">V_{k+1}={\cal R}^{\pi}+\gamma{\cal P}^{\pi}V_k</span></p>
<p>Define Bellman Evaluation Operator <span class="math inline">(\mathcal L^\pi: \mathcal V \rightarrow \mathcal V)</span> as, <span class="math display">L^{\pi}(v)={\cal R}^{\pi}+\gamma{\cal P}^{\pi}v</span></p>
</section>
<section id="bellman-optimality-operator" class="level3" data-number="12.9">
<h3 data-number="12.9" class="anchored" data-anchor-id="bellman-optimality-operator"><span class="header-section-number">12.9</span> Bellman Optimality Operator</h3>
<p><span class="math display">V_{k+1}(s)=\max_a\left[\sum_{s^{\prime}\in{\cal S}}{\cal P}_{s s^{\prime}}^{a}\left({\cal R}_{s s^{\prime}}^{a}+\gamma V_{k}(s^{\prime})\right)\right]</span></p>
<p>Denote,</p>
<p><span class="math display">{\mathcal{P}}^{a}(s)=\sum_{s'\in\mathcal{S}}{\mathcal{P}}_{s s^{\prime}}^{a}</span></p>
<p>and</p>
<p><span class="math display">\mathcal R^{a}(s)= \sum_{s'\in\mathcal{S}}\mathcal{P}_{s s^{\prime}}^{a}\mathcal{R}_{s s^{\prime}}^{a}</span></p>
<p>Then , we can write,</p>
<p><span class="math display">V_{k+1}=\max_{a \in \mathcal A}\left[ {\cal R}^{a}+\gamma{\cal P}^{a}V_k \right]</span></p>
<p>Define Bellman Optimality Operator : <span class="math inline">(\mathcal L: \mathcal V \rightarrow \mathcal V)</span> as,</p>
<p><span class="math display">L(v)=\max_{a\in \mathcal A}[{\cal R}^{a}+\gamma{\cal P}^{a}v]</span></p>
<ul>
<li>Note that since value functions are a mapping from state space to real numbers one can also think of <span class="math inline">\mathcal L^\pi</span> and <span class="math inline">\mathcal L</span> as mappings from <span class="math inline">\mathbb R_d \to \mathbb R_d</span></li>
</ul>
</section>
<section id="fixed-points-of-maps-mathcal-lpi-and-mathcal-l" class="level3" data-number="12.10">
<h3 data-number="12.10" class="anchored" data-anchor-id="fixed-points-of-maps-mathcal-lpi-and-mathcal-l"><span class="header-section-number">12.10</span> Fixed Points of Maps <span class="math inline">\mathcal L^\pi</span> and <span class="math inline">\mathcal L</span></h3>
<p>We can see that <span class="math inline">V^\pi</span> is a fixed point of function <span class="math inline">\mathcal L^\pi</span></p>
<p><span class="math display">\mathcal L^\pi V^\pi=V^\pi</span></p>
<p>and <span class="math inline">V_*</span> is fixed point of operator <span class="math inline">\mathcal L</span></p>
<p><span class="math display">\mathcal L V_*=V_*</span></p>
</section>
<section id="bellman-evaluation-operator-is-a-contraction" class="level3" data-number="12.11">
<h3 data-number="12.11" class="anchored" data-anchor-id="bellman-evaluation-operator-is-a-contraction"><span class="header-section-number">12.11</span> Bellman Evaluation Operator is a Contraction</h3>
<p>Recall that Bellman evaluation operator is given by <span class="math inline">(\mathcal L^\pi: \mathcal V \rightarrow \mathcal V)</span> <span class="math display">L^{\pi}(v)={\cal R}^{\pi}+\gamma{\cal P}^{\pi}v</span></p>
<ul>
<li>This operator is <span class="math inline">\lambda</span> contraction. i.e., it makes value fuctions closer by at least <span class="math inline">\lambda</span>.</li>
</ul>
<p>Proof</p>
<ul>
<li>For any two value functions <span class="math inline">u</span> and <span class="math inline">v</span> in the space <span class="math inline">V</span>, we have,</li>
</ul>
<p><span class="math display">\begin{align*}
    \|L^\pi(u)-L^\pi(v)\|_\infty &amp;= \|(\mathcal R^\pi + \lambda \mathcal P^\pi u)-(\mathcal R^\pi + \lambda \mathcal P^\pi v)\|_\infty \\
    &amp;= \|\lambda \mathcal P^\pi(u-v)\|_\infty\\
    &amp;\le \gamma \| \mathcal P ^\pi \|_\infty \|(u-v)\|_\infty = \gamma \|u-v\|_\infty\\
\end{align*}</span></p>
<p>we used the property that for every <span class="math inline">x \in \mathbb R^n,</span> and <span class="math inline">A</span> , a <span class="math inline">m \times n</span> matrix, <span class="math inline">\|Ax\|_\infty \le \|A\|_\infty \|x\|_\infty</span></p>
</section>
<section id="convergence-of-bellman-updates" class="level3" data-number="12.12">
<h3 data-number="12.12" class="anchored" data-anchor-id="convergence-of-bellman-updates"><span class="header-section-number">12.12</span> Convergence of bellman Updates</h3>
<ul>
<li>Banach fixed-point theorem guarantees that iteratively applying evaluation operator <span class="math inline">\mathcal L^\pi</span> to any function <span class="math inline">V \in \mathcal V</span> will converge to a unique function <span class="math inline">V^\pi \in V</span></li>
<li>Similarly, the Bellman optimality operator <span class="math inline">(\mathcal L : \mathcal V \to \mathcal V)</span> <span class="math display">L(v)=\max_{a\in \mathcal A}[{\cal R}^{a}+\gamma{\cal P}^{a}v]</span> is also (A similar argument as <span class="math inline">\mathcal L^\pi</span>) a <span class="math inline">\gamma</span> contraction and hence iteratively applying optimality operator <span class="math inline">\mathcal L</span> to any function <span class="math inline">V \in \mathcal V</span> will converge to a unique function <span class="math inline">V_* \in V</span></li>
<li>Also, <span class="math inline">V_* = \max_\pi V^\pi(\cdot)</span></li>
</ul>
</section>
</section>
<section id="policy-iteration" class="level2" data-number="13">
<h2 data-number="13" class="anchored" data-anchor-id="policy-iteration"><span class="header-section-number">13</span> Policy Iteration</h2>
<p>We can arrive at <span class="math inline">\pi_*</span> starting form an arbitrary policy <span class="math inline">\pi</span></p>
<ul>
<li>Evaluate the policy <span class="math inline">\pi</span>
<ul>
<li>Compute <span class="math inline">V^{\pi}(s)={\mathbb{E}}_{\pi}(r_{t+1}+{\gamma}{r_{t+2}}+{\gamma}^{2}r_{t+3}+\cdot\cdot\cdot\cdot\cdot\vert s_{t}=s)</span></li>
</ul></li>
<li>Improve policy <span class="math inline">\pi</span>
<ul>
<li><span class="math inline">\pi'(s)=\text{greedy}(V^\pi(s) )</span></li>
</ul></li>
</ul>
<section id="policy-evaluation" class="level3" data-number="13.1">
<h3 data-number="13.1" class="anchored" data-anchor-id="policy-evaluation"><span class="header-section-number">13.1</span> Policy Evaluation</h3>
<ul>
<li><p>Problem : Evaluate a given policy <span class="math inline">\pi</span></p></li>
<li><p>Compute <span class="math inline">V^{\pi}(s)={\mathbb{E}}_{\pi}(r_{t+1}+{\gamma}{r_{t+2}}+{\gamma}^{2}r_{t+3}+\cdot\cdot\cdot\cdot\cdot\vert s_{t}=s)</span></p></li>
<li><p>Solution 1: solve a system of linear equation using any solver</p></li>
<li><p>Solution 2: Iterative application of Bellman Evaluation Equation</p>
<ul>
<li>Iterative update rule : <span class="math display">V_{k+1}^{\pi}(s)\leftarrow\sum_{a}\pi(a|s)\sum_{s^{\prime}}\mathcal{P}_{s s^{\prime}}^{a}\left[\mathcal{R}_{s s^{\prime}}^{a}+\gamma\mathcal{V}_{k}^{\pi}(s^{\prime})\right]</span></li>
<li>The sequence of value function <span class="math inline">\{V_1^\pi,V_2^\pi, \cdots ,\}</span> converges to <span class="math inline">V^\pi</span></li>
</ul></li>
</ul>
</section>
<section id="policy-improvement" class="level3" data-number="13.2">
<h3 data-number="13.2" class="anchored" data-anchor-id="policy-improvement"><span class="header-section-number">13.2</span> Policy Improvement</h3>
<p>Suppose we know <span class="math inline">V^\pi</span> . How to improve policy <span class="math inline">\pi</span></p>
<p>The answer lies in the definition of action value function <span class="math inline">Q^\pi (s,a)</span>, Recall that,</p>
<p><span class="math display">\begin{align*}
    Q^{\pi}(s,a)&amp;=\mathbb{E}_{\pi}\left(\sum_{k=0}^{\infty}\gamma^{k}r_{t+k+1}|s_{t}=s,a_{t}=a\right)\\
    &amp;= \mathbb{E}(r_{t+1}+\gamma V^{\pi}(s_{t+1})|s_{t}=s,a_{t}=a)\\
    &amp;=\sum_{s^{\prime}\in{\mathcal{S}}}{\mathcal{P}}_{s s^{\prime}}^{a}\left[{\mathcal{R}}_{s s^{\prime}}^{a}+\gamma\mathcal{V}^{\pi}\left(s^{\prime}\right)\right]
\end{align*}</span></p>
<ul>
<li>If <span class="math inline">\mathcal{Q}^{\pi}(s,a)&gt;V^{\pi}(s) \implies</span> Better to select action <span class="math inline">a</span> in state <span class="math inline">s</span> and thereafter follow the policy <span class="math inline">\pi</span></li>
<li>This is a special case of the policy improvement theorem.</li>
</ul>
<p><strong>Policy Improvement Theorem</strong></p>
<div class="callout callout-style-default callout-note callout-titled" title="Theorem">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\pi</span> and <span class="math inline">\pi^*</span> be any pair of deterministic policy such that, for all <span class="math inline">s \in \mathcal S</span>,</p>
<p><span class="math display">Q^{\pi}(s,\pi^{\prime}(s))\ge V^{\pi}(s).</span> Then <span class="math inline">V^{\pi^{\prime}}(s)\geq V^{\pi}(s) \quad \forall s \in \mathcal S</span></p>
</div>
</div>
<p>Proof :</p>
<p><span class="math display">\begin{align*}
    V^\pi(s) &amp;\le {\cal{Q}}^{\pi}(s,\pi^{\prime}(s))=\mathbb{E}_{\pi^{\prime}}(r_{t+1}+\gamma V^{\pi}(s_{t+1})|s_{t}=s)\\
    &amp;\le \mathbb{E}_{\pi^{\prime}}(r_{t+1}+\gamma Q^{\pi}(s_{t+1},\pi^{\prime}(s_{t+1}))|s_{t}=s)\\
    &amp;=\mathbb{E}_{\pi^{\prime}}(r_{t+1}+\gamma r_{t+2}+\gamma^{2}V^{\pi}(s_{t+2})|s_{t}=s)\\
    &amp;\le \mathbb{E}_{\pi^{\prime}}(r_{t+1}+\gamma r_{t+2}+\gamma^{2}Q^{\pi}(s_{t+2},\pi^{\prime}(s_{t+2}))|s_{t}=s)\\
    &amp;\le \mathbb{E}_{\pi^{\prime}}(r_{t+1}+\gamma r_{t+2}+\gamma^{2}r_{t+3}+\cdot\cdot\cdot\cdot\left|s_{t}=s\right)=V^{\pi^{'}}(s)
\end{align*}</span></p>
<ul>
<li>Now consider the greedy policy <span class="math inline">\pi' = \text{greedy}(V^\pi)</span></li>
<li>Then, <span class="math inline">\pi' \ge \pi</span>, That is <span class="math inline">V^{\pi'}(S) \ge V^{\pi}(S) \quad \forall s \in \mathcal S</span>
<ul>
<li>By definition of <span class="math inline">\pi'</span>, at state s, the action chosen by policy <span class="math inline">\pi'</span> is given by the greedy operator <span class="math display">\pi'(s) = \argmax_a Q^\pi (s, a)</span></li>
<li>This improves the value from any state <span class="math inline">s</span> over one step <span class="math display">Q^{\pi}(s,\pi^{\prime}(s))=\operatorname*{max}_{a}Q^{\pi}(s,a)\geq Q^{\pi}(s,\pi(s))=V^{\pi}(s)</span></li>
<li>It therefore improves the value function, <span class="math inline">V^{\pi^{\prime}}(s)\ge V^{\pi}\left(s\right)</span></li>
</ul></li>
<li>Policy <span class="math inline">\pi'</span> is at least as good as policy <span class="math inline">\pi</span></li>
<li>If improvement stops, <span class="math display">Q^{\pi}(s,\pi^{\prime}(s))=\operatorname*{max}_{a}Q^{\pi}(s,a)=Q^{\pi}(s,\pi(s))=V^{\pi}(s)</span></li>
<li>Bellman Optimality equation is satisfied as, <span class="math display">V^\pi(s)=\max_a Q^\pi (s,a)</span></li>
<li>The policy <span class="math inline">\pi</span> for which the improvement stops is the optimal policy. <span class="math display">V^\pi(s)=V_*(s)\quad \forall s \in \mathcal S</span></li>
</ul>
</section>
<section id="policy-iteration-algorithm" class="level3" data-number="13.3">
<h3 data-number="13.3" class="anchored" data-anchor-id="policy-iteration-algorithm"><span class="header-section-number">13.3</span> Policy Iteration : Algorithm</h3>
<div class="alert alert-dismissible alert-success">
<table class="table">
<colgroup>
<col style="width: 8%">
</colgroup>
<tbody>
<tr class="odd">
<td>Algorithm Policy Iteration</td>
</tr>
</tbody>
</table>
<ol type="1">
<li>Start with an initial policy <span class="math inline">\pi_1</span></li>
<li>For <span class="math inline">i=1,2,\cdots,N</span> do</li>
<li>Evaluate <span class="math inline">V^{\pi_i}(s)\quad \forall s \in \mathcal S</span>. That is,</li>
<li>For <span class="math inline">k=1,2,\cdots K</span> do</li>
<li>For all <span class="math inline">s \in \mathcal S</span> calculate <span class="math display">V_{k+1}^{\pi_{i}}(s)\leftarrow\sum_{a}\pi(a|s)\sum_{s^{\prime}}\mathcal{P}_{s s^{\prime}}^{a}\ \left[{\mathcal{R}}_{s s^{\prime}}^{a}\ +\gamma V_{k}^{\pi_{i}}(s^{\prime})\right]</span></li>
<li>end for</li>
<li>Perform policy Improvement <span class="math display">\pi_{i+1}=\text{greedy}(V^{\pi_i})</span></li>
<li>End for</li>
</ol>
<hr>
</div>
<p><img src="CS6140_images/2023-09-03_05-50.png" class="img-fluid"> <img src="CS6140_images/2023-09-03_05-24.png" class="img-fluid"></p>
</section>
<section id="policy-iteration-schematic-representation" class="level3" data-number="13.4">
<h3 data-number="13.4" class="anchored" data-anchor-id="policy-iteration-schematic-representation"><span class="header-section-number">13.4</span> Policy Iteration : Schematic Representation</h3>
<p><br> <img src="CS6140_images/2023-09-03_05-53.png" class="img-fluid"></p>
<ul>
<li>The sequence <span class="math inline">\{\pi_1,\pi_2,\cdots\}</span> is guaranteed to converge.</li>
<li>At convergence, both current policy and the value function associated with the policy are optimal.</li>
</ul>
</section>
<section id="modified-policy-iteration" class="level3" data-number="13.5">
<h3 data-number="13.5" class="anchored" data-anchor-id="modified-policy-iteration"><span class="header-section-number">13.5</span> Modified Policy Iteration</h3>
<p>Can we computationally simplify policy iteration process</p>
<ul>
<li>We need not wait for policy evaluation to converge to <span class="math inline">V^\pi</span></li>
<li>We can have a stopping criterion like <span class="math inline">\epsilon</span>- convergence of value function evaluation or <span class="math inline">K</span> iteration of policy evaluation</li>
<li>Extreme case of <span class="math inline">K=1</span> is <strong>value iteration</strong>, we update the policy every iteration.</li>
</ul>
</section>
</section>
<section id="possible-extension" class="level2" data-number="14">
<h2 data-number="14" class="anchored" data-anchor-id="possible-extension"><span class="header-section-number">14</span> Possible Extension</h2>
<section id="asynchronous-dynamic-programming" class="level3" data-number="14.1">
<h3 data-number="14.1" class="anchored" data-anchor-id="asynchronous-dynamic-programming"><span class="header-section-number">14.1</span> Asynchronous Dynamic Programming</h3>
<ul>
<li>Update to states are done individually, in any order</li>
<li>For each selected state, apply the appropriate backup</li>
<li>Can significantly reduce computation</li>
<li>Convergence guaranteed exist, if all states are selected sufficient number of times.</li>
</ul>
</section>
<section id="real-time-dynamic-programming" class="level3" data-number="14.2">
<h3 data-number="14.2" class="anchored" data-anchor-id="real-time-dynamic-programming"><span class="header-section-number">14.2</span> Real Time Dynamic Programming</h3>
<ul>
<li>Idea : Update only states that are relevant to agent</li>
<li>After each time step, we get <span class="math inline">s_t,a_t,r_{t+1}</span></li>
<li>perform the following update <span class="math display">V{\big(}s_{t}{\big)}\leftarrow{\mathrm{max}}\left[\sum_{s^{\prime}\in{\cal S}}{\mathcal{P}}_{s_{t}s^{\prime}}^{a}\left({\mathcal{R}}_{s_{t}s^{\prime}}^{a}+\gamma V(s^{\prime})\right)\right]</span></li>
</ul>
</section>
</section>
<section id="remarks" class="level2" data-number="15">
<h2 data-number="15" class="anchored" data-anchor-id="remarks"><span class="header-section-number">15</span> Remarks</h2>
<ul>
<li>MDP Setting : The agent has knowledge of the state transition matrices <span class="math inline">\mathcal P_{ss'}</span> and the reward function <span class="math inline">\mathcal R</span></li>
<li>RL Setting : The agent does not have knowledge of the state transition matrices <span class="math inline">\mathcal P^a_{ss'}</span> and the reward function <span class="math inline">\mathcal R</span>
<ul>
<li>The goal in both cases are same; Determine optimal sequence of actions such that the total discounted future reward is maximum.</li>
<li>Although, this course would assume Markovian structure to state transitions, in many (sequential) decision making problems we may have to consider the history as well.</li>
</ul></li>
</ul>
<p>Prediction and Control using Dynamic Programming :</p>
<ul>
<li>Dynamic Programming assumes full knowledge of MDP</li>
<li>Used for both prediction and control in an MDP</li>
<li>Prediction
<ul>
<li>Input MDP <span class="math inline">\langle \mathcal S, \mathcal A, \mathcal P, \mathcal R, \gamma \rangle</span> and policy <span class="math inline">\pi</span></li>
<li>output <span class="math inline">v^\pi(\cdot)</span></li>
</ul></li>
<li>Control
<ul>
<li>Input MDP <span class="math inline">\langle \mathcal S, \mathcal A, \mathcal P, \mathcal R, \gamma \rangle</span></li>
<li>Output Optimal value function <span class="math inline">V_*(\cdot)</span> or optimal policy <span class="math inline">\pi_*</span></li>
</ul></li>
</ul>
<p><br><br><br> <span class="math inline">\tiny {\textcolor{#808080}{\boxed{\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}</span></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="abhiyantaabhishek/IITH-Data-Science" data-repo-id="R_kgDOILoB8A" data-category="Announcements" data-category-id="DIC_kwDOILoB8M4CSJcL" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../Data_Science_Notes/Reinforcement-Learning/2023-08-19-CS6140-week3.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Reinforcement Learning 3</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../Data_Science_Notes/Reinforcement-Learning/2023-09-02-CS6140-week5.html" class="pagination-link">
        <span class="nav-page-text">Reinforcement Learning 5</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Copyright 2022, Abhishek Kumar Dubey</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/abhiyantaabhishek">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/abhishek-kumar-dubey-585a86179/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>



</body></html>