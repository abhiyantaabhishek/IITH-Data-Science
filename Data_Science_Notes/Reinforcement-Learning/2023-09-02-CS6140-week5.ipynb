{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "author: Abhishek Kumar Dubey\n",
    "badges: false\n",
    "categories:\n",
    "- Reinforcement Learning\n",
    "date: '2023-09-02'\n",
    "description: Value Functions with Policy, Decomposition of State Value Function, MDP with Policy, Optimal Policy, Solution to an MDP, Greedy Policy\n",
    "image: CS6140_images/Acrobat_9thkiyFyb5.png\n",
    "title: Reinforcement Learning 5\n",
    "toc: true\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DP Algorithms: A closer Look\n",
    "\n",
    "### Terminology \n",
    "\n",
    "Let us consider the policy evaluation formula\n",
    "<br><br>\n",
    "\n",
    "$$V_{k+1}(s)\\leftarrow\\sum_{a}\\pi(s,a)\\sum_{s^{\\prime}}\\mathcal{P}_{s s^{\\prime}}^{a}\\left[\\mathcal{R}_{s s^{\\prime}}^{a}+\\gamma V_{k}(s^{\\prime})\\right]$$\n",
    "\n",
    "![](CS6140_images/2023-09-03_06-38.png)\n",
    "\n",
    "### Schematic View\n",
    "\n",
    "![](CS6140_images/2023-09-03_06-40.png)\n",
    "\n",
    "$$V(s) = \\sum_{a}\\pi(s,a)\\sum_{s^{\\prime}}\\mathcal{P}_{s s^{\\prime}}^{a}\\left[\\mathcal{R}_{s s^{\\prime}}^{a}+\\gamma V(s^{\\prime})\\right]$$\n",
    "\n",
    "$$V_{k+1}(s)\\leftarrow\\sum_{a}\\pi(s,a)\\sum_{s^{\\prime}}\\mathcal{P}_{s s^{\\prime}}^{a}\\left[\\mathcal{R}_{s s^{\\prime}}^{a}+\\gamma V_{k}(s^{\\prime})\\right]$$\n",
    "\n",
    "### Drawbacks of DP Algorithms\n",
    "\n",
    "- Requires full prior knowledge of the dynamics of the environment\n",
    "- Can be implemented only on small or medium sized discrete state spaces\n",
    "  - For large problems, DP suffers from Bellmanâ€™s curse of dimensionality\n",
    "- DP uses full width back-ups\n",
    "  - Every successor state and action is considered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Free Prediction : Monte Carlo Methods\n",
    "\n",
    "### MC: key Idea\n",
    "\n",
    "$$\\begin{align*}\n",
    "  V^{\\pi}(s)\\quad & \\stackrel{\\mathrm{def}}{=}\\quad\\mathbb{E}_{\\pi}(G_{t}|s_{t}=s)=\\mathbb{E}_{\\pi}\\left(\\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k+1}|s_{t}=s\\right)  \\\\\n",
    "  &= \\quad \\mathbb E_\\pi[r_{t+1}, \\gamma V^\\pi (s_{t+1})|s_t=S]\n",
    "\\end{align*}$$\n",
    "\n",
    "- To estimate the expectations use samples\n",
    "\n",
    "### MC: Policy Evaluation \n",
    "\n",
    "- Goal : Evaluate $V^\\pi (s)$ using experiences (or trajectories) under policy $\\pi$ \n",
    "  $$s_0,a_0,r_0,s_1,a_1,r_1,\\cdots,s_T$$ \n",
    "- Recall that \n",
    "  $$V^{\\pi}(s) =\\mathbb{E}_{\\pi}(G_{t}|s_{t}=s)=\\color{red}\\mathbb{E}_{\\pi}\\left(\\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k+1}|s_{t}=s\\right) $$\n",
    "- The idea is to calculate sample mean return $(G_t)$ starting from state $s$ instead of expected mean return.\n",
    "\n",
    "### MC: Evaluation schematics\n",
    "\n",
    "![](CS6140_images/2023-09-03_07-23.png)\n",
    "\n",
    "- Use $G_1$ to update $V^\\pi(s_1)$\n",
    "- Use $G_2$ to update $V^\\pi(s_2)$\n",
    "- Use $G_3$ to update $V^\\pi(s_3)$\n",
    "\n",
    "### First-visit Monte Carlo Policy Evaluation\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "$\\tiny  {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0e5e69b8442e8f020791fad6bfcef3777f0e89e0f1a2517b6b628b2eaf0fe66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
