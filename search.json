[
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-11-05-CS5590-week9.html",
    "href": "Data_Science_Notes/Machine-Learning/2022-11-05-CS5590-week9.html",
    "title": "Machine Learning 9",
    "section": "",
    "text": "Given training data \\{x_i,y_i\\}_{i=1}^n\nFind w_1 and b  such that y=w_1+b optimally describes the data: \nThe thinner the tube more complex the model.\n\nLazy case (underfitting): \nSuspiciously smart case (overfitting): \nCompromising case (good generalizability) \n\nFormulation:\n\\min_{w_1 ,b,\\xi_i ,{\\xi_i }^* } \\frac{1}{2}w_1^2 +C\\sum_i \\left(\\xi_i +\\xi_i^* \\right) subject to : \\begin{align*}\ny_i-(w_ix_{i1}) -b &\\leq \\epsilon +\\xi_i \\;\\;\\text{green line area below}\\\\\n(w_ix_{i1}) +b -y_i&\\leq \\epsilon +\\xi_i^* \\;\\;\\text{red line area below}\\\\\n\\xi_i,\\xi_i^*\\ge 0 \\quad & \\quad i=1,2,\\dots,n\n\\end{align*}    \\epsilon controls width of the tube.\nRole of C\n\nSmall C \nBig C \n\n\n\n\n\\min_{w_1 ,b,\\xi_i ,{\\xi_i }^* } \\frac{1}{2}(w_1^2 +w_2^2 )+C\\sum_i \\left(\\xi_i +\\xi_i^* \\right) subject to : \\begin{align*}\n  y_i-(\\mathbf{w}'\\phi( x_{i1})) -b &\\leq \\epsilon +\\xi_i \\\\\n  (\\mathbf{w}'\\phi( x_{i1})) +b -y_i&\\leq \\epsilon +\\xi_i^* \\\\\n  \\xi_i,\\xi_i^*\\ge 0 \\;&\\; i=1,2,\\dots,n\n  \\end{align*}\n\n\n\n\\min_{w_1 ,b,\\xi_i ,{\\xi_i }^* } \\frac{1}{2}(\\left\\lVert \\mathbf{w} \\right\\rVert)+C\\sum_i \\left(\\xi_i +\\xi_i^* \\right) subject to : \\begin{align*}\ny_i-(\\mathbf{w}'\\phi( x_{i})) -b &\\leq \\epsilon +\\xi_i \\\\\n(\\mathbf{w}'\\phi( x_{i})) +b -y_i&\\leq \\epsilon +\\xi_i^* \\\\\n\\xi_i,\\xi_i^*\\ge 0 \\;&\\; i=1,2,\\dots,n\n\\end{align*} Lagrangian: \\begin{align*}\nL:=\\;&\\frac{1}{2}(\\left\\lVert \\mathbf{w} \\right\\rVert)+C\\sum_i \\left(\\xi_i +\\xi_i^* \\right) \\\\\n&-\\sum_i(\\eta_i\\xi_i+\\eta_i^*\\xi_i^*)\\\\\n&-\\sum_i \\alpha_i(\\epsilon +\\xi_i -y_i+(\\mathbf{w}'\\phi( x_{i})) +b  )\\\\\n&-\\sum_i \\alpha_i^*(\\epsilon +\\xi_i^* +y_i -(\\mathbf{w}'\\phi( x_{i})) -b )\n\\end{align*} Minimize with respect to \\mathbf{w},b,\\xi_i,\\xi_i^* Maximize with respect to \\alpha_i,\\alpha_i^*,\\eta_i,\\eta_i^*\n\n\n\n\n\n\nNo local minima\nScales relatively well to high-dimensional data\nTrade-off between classifier complexity and error can be controlled explicitly via C and \\epsilon\nOverfitting is avoided (for any fixed C and \\epsilon)\nThe “curse of dimensionality” is avoided through kernel functions\n\n\n\n\n\nWhat is the best trade-off parameter C and best \\epsilon ?\nWhat is a good transformation of the original space ?"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-11-05-CS5590-week9.html#logistic-regression",
    "href": "Data_Science_Notes/Machine-Learning/2022-11-05-CS5590-week9.html#logistic-regression",
    "title": "Machine Learning 9",
    "section": "2 Logistic Regression",
    "text": "2 Logistic Regression\n\nTo predict an outcome variable that is categorical from one or more categorical or continuous predictor variables.\nUsed because having a categorical outcome variable violates the assumption of linearity in normal regression.\nLet X be the data instance, and Y be the class label: Learn P(Y\\mid X) directly\n\nLet W = (W_1, W_2,\\dots, W_n), X=(X_1, X_2, \\dots , X_n), \\mathbf W\\cdot \\mathbf X is the dot product\nSigmoid function: P(Y=1\\mid \\mathbf{X})=\\frac{1}{1+e^{-\\mathbf{WX}}}\n\nGenerative models, e.g., Naïve Bayes:  If we estimate P(X\\mid Y),P(Y) from the data and use bayesian rule to find P(Y\\mid X=x) it can be considered as generative modeling. It can also generate the data P(X)=\\sum_yP(y)P(X\\mid y)\nDiscriminative models, e.g., Logistic Regression: If we estimate P(Y\\mid X) directly from the data then it can be considered as discriminative modeling.\nIn logistic regression, we learn the conditional distribution P(Y\\mid X)\nLet P_y(X;W) be our estimate of P(Y\\mid X), where W is a vector of adjustable parameters.\nAssume there are two classes, y = 0 and y = 1 and P_1( \\mathbf{X};\\mathbf{W})=\\frac{1}{1+e^{-\\mathbf{WX}}} and P_0( \\mathbf{X};\\mathbf{W})=1-\\frac{1}{1+e^{-\\mathbf{WX}}} log odd \\begin{align*}\n\\log\\frac{P_1( \\mathbf{X};\\mathbf{W})}{P_0( \\mathbf{X};\\mathbf{W})}&=\\log\\frac{\\frac{1}{1+e^{-\\mathbf{WX}}}}{1-\\frac{1}{1+e^{-\\mathbf{WX}}}}\\\\\n&=\\log\\frac{\\frac{1}{1+e^{-\\mathbf{WX}}}}{\\frac{1+e^{-\\mathbf{WX}}-1}{1+e^{-\\mathbf{WX}}}}\\\\\n&=\\log e^\\mathbf{WX}\\\\\n&=\\mathbf{WX}\n\\end{align*} That is, the log odds of class 1 is a linear function of \\mathbf{X}\nWe find \\mathbf{W} using Conditional data likelihood — Probability of observed Y values in the training data, conditioned on corresponding X values.\nWe choose parameters \\mathbf{W} that satisfy. \\mathbf{W}=\\argmax_\\mathbf{W}\\prod_lP(y^l \\mid \\mathbf{X}^l,\\mathbf{W}) where\n\nw = <w_0,w_1,\\dots,w_n> is the vector of parameters to be estimated,\ny^l denotes the observed value of Y in the l^{\\text{th}} training example, and\n\\mathbf{X}^l denotes the observed value of X in the l^{\\text{th}} training example\n\nEquivalently, we can work with log of conditional likelihood: \\mathbf{W}=\\argmax_\\mathbf{W}\\sum_l \\ln P(y^l \\mid \\mathbf{X}^l,\\mathbf{W})\nConditional data log likelihood, l(W), can be written as l(W)=\\sum_l y^l \\ln P(y^l=1 \\mid \\mathbf{X}^l,\\mathbf{W})+(1-y^l) \\ln P(y^l=0 \\mid \\mathbf{X}^l,\\mathbf{W})\nNote here that Y can take only values 0 or 1, so only one of the two terms in the expression will be non-zero for any given y^l\n\n\n2.1 Training\n\nWe need to estimate: \\mathbf{W}=\\argmax_\\mathbf{W}\\sum_l \\ln P(y^l \\mid \\mathbf{X}^l,\\mathbf{W})\nEquivalently, we can minimize negative log likelihood\nThis is convex – so, unique global minimum\nNo closed-form solution though. Iterative method required.\nUse gradient ascent (descent) for the maximization (min) problem\nThe i^{\\text{th}} component of the vector gradient has the form \\frac{\\partial }{\\partial w_i }l\\left(\\mathbf{W}\\right)=\\sum_l x_i^l \\left(y^l -\\underbrace{\\hat{P} \\left(y^l  =1\\mid {\\mathit{\\mathbf{X}}}^l ,\\mathit{\\mathbf{W}}\\right)}_{\\text{Logistic regression prediction}}\\right)\nBeginning with initial weights, we repeatedly update the weights in the direction of the gradient, changing the i^{\\text{th}} weight according to w_i \\leftarrow w_i+\\eta \\sum_l x_i^l \\left(y^l -\\hat{P} \\left(y^l =1\\mid {\\mathit{\\mathbf{X}}}^l ,\\mathit{\\mathbf{W}}\\right)\\right)\nOverfitting can arise especially when data has very high dimensions and is sparse\nOne approach -> modified “penalized log likelihood function,” which penalizes large values of \\mathbf{W}, as before. \\mathbf{W}=\\argmax_\\mathbf{W}\\sum_l \\ln P(y^l \\mid \\mathbf{X}^l,\\mathbf{W})-\\frac{\\lambda}{2}\\left\\lVert \\mathbf{W} \\right\\rVert^2\nDerivative then becomes: \\frac{\\partial }{\\partial w_i }l\\left(\\mathit{\\mathbf{W}}\\right)=\\sum_l x_i^l \\left(y^l -\\hat{P} \\left(y^l =1\\mid {\\mathit{\\mathbf{X}}}^l ,\\mathit{\\mathbf{W}}\\right)\\right)-\\lambda w_i\n\n\n\n2.2 Summary\n\nIn general, NB and LR make different assumptions\n\nNB: Features independent given class -> assumption on P(X\\mid Y)\nLR: Functional form of P(Y\\mid X), no assumption on P(X\\mid Y)\n\nLR is a linear classifier\n\ndecision rule is a hyperplane\n\nLR optimized by conditional likelihood\n\nno closed-form solution\nConcave (convex) -> global optimum with gradient ascent (descent)\n\nLR can be extended to multiple class using softmax."
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-11-05-CS5590-week9.html#clustering",
    "href": "Data_Science_Notes/Machine-Learning/2022-11-05-CS5590-week9.html#clustering",
    "title": "Machine Learning 9",
    "section": "3 Clustering",
    "text": "3 Clustering\nTypes of Clustering Methods\n\nIn terms of objective:\n\nMonothetic: cluster members have some common property\n\nE.g. All are males aged 20-35, or all have X% response to test B\n\nPolythetic: cluster members are similar to each other\n\nDistance between elements defines membership\n\n\nIn terms of overlap of clusters\n\nHard clustering: clusters do not overlap\nSoft clustering: clusters may overlap\n\n“Strength of association” between element and cluster\n\n\nIn terms of methodology\n\nFlat/partitioning (vs) hierarchical: Set of groups (vs) taxonomy\nDensity-based (vs) Model/Distribution-based: DBSCAN vs GMMs\nConnectionist (vs) Centroid-based: k-means vs Hierarchical clustering\n\n\nOutline\n\nK-Means\nHierarchical Clustering\nGraph-based/Spectral Clustering\nDBSCAN\nModel-based Clustering (GMM and Expectation Maximization)\nEvaluation of Clustering Algorithms\n\n\n3.1 k-Means Clustering\n\nPartitional clustering approach\nEach cluster is associated with a centroid (center point)\nEach point is assigned to the cluster with the closest centroid\nNumber of clusters, K, must be specified\nThe basic algorithm is very simple\n\nSelect K points as initial centroids\nRepeat:\n\nForm K clusters by assigning all point to the closest centroid\nRecompute centroid for each cluster.\n\nuntil The centroid doesn’t change.\n\nInitial centroids are often chosen randomly.\n\nClusters produced can vary from one run to another.\nThe centroid is (typically) the mean of the points in the cluster.\n\n‘Closeness’ is measured by Euclidean distance, cosine similarity,correlation, etc.\nK-means will converge for common similarity measures mentioned above (local minimum though)\nMost of the convergence happens in the first few iterations.\n\nOften the stopping condition is changed to ‘Until relatively few points change clusters’\n\nNearby points may not end up in the same cluster\n\n\n3.1.1 Selecting Initial Centroids\nHow difficult is this?\n\nIf there are K ‘real’ clusters then the chance of selecting one centroid from each cluster is small\n\nChance is relatively small when K is large\nIf clusters are the same size, n, then \\begin{align*}\np&=\\frac{\\text{number of ways to select one centroid from each cluster}}{\\text{Number of ways to select }K \\text{ centroid}}\\\\\n&=\\frac{K!n^K}{(Kn)^K}\\\\\n&=\\frac{K!}{K^K}\n\\end{align*}\nFor example, if K = 10, then probability = 10!/10^{10} = 0.00036\n\n\n\n\n3.1.2 Possible Solutions\n\nMultiple runs\n\nHelps, but probability is not on our side\n\nSample and use hierarchical clustering to determine initial centroids\nSelect more than k initial centroids and then select among these initial centroids\n\nSelect most widely separated\n\nBisecting $K-means\n\nNot as susceptible to initialization issues\n\n\n\n\n3.1.3 Evaluating k-Means Clusters\n\nMost common measure is Sum of Squared Error (SSE)\n\nFor each point, the error is the distance to the nearest cluster\nTo get SSE, we square these errors and sum them. \\mathrm{SSE}=\\sum_{i=1}^K\\sum_{i\\in C_i}\\mathrm{dist}^2(m_i,x)\n\nx is a data point in cluster C_i and m_i is the representative point for cluster C_i\nCan show that m_i corresponds to the center (mean) of the cluster\nGiven two clusterings, we can choose the one with the smaller error\nOne easy way to reduce SSE is to increase K, the number of clusters\nA good clustering with smaller K can have a lower SSE than a poor clustering with higher K\nRelatively faster than other clustering methods: O( # iterations * # clusters * # instances * # dimensions )\n\n\n\n3.1.4 Limitations\n\nk-Means has problems when clusters are of differing\n\nSizes, Densities, Non-globular shapes\n\nSensitive to outliers\nThe number of clusters (K) is difficult to determine \n\n\n\n3.1.5 Extensions\n\nUse of various distance metrics\n\nEuclidean distance  d(x,y)=\\sqrt{\\sum_{i=1}^n \\left\\lvert x_i-y_i \\right\\rvert^2}\nManhattan (city-block) distance d(x,y)=\\sum_{i=1}^n \\left\\lvert x_i-y_i \\right\\rvert\nCosine distance \\begin{align*}\n  &\\cos(x,y)=\\frac{\\sum_{i=1}^nx_iy_i}{\\sqrt{\\sum_{i=1}^nx_i^2}\\sqrt{\\sum_{i=1}^ny_i^2}}\\\\\n  &d(x,y)=1-\\cos(x,y)\n  \\end{align*}\nChebyshev distance \\mathrm{dist}(x_i,x_j)=\\max(\\left\\lvert x_{i1}-x_{j1} \\right\\rvert ,\\left\\lvert x_{i2}-x_{j2} \\right\\rvert ,\\dots ,\\left\\lvert x_{ir}-x_{jr} \\right\\rvert) \n\nk-Medioids\nBisecting k-Means\nk-Means ++\n\n\n\n\n3.2 Hierarchical Clustering\nAs we do not know how many clusters to use, so the idea of hierarchical clustering is to use all k clusters hierarchically.\n\nTypes of Clustering Methods\n\n\nHierarchical Clustering\n\nProduces a set of nested clusters organized as a hierarchical tree\nCan be visualized as a dendrogram\n\nA tree like diagram that records the sequences of merges or splits \n\n\n\n3.2.1 Strengths\n\nDo not have to assume any particular number of clusters\n\nAny desired number of clusters can be obtained by ‘cutting’ the dendrogram at the proper level\n\nThey may correspond to meaningful taxonomies\n\nExample in biological sciences (e.g., animal kingdom, phylogeny reconstruction, …)\n\n\n\n\n3.2.2 Two main types of hierarchical clustering\n\nAgglomerative:\n\nStart with the points as individual clusters\nAt each step, merge the closest pair of clusters until only one cluster (or k clusters) left\n\nDivisive:\n\nStart with one, all-inclusive cluster\nAt each step, split a cluster until each cluster contains a point (or there are k clusters)\n\n\n\nTraditional hierarchical algorithms use a similarity or distance matrix\n\nMerge or split one cluster at a time\n\n\n\n\n3.2.3 Agglomerative Clustering Algorithm\n\nMore popular hierarchical clustering technique\nBasic algorithm is straightforward\n\nCompute the proximity matrix\nLet each data point be a cluster\nRepeat\n\nMerge the two closest clusters\nUpdate the proximity matrix\n\nUntil only a single cluster remains\n\nKey operation is the computation of the proximity of two clusters\n\nDifferent approaches to defining the distance between clusters distinguish the different algorithms\n\n\n\n\n3.2.4 Methodology\n\nStart with clusters of individual points and a proximity matrix (pairwise distance between each data point) \nAfter some merging steps, we have some clusters \nWe want to merge the two closest clusters (C2 and C5) and update the proximity matrix. \nThe question is “How do we update the proximity matrix?” \n\nWe can update the proximity matrix using Inter-cluster Similarity\n\n\n3.2.5 Inter-cluster Similarity\n\nMIN (Single-link) \nMAX (Complete-link) \nGroup Average (Average-link) \nDistance Between Centroids \n\n\n\n3.2.6 Limitations\n\nOnce a decision is made to combine two clusters, it cannot be undone\nNo objective function is directly minimized\nDifferent schemes have problems with one or more of the following:\n\nSensitivity to noise and outliers (MIN)\nDifficulty handling different sized clusters and non-convex shapes (Group average, MAX)\nBreaking large clusters (MAX)"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-11-05-CS5590-week9.html#graph-basedspectral-clustering",
    "href": "Data_Science_Notes/Machine-Learning/2022-11-05-CS5590-week9.html#graph-basedspectral-clustering",
    "title": "Machine Learning 9",
    "section": "4 Graph-based/Spectral Clustering",
    "text": "4 Graph-based/Spectral Clustering\n\nAssociate each data item with a vertex in a weighted graph\n\nweights on the edges between elements are large if the elements are similar and small if they are not.\n\nCut the graph into connected components with relatively large interior weights by cutting edges with relatively low weights.\nClustering becomes a graph cut problem."
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-08-06-CS5590-week1.html",
    "href": "Data_Science_Notes/Machine-Learning/2022-08-06-CS5590-week1.html",
    "title": "Machine Learning 1",
    "section": "",
    "text": "Supervised Learning\n\nclassification, regression\n\nUnsupervised learning\nOther settings of ML\n\nReinforcement learning\nSemi-supervised learning\nActive learning,Transfer learning,Structured learning\n\nDimensionality Reduction (unsupervised Learning)\n\nLarge sample size is required for high dimensional data\nQuery accuracy and efficiency degrade rapidly as the dimension increases\nstrategies:\n\nFeature reduction, Feature selection, Manifold learning, Kernel learning"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-08-06-CS5590-week1.html#iid-assumption",
    "href": "Data_Science_Notes/Machine-Learning/2022-08-06-CS5590-week1.html#iid-assumption",
    "title": "Machine Learning 1",
    "section": "2 IID Assumption",
    "text": "2 IID Assumption\n\nIdentically independently distributed : This is the assumption that the training data and testing data comes from the same distribution"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-08-06-CS5590-week1.html#types-of-models",
    "href": "Data_Science_Notes/Machine-Learning/2022-08-06-CS5590-week1.html#types-of-models",
    "title": "Machine Learning 1",
    "section": "3 Types of Models",
    "text": "3 Types of Models\n\nInduction : Model Learns by Induction, ( creating it’s own rules for example, if we do extensive research while buying mobile, we create set rules, it is called induction).\nTransductions: Model learns from references ( for example, if we ask our friends about mobile and we buy according to their suggestion, it is called Transduction).\nOnline : data could be a stream, data keeps coming over time.\nOffline: data is already acquired and trained offline.\nGenerative: Learns the distribution, the model learns joint probability distribution.\nDiscriminative:Learns to discriminate without learning distribution.\nParametric : The model have parameters like \\mu and \\sigma\nNon parametric: The model doesn’t have parameters, as in K nearest neighbor (KNN)"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-08-06-CS5590-week1.html#classifier-evaluation",
    "href": "Data_Science_Notes/Machine-Learning/2022-08-06-CS5590-week1.html#classifier-evaluation",
    "title": "Machine Learning 1",
    "section": "4 Classifier evaluation",
    "text": "4 Classifier evaluation\n\n\nTraining Error\n\nNot very useful\nRelatively easy to obtain low error\nE_{\\mathrm{train}} =\\frac{1}{n}\\sum_{i=1}^n \\mathrm{error}\\left(f_D \\left(X_i \\right),y_i \\right)\n\nGeneralization Error\n\nMeasure of how well do we do on unseen data\nE_{\\mathrm{gen}} =\\int \\mathrm{error}\\left(f_D \\left(X\\right),y\\right)p\\left(y,X\\right)\\mathrm{dX}"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-08-06-CS5590-week1.html#stratified-sampling",
    "href": "Data_Science_Notes/Machine-Learning/2022-08-06-CS5590-week1.html#stratified-sampling",
    "title": "Machine Learning 1",
    "section": "5 Stratified sampling",
    "text": "5 Stratified sampling\n\nFirst stratify instances by class, then randomly select instances from each class proportionally\nIt ensures that the proportion of each class remains same in training and validation set."
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-08-06-CS5590-week1.html#model-selection",
    "href": "Data_Science_Notes/Machine-Learning/2022-08-06-CS5590-week1.html#model-selection",
    "title": "Machine Learning 1",
    "section": "6 Model Selection",
    "text": "6 Model Selection\n\nRe-Substitution : not useful as it suggests to re- substitute the train data for validation as well\nK - Fold cross-validation : Divide the data in K fold using stratified sampling, and the select some set for training and some for validation in each iteration.\nLeave-one-out\n\nN-fold cross-validation\n\n\n \\tiny {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-08-20-CS5590-week2.html",
    "href": "Data_Science_Notes/Machine-Learning/2022-08-20-CS5590-week2.html",
    "title": "Machine Learning 2",
    "section": "",
    "text": "Classification\n\nIt is a measure of being right/wrong,0-1, eg: hinge loss, cross entropy loss\n\nRegression loss\n\nIt is a measure if how close we are to target, eg: MEA, MES\n\nRanking/search\n\nIt is a measure of top K search\n\nClustering\n\nHow well we have described the data ( not straight forward)"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-08-20-CS5590-week2.html#is-accuracy-adequate",
    "href": "Data_Science_Notes/Machine-Learning/2022-08-20-CS5590-week2.html#is-accuracy-adequate",
    "title": "Machine Learning 2",
    "section": "2 Is accuracy adequate",
    "text": "2 Is accuracy adequate\nAccuracy may not not be useful in cases where:\n\nThere is a large class skew.\nThere are differential misclassification cost, say getting a positive wrong costs more than getting a negative wrong.\nwe are most interested in a subset of high confidence predictions."
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-08-20-CS5590-week2.html#classification-error",
    "href": "Data_Science_Notes/Machine-Learning/2022-08-20-CS5590-week2.html#classification-error",
    "title": "Machine Learning 2",
    "section": "3 Classification Error",
    "text": "3 Classification Error\n\n\n\n\n\n\n\nTip\n\n\n\nPrecision = How many retrieved items are relevant?  Recall = How many relevant items are retrieved?\n\n\n\n\n\n\n\n\nTip\n\n\n\nsensitivity = Probability of positive test given a patient has a disease. Specificity = Probability of a negative test given a patient is well.Specificity = 1 - False Alarm"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-08-20-CS5590-week2.html#utility-and-cost",
    "href": "Data_Science_Notes/Machine-Learning/2022-08-20-CS5590-week2.html#utility-and-cost",
    "title": "Machine Learning 2",
    "section": "4 Utility and cost",
    "text": "4 Utility and cost\n\nDetection Cost:\n\ncost = C_{\\mathrm{FP}} \\times \\mathrm{FP}+C_{\\mathrm{FN}} \\times \\mathrm{FN}\n\nFmeasure\n\nF1=\\frac{2\\times \\left(\\mathrm{Recall}\\times \\mathrm{Precision}\\right)}{\\mathrm{Recall}+\\mathrm{Precision}}"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-08-20-CS5590-week2.html#roc-curve",
    "href": "Data_Science_Notes/Machine-Learning/2022-08-20-CS5590-week2.html#roc-curve",
    "title": "Machine Learning 2",
    "section": "5 ROC curve",
    "text": "5 ROC curve\n\nReceiver Operative Curve\nPlot between True positive rate on y axis and False positive rate on x axis\nAUC : Area under the curve, higher the area, better the performance\n\nA Receiver Operating Characteristic (ROC) curve plots the TP rate vs. the FP rate as a threshold on the confidence of an instance being positive is varied :"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-08-20-CS5590-week2.html#precision-recall-curve",
    "href": "Data_Science_Notes/Machine-Learning/2022-08-20-CS5590-week2.html#precision-recall-curve",
    "title": "Machine Learning 2",
    "section": "6 Precision Recall Curve",
    "text": "6 Precision Recall Curve\n\nPlot between Precision on y axis and recall (TPR) on x axis. It is used for class imbalanced dataset mostly, It is also used when we can not calculate True Negative.\n\nA precision/recall curve plots the precision vs. recall (TP rate) as a threshold on the confidence of an instance being positive is varied."
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-08-20-CS5590-week2.html#a-nice-way-to-define-precision-recall-etc.",
    "href": "Data_Science_Notes/Machine-Learning/2022-08-20-CS5590-week2.html#a-nice-way-to-define-precision-recall-etc.",
    "title": "Machine Learning 2",
    "section": "7 A nice way to define precision recall etc.",
    "text": "7 A nice way to define precision recall etc.\nConsider Y as true label and \\hat{Y} as predicted label.\n\nprecision =P\\left(Y=1|\\;\\hat{Y} =1\\right)\nRecall (TPR) =P\\left(\\hat{Y} =1|\\;Y=1\\right)\nFalse positive Rate (FPR) =P\\left(\\hat{Y} =1|\\;Y=0\\right)\nTrue Negative Rate (TNR)= =P\\left(\\hat{Y} =0|\\;Y=0\\right) \n\n\n\n\n\n\n\nNote\n\n\n\nNotice that TPR and FPR which makes ROC curve is conditioned on the true value, and precision which is used in PR curve is conditioned on predicted label. This is the reason, PR curve is used for class imbalanced dataset, or where positive class is more interesting then negative class. If question is: “How meaningful is a positive result from my classifier given the baseline probabilities of my problem?”, use a PR curve. If question is, “How well can this classifier be expected to perform in general, at a variety of different baseline probabilities?”, go with a ROC curve."
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-08-20-CS5590-week2.html#roc-curve-vs-pr-curve",
    "href": "Data_Science_Notes/Machine-Learning/2022-08-20-CS5590-week2.html#roc-curve-vs-pr-curve",
    "title": "Machine Learning 2",
    "section": "8 ROC curve vs PR curve",
    "text": "8 ROC curve vs PR curve\nConsider a dataset having 100 positive cases and 10,000 negative cases. Now consider 2 classifiers A and B. A predicts 9 as true positive, 40 as false positive whereas B predicts 9 as true positive 1000 as false positive. We can observe that as both the classifier predicts 9 out of 10 as true positive so both has same recall value, also FPR is small(as it can be seen in below picture), so the ROC curve which is drawn between recall (TPR) and FPR, will not differentiate among the two classifier.  But PR curve which is drawn between precision and recall, will look totally different here as both has different precision.\n\n\nCode\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef get_conf_matrix_labels(cf_matrix):\n    group_names = ['True Neg','False Pos','False Neg','True Pos']\n    group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]\n    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n    labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(cf_matrix.shape[0],cf_matrix.shape[1])\n    accuracy  = np.trace(cf_matrix) / float(np.sum(cf_matrix))\n    precision = cf_matrix[1,1] / sum(cf_matrix[:,1])\n    recall    = cf_matrix[1,1] / sum(cf_matrix[1,:])\n    fpr    = cf_matrix[0,1] / sum(cf_matrix[0,:])\n    stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nFPR={:0.4f}\".format(\n                accuracy,precision,recall,fpr)    \n    return labels,stats_text\n\n\nYTrue = np.hstack([np.ones(10),np.zeros(100000)]) # data with 100 positive and 10,000 negative cases\nyPredA = np.hstack([np.ones(9),np.zeros(1),np.ones(40),np.zeros(99960)]) # A predicts 9 True positive, 40 False  Positive,\nyPredB = np.hstack([np.ones(9),np.zeros(1),np.ones(1000),np.zeros(99000)]) # B predicts 9 True positive, 1000 False positive\ncf_matrixA = confusion_matrix(YTrue, yPredA)\ncf_matrixB = confusion_matrix(YTrue, yPredB)\n\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\naxis_labels = ['Negative Class','Positive Class']\nlabels,stats_text=get_conf_matrix_labels(cf_matrixA)\nsns.heatmap(cf_matrixA, annot=labels, fmt='', cmap='Blues',ax=ax[0],cbar=False,xticklabels=axis_labels, yticklabels=axis_labels)\nax[0].set_title('Classifier A')\nax[0].set_xlabel('Predicted label'+stats_text)\nax[0].set_ylabel('True label')\nlabels,stats_text=get_conf_matrix_labels(cf_matrixB)\nsns.heatmap(cf_matrixB, annot=labels, fmt='', cmap='Blues',ax=ax[1],cbar=False,xticklabels=axis_labels, yticklabels=axis_labels)\nax[1].set_title('Classifier B')\nax[1].set_xlabel('Predicted label'+stats_text)\nax[1].set_ylabel('True label');\n\n\n\n\n\nAccuracy also is not a good measure in above case. As both the classifier has similar accuracy."
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-08-20-CS5590-week2.html#other-performance-measures",
    "href": "Data_Science_Notes/Machine-Learning/2022-08-20-CS5590-week2.html#other-performance-measures",
    "title": "Machine Learning 2",
    "section": "9 Other performance measures",
    "text": "9 Other performance measures\n\nKullback-Leibler Diverfence : D_{\\mathrm{KL}} \\left(P\\|Q\\right)=\\sum_i P\\left(i\\right)\\log \\frac{P\\left(i\\right)}{Q\\left(i\\right)}\nGini Statistic : 2\\times \\mathrm{AUC}-1\nF1 score: \\frac{2\\times \\left(\\mathrm{Recall}\\times \\mathrm{Precision}\\right)}{\\mathrm{Recall}+\\mathrm{Precision}}\nAkaike Information Criterion (AIC): 2k-2\\ln \\left(L\\right), here k is number of model parameters, L is max value of the Likelihood function for the model"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-08-20-CS5590-week2.html#important-points",
    "href": "Data_Science_Notes/Machine-Learning/2022-08-20-CS5590-week2.html#important-points",
    "title": "Machine Learning 2",
    "section": "10 Important points",
    "text": "10 Important points\n\nRandomization of data is essential so that held-aside test data can be really representative of new data.\nTest set should never be used in any way for normalization, hyper parameter tuning etc.\nAny preprocessing done over entire data set ( feature selection, parameter tuning, threshold selection ) must not use labels from test set."
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-08-20-CS5590-week2.html#k-nearest-neighbors",
    "href": "Data_Science_Notes/Machine-Learning/2022-08-20-CS5590-week2.html#k-nearest-neighbors",
    "title": "Machine Learning 2",
    "section": "11 K-Nearest Neighbors",
    "text": "11 K-Nearest Neighbors\n\n11.1 basic idea\n\nIf it walks like a duck, quacks like a duck , then it’s probably a duck.\nIf data points are represented well then KNN works well.\nchoosing K is important, if K is too small then it becomes sensitive to noise point. If k is too large, neighborhood may incudes points from other class.\nEuclidean distance between two instance d\\left(X_i ,X_j \\right)=\\sqrt{\\sum_{r=1}^n {\\left(a_r \\left(X_i \\right)-a_r{\\left(X_j \\right)} \\right)}^2 } here a_i \\left(X\\right)\\; denotes features.\nIn case of continuous valued target function, Mean value of K nearest training examples is taken\n\n\n\n11.2 How to determinke K\n\nexperiment with different value of K starting form 1 on test set to validate the error, in case of binary classification use odd number for k to avoid ties.\n\nKNN is a transductive method, there is no training involved , it is refereed as Lazy learning, Learning is just storing all the training instances  Similar Keywords: KNN, Memory Based Reasoning, Example Based Reasoning, Instance Based Learning, Case Based Reasoning, Lazy Learning Voronoi Diagram: Decision surface formed by the training Examples for 1 nearest neighbors classifier\n\n\n11.3 Improvements\n\nDistance weighted Nearest Neighbors\nScaling (normalization) attributes for fair computation fo distances\nMeasure “closeness” differently\nFinding “close” example in large training set quickly , eg Efficient memory indexing using kd-tree\n\n\n\n11.4 Pros\n\nHighly effective transductive inference method for noisy training data and complex target functions.\nTarget function for a whole space may be described as a combinations of less complex local approximations\nTrains very fast (Lazy Learner)\n\n\n\n11.5 Cons\n\nCurse of dimensionality\nStorage: all training example are saved in memory\nslow at query time, can be overcome by pre sorting and indexing training samples.\n\n\n\n11.6 Convergence of 1-NN\nP\\left(\\mathrm{KNNError}\\right)=2\\left(\\mathrm{Bayes}\\;\\mathrm{Optimal}\\;\\mathrm{Error}\\;\\mathrm{Rate}\\right) Probability of K NN error is at most twice the bayes optimal error, bayes optimal error is best(least) error we can get using machine learning  It is Possible to show that: as the size of training data set approaches infinity, the one nearest neighbor classifier guarantees an error rate of no worse than twice the bayes error rate ( the minimum achievable error rate given the distribution of the data).\n\n\n11.7 Density Estimation using KNN\nNon parametric Density Estimation using KNN.  nstead of fixing bin width h and counting the mumber of instances, fix the instances(neighbors) k and check bin width \\hat{p} \\left(X\\right)=\\frac{k}{2{\\mathrm{Nd}}_k \\left(X\\right)} here d_k \\left(X\\right) is the k_{\\mathrm{th}} closest distance to to X , This is also known as Parzen density estimation.\n\n\n11.8 KNN example using sklearn\nImport libraries\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn import neighbors, datasets\niris = datasets.load_iris()\n\nPrint shapes and class information\n\nprint('features: ',iris.feature_names)\nprint('target: ',iris.target)\nprint('classess: ',iris.target_names)\nX = iris.data\ny = iris.target\nprint('input  data shape:',X.shape)\nprint('target shape: ',y.shape)\n\nfeatures:  ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\ntarget:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\nclassess:  ['setosa' 'versicolor' 'virginica']\ninput  data shape: (150, 4)\ntarget shape:  (150,)\n\n\nPlot the data\n\nplt.scatter(iris.data[:,1],iris.data[:,2],c=iris.target, cmap=plt.cm.Paired)\nplt.xlabel(iris.feature_names[1])\nplt.ylabel(iris.feature_names[2])\nplt.show()\n\nplt.scatter(iris.data[:,0],iris.data[:,3],c=iris.target, cmap=plt.cm.Paired)\nplt.xlabel(iris.feature_names[0])\nplt.ylabel(iris.feature_names[3])\nplt.show()\n\n\n\n\n\n\n\nsplit the dataset into test set and train set\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)\n\nFit the classifier with different values of k\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nk_range = range(1,26)\nscores = {}\nscores_list = []\nfor k in k_range:\n        knn = KNeighborsClassifier(n_neighbors=k)\n        knn.fit(X_train,y_train)\n        y_pred=knn.predict(X_test)\n        scores[k] = metrics.accuracy_score(y_test,y_pred)\n        scores_list.append(metrics.accuracy_score(y_test,y_pred))\n\nPlot the scores:\n\n#plot the relationship between K and the testing accuracy\nplt.plot(k_range,scores_list)\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Testing Accuracy');\n\n\n\n\nchose the best value of K for final model.\n\n\n\n\n\n\nTip\n\n\n\nIn KNN, finding the value of k is not easy. A small value of k means that noise will have a higher influence on the result and a large value make it computationally expensive. Data scientists usually choose as an odd number if the number of classes is 2 and another simple approach to select k is set k=sqrt(n). There is one more widely used method called Elbow Method which is also used to find the value of K, here we plot error rate vs k value and chose k value at elbow point.\n\n\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train,y_train);\n\n\n# Confusion metrics\nfrom sklearn.metrics import confusion_matrix\nprint('confusion matrix on test data: ')\nprint(confusion_matrix(y_test, y_pred))\n\nconfusion matrix on test data: \n[[11  0  0]\n [ 0 13  0]\n [ 0  0  6]]\n\n\n\nfrom sklearn.metrics import classification_report\nprint('classification report on test data:')\nprint(classification_report(y_test, y_pred, target_names=iris.target_names))\n\nclassification report on test data:\n              precision    recall  f1-score   support\n\n      setosa       1.00      1.00      1.00        11\n  versicolor       1.00      1.00      1.00        13\n   virginica       1.00      1.00      1.00         6\n\n    accuracy                           1.00        30\n   macro avg       1.00      1.00      1.00        30\nweighted avg       1.00      1.00      1.00        30\n\n\n\n \\tiny {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-09-10-CS5590-week4.html",
    "href": "Data_Science_Notes/Machine-Learning/2022-09-10-CS5590-week4.html",
    "title": "Machine Learning 4",
    "section": "",
    "text": "It is a discriminative classifier.\nInspired by Statistical Learning.\nDeveloped in 1992 by Vapnik, Guyon, Boser\nWas one of the go-to methods in ML since mid 1990s (only recently displaced by deep learning.)"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-09-10-CS5590-week4.html#maximum-margin-classifier",
    "href": "Data_Science_Notes/Machine-Learning/2022-09-10-CS5590-week4.html#maximum-margin-classifier",
    "title": "Machine Learning 4",
    "section": "2 Maximum Margin Classifier",
    "text": "2 Maximum Margin Classifier\n\nFormulation \n\nf(\\mathbf{X},\\mathbf{W},b)=\\mathrm{sign} (\\mathbf{W} \\cdot \\mathbf{X}+b) \nBasic formulation of SVM can only handle two classes.\nThere are improvised method to handle more than tow class.\nThe Maximum margin classifier is the linear classifier with the maximum margin. This is the simplest kind of SVM ( called an LSVM)."
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-09-10-CS5590-week4.html#estimate-the-margin",
    "href": "Data_Science_Notes/Machine-Learning/2022-09-10-CS5590-week4.html#estimate-the-margin",
    "title": "Machine Learning 4",
    "section": "3 Estimate the Margin",
    "text": "3 Estimate the Margin\n\nThe points those lies on the two margin lines are called support vector.\nThe model is immune to removal of any non-support-vector data points.\nThe equation of the line is given by \\mathbf{W}^T \\cdot \\mathbf{X}+b=0 \n\\mathbf{W} is always normal to the line \\mathbf{W}^T \\cdot \\mathbf{X}+b=0 This can be proved by taking two vector \\mathbf{X_1} and \\mathbf{X_2} on line \\mathbf{W}^T \\cdot \\mathbf{X}+b=0, Now if we subtract the two vector we get \\mathbf{W}^T(\\mathbf{X_1}-\\mathbf{X_2})=\\mathbf{0} \\Leftrightarrow (\\mathbf{X_1}-\\mathbf{X_2}) \\perp \\mathbf{W}. The same is explained on stack overflow.\nDotted line \\mathbf{X'-X} is perpendicular to decision boundary so parallel to \\mathbf{W} let it’s length (magnitude) be r\nThe Unit vector along Dotted line \\mathbf{X'-X} is given by \\frac{\\mathit{\\mathbf{W}}}{\\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert }\nThe equation of the dotted line \\mathbf{X'-X} can be also given by magnitude multiplied by unit vector :  \\displaystyle r\\cdot \\frac{ \\mathit{\\mathbf{W}}}{\\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert }\nBut as the dotted line can be on any side of the main line so we need to multiply with y, as y takes value of 1 or -1 depending on the side:  \\displaystyle \\mathbf{X'-X} = yr\\cdot \\frac{ \\mathit{\\mathbf{W}}}{\\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert } \\displaystyle \\mathbf{X'} = \\mathbf{X} - yr\\cdot \\frac{ \\mathit{\\mathbf{W}}}{\\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert }\nNow since \\mathbf{X'} lies on the line so we can write \\mathbf{W}^T \\cdot \\mathbf{X'}+b=0\nSubstituting value of \\mathbf{X'} in \\mathbf{W}^T \\cdot \\mathbf{X'}+b=0 we get:  \\displaystyle \\mathbf{W}^T \\cdot \\left( \\mathbf{X} - yr\\cdot \\frac{ \\mathit{\\mathbf{W}}}{\\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert } \\right)+b=0\nsubstituting \\displaystyle \\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert =\\sqrt{\\mathit{\\mathbf{W}}^T \\mathit{\\mathbf{W}}} in above equation we get:  \\displaystyle \\mathbf{W}^T \\cdot \\left( \\mathbf{X} - yr\\cdot \\frac{ \\mathit{\\mathbf{W}}}{\\sqrt{\\mathit{\\mathbf{W}}^T \\mathit{\\mathbf{W}}}} \\right)+b=0 \\displaystyle \\left( \\mathbf{W}^T \\mathbf{X} - yr\\cdot \\frac{ \\mathit{\\mathbf{W}}^T\\mathit{\\mathbf{W}}}{\\sqrt{\\mathit{\\mathbf{W}}^T \\mathit{\\mathbf{W}}}} \\right)+b=0 \\displaystyle \\left( \\mathbf{W}^T \\mathbf{X} - yr\\cdot \\sqrt{\\mathit{\\mathbf{W}}^T \\mathit{\\mathbf{W}}} \\right)+b=0 \\displaystyle \\mathbf{W}^T \\mathbf{X} - yr\\cdot \\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert +b=0 \\displaystyle \\mathbf{W}^T \\mathbf{X} +b = yr\\cdot \\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert \\displaystyle r = \\frac{\\mathbf{W}^T \\mathbf{X} +b}{y\\cdot \\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert}\nSince y takes value of only 1 or -1, hence we can bring y to numerator. \n\\displaystyle r = y \\frac{\\mathbf{W}^T \\mathbf{X} +b}{\\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert}\nSince \\mathbf{W}^T \\cdot \\mathbf{X}+b=0 and c\\left(\\mathbf{W}^T \\cdot \\mathbf{X}\\right)+b=0 define the same plane, we have the freedom to choose the normalization of \\mathbf{W}\nLet us choose normalization such that \\mathbf{W}^T \\cdot \\mathbf{X}_+ + b = +1 and \\mathbf{W}^T \\cdot \\mathbf{X}_- +b = -1 for the positive and negative support vectors respectively.  Hence, Margin now is:  \\displaystyle \\left( +1 \\right) \\frac{\\mathbf{W}^T \\mathbf{X_+} +b}{ \\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert} + \\left( -1 \\right) \\frac{\\mathbf{W}^T \\mathbf{X_-} +b}{ \\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert} Since \\mathbf{W}^T \\mathbf{X_+} +b=+1 and \\mathbf{W}^T \\mathbf{X_-} +b=-1, substituting these in above equation we get:  \\displaystyle \\left( +1 \\right) \\frac{\\mathbf{W}^T \\mathbf{X_+} +b}{ \\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert} + \\left( -1 \\right) \\frac{\\mathbf{W}^T \\mathbf{X_-} +b}{ \\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert}=\\displaystyle \\left( +1 \\right) \\frac{+1}{ \\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert} + \\left( -1 \\right) \\frac{-1}{ \\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert}=\\frac{2}{\\left\\lVert \\mathbf{W}\\right\\rVert}\n\n\n\n\n\n\n\nTip\n\n\n\nMargin between the two support vector is given by: \\displaystyle    \\frac{2}{\\left\\lVert \\mathbf{W}\\right\\rVert}"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-09-10-CS5590-week4.html#maximize-the-margin",
    "href": "Data_Science_Notes/Machine-Learning/2022-09-10-CS5590-week4.html#maximize-the-margin",
    "title": "Machine Learning 4",
    "section": "4 Maximize the Margin",
    "text": "4 Maximize the Margin\n\nNow we know the margin between the two support vector.\nWe need to maximize the margin in such a way that +1 class points lies on one side of the margin and -1 class points lies on the other side of the margin.\nWe can formulate this as the quadratic optimization problem: Find \\mathbf{W} such that   \\displaystyle \\rho = \\frac{2}{\\left\\lVert \\mathbf{W}\\right\\rVert } is maximized; and for all \\left\\{ \\left( \\mathbf{X}_i,\\mathbf{y}_i \\right) \\right\\}  and \\mathbf{W}^T \\cdot \\mathbf{X}_i+b \\ge 1 if y_i=+1  and \\mathbf{W}^T \\cdot \\mathbf{X}_i+b \\le -1 if y_i=-1\n\nA better formulation is to minimize inverse of \\rho instead of maximizing it.\n\nWe know that  \\displaystyle \\max \\frac{2}{\\left\\lVert \\mathbf{W}\\right\\rVert } =\\min \\frac{\\left\\lVert \\mathbf{W}\\right\\rVert}{2} =\\min\\frac{\\sqrt{ \\mathbf{W}^T\\mathbf{W}}}{2}\nInstead of minimizing \\displaystyle \\frac {\\left\\lVert \\mathbf{W}\\right\\rVert}{2} we minimize \\displaystyle \\frac {\\left\\lVert \\mathbf{W}\\right\\rVert^2}{2} = \\displaystyle \\frac{ \\mathbf{W}^T\\mathbf{W}}{2} as both (with or without square) are equivalent. we select square one as math (derivative) becomes easy.\n\n\n\n\n\n\n\nTip\n\n\n\nMaximization problem can be written in terms of minimization as follows:Find \\mathbf{W} and b such that\\displaystyle \\frac{\\mathbf{W}^T\\mathbf{W}}{2} is minimized.and for all \\left\\{ \\left( \\mathbf{X}_i,\\mathbf{y}_i \\right) \\right\\} : \\displaystyle y_i\\left( \\mathbf{W}^T\\mathbf{X}_i +b\\right) \\ge 1"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-09-10-CS5590-week4.html#using-lagrange-multipliers",
    "href": "Data_Science_Notes/Machine-Learning/2022-09-10-CS5590-week4.html#using-lagrange-multipliers",
    "title": "Machine Learning 4",
    "section": "5 Using Lagrange Multipliers",
    "text": "5 Using Lagrange Multipliers\n\n5.1 Basics of Lagrange Multipliers\n\nOptimization problem: Minimize : \\displaystyle f\\left( \\overrightarrow{x} \\right)  Such that for all i, \\displaystyle g_i\\left( \\overrightarrow{x} \\right)\\le 0 \nTo solve the above problem we create augmented Lagrange function: \\displaystyle L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda} \\right):=f\\left( \\overrightarrow{x} \\right)+\\sum_{i=1}^{n}\\lambda_ig_i\\left( \\overrightarrow{x} \\right)  \\displaystyle \\underbrace{L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda} \\right)}_{\\text{lagrange function}} :=f\\left( \\overrightarrow{x} \\right)+\\sum_{i=1}^{n}\\underbrace{\\lambda_i}_{\\text{lagrange variable  or dual varialbe }}g_i \\left( \\overrightarrow{x} \\right) \nObservation: For any feasible x and all \\lambda_i \\ge 0,  \\displaystyle L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda} \\right):=f\\left( \\overrightarrow{x} \\right)+\\overbrace{\\sum_{i=1}^{n}\\overbrace{\\lambda_i}^{\\text{this is positve}} \\underbrace{g_i\\left( \\overrightarrow{x} \\right)}_{\\text{this is negative}}}^{\\text{This is negative}}  Hence , \\displaystyle L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda} \\right) \\le f\\left( \\overrightarrow{x} \\right)  \\displaystyle \\Longrightarrow \\max_{\\lambda_i \\ge 0} L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda} \\right) \\le f\\left( \\overrightarrow{x} \\right) \nSo, the optimal value to the constrained optimization: \\displaystyle p^*:=\\min_{\\overrightarrow{x} } \\max_{\\lambda_i \\ge 0} L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda} \\right)  We can see that now problem becomes unconstrained in x  Also p^* is called The primal problem\nObservation: consider a function: \\displaystyle \\min_{\\overrightarrow{x} } L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda} \\right) Since p^* is solution for maximum possible \\lambda so for any feasible x and all \\lambda_i \\ge 0 \\displaystyle p^* \\ge \\min_{\\overrightarrow{x} } L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda} \\right)  Thus: \\displaystyle d^*:= \\max_{\\lambda_i \\ge 0} \\min_{\\overrightarrow{x} } L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda} \\right) \\le p^*  Also d^* is called The dual problem\n\nIn short:\n\n\n\n\n\n\nNote\n\n\n\nOptimization problem:Minimize : \\displaystyle f\\left( \\overrightarrow{x} \\right) Such that for all i, \\displaystyle g_i\\left( \\overrightarrow{x} \\right)\\le 0 Lagrange Function : \\displaystyle L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda} \\right):=f\\left( \\overrightarrow{x} \\right)+\\sum_{i=1}^{n}\\lambda_ig_i\\left( \\overrightarrow{x} \\right)  Primal: \\displaystyle p^*:=\\min_{\\overrightarrow{x} } \\max_{\\lambda_i \\ge 0} L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda} \\right) Dual: \\displaystyle d^*:= \\max_{\\lambda_i \\ge 0} \\min_{\\overrightarrow{x} } L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda} \\right) \n\n\n\nTheorem (weak Lagrangian duality): d^* \\le p^* This is also called as minimax inequality p^*-d^* is called duality gap\nThere are certain condition when duality gap becomes zero, for that we need to understand convexity\n\nA function f:\\mathbb{R}^d \\rightarrow \\mathbb{R} is called convex iff for any two point x and x' and \\beta \\in \\left[ 0,1 \\right] f\\left( \\beta\\overrightarrow{x}+\\left( 1-\\beta \\right)\\overrightarrow{x} \\right) \\le \\beta f\\left( \\overrightarrow{x} \\right)+\\left( 1-\\beta \\right)f\\left( \\overrightarrow{x} \\right) \nA set S \\subset\\mathbb{R}^d is called conved iff for any tow points x, x' \\in S and any \\beta \\in \\left[ 0,1 \\right] \\beta \\overrightarrow{x}+\\left( 1-\\beta \\right)\\overrightarrow{x} \\in S \nConvex Optimization problem  \\displaystyle \\min_{\\overrightarrow{x} \\in \\mathbb{R}^d } f\\left( \\overrightarrow{x} \\right)  subject to: \\displaystyle g_i\\left( \\overrightarrow{x} \\right)\\le 0 for 1 \\le i \\le n is called convex optimization problem if:\n\nThe objective function f\\left( \\overrightarrow{x} \\right) is convex function, and\nthe feasible set induced by the constraints g_i is a convex set.\n\n\nTheorem (strong Lagrangian duality): if f is convex and for a feasible point x^* g_i\\left( \\overrightarrow{x^*} \\right)<0, or  g_i\\left( \\overrightarrow{x^*} \\right) \\le 0 when g is affine Then d^*=p^* This is called Slater’s condition.\n\n\n\n5.2 SVM standard (primal) form\n\\displaystyle \\min_{w,b}\\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2  such that: \\forall i, y_i\\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i} +b \\right) \\ge 1\n\nObservations :\n\nObjective function is convex\nthe constraints are affine, inducing a polytope constraint set.\n\nSo SVM is a convex optimization problem (in fact a quadratic program)\nMoreover, strong duality holds.\nLagrangian for SVM\n\nFor Lagrangian the constraint should always written as less than 0 format:  y_i\\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i} +b \\right) -1 \\ge 0  - y_i\\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i} +b \\right) +1 \\le 0  1 - y_i\\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i} +b \\right) \\le 0 \nNow the Lagrangian for SVM can be written as: \\displaystyle L\\left( \\overrightarrow{w},b,\\overrightarrow{\\alpha} \\right)=\\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 + \\underbrace{\\sum_{i = 1}^{n}\\alpha_i\\left( 1-y_i\\left( \\overrightarrow{w}\\cdot\\overrightarrow{x_i}+b \\right) \\right)}_{\\text{appears like a hinge loss}}\n\n\n\n\n5.3 SVM Dual\n\nPrimal \\displaystyle \\min_{\\overrightarrow{w},b }\\max_{\\overrightarrow{\\alpha } \\ge 0 }\\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 + \\sum_{i = 1}^{n}\\alpha_i\\left( 1-y_i\\left( \\overrightarrow{w}\\cdot\\overrightarrow{x_i}+b   \\right) \\right) \nDual \\displaystyle\\max_{\\overrightarrow{\\alpha } \\ge 0 } \\min_{\\overrightarrow{w},b }\\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 + \\sum_{i = 1}^{n}\\alpha_i\\left( 1-y_i\\left( \\overrightarrow{w}\\cdot\\overrightarrow{x_i}+b   \\right) \\right) \nSlater’s condition from convex optimization guarantees that these two optimization problems are equivalent!\n\n\n\n5.4 Solving using KKT condition\nKKT stands for Karush-Kuhn-Tucker Condition - We solve Dual problem: \\displaystyle\\max_{\\overrightarrow{\\alpha } \\ge 0 } \\min_{\\overrightarrow{w},b }\\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 + \\sum_{i = 1}^{n}\\alpha_i\\left( 1-y_i\\left( \\overrightarrow{w}\\cdot\\overrightarrow{x_i}+b \\right) \\right)\n\nWe can solve for optimal w, b as function of \\alpha \\displaystyle \\frac{\\partial L }{\\partial \\overrightarrow{w}}= w - \\sum_{i}\\alpha_iy_i\\overrightarrow{x}_i=0 \\Rightarrow w = \\sum_{i}\\alpha_iy_i\\overrightarrow{x}_i \\displaystyle \\frac{\\partial L }{\\partial b}= \\sum_{i}\\alpha_iy_i=0 \\Rightarrow \\sum_{i}\\alpha_iy_i =0\nsubstituting these values back in Dual we get:  \\displaystyle \\max_{\\overrightarrow{\\alpha } \\ge 0 } \\frac{1}{2} \\left( \\sum_{i}\\alpha_iy_i\\overrightarrow{x}_i \\right) \\cdot \\left( \\sum_{j}\\alpha_jy_j\\overrightarrow{x}_j \\right) + \\sum_{i = 1}^{n}\\alpha_i\\left( 1-y_i\\left( \\sum_{j}\\alpha_jy_j\\overrightarrow{x}_j \\cdot\\overrightarrow{x_i}+b \\right) \\right) \\displaystyle \\max_{\\overrightarrow{\\alpha } \\ge 0 } \\frac{1}{2} \\sum_{i,j}\\alpha_i\\alpha_jy_iy_j\\overrightarrow{x}_i\\cdot\\overrightarrow{x} _j + \\sum_{i = 1}^{n}\\alpha_i-\\sum_{i = 1}^{n} \\left( \\alpha_iy_i\\left( \\sum_{j}\\alpha_jy_j\\overrightarrow{x} _j\\cdot\\overrightarrow{x_i}+b\\right) \\right) \\displaystyle \\max_{\\overrightarrow{\\alpha } \\ge 0 } \\frac{1}{2} \\sum_{i,j}\\alpha_i\\alpha_jy_iy_j\\overrightarrow{x}_i\\cdot\\overrightarrow{x} _j + \\sum_{i = 1}^{n}\\alpha_i- \\sum_{i = 1}^{n}\\alpha_iy_i\\sum_{j}\\alpha_jy_j\\overrightarrow{x} _j\\cdot\\overrightarrow{x_i}+ \\sum_{i = 1}^{n}\\alpha_iy_ib \\displaystyle \\max_{\\overrightarrow{\\alpha } \\ge 0 } -\\frac{1}{2} \\sum_{i,j}\\alpha_i\\alpha_jy_iy_j\\overrightarrow{x}_i\\cdot\\overrightarrow{x} _j + \\sum_{i = 1}^{n}\\alpha_i + \\sum_{i = 1}^{n}\\alpha_iy_ib \\displaystyle \\max_{\\overrightarrow{\\alpha } \\ge 0 } \\sum_{i = 1}^{n}\\alpha_i -\\frac{1}{2} \\sum_{i,j}\\alpha_i\\alpha_jy_iy_j\\overrightarrow{x}_i\\cdot\\overrightarrow{x} _j\nThe above equation can also be written as:  \\displaystyle \\max \\sum_{k = 1}^{R}\\alpha_k - \\frac{1}{2}\\sum_{k=1}^{R}\\sum_{l = 1}^{R}\\alpha_k \\alpha_l Q_{kl}, where \\displaystyle Q_{kl} =y_ky_l\\left( \\mathbf{X_k}\\cdot \\mathbf{X_l} \\right) subject to constrains:\\alpha_k \\ge 0, and \\forall k , \\displaystyle \\sum_{k=1}^{R}\\alpha_ky_k=0\nAbove problem can be solved using SMO (Sequential minimal optimization) or any other quadratic programming or gradient descent.\nOnce we solve we get optimum \\alpha^*\nUsing \\alpha^* we can get w^* as below: \\displaystyle w^* = \\sum_{i}\\alpha_i^*y_i\\overrightarrow{x}_i\nb^* can be calculated as follows: y_i\\left( \\overrightarrow{w}^* \\cdot \\overrightarrow{x_i} +b \\right) = 1  y_i\\left( \\overrightarrow{w}^* \\cdot \\overrightarrow{x_i} \\right) + y_ib = 1  Multiplying y_i both the sides:  y_i y_i\\left( \\overrightarrow{w}^* \\cdot \\overrightarrow{x_i} \\right) + y_i y_ib = y_i  y_i y_ib = y_i -y_i y_i\\left( \\overrightarrow{w}^* \\cdot \\overrightarrow{x_i} \\right)  y_i y_i b can be written as b because y_i can be only \\pm 1 so in either case y_i y_i =1 b = y_i \\left( 1- y_i\\left( \\overrightarrow{w}^* \\cdot \\overrightarrow{x_i} \\right) \\right)  b^* = - y_i \\left( y_i\\left( \\overrightarrow{w}^* \\cdot \\overrightarrow{x_i} \\right)- 1 \\right) \nNow we can classify with: f(\\mathbf{X},\\mathbf{W}^*,b^*)=\\mathrm{sign} (\\mathbf{W}^* \\cdot \\mathbf{X}+b^*)"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-09-10-CS5590-week4.html#soft-margin-svm",
    "href": "Data_Science_Notes/Machine-Learning/2022-09-10-CS5590-week4.html#soft-margin-svm",
    "title": "Machine Learning 4",
    "section": "6 Soft Margin SVM",
    "text": "6 Soft Margin SVM\n\nTill now what we discussed is called Hard margin SVM.\nIn practice data is not always separable\nWe allow misclassification of the data point in soft margin SVM.\nSoft margin SVM is also called as C-SVM\nNow our Lagrangian is formulated as below: \\displaystyle \\min_{w,b,\\varepsilon}\\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 + c \\sum_{j=1}^{N}\\varepsilon_j  such that: \\forall i, y_i\\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i} +b \\right) \\ge 1-\\varepsilon _i, and \\varepsilon _i\\ge0 \nSignificance of \\varepsilon\n\n\\displaystyle \\min_{w,b,\\varepsilon}\\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 + \\overbrace{c}^{\\text{Controls amount of misclassification}} \\underbrace{\\sum_{j=1}^{N}\\varepsilon_j }_{\\text{Minimize }\\varepsilon \\text{ also  } } such that: \\forall i, y_i\\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i} +b \\right) \\ge 1-\\underbrace{\\varepsilon _i}_{\\text{to allow misclassification}} , and \\overbrace{\\varepsilon _i\\ge0}^{\\text{keep } \\varepsilon \\text{ positive} } \n\\varepsilon_i \\ge 1 \\Longleftrightarrow y_i \\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i} +b \\right) < 0, i.e., misclassification.\n0<\\varepsilon_i < 1 \\Longleftrightarrow x_i is correctly classified, but lies inside the margin\n\\varepsilon_i =0 \\Longleftrightarrow x_i is correctly classified, and lies outside of margin\n\\sum_{j=1}^{N}\\varepsilon_j is an upper bound on training errors.\n\n\n\n6.1 Lagrangian for Soft Margin SVM\n\nWe need to Solve: \\displaystyle \\min_{w,b,\\varepsilon}\\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 + c \\sum_{i=1}\\varepsilon_i  such that: \\forall i, y_i\\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i} +b \\right) \\ge 1-\\varepsilon _i, and \\varepsilon _i\\ge0 \nFor Lagrangian the constraint should always written as less than 0 condition:  y_i\\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i} +b \\right) \\ge 1-\\varepsilon _i, and \\varepsilon _i\\ge0  y_i\\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i} +b \\right) - 1 + \\varepsilon _i \\ge 0, and -\\varepsilon _i \\le 0  1 - \\varepsilon _i -y_i\\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i} +b \\right) \\le 0, and -\\varepsilon _i \\le 0 \nLagrangian formulation of above problem: \\displaystyle L\\left( \\overrightarrow{w},b,\\overrightarrow{\\alpha},\\overrightarrow{\\beta},\\overrightarrow{\\varepsilon } \\right)= \\\\ \\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 +c\\sum_{i}\\varepsilon_i + \\sum_{i = 1}^{n}\\alpha_i\\left( 1- \\varepsilon _i - y_i\\left( \\overrightarrow{w}\\cdot\\overrightarrow{x_i}+b \\right) \\right) -\\sum_{i}\\beta_i \\varepsilon_i\nPrimal \\displaystyle  \\min_{ \\left(  \\overrightarrow{w},b,\\overrightarrow{\\varepsilon } \\right)}\\max_{\\left(  \\overrightarrow{\\alpha } \\ge 0 ,\\overrightarrow{\\beta} \\ge 0  \\right)}\\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 +c\\sum_{i}\\varepsilon_i + \\sum_{i = 1}^{n}\\alpha_i\\left( 1- \\varepsilon _i - y_i\\left( \\overrightarrow{w}\\cdot\\overrightarrow{x_i}+b   \\right) \\right) -\\sum_{i}\\beta_i \\varepsilon_i   \nDual \\displaystyle  \\max_{\\left( \\overrightarrow{\\alpha } \\ge 0 , \\overrightarrow{\\beta} \\ge 0  \\right)} \\min_{\\left( \\overrightarrow{w},b,\\overrightarrow{\\varepsilon }  \\right)}\\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 +c\\sum_{i}\\varepsilon_i + \\sum_{i = 1}^{n}\\alpha_i\\left( 1- \\varepsilon _i - y_i\\left( \\overrightarrow{w}\\cdot\\overrightarrow{x_i}+b   \\right) \\right) -\\sum_{i}\\beta_i \\varepsilon_i   \nSame as Hard margin SVM we use KKT condition and solve minimization of dual: \\displaystyle \\frac{\\partial L }{\\partial \\overrightarrow{w}}= w - \\sum_{i}\\alpha_iy_i\\overrightarrow{x}_i=0 \\Rightarrow w = \\sum_{i}\\alpha_iy_i\\overrightarrow{x}_i \\displaystyle \\frac{\\partial L }{\\partial b}= \\sum_{i}\\alpha_iy_i=0 \\Rightarrow \\sum_{i}\\alpha_iy_i =0 \\displaystyle \\frac{\\partial L }{\\partial \\varepsilon _i}= c -\\alpha_i - \\beta_i = 0\\Rightarrow c=\\beta_i+\\alpha_i\n\nObservation: \\overbrace{c}^{\\text{Upper bound of }\\alpha \\text{ and }\\beta} =\\underbrace{\\beta_i}_{\\text{always +ve}} +\\underbrace{\\alpha_i}_{\\text{always +ve}} so we can say: 0 \\le \\alpha_i \\le c \\forall i\n\nsubstituting these values back in Dual we get:  \\displaystyle \\Rightarrow \\max_{\\overrightarrow{\\alpha } \\ge 0 , \\overrightarrow{\\beta} \\ge 0} \\frac{1}{2}\\left( \\sum_{i}\\alpha_iy_i\\overrightarrow{x}_i \\right) \\cdot \\left( \\sum_{j}\\alpha_jy_j\\overrightarrow{x}_j \\right) +\\sum_{i}\\varepsilon_i\\left( \\beta_i+\\alpha_i \\right) + \\sum_{i = 1}^{n}\\alpha_i\\left( 1- \\varepsilon _i - y_i\\left( \\sum_{j}\\alpha_jy_j\\overrightarrow{x}_j \\cdot\\overrightarrow{x_i}+b\\right) \\right) -\\sum_{i}\\beta_i \\varepsilon_i \\displaystyle \\Rightarrow \\max_{\\overrightarrow{\\alpha } \\ge 0 , \\overrightarrow{\\beta} \\ge 0} \\frac{1}{2}\\left( \\sum_{i}\\alpha_iy_i\\overrightarrow{x}_i \\right) \\cdot \\left( \\sum_{j}\\alpha_jy_j\\overrightarrow{x}_j \\right) +\\sum_{i}\\varepsilon_i \\beta_i+\\sum_{i}\\varepsilon_i \\alpha_i + \\sum_{i = 1}^{n}\\alpha_i - \\sum_{i = 1}^{n}\\alpha_i \\varepsilon _i - \\sum_{i = 1}^{n}\\alpha_i y_i\\left( \\sum_{j}\\alpha_jy_j\\overrightarrow{x}_j \\cdot\\overrightarrow{x_i}+b \\right) -\\sum_{i}\\beta_i \\varepsilon_i \\displaystyle \\Rightarrow \\max_{\\overrightarrow{\\alpha } \\ge 0 , \\overrightarrow{\\beta} \\ge 0} \\frac{1}{2} \\sum_{i}\\alpha_i \\alpha_j y_i y_j \\overrightarrow{x}_i \\overrightarrow{x}_j +\\sum_{i}\\varepsilon_i \\beta_i+\\sum_{i}\\varepsilon_i \\alpha_i + \\sum_{i = 1}^{n}\\alpha_i - \\sum_{i = 1}^{n}\\alpha_i \\varepsilon _i - \\sum_{i = 1}^{n}\\alpha_i y_i \\sum_{j}\\alpha_jy_j\\overrightarrow{x}_j \\cdot\\overrightarrow{x_i}+ \\sum_{i = 1}^{n}\\alpha_i y_ib -\\sum_{i}\\beta_i \\varepsilon_i \\displaystyle \\Rightarrow \\max_{\\overrightarrow{\\alpha } \\ge 0 } \\frac{1}{2} \\sum_{i}\\alpha_i \\alpha_j y_i y_j \\overrightarrow{x}_i \\overrightarrow{x}_j + \\sum_{i = 1}^{n}\\alpha_i - \\sum_{i = 1}^{n}\\alpha_i y_i \\sum_{j}\\alpha_jy_j\\overrightarrow{x}_j \\cdot\\overrightarrow{x_i}+ \\sum_{i = 1}^{n}\\alpha_i y_ib \\displaystyle \\Rightarrow \\max_{\\overrightarrow{\\alpha } \\ge 0 } \\frac{1}{2} \\sum_{i}\\alpha_i \\alpha_j y_i y_j \\overrightarrow{x}_i \\overrightarrow{x}_j + \\sum_{i = 1}^{n}\\alpha_i - \\sum_{i=1,j=1}^{n}\\alpha_i \\alpha_j y_i y_j \\overrightarrow{x_i} \\cdot \\overrightarrow{x_j}+ \\sum_{i = 1}^{n}\\alpha_i y_ib \\displaystyle \\Rightarrow \\max_{\\overrightarrow{\\alpha } \\ge 0 } \\sum_{i = 1}^{n}\\alpha_i -\\frac{1}{2} \\sum_{i}\\alpha_i \\alpha_j y_i y_j \\overrightarrow{x}_i \\overrightarrow{x}_j\nNotice neither \\overrightarrow{\\beta} nor \\overrightarrow{\\varepsilon } appears in the above equation.\nThe above equation can also be written as:  \\displaystyle \\max \\sum_{k = 1}^{R}\\alpha_k - \\frac{1}{2}\\sum_{k=1}^{R}\\sum_{l = 1}^{R}\\alpha_k \\alpha_l Q_{kl}, where \\displaystyle Q_{kl} =y_ky_l\\left( \\mathbf{X_k}\\cdot \\mathbf{X_l} \\right) subject to constrains:0 \\le \\alpha_k \\le c, and \\forall k , \\displaystyle \\sum_{k=1}^{R}\\alpha_ky_k=0\n\n\n\n\n\n\n\nNote\n\n\n\nOne of the constraint of soft margin SVM is 0 \\le \\alpha_k \\le c which is different for hard margin SVM constraint \\alpha_k \\ge 0"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-09-10-CS5590-week4.html#multi-class-classification-with-svms",
    "href": "Data_Science_Notes/Machine-Learning/2022-09-10-CS5590-week4.html#multi-class-classification-with-svms",
    "title": "Machine Learning 4",
    "section": "7 Multi-class Classification with SVMs",
    "text": "7 Multi-class Classification with SVMs\n\nSVM can handle only tow-class outputs.\nwhat to do for multi-class case:\n\none vs all SVM\n\nLearn N SVMs\nSVM 1 learns class 1 vs not class 1\nSVM 2 learns class 2 vs not class 2 and so on.\nThen to predict the output for a new point, just predict with each SVM and fond out which one puts the prediction the furthest into the positive region.\n\nOther approaches:\n\npair-wise SVM\nTree-structured SVM"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-09-10-CS5590-week4.html#kernel-trick",
    "href": "Data_Science_Notes/Machine-Learning/2022-09-10-CS5590-week4.html#kernel-trick",
    "title": "Machine Learning 4",
    "section": "8 Kernel Trick",
    "text": "8 Kernel Trick\n\n8.1 Why do we require the Kernel Trick\n\nWe found that after solving minimization problem of dual of SVM we get following:  \\displaystyle \\max \\sum_{k = 1}^{R}\\alpha_k - \\frac{1}{2}\\sum_{k=1}^{R}\\sum_{l = 1}^{R}\\alpha_k \\alpha_l Q_{kl}, where \\displaystyle Q_{kl} =y_ky_l\\left( \\mathbf{X_k}\\cdot \\mathbf{X_l} \\right) subject to constrains:0 \\le \\alpha_k \\le c, and \\forall k , \\displaystyle \\sum_{k=1}^{R}\\alpha_ky_k=0\nBut if the data can’t be separated linearly we transform the data to higher dimension space using the transformation \\phi. So that the data can be separated using a hyper-plane in higher dimension space. In that case the above equation changes as below :  \\displaystyle \\max \\sum_{k = 1}^{R}\\alpha_k - \\frac{1}{2}\\sum_{k=1}^{R}\\sum_{l = 1}^{R}\\alpha_k \\alpha_l Q_{kl}, where \\displaystyle Q_{kl} =y_ky_l \\underbrace{\\left(\\mathbf{\\Phi} \\left( \\mathbf{X}_k \\right)\\cdot \\mathbf{\\Phi} \\left( \\mathbf{X}_l \\right)\\right)}_{\\text{Notice the term } \\Phi } subject to constrains:0 \\le \\alpha_k \\le c, and \\forall k , \\displaystyle \\sum_{k=1}^{R}\\alpha_ky_k=0\nThen compute :  \\displaystyle \\mathbf{W} = \\sum_{\\text{k s.t } \\alpha_k >0 }\\alpha_k^*y_k \\mathbf{\\Phi} \\left( \\mathbf{X}_k \\right)\nThen classify with  \\displaystyle f(\\mathbf{X},w,b)=\\mathrm{sign}\\left( \\mathbf{W} \\cdot \\mathbf{\\Phi}(\\mathbf{X})+b \\right)\nMost important change :  \\mathbf{X} \\rightarrow \\mathbf{\\Phi}(\\mathbf{X})\nNotice that in the term \\displaystyle Q_{kl} =y_ky_l \\left(\\mathbf{\\Phi} \\left( \\mathbf{X}_k \\right)\\cdot \\mathbf{\\Phi} \\left( \\mathbf{X}_l \\right)\\right) we must do \\frac{R^2}{2} dot products to get this matrix ready. Assuming a quadratic polynomial kernel, each dot product requires \\frac{m^2}{2} addition and multiplication ( where m is the dimension of X)  The whole thing costs \\frac{R^2m^2}{4} This is the reason we require a trick so that we need not do this large computation.\n\n\n\n8.2 How do we do the kernel Trick\nTo understand we create a data in circular fashion as shown below:\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nimport math\ndef get_points(rl,rh):\n    npoints = 1000 # points to chose from\n    r =  np.random.uniform(low=rl, high=rh, size=npoints)\n    t = np.linspace(0, 2*np.pi, npoints, endpoint=False)\n    x = r * np.cos(t)\n    y = r * np.sin(t)\n    return x,y\n\nfig = plt.figure(figsize=(4,4))\nx11,x21=get_points(2,4)\nplt.scatter(x11,x21);\nx12,x22=get_points(6,8)\nplt.scatter(x12,x22);\nplt.xlabel('x1')\nplt.ylabel('x2');\n\n\n\n\n\n\nThe two circle can’t be separated by a line.\nNow we transform the data to 3 dimension using below function. \\phi\\left( \\mathbf{X}\\right) = \\phi\\left( \\left( \\begin{array}{c}  x_1\\\\  x_2 \\end{array} \\right) \\right) = \\left( \\begin{array}{c} x_1^2 \\\\ \\sqrt{2}x_1x_2 \\\\ x_2^2 \\end{array} \\right)\nPython implementation of the same is shown below:\n\n\ndef transform(x1,x2):\n    return np.square(x1),np.sqrt(2)*x1*x2,np.square(x2)\n\n\nWe can see in below pic that how data becomes sparable in 3 dimensional space.\n\n\n\nCode\nfrom mpl_toolkits import mplot3d\nimport matplotlib.pyplot as plt\nfig = plt.figure(figsize=(8,8))\nax = plt.axes(projection='3d')\nx11,x21,x31 = transform(x11,x21)\nx12,x22,x32 = transform(x12,x22)\nax.scatter3D(x11,x21,x31);\nax.scatter3D(x12,x22,x32);\nax.set_xlabel('x1')\nax.set_ylabel('x2')\nax.set_zlabel('x3');\nax.view_init(10, 80);\n\n\n\n\n\n\nIn case of SVM we need the dot product of the transformed data, not the transformed data itself.\nConsider two vectors \\mathbf{a} and \\mathbf{b}, we will apply following transformation on it : \\phi\\left( \\mathbf{X}\\right) = \\phi\\left( \\left( \\begin{array}{c}  x_1\\\\  x_2 \\end{array} \\right) \\right) = \\left( \\begin{array}{c} x_1^2 \\\\ \\sqrt{2}x_1x_2 \\\\ x_2^2 \\end{array} \\right)  We get below result:  \\phi\\left( \\mathbf{a}\\right)^T\\phi\\left( \\mathbf{b}\\right)= \\left( \\begin{array}{c} a_1^2 \\\\ \\sqrt{2}a_1a_2 \\\\ a_2^2 \\end{array} \\right)^T \\cdot \\left( \\begin{array}{c} b_1^2 \\\\ \\sqrt{2}b_1b_2 \\\\ b_2^2 \\end{array} \\right)= a_1^2 b_1^2 + 2 a_1 b_1 a_2 b_2 +a_2^2 b_2^2= \\left( a_1b_1+a_2b_2\\right)^2= \\left( \\left( \\begin{array}{c}  a_1\\\\  a_2 \\end{array} \\right)^T \\cdot \\left( \\begin{array}{c}  b_1\\\\  b_2 \\end{array} \\right) \\right)^2 = \\left( \\mathbf{a}^T \\cdot \\mathbf{b} \\right)^2\nThe kernel function here is polynomial function.\nwe can see that we don’t even need to use \\phi we can get the result just from \\left( \\mathbf{a}^T \\cdot \\mathbf{b} \\right)^2\nSo we never need to transformed the data to higher domain still we get the same benefit at the less computation cost.\nThe same is explained below:\n\n\n\nCode\nfig = plt.figure(figsize=(16,16))\nx11,x21=get_points(2,4)\nx12,x22=get_points(6,8)\nsame_domain_result = np.square(x11*x12+x21*x22)\nax = fig.add_subplot(2, 2, 1)\nax.scatter(x11,x21);\nax.scatter(x12,x22);\nax.set_xlabel('x1')\nax.set_ylabel('x2');\nax.set_title(' Original domain')\n\nx11,x21,x31 = transform(x11,x21)\nx12,x22,x32 = transform(x12,x22)\ntransformed_domain_result = x11*x12+x21*x22+x31*x32\nax = fig.add_subplot(2, 2, 2, projection='3d')\nax.scatter3D(x11,x21,x31);\nax.scatter3D(x12,x22,x32);\nax.set_xlabel('x1')\nax.set_ylabel('x2')\nax.set_zlabel('x3');\nax.view_init(10, 80);\nax.set_title('Transformed domain ')\n\nax = fig.add_subplot(2, 2, 3)\nax.scatter(range(len(same_domain_result)),same_domain_result)\nax.set_title('Result in original domain $( \\mathbf{a}^T \\cdot \\mathbf{b} )^2$')\n\nax = fig.add_subplot(2, 2, 4)\nax.scatter(range(len(transformed_domain_result)),transformed_domain_result)\nax.set_title('Result in transformed domain $\\phi( \\mathbf{a})^T\\phi( \\mathbf{b})$ ');\n\n\n\n\n\n\n\nFrom above figure we can see that we get exact same result in original domain without transforming data to the higher dimensional space\nNot all functions are kernel functions.\n\nNeed to be decomposable: K(a,b)=\\phi(a)\\cdot \\phi(b)\n\nMercer’s condition : To expand kernel function K(X,Y) into a dot product, i.e. K(x,y)=\\Phi(x).\\Phi(y), K(x,y) has to be positive semi-definite function, i.e., for any function f(X) whose \\displaystyle \\int f^2(x)dx is finite, the following inequality holds: \\displaystyle \\int dx dy f(x) K(x,y) f(y) \\ge 0\nIt is not easy to select the kernel function which will work best for the given data.\nRBF kernels are considered good in general, especially for images (and other smooth functions/data).\nFor discrete data, chi-square kernel is preferred.\nwe can also do Multiple Kernel learning.\nIf still it doesn’t work we can use cross-validation to select a kernel function from some basic options.\nSame kernel trick can also be applied to other methods including:\n\nKernel k-NN\nKernel Perceptron\nKernelized Linear Regression\netc.\n\n\n \\tiny {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-10-29-CS5590-week8.html",
    "href": "Data_Science_Notes/Machine-Learning/2022-10-29-CS5590-week8.html",
    "title": "Machine Learning 8",
    "section": "",
    "text": "Observe a phenomenon\nConstruct a model from observations\nUse that model to make decisions/predictions\n\n\n\n\nPhenomenon of interest:\n\nInput space: X; Output space: Y\nThere is an unknown distribution D over (X,Y)\nThe learner observes m examples (x_1 ,y_1,\\dots, x_m ,y_m ) drawn from D\n\nConstruct a model:\n\nLet F be a collection of models, where each f: X \\rightarrow Y predicts y given x\nFrom m observations, select a model f_m in F which predicts well.\nGeneralization error of f: \\mathrm{err}(f):=\\mathbb{P}_{(x,y)\\sim D}\\left[f(x)\\ne y\\right] Notice this error is calculated on the whole distribution D\nWe can say that we have learned a phenomenon if \\mathrm{err}(f_m)-\\mathrm{err}(f^*)\\le \\epsilon \\quad f^*:=\\argmin_{f \\in F}\\mathrm{err}(f) for any tolerance level \\epsilon of our choice.\n\nFor all tolerance levels \\epsilon > 0, and all confidence levels \\delta > 0, if there exists some model selection algorithm \\mathcal{A} that selects f_m^\\mathcal{A} \\in \\mathcal{F} from m observations i.e. \n\n\\mathcal{A}:(x_i,y_i)_i^m \\mapsto f_m^\\mathcal{A}\nAnd \\mathrm{err}(f_m^\\mathcal{A})-\\mathrm{err}(f^*)\\le \\epsilon with probability at least 1-\\delta over the draw of the sample.\n\nWe call\n\nThe model class \\mathcal{F} is PAC-Learnable. (Probably Approximate Correct)\nIf m is polynomial in \\frac{1}{\\epsilon} and \\frac{1}{\\delta} then \\mathcal{F} is Efficiently PAC-Learnable.\n\nA popular algorithm:\n\nEmpirical risk minimization (ERM) algorithm. f_m^{\\text{ERM}}:=\\argmin_{f \\in \\mathcal{F}}\\frac{1}{m}\\sum_{i=1}^m\\mathbf{1}\\{f(x_i)\\ne y_i\\}"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-10-29-CS5590-week8.html#pac-learning-simple-model-classes",
    "href": "Data_Science_Notes/Machine-Learning/2022-10-29-CS5590-week8.html#pac-learning-simple-model-classes",
    "title": "Machine Learning 8",
    "section": "2 PAC Learning Simple Model Classes",
    "text": "2 PAC Learning Simple Model Classes\nTheorem (finite seize \\mathcal{F} ):\n\nPick any tolerance level \\epsilon > 0, and any confidence level \\delta > 0 let (x_1,y_1),\\dots,(x_m,y_m) be m examples drawn from an unknown \\mathcal{D} if \\displaystyle m \\ge C \\cdot \\frac{1}{\\epsilon^2}\\ln\\frac{\\lvert \\mathcal{F}\\rvert}{\\delta}, then with the probability at least 1- \\delta \\mathrm{err}(f_m^\\mathrm{ERM})-\\mathrm{err}(f^*)\\le \\epsilon \\boxed{\\mathcal{F}\\text{ is efficiently PAC-learnable}}\nProof Sketch\n\nDefine Generalization error of f \\text{err}(f):=\\mathbb{E}_{(x,y)\\sim \\mathcal{D}}\\left[\\mathbf{1}\\{f(x_i)\\ne y_i\\}\\right]\nDefine sample error of f \\text{err}_m(f):=\\frac{1}{m}\\sum_{i=1}^m\\left[\\mathbf{1}\\{f(x_i)\\ne y_i\\}\\right] Fix any f \\in \\mathcal{F} and sample (x_i,y_i), define random variable \\mathbf{Z}_i^f=\\mathbf{1}\\{f(x_i)\\ne y_i\\} Now we can re-write generalization error and sample error as below\nGeneralization error of f \\text{err}(f):=\\mathbb{E}_{(x,y)\\sim \\mathcal{D}}\\left[\\mathbf{Z}_1^f\\right]\nsample error of f \\text{err}_m(f):=\\frac{1}{m}\\sum_{i=1}^m\\left[\\mathbf{Z}_i^f\\right]\n\n\n\n2.1 Lemma (Chernoff-Hoeffding bound’63)\nLet \\mathbf{Z}_1,\\dots,\\mathbf{Z}_m be m Bernouli r.v. drawn independently from \\mathbf{B(p)}, for any tolerance level \\epsilon > 0 {\\mathcal{P}}_{{\\mathit{\\mathbf{Z}}}_i } \\left\\lbrack \\left \\lvert \\frac{1}{m}\\sum_{i=m}^m \\left\\lbrack {\\mathit{\\mathbf{Z}}}_i \\right\\rbrack -\\mathbb{E}\\left\\lbrack \\mathbf{Z}_1 \\right\\rbrack \\right \\rvert\\ge \\epsilon \\right\\rbrack \\le 2e^{-2\\epsilon^2 m}\nAnalyze \\begin{align*}{}\n&{\\mathcal{P}}_{\\left(x_i ,y_i \\right)} \\left\\lbrack \\mathrm{exists}\\;f\\in \\mathcal{F},\\left \\lvert \\frac{1}{m}\\sum_{i=m}^m \\left\\lbrack {\\mathit{\\mathbf{Z}}}_i^f \\right\\rbrack -\\mathbb{E}\\left\\lbrack {\\mathbf{Z}}_1^f \\right\\rbrack \\right \\rvert\\ge \\epsilon \\right\\rbrack \\\\\n&\\qquad \\quad \\le \\sum_{f\\in \\mathcal{F}} {\\mathcal{P}}_{\\left(x_i ,y_i \\right)} \\left\\lbrack \\left \\lvert \\frac{1}{m}\\sum_{i=m}^m \\left\\lbrack {\\mathit{\\mathbf{Z}}}_i^f \\right\\rbrack -\\mathbb{E}\\left\\lbrack {\\mathbf{Z}}_1^f \\right\\rbrack  \\right \\rvert\\ge \\epsilon \\right\\rbrack \\\\\n&\\qquad  \\quad \\le 2{\\left \\lvert \\;\\mathcal{F}\\right \\rvert e}^{-2\\epsilon^2 m} \\\\\n&\\qquad  \\quad \\le \\delta\n\\end{align*}\nEquivalently by choosing \\displaystyle m \\ge \\frac{1}{2 \\epsilon ^2}\\ln\\frac{2\\mathcal{F}}{\\delta} with probability at least\n1-\\delta, for all f \\in \\mathcal{F} \\left \\lvert \\frac{1}{m}\\sum_{i=m}^m \\left\\lbrack {\\mathit{\\mathbf{Z}}}_i^f \\right\\rbrack -\\mathbb{E}\\left\\lbrack {\\mathbf{Z}}_1^f \\right\\rbrack  \\right \\rvert=\\left \\lvert \\mathrm{err}_m(f)-\\mathrm{err}(f) \\right \\rvert \\le \\epsilon"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-10-29-CS5590-week8.html#learning-general-concepts",
    "href": "Data_Science_Notes/Machine-Learning/2022-10-29-CS5590-week8.html#learning-general-concepts",
    "title": "Machine Learning 8",
    "section": "3 Learning general concepts",
    "text": "3 Learning general concepts\n\n3.1 VC dimension\nVC dimension is also known as Vapnik-Chervonenkis dimension.\n\nDefinition  We say that a model class \\mathcal{F} has VC dimension d, if d is the largest set of points x_1,\\dots,x_d \\subset X Such that for all possible labelling of x_1,\\dots,x_d there exists some f \\in \\mathcal{F} that achieves that labelling.\n\nExample: \\mathcal{F}= Linear classifier in \\mathbb{R}^2  \\text{VC}(\\mathcal{F})=3 Notice that we can change the structure of the data, for e.g. on the left side, data is in the from of triangle, we can not change it to form a line, i.e. we can not change the position of the data, but we can change the label as we want.\n\n\n\n\n3.2 VC Theory\n\nTheorem (Vapnik-Chervonenkis’71) Chose any tolerance level \\epsilon >0, and any confidence level \\delta>0 let (x_1,y_1),\\dots,(x_m,y_m) be m examples drawn from an unknown \\mathcal{D},  if \\displaystyle m>C.\\frac{\\text{VC}(\\mathcal{F})\\ln(1/\\delta)}{\\epsilon^2}, then with probability at least 1-\\delta \\mathrm{err}(f_m^{\\mathrm{ERM}})-\\mathrm{err}(f^*)\\le\\epsilon \\boxed{\\mathcal{F} \\text{ is efficiently PAC-learnable}}\n\n\n\n3.3 Tightness of VC Bound\nTheorem (VC lower bound) Let \\mathcal{A} be any model selection algorithm that given m samples, returns a model from \\mathcal{F}, that is \\mathcal{A}:(x_i,y_i)_{i=1}^m \\mapsto f_m^\\mathcal{A} For all tolerance level 0<\\epsilon <1, and all confidence levels 0<\\delta<1/4, there exists a distribution \\mathcal {D} such that if \\displaystyle m \\leq C \\cdot \\frac{\\mathrm{VC}(\\mathcal{F})}{\\epsilon^2} \\mathbb{P}_{(x_i,y_i)}\\left[ \\left \\lvert \\mathrm{err}(f_m^{\\mathcal{A}})-\\mathrm{err}(f^*) \\right \\rvert > \\epsilon \\right]> \\delta\n\n\n3.4 Facts of VC dimension\n\nVC dimension:\n\nA combinatorial concept to capture the true richness of \\mathcal{F}\nOften (but not always!) proportional to the degrees of freedom or the number of independent parameters in \\mathcal{F}\n\nOther Observations\n\nVC dimension of a model class fully characterizes its learning ability!\nResults are agnostic to the underlying distribution."
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-10-29-CS5590-week8.html#erm",
    "href": "Data_Science_Notes/Machine-Learning/2022-10-29-CS5590-week8.html#erm",
    "title": "Machine Learning 8",
    "section": "4 ERM",
    "text": "4 ERM\nFrom the discussion it may seem that ERM algorithm is universally consistent. Not really though!  Below is a theorem which shows that error will always greater than some amount no matter what we do\n\nTheorem (no free lunch, Devroye’82): Pick any sample size m, any algorithm \\mathcal{A} any tolerance \\epsilon>0 there exists a distribution \\mathcal {D} such that: \\mathrm{err}(f_m^{\\mathcal{A}})A>1/2-\\epsilon while base optimal error, \\displaystyle \\min_f \\mathrm{err}(f)=0\n\n\n4.1 Further\n\nHow to do model class selection? Structural risk results.\nDealing with kernels Fat margin theory\nIncorporating priors over the models PAC Bayes theory\nIs it possible to get distribution dependent bound? It is also known as Rademacher complexity.\nHow about regression ? Can derive similar results for nonparametric regression."
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-10-29-CS5590-week8.html#regression-formulation",
    "href": "Data_Science_Notes/Machine-Learning/2022-10-29-CS5590-week8.html#regression-formulation",
    "title": "Machine Learning 8",
    "section": "5 Regression Formulation",
    "text": "5 Regression Formulation\ny \\rightarrow True label  \\hat y \\rightarrow Predicted label  X \\rightarrow Input data  L(\\hat y,y):=\\lvert \\hat y-y \\rvert \\rightarrow Absolute error L(\\hat y,y):= (\\hat y-y)^2 \\rightarrow Squared error\nA Liner predictor can be defined by slop w and intercept w_0 \\hat f(\\vec x)=\\vec w \\cdot \\vec x+ w_0\nWhich minimizes the loss \\min_{w,w_0} \\mathbb{E}_{(\\vec x,y)}[L(\\hat f(\\vec x),y)]  The intercept can be absorbed via lifting and now it can be written as \\hat f(\\vec x)=\\vec w \\cdot \\vec x \\tag{1} Which minimizes the loss \\min_{w} \\mathbb{E}_{(\\vec x,y)}[L(\\hat f(\\vec x),y)] \\tag{2} - Parametric Regressor: Here we assume a particular form of the regressor and goal is to learn the parameter which minimizes the loss. - Non-Parametric Regressor: Here we do not assume any specific form of the regressor and the goal here is to learn the predictor directly from the input data so the error is minimized.\nwe want to find a linear predictor \\hat f given by equation (1) which minimizes the loss given by equation (2)  We estimate the parameter s by minimizing the corresponding loss on the training data: \\begin{align*}\n&\\argmin_w \\frac{1}{n}\\sum_{i=1}^n L(\\vec w\\cdot \\vec x_i, y_i)\\\\\n=&\\argmin_w \\frac{1}{n}\\sum_{i=1}^n(\\vec w\\cdot \\vec x_i-y_i)^2\\\\\n=&\\argmin_w  \\left\\lVert \\left\\lbrack \\begin{array}{c}\n\\dots X_1 \\dots\\\\\n\\dots X_i \\dots\\\\\n\\dots X_n \\dots\n\\end{array}\\right\\rbrack \\left\\lbrack \\begin{array}{c}\n\\;\\\\\nw\\\\\n\\;\n\\end{array}\\right\\rbrack -\\left\\lbrack \\begin{array}{c}\ny_1 \\\\\ny_i \\\\\ny_n\n\\end{array}\\right\\rbrack \\right\\rVert^2 \\\\\n=&\\argmin_w \\left\\lVert X \\vec w - \\vec y\\right\\rVert_2^2\n\\end{align*} Notice that every\n\\left\\lbrack \\begin{array}{c}\n\\dots X_i \\dots\\\\\n\\\\\n\\\\\n\\end{array}\\right\\rbrack \\left\\lbrack \\begin{array}{c}\n\\;\\\\\nw\\\\\n\\;\n\\end{array}\\right\\rbrack produces a single value as it is just a dot product.\nThis is unconstrained problem, We can take the gradient and examine the stationary points.\n\\begin{align*}\n&&\\frac{\\partial}{\\partial \\vec w} \\left\\lVert X \\vec w - \\vec y\\right\\rVert^2 &=0\\\\\n&\\Rightarrow& 2X^T(X\\vec w-\\vec y) &=0 \\\\\n&\\Rightarrow& X^T(X\\vec w-\\vec y) &=0 \\\\\n&\\Rightarrow& X^TX\\vec w &=X^T\\vec y \\\\\n&\\Rightarrow& \\vec w &=(X^TX)^\\dagger X^T\\vec y \\\\\n\\end{align*}\nHere (\\cdot)^\\dagger is called pseudo-inverse. The above equation is also called Ordinary Least Squares \\vec w_{ols} =(X^TX)^\\dagger X^T\\vec y  The solution is unique and stable when X^TX is invertible.\n Now consider the column space view of the data \\mathbf X: \\left\\lbrack \\begin{array}{c}\n\\dots X_1 \\dots\\\\\n\\dots X_i \\dots\\\\\n\\dots X_n \\dots\n\\end{array}\\right\\rbrack \\rightarrow \\left\\lbrack \\begin{array}{c|c|c}\n\\ddot x_1  & \\cdots  & \\ddot x_d \\\\\n\\vdots  &  & \\vdots \\\\\n&  &\n\\end{array}\\right\\rbrack Find a w such that the linear combination of X is minimized.\n\\frac{1}{n} \\left\\lVert \\vec y-\\sum_{i=1}^d w_i \\ddot x_i \\right\\rVert:=\\text{residual} Say \\hat y is the solution \\hat y:=X\\vec w_{ols}=\\sum_{i=1}^d w_{ols,i}\\ddot x_i\n\nThus \\hat y is the orthogonal projection of y onto the \\text{span}(\\ddot x_1,\\dots,\\ddot x_d) \\hat y = X \\vec w_{ols}=\\underbrace{X(X^TX)^\\dagger X^T}_{\\text{Projection Matrix }\\prod} \\vec y\n\nBelow pic shows the same:"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-10-29-CS5590-week8.html#regression-statistical-modeling-view",
    "href": "Data_Science_Notes/Machine-Learning/2022-10-29-CS5590-week8.html#regression-statistical-modeling-view",
    "title": "Machine Learning 8",
    "section": "6 Regression Statistical Modeling View",
    "text": "6 Regression Statistical Modeling View\nConsider y_{\\text{clean}} is computed as w\\cdot x_i and level y_i is formed by corrupting y_{\\text{clean}} by gaussian noise \\epsilon_i \\sim \\mathcal{N}(0,\\sigma ^2) y_i:=y_{\\text{clean}}+\\epsilon_i=w\\cdot x_i + \\epsilon_i we can observe that y_i \\sim w\\cdot x_i +\\mathcal{N}(0,\\sigma ^2)=\\mathcal{N}(w\\cdot x_i,\\sigma ^2) Consider our data pair (\\vec x_1,y_2),(\\vec x_2,y_2),\\cdots,(\\vec x_n,y_n) is drawn independently from a fixed underlying distribution ( also called the i.i.d assumption ).\nwe need to select optimal model \\vec f from all possible pool of the model \\mathcal{F} such that we get\n\nMaximum likelihood ( best fits the data)\nMaximum a posteriori ( best fits the dta but incorporates prior assumptions)\nOptimization of loss criterion ( best discriminates the labels)\n\nGiven some i.i.d data say we have a model class \\mathcal{P} = \\{\\mathcal{p}_\\theta \\mid \\theta \\in \\Theta \\}  We need to find the parameter settings \\theta that best fits the data.\nWE can find the best fitting model via Maximum likelihood extimation. \\mathcal{L}(\\theta \\mid X) := P(X\\mid\n\\theta) = P(\\vec x_1, \\cdots,\\vec x_n\\mid \\theta) \\overset{\\text{i.i.d}}{=} \\prod_{i=1}^n \\mathcal{P}(\\vec x_i \\mid \\theta)= \\prod_{i=1}^n \\mathcal{P}_\\theta(\\vec x_i) Interpretation : How probable is the data given the model p_\\theta\nParameter setting that maximizes \\mathcal{L}(\\theta \\mid X) \\argmax_\\theta  \\prod_{i=1}^n \\mathcal{P}_\\theta(\\vec x_i)\nConsider {\\mathcal{P}}_{\\left\\lbrace \\mu ,\\sigma^2 \\right\\rbrace } \\left(x\\right) is given as\n{\\mathcal{P}}_{\\left\\lbrace \\mu ,\\sigma^2 \\right\\rbrace } \\left(x\\right):=\\frac{1}{\\sqrt{2\\pi \\sigma^2 }}\\exp \\left(-\\frac{{\\left(x-\\mu \\right)}^2 }{2\\sigma^2 }\\right) Then\n\\begin{align*}{}\n\\arg \\;\\max_{\\theta } \\mathcal{L}\\left(\\theta \\mid X\\right)&=\\arg \\;\\max_{\\mu ,\\sigma^2 } \\prod_{i=1}^n {\\mathcal{P}}_{\\left\\lbrace \\mu ,\\sigma^2 \\right\\rbrace } \\left(x_i \\right)\\\\\n&=\\arg \\;\\max_{\\mu ,\\sigma^2 } \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2 }}\\exp \\left(-\\frac{{\\left(x_i -\\mu \\right)}^2 }{2\\sigma^2 }\\right)\n\\end{align*}\nNow, we know that the arg max of a function and it’s log is same, so\n\\begin{align*}{}\n\\arg \\;\\max_{\\theta } \\mathcal{L}\\left(\\theta |X\\right)&=\\arg \\;\\max_{\\theta } \\;\\log \\mathcal{L}\\left(\\theta \\mid X\\right)\\\\\n&=\\arg \\;\\max_{\\mu ,\\sigma^2 } \\log \\left(\\prod_{i=1}^n {\\mathcal{P}}_{\\left\\lbrace \\mu ,\\sigma^2 \\right\\rbrace } \\left(x_i \\right)\\right)\\\\\n&=\\arg \\;\\max_{\\mu ,\\sigma^2 } \\log \\left(\\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2 }}\\exp \\left(-\\frac{{\\left(x_i -\\mu \\right)}^2 }{2\\sigma^2 }\\right)\\right)\\\\\n&=\\arg \\;\\max_{\\mu ,\\sigma^2 } \\sum_{i=1}^n \\log \\left(\\frac{1}{\\sqrt{2\\pi \\sigma^2 }}\\exp \\left(-\\frac{{\\left(x_i -\\mu \\right)}^2 }{2\\sigma^2 }\\right)\\right)\\\\\n&=\\arg \\;\\max_{\\mu ,\\sigma^2 } \\sum_{i=1}^n \\left\\lbrack -\\frac{1}{2}\\log \\left(2\\pi \\sigma^2 \\right)-\\frac{{\\left(x_i -\\mu \\right)}^2 }{2\\sigma^2 }\\right\\rbrack \\\\\n&=\\arg \\;\\max_{\\mu ,\\sigma^2 } \\left\\lbrack -\\frac{n}{2}\\log \\left(2\\pi \\sigma^2 \\right)-\\sum_{i=1}^n \\left\\lbrack \\frac{{\\left(x_i -\\mu \\right)}^2 }{2\\sigma^2 }\\right\\rbrack \\right\\rbrack \\\\\n&=\\arg \\;\\max_{\\mu ,\\sigma^2 } \\left\\lbrack -\\frac{n}{2}\\log \\left(2\\pi \\right)-\\frac{n}{2}\\log \\left(\\sigma^2 \\right)-\\frac{1}{2\\sigma^2 }\\sum_{i=1}^n {\\left(x_i -\\mu \\right)}^2 \\right\\rbrack\n\\end{align*}\nNow find derivative of above with \\mu\n\\begin{align*}{}\n\\nabla_{\\mu } \\left(\\mathcal{L}\\left(\\theta |X\\right)\\right)&=-\\frac{1}{2\\sigma^2 }\\sum_{i=1}^n \\left\\lbrack \\left(x_i -\\mu \\right)\\times \\left(-2\\mu \\right)\\right\\rbrack \\\\\n&=\\frac{\\mu }{\\sigma^2 }\\sum_{i=1}^n \\left(x_i -\\mu \\right)\\\\\n&=\\frac{\\mu }{\\sigma^2 }\\sum_{i=1}^n x_i -\\frac{\\mu }{\\sigma^2 }n\\mu\n\\end{align*}\nTo find minimum equate it to zero\n\\begin{align*}{}\n&\\nabla_{\\mu } \\left(\\mathcal{L}\\left(\\theta |X\\right)\\right)=0\\\\\n\\Rightarrow \\quad& \\frac{\\mu }{\\sigma^2 }\\sum_{i=1}^n x_i -\\frac{\\mu }{\\sigma^2 }n\\mu =0\\\\\n\\Rightarrow \\quad& \\frac{\\mu }{\\sigma^2 }\\sum_{i=1}^n x_i =\\frac{\\mu }{\\sigma^2 }n\\mu \\\\\n\\Rightarrow \\quad& \\mu =\\frac{1}{n}\\sum_{i=1}^n x_i\n\\end{align*}\nNow find derivative of the same with \\sigma\n\\begin{align*}{}\n\\nabla_{\\mu } \\left(\\mathcal{L}\\left(\\theta |X\\right)\\right)&=-\\frac{n}{2}\\times \\frac{1}{\\sigma^2 }\\times 2\\sigma +\\frac{2}{2\\sigma^3 }\\sum_{i=1}^n {\\left(x_i -\\mu \\right)}^2 \\\\\n&=\\frac{-n}{\\sigma }+\\frac{1}{\\sigma^3 }\\sum_{i=1}^n {\\left(x_i -\\mu \\right)}^2\n\\end{align*}\nTo find minimum equate it to zero\n\\begin{align*}{}\n&\\nabla_{\\mu } \\left(\\mathcal{L}\\left(\\theta |X\\right)\\right)=0\\\\\n\\Rightarrow \\quad&\\frac{-n}{\\sigma }+\\frac{1}{\\sigma^3 }\\sum_{i=1}^n {\\left(x_i -\\mu \\right)}^2 =0\\\\\n\\Rightarrow \\quad&\\frac{n}{\\sigma }=\\frac{1}{\\sigma^3 }\\sum_{i=1}^n {\\left(x_i -\\mu \\right)}^2 \\\\\n\\Rightarrow \\quad&\\sigma^2 =\\frac{1}{n}\\sum_{i=1}^n {\\left(x_i -\\mu \\right)}^2\n\\end{align*}\nBack to linear Regression: How can we determine w from Gaussian noise corrupted observations? S=(x_1,y_1),\\dots,(x_n,y_n)  Observation: y_i\\sim w\\cdot x_i + \\mathcal{N}(o,\\sigma^2)=\\mathcal{N}(w\\cdot x_i,\\sigma^2) Let’s try to use Maximum likelihood estimation to estimate Gaussian Noise Parameter: \\begin{align*}{}\n\\log \\;\\mathcal{L}\\left(w\\mid S\\right)&=\\sum_{i=1}^n \\log \\;p\\left(y_i \\mid w\\right)\\\\\n&\\propto\\sum \\frac{-{\\left(w\\cdot x_i -y_i \\right)}^2 }{2\\sigma^2 }\n\\end{align*} Ignoring the terms independent of w and optimizing for w yields the same ols results.\nwe can say that Minimizing Ordinary sum of square is same as maximizing the gaussian log likelihood"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-10-29-CS5590-week8.html#regularized-least-squared-regression",
    "href": "Data_Science_Notes/Machine-Learning/2022-10-29-CS5590-week8.html#regularized-least-squared-regression",
    "title": "Machine Learning 8",
    "section": "7 Regularized Least-Squared Regression",
    "text": "7 Regularized Least-Squared Regression\n\ncomplex models has a lot of parameters, so pron to overfitting.\nOverfitting can be reduced by imposing a constraint on the overall magnitude of the parameters.\nTwo common types of regularization in linear regression:\n\nL2 regularization (a.k.a. ridge regression): Find w which minimizes: {\\sum_{j=1}^N \\left(y_j -\\sum_{i=0}^d w_i \\cdot x_i \\right)}^2 +\\lambda \\sum_{i=1}^d w_i^2 Bigger \\lambda imposes more constraints.\nL1 regularization (a.k.a. lasso): Find w which minimizes: {\\sum_{j=1}^N \\left(y_j -\\sum_{i=0}^d w_i \\cdot x_i \\right)}^2 +\\lambda \\sum_{i=1}^d \\left\\lvert w_i \\right\\rvert"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-10-29-CS5590-week8.html#solving-ridge-regression",
    "href": "Data_Science_Notes/Machine-Learning/2022-10-29-CS5590-week8.html#solving-ridge-regression",
    "title": "Machine Learning 8",
    "section": "8 Solving Ridge Regression",
    "text": "8 Solving Ridge Regression\nRidge regression can also be written as  \\left\\lVert \\mathbf{y}- \\mathbf{Xw} \\right\\rVert ^2 + \\lambda \\left\\lVert \\mathbf{w} \\right\\rVert^2 Solve above for minimum \\mathbf{ w}  \\begin{align*}\n&&&\\frac{\\partial}{\\partial \\vec w} \\left(\\left\\lVert  \\mathbf{y}- \\mathbf{Xw} \\right\\rVert ^2 + \\lambda \\left\\lVert \\mathbf{w} \\right\\rVert^2\\right) =0\\\\\n\\Rightarrow&&& 2\\mathbf{X}^T(\\mathbf{Xw}- \\mathbf y) +2\\lambda \\mathbf{w}=0 \\\\\n\\Rightarrow&&& 2{\\mathbf{X}}^T \\mathbf{X}\\mathbf{w}+2\\lambda \\mathbf{w}=2{\\mathbf{X}}^T \\mathbf{y}\\\\\n\\Rightarrow&&& \\left({\\mathbf{X}}^T \\mathbf{X}+\\lambda I\\right)\\mathbf{w}={\\mathbf{X}}^T \\mathbf{y}\\\\\n\\Rightarrow&&& \\mathbf{w}={{ \\left({\\mathbf{X}}^T \\mathbf{X}+\\lambda I \\right)}^{-1} \\mathbf{X}}^T \\mathbf{y}\n\\end{align*}\nInverse always exists for any \\lambda >0"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-10-29-CS5590-week8.html#total-least-squares-and-partial-least-squares",
    "href": "Data_Science_Notes/Machine-Learning/2022-10-29-CS5590-week8.html#total-least-squares-and-partial-least-squares",
    "title": "Machine Learning 8",
    "section": "9 Total Least Squares and Partial Least Squares",
    "text": "9 Total Least Squares and Partial Least Squares\n\nTotal least square models error in output and input. \nPartial Least Squares:\n\nSeeks to address the correlation between predictor variables in it’s model\nAlso called “Projection to Latent Structures” (PLS)"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-10-29-CS5590-week8.html#regression-methods",
    "href": "Data_Science_Notes/Machine-Learning/2022-10-29-CS5590-week8.html#regression-methods",
    "title": "Machine Learning 8",
    "section": "10 Regression Methods",
    "text": "10 Regression Methods\n\nLinear Least-Squares Regression\n\nPartial Least-Squares\nTotal Least-Squares\nRidge Regression, LASSO\n\nKernel Regression\nk-NN Regression\nRegression Trees\nSupport Vector Regression\nLogistic Regression"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-10-29-CS5590-week8.html#non-linear-regression",
    "href": "Data_Science_Notes/Machine-Learning/2022-10-29-CS5590-week8.html#non-linear-regression",
    "title": "Machine Learning 8",
    "section": "11 Non-Linear Regression",
    "text": "11 Non-Linear Regression\n\nit is also called kernel regression, recall kernel trick.\nKey Idea: Map data to higher dimensional space (feature space) and perform linear regression in embedded space.\n\n\n11.1 Kernel Regression\n\\begin{align*}{}\n&&& \\mathbf{w}={{\\left({\\mathbf{X}}^T \\mathbf{X}+\\lambda I\\right)}^{-1} \\mathbf{X}}^T \\mathbf{y}\\\\\n\\Rightarrow&&&\\left({\\mathbf{X}}^T \\mathbf{X}+\\lambda I\\right)\\mathbf{w}={\\mathit{\\mathbf{X}}}^T \\mathit{\\mathbf{y}}\\\\\n\\Rightarrow&&& {\\mathbf{X}}^T \\mathbf{X}\\mathbf{w}+\\lambda \\mathbf{w} ={\\mathit{\\mathbf{X}}}^T \\mathit{\\mathbf{y}}\\\\\n\\Rightarrow&&& \\lambda \\mathbf{w} ={\\mathit{\\mathbf{X}}}^T \\mathit{\\mathbf{y}}-{\\mathbf{X}}^T \\mathbf{X}\\mathbf{w}\\\\\n\\Rightarrow&&& \\mathbf{w}=\\lambda^{-1} \\left({\\mathit{\\mathbf{X}}}^T \\mathit{\\mathbf{y}}-{\\mathbf{X}}^T \\mathbf{X}\\mathbf{w}\\right)\\\\\n\\Rightarrow&&& \\mathbf{w}=\\lambda^{-1} {\\mathit{\\mathbf{X}}}^T \\left(\\mathit{\\mathbf{y}}-\\mathbf{Xw}\\right)\\\\\n\\Rightarrow&&& \\mathbf{w}= {\\mathit{\\mathbf{X}}}^T \\alpha\n\\end{align*} where \\alpha =\\lambda^{-1} \\left(\\mathit{\\mathbf{y}}-\\mathbf{Xw}\\right)  now put value of \\mathbf{w} in the above equation  \\begin{align*}{}\n&&&\\alpha =\\lambda^{-1} \\left(\\mathit{\\mathbf{y}}-{\\mathbf{XX}}^T \\alpha \\right)\\\\\n\\Rightarrow&&& \\lambda \\alpha =\\left(\\mathit{\\mathbf{y}}-{\\mathbf{XX}}^T \\alpha \\right)\\\\\n\\Rightarrow&&& \\lambda \\alpha +{\\mathbf{XX}}^T \\alpha =\\mathit{\\mathbf{y}}\\\\\n\\Rightarrow&&& \\alpha ={\\left({\\mathbf{XX}}^T +\\lambda I\\right)}^{-1} \\mathit{\\mathbf{y}}\\\\\n\\Rightarrow&&& \\alpha ={\\left(\\mathbf{G}+\\lambda I\\right)}^{-1} \\mathit{\\mathbf{y}}\n\\end{align*} where \\mathit{\\mathbf{G}}={\\mathbf{XX}}^T, or G_{i,j} =\\langle {\\mathit{\\mathbf{X}}}_i ,{\\mathit{\\mathbf{X}}}_j \\rangle Need to only compute G, the Gram Matrix (or the inner products between data points)  Solving \\mathbf{w}={{\\left({\\mathbf{X}}^T \\mathbf{X}+\\lambda I\\right)}^{-1} \\mathbf{X}}^T \\mathbf{y} takes O(d^3) but solving \\mathit{\\mathbf{G}}={\\mathbf{XX}}^T takes O(n^3) so if we have a problem where dimension is high and sample is less we go for gram matrix one, if we have less dimension then we can solve using original formulation.  Notice: \\mathbf{X}\\rightarrow n\\times d \\mathbf{X}^T\\mathbf{X}\\rightarrow d\\times d \\mathbf{X}\\mathbf{X}^T\\rightarrow n\\times n"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-10-29-CS5590-week8.html#k-nn-regression",
    "href": "Data_Science_Notes/Machine-Learning/2022-10-29-CS5590-week8.html#k-nn-regression",
    "title": "Machine Learning 8",
    "section": "12 k-NN Regression",
    "text": "12 k-NN Regression\nIn case of k-NN classification we used to find most common value (majority vote), but in case of regression we calculate mean value instead\n\\hat f(x_q)\\leftarrow\\frac{\\sum_{i=1}^kf(x_i)}{k}"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-10-29-CS5590-week8.html#regression-trees",
    "href": "Data_Science_Notes/Machine-Learning/2022-10-29-CS5590-week8.html#regression-trees",
    "title": "Machine Learning 8",
    "section": "13 Regression Trees",
    "text": "13 Regression Trees\n\nTree for regression: exactly the same model but with a number in each leaf instead of a class \nA regression tree is a piecewise constant function of the input attributes \nTo minimize the square error on the learning sample, the prediction at a leaf is the average output of the learning cases reaching that leaf\nImpurity of a sample is defined by the variance of the output in that sample I\\left(\\mathrm{LS}\\right)={\\mathrm{Var}}_{y\\mid \\mathrm{LS}} \\left\\lbrace y\\right\\rbrace =E_{y\\mid \\mathrm{LS}} \\left\\lbrack {\\left(y-E_{y\\mid \\mathrm{LS}} \\left\\lbrack y\\right\\rbrack \\right)}^2 \\right\\rbrack where \\mathrm{LS} is the dataset.\nThe best split is the one that reduces the most variance: \\Delta I\\left(\\mathrm{LS},A\\right)={\\mathrm{Var}}_{y\\mid \\mathrm{LS}} \\left\\lbrace y\\right\\rbrace -\\sum_a \\frac{\\left\\lvert { \\mathrm{LS}}_a \\right\\rvert }{\\mathrm{\\left\\lvert LS \\right\\rvert}}{\\mathrm{Var}}_{y\\mid {\\mathrm{LS}}_a } \\left\\lbrace y\\right\\rbrace where \\mathrm{LS_a} is the subset of the dataset.\n\n\n13.1 Regression Tree Pruning\n\nExactly the same algorithms apply as regression tree classification: pre-pruning and post-pruning.\nIn practice, pruning is more important in regression because full trees are much more complex\n\nEach data instance can have a different output value and hence the full tree has as many leaves as there are training instances\n\n\n \\tiny {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-10-08-CS5590-week6.html",
    "href": "Data_Science_Notes/Machine-Learning/2022-10-08-CS5590-week6.html",
    "title": "Machine Learning 6",
    "section": "",
    "text": "Ensemble classification combines multiple classifiers to improve accuracy\nAdvantage\n\nLarge datasets: if dataset is too large we can train different models on subset of the data.\nSmall datasets: Can handle them with bootstrapping (random sampling)\nCan solve complicated problems which can’t be solved using single classifier."
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-10-08-CS5590-week6.html#types-of-ensemble-classifiers",
    "href": "Data_Science_Notes/Machine-Learning/2022-10-08-CS5590-week6.html#types-of-ensemble-classifiers",
    "title": "Machine Learning 6",
    "section": "1 Types of Ensemble Classifiers",
    "text": "1 Types of Ensemble Classifiers\n\nBagging (bootstrap aggregating)\n\nTrain several models using bootstrapped datasets\nThe majority classification is selected\n\nBoosting\n\nUse several weak classifiers to create a strong classifier\nResample previously misclassified points\n\nStacking (stacked generalization)\n\nTrain multiple tiers of classifiers\nHigher tiers can correct lower tiers"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-10-08-CS5590-week6.html#bagging",
    "href": "Data_Science_Notes/Machine-Learning/2022-10-08-CS5590-week6.html#bagging",
    "title": "Machine Learning 6",
    "section": "2 Bagging",
    "text": "2 Bagging\n\nProblem: we have only one dataset.\nSolution: generate new ones of size n by bootstrapping, i.e. sampling it with replacement\nBagging works because it reduces variance by voting/averaging\nUsually, the more classifiers the better\nSome candidates:\n\nDecision tree, decision stump, SVMs.\nCan do this with regression too: Regression tree, linear regression\n\n\n\n2.1 Example: Random Forests\n\nRandom forests (RF) are a combination of tree predictors, it’s a variant of bagging.\nExtremely successful, especially on Kaggle challenges.\nEach tree depends on the values of a random vector sampled independently\nThe generalization error depends on the strength of the individual trees and the correlation between them\nUsing a random selection of features yields results robust w.r.t. noise\n\n\n2.1.1 Random Forests: Algorithm\n\nGiven a training set S\nFor i = 1 to k do:\n\nBuild subset S_i by sampling with replacement from S\nLearn tree T_i from S_i\n\nAt each node:\n\nChoose best split from random subset of F features\n\nEach tree grows to the largest extent, and no pruning\n\n\nMake predictions according to majority vote of the set of k trees.\nIf there are M input variables, a number m is specified such that at each node, m variables are selected at random out of the M and the best split on these m is used to split the node. The value of m is held constant during the forest growing.\nDepending upon the value of m, there are three slightly different systems:\n\nRandom splitter selection: m =1\nBreiman’s bagger: m = total number of predictor variables\nRandom forest: m << number of predictor variables. Breiman suggests three possible values for m:\\frac{1}{2}\\sqrt{M}, \\sqrt{M}, \\text{ and } 2\\sqrt{M}\n\n\n\n\n2.1.2 Features of Random Forests\n\nOne of the best in the business\nIt runs efficiently on large data bases\nIt can handle thousands of input variables without variable deletion/reduction\nIt gives estimates of what variables are important in the classification\nDoes not overfit by design\nThe generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. ( if correlation is high it means trees are similar and not so useful)\n\n\n\n\n2.2 Bagging: when\n\nCan help if data is noisy.\nIf learning algorithm is unstable, i.e. if small changes to the training set cause large changes in the learned classifier.\n\n\n\n2.3 Bagging: Why\n\nLet S= \\{(x , y ), i=1\\dots N\\} be the training dataset\nLet \\{S_k \\} be a sequence of training sets containing a sub-set of S\nLet P be the underlying distribution of S.\nBagging replaces the prediction of the model with the majority of the predictions given by the classifiers S. \\phi(x,P)=E_s(\\phi (x,S_k))"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-10-08-CS5590-week6.html#boosting",
    "href": "Data_Science_Notes/Machine-Learning/2022-10-08-CS5590-week6.html#boosting",
    "title": "Machine Learning 6",
    "section": "3 Boosting",
    "text": "3 Boosting\n\nUse several weak classifiers to create a strong classifier\nResample previously misclassified points. (This is not done by bagging)\nStrong Learner: This is the objective of machine learning where we take labelled data for training and produce classifier which can be arbitrarily accurate.\nWeak Learner: Take labelled data for training and produce a classifier which is more accurate than random guessing.\n\nCan a set of weak learners create a single strong learner?\n\n3.1 Key Idea\n\nAn algorithm for constructing a “strong” classifier as linear combination of “simple” “weak” classifier f(x)=\\sum_{t=1}^T \\alpha_th_t(x)\nFinal classification based on weighted vote of weak classifiers\n\n\n\n3.2 Adaboost Algorithm\nGiven (x_1,y_1),\\dots (x_m,y_m) where x_i \\in X,y_i \\in Y =\\{-1,+1\\} Initialize D(i)=\\frac{1}{m} For t=1,\\dots T:\n\nFind the classifier h_t:X \\rightarrow \\{-1,+1\\} that minimizes the error with respect to the distribution D_t: \\displaystyle h_t = \\argmin _{h_j \\in H} \\epsilon_j, where \\displaystyle\\epsilon_j=\\sum_{i=1}^m D_t(i)[y_i \\ne h_j(x_i)]\nPrerequisite: \\epsilon_t<0.5, otherwise stop.\nChoose \\alpha_t \\in \\mathbb{R}, typically \\displaystyle \\alpha_t=\\frac{1}{2}\\ln \\frac{1-\\epsilon_t}{\\epsilon_t}, where \\epsilon_t is the weighted error rate of the classifier h_t \\boxed{\\alpha_t \\text{ stays same for all data points}}\nUpdate  D_{t+1} \\left(i\\right)=\\frac{D_t \\left(i\\right)\\exp \\left(-\\alpha_t y_t h_t \\left(x_i \\right)\\right)}{Z_t } where Z_t is a normalization factor (chosen such that D_{t+1} will be a distribution)\n\noutput of the final classifier : H(x)=\\text{Sign}\\sum_{t=1}^T\\alpha_t h_t(x)\nReweighting \\begin{align*}{}\nD_{t+1} \\left(i\\right)&=\\frac{D_t \\left(i\\right)\\exp \\left(-\\alpha_t y_t h_t \\left(x_i \\right)\\right)}{Z_t }\\\\\n&=\\frac{D_t \\left(i\\right)\\exp \\left(-y_i \\sum_{q=1}^t \\alpha_q h_q \\left(x_i \\right)\\right)}{m\\prod_{q=1}^t Z_q }\n\\end{align*}\nNotice\n\\exp \\left(-\\alpha_t y_t h_t \\left(x_i \\right)\\right)  \\begin{cases}\n    \\le1 &\\text{if } y_t=h_t(x_i) \\xleftarrow{y\\times h(x)=1} \\\\\n    >1 &\\text{if } y_t\\ne h_t(x_i) \\xleftarrow{y\\times h(x)=-1}\n\\end{cases}\nwe can see that weight of wrongly classified example is increased and weight of correctly classified example is decreased.\n\n\n\n\n\n\nAdaboost vs Random Forests\n\n\n\nDietterich (1998) showed that when a fraction of the output labels in the training set are randomly altered, the accuracy of Adaboost degenerates, while bagging is more immune to the noise.\n\n\n\n\n3.3 A good weak learner\n\nThe set of weak rules (features) should be flexible enough to be (weakly) correlated with most conceivable relations between feature vector and label.\nSmall enough to allow exhaustive search for the minimal weighted training error.\nSmall enough to avoid over-fitting.\nShould be able to calculate predicted label very efficiently\nRules can be “specialists” – predict only on a small subset of the input space and abstain from predicting on the rest (output 0).\n\n\n\n3.4 Gradient Boosting\nGradient Boosting = Gradient Descent + Boosting\n\nFit an additive model (ensemble) in a forward stage-wise manner.\nIn each stage, introduce a weak learner to compensate the shortcomings of existing weak learners.\nIn Gradient Boosting, “shortcomings” are identified by gradients.\nRecall that, in Adaboost, “shortcomings” are identified by high-weight data points.\nBoth high-weight data points and gradients tell us how to improve our model.\n\n\n3.4.1 Gradient Boosting Algorithm\nInput: Training set {\\left\\lbrace \\left(x_i ,y_i \\right)\\right\\rbrace }_{i=1}^n, a differentiable loss function L(y,F(x)),Number of iteration M  Algorithm: \n\nInitialize model with constant value:\nF_0(x)=\\argmin_\\gamma \\sum_{i=1}^nL(y_i,\\gamma)\nFor m=1 to M\n\nCompute pseudo-residuals: r_{im}=-\\left[\\frac{\\partial L(y_i,F(x_i))}{\\partial F(x_i)} \\right]_{F(x)=F_{m-1}(x)}\\;\\text{for }i=1,\\dots,n\nFit a base lerner (e.g. tree) h_m(x) to pseudo-residual, i.e. train it using the set {\\left\\lbrace \\left(x_i ,\\boxed{r_{im}} \\right)\\right\\rbrace }_{i=1}^n\\;\\; Notice the training set now contains residual instead of original data.\nCompute multiplier \\gamma_m by solving the following one-dimensional optimization problem: \\gamma_m=\\argmin_\\gamma \\sum_{i=1}^nL(y_i,\\underbrace{F_{m-1}(x_i)}_{\\text{previous model}}+\\gamma \\times \\underbrace{h_m(x_i)}_{\\text{current model}})\nUpdate the model: F_m(x)=F_{m-1}(x)+\\gamma h_m(x)\n\nOutput F_M(x)"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-10-08-CS5590-week6.html#stacking-stacked-generalization",
    "href": "Data_Science_Notes/Machine-Learning/2022-10-08-CS5590-week6.html#stacking-stacked-generalization",
    "title": "Machine Learning 6",
    "section": "4 Stacking (stacked generalization)",
    "text": "4 Stacking (stacked generalization)\n\nTrain multiple tiers of classifiers\nHigher tiers can correct lower tiers\nCombiner f () is another learner (Wolpert, 1992)\nIdea:\n\nGenerate component (level 0) classifiers with part of the data (half, three quarters)\nTrain combiner (level 1) classifier to combine predictions of components using remaining data\nRetrain component classifiers with all of training data\n\nIn practice, often equivalent to voting  \n\n \\tiny {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-08-27-CS5590-week3.html",
    "href": "Data_Science_Notes/Machine-Learning/2022-08-27-CS5590-week3.html",
    "title": "Machine Learning 3",
    "section": "",
    "text": "An efficient nonparametric method.\nA hierarchical model.\nDivide and conquer strategy.\nInternal decision nodes\n\nUnivariate : It uses a single attribute X_i\n\nNumeric X_i:\n\nIf numeric data perform Binary split:X_i>w_m\n\nDiscrete X_i:\n\nFor discrete data perform n- way split for n possible values\n\n\nMultivariate: It uses more than one attributes, X\n\nLeaves\n\nClassification : Class labels, or proportions\nRegression : Numeric, r average, or local fit\n\nLearning is greedy; find the best split recursively.\nFor node m, N_m instances reach m, N_m^i belong to C_i  \\hat P(C_i|X,m)\\equiv p_m^i = \\frac{N_m^i}{N_m}\nNode m is pure if p_m^i is 0 or 1\nMeasure if impurity is entropy \\displaystyle I_m=-\\sum_{i=1}^kp_m^i \\log_2p_m^i\n\ncompare probability distributions vs entropy\n\n\nCode\nfrom matplotlib import pyplot\nimport matplotlib.pyplot as plt\nimport numpy as np\n# calculate entropy\ndef entropy(events, ets=1e-15):\n    return -sum([p * np.log2(p + ets) for p in events])\n\n# define probabilities\nprobs = np.arange(0.0001,1,0.001) \n# create probability distribution\ndists = [[p, 1.0 - p] for p in probs]\n# calculate entropy for each distribution\nents = [entropy(d) for d in dists]\n# plot probability distribution vs entropy\nplt.plot(probs, ents)\nplt.title('Probability Distribution vs Entropy for 2 class problem')\nplt.xlabel('Probability Distribution')\nplt.ylabel('Entropy (bits)')\nplt.show()\n\n\n\n\n\nEntropy in information theory specifies the average (expected) amount of information derived from observing an event .\n\n\n\nSelect a root not which divides the data best based on impurity measures.\nIf node is pure, generate a leaf and stop, otherwise split and continue recursively.\nImpurity after split:\n\nIt is probability weighted entropy given by:\n\n\\displaystyle I_m^{\\prime }=-\\sum_{j=1}^{n}\\frac{N_{mj}}{N_m}\\sum_{i=1}^kp_{mj}^i\\log_2p_{mj}^i, here, N_{mj} is j^{th} branch of N_m and N_{mj}^i belongs to i^{th} class.\n\n\nInformation gain: Expected reduction in impurity measure after split. Chose the attribute with maximum information gain.\nOther impurity measure method - Gini impurity/index : \\displaystyle 1- \\sum_{j=1}^cp_j^2\n\n\n\n\n\nNoisy training example or if only small number of samples are associated leaf nodes can cause overfitting.\nUsing Pruning for better generalization\n\nPruning is the process of removing subtree.\n\nPre-pruning: Early stopping, after a predetermined performance.\nPost-pruning: Grow the whole then prune the subtree which overfit on the pruning set\n\nPre-pruning is faster, post-pruning is more accurate.\n\n\n\n\n\nWhen multiple hypotheses can solve the problem chose the simplest one\n\n\n\n\nMeasure performance over training and separate validation data set\nMinimum Description Length : Minimize size(tree)+size(miscalssifications(tree))\n\n\n\n\n\nConvert tree to equivalent set of rules.(“if else” condition for example).\nPrune each rules independently of others, by removing any pre-conditions that result in improving its estimates accuracy.\nSort final rules into desired sequence for use.\n\n\nfrom sklearn.datasets import load_iris\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\niris = load_iris()\nX, y = iris.data, iris.target\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X, y)\n\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 10))\ntree = tree.plot_tree(clf,ax=ax)"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-08-27-CS5590-week3.html#naive-bayes",
    "href": "Data_Science_Notes/Machine-Learning/2022-08-27-CS5590-week3.html#naive-bayes",
    "title": "Machine Learning 3",
    "section": "2 Naive Bayes",
    "text": "2 Naive Bayes\n\n\nIn Naive Bayes classifier goal is to learn function f:X\\rightarrow y, where y is one of k classes and X=X_1,...,X_n: values of attributes (numeric or categorical)\nIt is a probabilistic classification\n\nmost probable class given observation: \\displaystyle \\hat{y}=\\arg \\max_y P(y|x)\nBayesian probability of a class: \\displaystyle P\\left( Y|X \\right)=\\frac{P\\left( X|Y \\right)P\\left( Y \\right)}{\\sum_{y'}P\\left( X|Y' \\right)P\\left( Y' \\right)}\n\n\n\n2.1 Formulation\n\nconsider a record with attributes A_1,A_2,\\dots ,A_n\nGoal is to predict class C\nSpecifically, we want to find the value of C that maximizes P(C|A_1,A_2,\\dots,A_n)\n\n\n\nwhat is Naive about Naive Bayes?\n\nThe attributes are considered independent of each other, this is Naive in Naive Bayes.\n\nAS we assume independence among attributes A_i so we can write: P(A_1,A_2,\\dots ,A_n|C_j)=P(A_i|C_j)P(A_2|C_j)\\dots P(A_n|C_j)\nNew point is classified to C_j if  P(C_j)\\prod_{j}P(A_i|C_j)=P(C_j)P(A_i|C_j)(A_2|C_j)\\dots P(A_n|C_j) is maximal .\nAssume that all hypotheses (classes) are equally probable a priori, i.e., P(C_i)=P(C_j) for all i,j\nThis is called assuming a uniform prior. It simplifies computing the posterior: \\displaystyle C_{ML}=\\arg \\max_c P(A_1,A_2,\\dots A_n|C)\nThis hypothesis is called the maximum likelihood hypothesis .\n\n\n\n2.2 Example\nGiven a data as shown in as shown in below data frame, Find if tennis will be played for a scenario given by X: X=( \\mathrm{ Outlook = Sunny, Temperature= Cool, Humidity =High, Wind= Strong})\n\nimport pandas as pd\nAttributes =['Day',    'Outlook',  'Temperature',      'Humidity',     'Wind',     'Play Tennis']\ndata      =[['D1',     'Sunny',    'Hot',              'High',         'Weak',       'No'         ],\n            ['D2',     'Sunny',    'Hot',              'High',         'Strong',     'No'         ],\n            ['D3',     'Overcast',  'Hot',              'High',        'Weak',       'Yes'        ],\n            ['D4',     'Rain',    'Mild',              'High',         'Weak',       'Yes'        ],\n            ['D5',     'Rain',    'Cool',              'Normal',       'Weak',       'Yes'        ],\n            ['D6',     'Rain',    'Cool',              'Normal',       'Strong',     'No'         ],\n            ['D7',     'Overcast', 'Cool',             'Normal',       'Strong',     'Yes'        ],\n            ['D8',     'Sunny',    'Mild',              'High',        'Weak',       'No'         ],\n            ['D9',     'Sunny',    'Cool',             'Normal',       'Weak',       'Yes'        ],\n            ['D10',    'Rain',    'Mild',              'Normal',       'Weak',       'Yes'        ],\n            ['D11',    'Sunny',    'Mild',             'Normal',       'Strong',     'Yes'        ],\n            ['D12',    'Overcast', 'Mild',              'High',        'Strong',     'Yes'        ],\n            ['D13',    'Overcast',  'Hot',             'Normal',       'Weak',       'Yes'        ],\n            ['D14',    'Rain',    'Mild',              'High',         'Strong',     'No'         ]]\ndf = pd.DataFrame(columns=Attributes,data=data)\ndf\n\n\n\n\n\n  \n    \n      \n      Day\n      Outlook\n      Temperature\n      Humidity\n      Wind\n      Play Tennis\n    \n  \n  \n    \n      0\n      D1\n      Sunny\n      Hot\n      High\n      Weak\n      No\n    \n    \n      1\n      D2\n      Sunny\n      Hot\n      High\n      Strong\n      No\n    \n    \n      2\n      D3\n      Overcast\n      Hot\n      High\n      Weak\n      Yes\n    \n    \n      3\n      D4\n      Rain\n      Mild\n      High\n      Weak\n      Yes\n    \n    \n      4\n      D5\n      Rain\n      Cool\n      Normal\n      Weak\n      Yes\n    \n    \n      5\n      D6\n      Rain\n      Cool\n      Normal\n      Strong\n      No\n    \n    \n      6\n      D7\n      Overcast\n      Cool\n      Normal\n      Strong\n      Yes\n    \n    \n      7\n      D8\n      Sunny\n      Mild\n      High\n      Weak\n      No\n    \n    \n      8\n      D9\n      Sunny\n      Cool\n      Normal\n      Weak\n      Yes\n    \n    \n      9\n      D10\n      Rain\n      Mild\n      Normal\n      Weak\n      Yes\n    \n    \n      10\n      D11\n      Sunny\n      Mild\n      Normal\n      Strong\n      Yes\n    \n    \n      11\n      D12\n      Overcast\n      Mild\n      High\n      Strong\n      Yes\n    \n    \n      12\n      D13\n      Overcast\n      Hot\n      Normal\n      Weak\n      Yes\n    \n    \n      13\n      D14\n      Rain\n      Mild\n      High\n      Strong\n      No\n    \n  \n\n\n\n\n\nfrom IPython.display import display_html \nall_tables=\"\"\nfor col in df.columns[1:-1]:\n    table=pd.DataFrame()\n    for c in df['Play Tennis'].unique():\n        for r in df[col].unique():\n            n = df.loc[(df[col]==r) & (df['Play Tennis']==c) ,[col]].count().to_numpy()[0]\n            d = df.loc[(df['Play Tennis']==c) ,[col]].count().to_numpy()[0]\n            table.loc[col+'_'+str(r),'Play_Tennis'+'_'+c]=\"{}/{}\".format(n,d)\n            table_styler = table.style.set_table_attributes(\"style='display:inline'\").set_caption(col)\n    all_tables=all_tables+table_styler._repr_html_()\ndisplay_html (all_tables,raw=True)\n\n\n\n\n  Outlook\n  \n    \n       \n      Play_Tennis_No\n      Play_Tennis_Yes\n    \n  \n  \n    \n      Outlook_Sunny\n      3/5\n      2/9\n    \n    \n      Outlook_Overcast\n      0/5\n      4/9\n    \n    \n      Outlook_Rain\n      2/5\n      3/9\n    \n  \n\n\n\n  Temperature\n  \n    \n       \n      Play_Tennis_No\n      Play_Tennis_Yes\n    \n  \n  \n    \n      Temperature_Hot\n      2/5\n      2/9\n    \n    \n      Temperature_Mild\n      2/5\n      4/9\n    \n    \n      Temperature_Cool\n      1/5\n      3/9\n    \n  \n\n\n\n  Humidity\n  \n    \n       \n      Play_Tennis_No\n      Play_Tennis_Yes\n    \n  \n  \n    \n      Humidity_High\n      4/5\n      3/9\n    \n    \n      Humidity_Normal\n      1/5\n      6/9\n    \n  \n\n\n\n  Wind\n  \n    \n       \n      Play_Tennis_No\n      Play_Tennis_Yes\n    \n  \n  \n    \n      Wind_Weak\n      2/5\n      6/9\n    \n    \n      Wind_Strong\n      3/5\n      3/9\n    \n  \n\n\n\n Above table shows conditional probabilities For example p(\\mathrm{outlook}=\\mathrm{sunny} \\mid \\mathrm{play Tennis} = \\mathrm{no}) is given by row outlook_Sunny and column Play_Tennis_No of table Outlook and  p(\\mathrm{Temperature}=\\mathrm{Cool} \\mid \\mathrm{play Tennis} = \\mathrm{Yes}) is given by row Temperature_Cool and column Play_Tennis_Yes of table Temperature  Also we can calculate below 2 probabilities of the two classes:  P(\\mathrm{Play}=\\mathrm{Yes})=\\frac{9}{14} P(\\mathrm{Play}=\\mathrm{No})=\\frac{5}{14}\n\nForm look up table:  P\\left(\\mathrm{Outlook}=\\mathrm{Sunny} \\mid \\mathrm{Play} = \\mathrm{Yes} \\right) = \\frac{2}{9}  P\\left(\\mathrm{Outlook}=\\mathrm{Sunny} \\mid \\mathrm{Play} = \\mathrm{No} \\right) = \\frac{3}{5}  P\\left(\\mathrm{Temperature}=\\mathrm{Cool} \\mid \\mathrm{Play} = \\mathrm{Yes} \\right) = \\frac{3}{9}  P\\left(\\mathrm{Temperature}=\\mathrm{Cool} \\mid \\mathrm{Play} = \\mathrm{No} \\right) = \\frac{1}{5} P\\left(\\mathrm{Humidity}=\\mathrm{High } \\mid \\mathrm{Play} = \\mathrm{Yes} \\right) = \\frac{3}{9}  P\\left(\\mathrm{Humidity}=\\mathrm{High } \\mid \\mathrm{Play} = \\mathrm{No} \\right) = \\frac{4}{5} P\\left(\\mathrm{Wind}=\\mathrm{Strong } \\mid \\mathrm{Play} = \\mathrm{Yes} \\right) = \\frac{3}{9}  P\\left(\\mathrm{Wind}=\\mathrm{Strong } \\mid \\mathrm{Play} = \\mathrm{No} \\right) = \\frac{3}{5}\nMAP Rule: P\\left(\\mathrm{Yes} \\mid X\\right)=\\\\  P\\left(\\mathrm{Outlook}=\\mathrm{Sunny} \\mid \\mathrm{Play} = \\mathrm{Yes} \\right) \\times \\\\  P\\left(\\mathrm{Temperature}=\\mathrm{Cool} \\mid \\mathrm{Play} = \\mathrm{Yes} \\right) \\times \\\\  P\\left(\\mathrm{Humidity}=\\mathrm{High } \\mid \\mathrm{Play} = \\mathrm{Yes} \\right) \\times \\\\  P\\left(\\mathrm{Wind}=\\mathrm{Strong } \\mid \\mathrm{Play} = \\mathrm{Yes} \\right) \\times \\\\  P(\\mathrm{Play}=\\mathrm{Yes})\\\\  =0.0053 P\\left(\\mathrm{No} \\mid X\\right)= \\\\  P\\left(\\mathrm{Outlook}=\\mathrm{Sunny} \\mid \\mathrm{Play} = \\mathrm{No} \\right) \\times \\\\  P\\left(\\mathrm{Temperature}=\\mathrm{Cool} \\mid \\mathrm{Play} = \\mathrm{No} \\right) \\times \\\\  P\\left(\\mathrm{Humidity}=\\mathrm{High } \\mid \\mathrm{Play} = \\mathrm{No} \\right) \\times \\\\  P\\left(\\mathrm{Wind}=\\mathrm{Strong } \\mid \\mathrm{Play} = \\mathrm{No} \\right) \\times \\\\  P(\\mathrm{Play}=\\mathrm{No}) \\\\  = 0.0206   Since P(\\mathrm{Play}=\\mathrm{Yes}) < P(\\mathrm{Play}=\\mathrm{No}) so X shall be labeled to be “No”, i.e. Given scenario X tennis will not be played.\n\n\n\n2.3 Pros and Cons\n\nCombines prior knowledge and observed data\nOutput is not only a classification but a probability distribution over all classes\nRobust to isolated noise points.\nHandle missing values by ignoring instance during probability estimate calculation.\nRobust to irrelevant attributes.\nwith each training example, the prior and the likelihood can be updated dynamically\nIndependence assumption may not hold always.\n\n\n\n2.4 Practical Issues\n\nDiscretize the range into bins\n\nOne ordinal attribute per bin\nViolates independence assumption\n\nTwo way split : (A < v) or (A < v)\n\nchoose only one of the two splits as new attribute.\n\nProbability density estimation:\n\nAssume attribute follows a parametrized distribution, e.g. normal distribution.\nUse data to estimate parameters of distribution, e.g. mean and standard deviation using maximum likelihood estimation.\nOnce probability distribution is known, can use it to estimate the conditional probability, P(A_i \\mid c)\n\n\n\n\n2.5 Bayesian Belief network ( Bayesian net )\nDescribe conditional independence among subset of variables (attributes) : combining prior knowledge about dependencies among variables with observed training data. For example consider below graph:\n\nHere Age, Occupation and Income determine if customer will byt this product, Given that customer buys product, whether there is interest in insurance is now independent of Age, Occupation, Income. P\\left(\\mathrm{Age,Occ,Inc,Buy,Ins}\\right)=P(\\mathrm{Age})P(\\mathrm{Occ})P\\left(\\mathrm{Inc}\\right)P\\left(\\mathrm{Buy} \\mid \\mathrm{Age, Occ, Inc}\\right)P\\left(\\mathrm{Int} \\mid \\mathrm{Buy}\\right)\n\n\n2.6 Naive Bayes Classifier category\n\nIt is an Inductive Learning.\nIt is a generative Modeling.\nIt can be Parametric or Non-parametric Models.\nIt can be online or offline Models.\n\n \\tiny {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-09-24-CS5590-week5.html",
    "href": "Data_Science_Notes/Machine-Learning/2022-09-24-CS5590-week5.html",
    "title": "Machine Learning 5",
    "section": "",
    "text": "Deep learning : A sub area of machine learning, that is today understood as representation learning.\nInspired by the human brain.\nHow do Neural Networks learn:\n\nWe initialize the weights with random value.\nThen present a trining pattern to the network.\nFeed it through tho get output. (feed forward)\ncompare with target output.\nAdjust weights based on the error.\nAnd so on …\n\nDeep learning models can learn complex decision boundaries."
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-09-24-CS5590-week5.html#perceptrons-linear-models",
    "href": "Data_Science_Notes/Machine-Learning/2022-09-24-CS5590-week5.html#perceptrons-linear-models",
    "title": "Machine Learning 5",
    "section": "2 Perceptrons (Linear Models)",
    "text": "2 Perceptrons (Linear Models)\n\nConsider the below pic:  Mathematical formulation is given as below: \\displaystyle  z = \\left\\lbrace\n\\begin{array}{ccc}\n  1 & \\text{if} & \\displaystyle  \\sum_{i=1}^n x_iw_i \\ge \\theta \\\\\n  0 & \\text{if} & \\displaystyle  \\sum_{i=1}^n x_iw_i < \\theta\n\\end{array} \\right.\nLearn weight such that the objective function is maximized.\nLoss calculation :  \\Delta W_i = c(t-z)X_i  where W_i is the weight from input i to perceptron node, c is the learning rate, t_j is the target for the current instance, z is the current output, and X_i is i^{th} input\nLeast perturbation principle\n\nonly change weights if there is an error\nsmall c sufficient to make current pattern corret\nscale by X_i\n\ncreate a perceptron node with n inputs.\nIteratively apply a pattern from the training set and apply the perceptron rule\nEach iteration through the training set is an epoch\ncontinue training until total training set error ceases to improve\nPeceptron Convergence Theorem : Guaranteed to find a solution in finite time if a solution exists"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-09-24-CS5590-week5.html#multi-layer-perceptrons",
    "href": "Data_Science_Notes/Machine-Learning/2022-09-24-CS5590-week5.html#multi-layer-perceptrons",
    "title": "Machine Learning 5",
    "section": "3 Multi Layer Perceptrons",
    "text": "3 Multi Layer Perceptrons\n\n3.1 MLP From PRML book\n\nNetwork with inputs, one hidden unit and outputs: \nThe output of the above network can be given as follows:   \\displaystyle y_{k}({\\bf x},{\\bf w})=\\sigma\\left(\\sum_{j=1}^{M}w_{k j}^{(2)}h\\left(\\sum_{i=1}^{D}w_{j i}^{(1)}x_{i}+w_{j0}^{(1)}\\right)+w_{k0}^{(2)}\\right) This equation is also interpreted as forward propagation of information through the newwork. It should be emphasized that these diagrams do not represent probabilistic graphical models, because the internal nodes represent deterministic variables rather than stochastic ones.\nThe above equation can be written as below if bias is absorbed into the set of weight by defining additional input variable x_0 whose value is clamped at x_0=1  \\displaystyle  y_{k}({\\bf x},{\\bf w})=\\sigma\\left(\\sum_{j=0}^{M}w_{k j}^{(2)}h\\left(\\sum_{i=0}^{D}w_{j i}^{(1)}x_{i}\\right)\\right)\nA key difference among neural network and perceptron, is that the neural network uses continuous sigmoidal non-linearities in the hidden units, whereas the perceptron uses step-function non-linearities.\nIf the activation functions of all the hidden units in a network are taken to be linear, then for any such network we can always find an equivalent network without hidden units.\nIn principle, a network with sigmoidal hidden units can always mimic skip layer connections by using a sufficiently small first-layer weight that, over its operating range.\nIn practice, however, it may be advantageous to include skip-layer connections explicitly.\n\n\n\n3.2 MLP Form lecture PDF\n\nExtension of perceptrons to multiple layers\n\nInitialize network with random weights\nFor all training cases ( called examples):\n\npresent training inputs to network and calculate output\nfor all layers (starting with output layer, back to input layer):\n\ncompare network output with correct putput\nAdapt weight in current layer\n\n\n\nMethod for Learning Weights in feed forward nets\n\nCan’t use Perceptron Rule\n\nNo teacher values (loss) are possible for hidden units.\n\nUse Gradient decent to minimize the error\n\nPropagate the deltas to adjust for errors backward from outputs to hidden layers to inputs\nThe algorithm can be summarized as follows:\n\nComputes the error term for the output units using the observed error.\nFrom output layer , repeat\n\nPropagating the error term back to the previous layer and updating the weights between the two layers until the earliest layer is reached.\n\n\n\n\nAlgorithm in detail:\n\nInitialize weights (typically random)\nKeep doing epoch\n\nFor each example e in the training set do\n\nForward Pass to compute\n\ny = neural new output (network , e)\nmiss = (T-y) at each output unit\n\nbackward pass to calculate deltas to weights\nupdate all weights\n\nend\n\nuntil tuning set error stops improving"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-09-24-CS5590-week5.html#error-backpropagation",
    "href": "Data_Science_Notes/Machine-Learning/2022-09-24-CS5590-week5.html#error-backpropagation",
    "title": "Machine Learning 5",
    "section": "4 Error Backpropagation",
    "text": "4 Error Backpropagation\n\n4.1 Backpropagation From PRML book\n\nThink of the N weights as a point in an N-dimensional space \nMany error functions fo practical interest, comprise a sum of terms, one for each data point in training set, so that  \\displaystyle E(\\mathbf{w})=\\sum_{n=1}^{N}E_{n}(\\mathbf{w})\nError function for one particular input patter n takes the form  \\displaystyle E_{n}={\\frac{1}{2}}\\sum_{k}(y_{n k}-t_{n k})^{2} where y_{n k}=y_{k}(\\mathbf{x}_{n},\\mathbf{w})\nThe gradient of this error with respect to a weight w_{ji} is given by  \\displaystyle {\\frac{\\partial E_{n}}{\\partial w_{j i}}}=(y_{n j}-t_{n j})x_{n i}\nIn a general feed-forward network, each unit computes a weighted sum of its inputs of the form:  \\displaystyle a_{j}=\\sum_{i}w_{j i}z_{i} where z_i is the activation of a unit, or input, that sends a connection to unit j, and w_{ji} is the weight associated with that connection\nA non-linear activation function h(.) transforms a_j to produce z_j of unit j in the form  z_{j}=h(a_{j}) Note z_i in equation a_{j}=\\sum_{i}w_{j i}z_{i} could be an input, and the unit j in equation z_{j}=h(a_{j}) could be an output\nNow we consider to evaluate derivative of E_n with respect to w_{ji}, E_n depends on the weight w_{ji} only via the summed input a_j to unit j. Applying chain rule for partial derivatives we get  \\displaystyle \\frac{\\partial E_{n}}{\\partial w_{j i}}=\\frac{\\partial E_{n}}{\\partial a_{j}}\\frac{\\partial a_{j}}{\\partial w_{j i}} \\tag{1}\nConsider a useful notation  \\displaystyle \\delta_{j}\\equiv\\frac{\\partial{ E}_{n}}{\\partial a_{j}} \\tag{2}\nwe can find derivative of a_j with respect to w_{ji} using a_{j}=\\sum_{i}w_{j i}z_{i}, we get  \\displaystyle {\\frac{\\partial a_{j}}{\\partial w_{j i}}}=z_{i} \\tag{3}\nUsing above equation (1),(2) \\text{ and },(3) we get  \\displaystyle \\frac{\\partial E_{n}}{\\partial w_{j i}}=\\delta_{j}z_{i}\nThis tells us required derivative is obtained simply by multiplying the value of \\delta for the unit at the output end of the weight by the value of z for the unit at the input end of the weight.\nFor output unit we have  \\delta_k = y_k - t_k\nFor hidden units, we again make use of chain rule for partial derivatives  \\displaystyle \\delta_{j}\\equiv\\frac{\\partial E_{n}}{\\partial a_{j}}=\\sum_{k}\\frac{\\partial E_{n}}{\\partial a_{k}}\\frac{\\partial a_{k}}{\\partial a_{j}} \\tag{4}\nNow we know that  \\displaystyle \\frac{\\partial E_{n}}{\\partial a_{k}} = \\delta _k\nalso, a_k =\\sum_j w_{kj} z_j \\;\\textrm{and}\\;z_j =h(a_j )\nso, \\displaystyle \\frac{\\partial a_{k}}{\\partial a_{j}} =  \\frac{\\partial \\sum_{j}w_{k j}h(a_{j})}{\\partial a_{j}} = w_{k j} h^\\prime(a_{j}) putting value of \\frac{\\partial E_{n}}{\\partial a_{k}} and \\frac{\\partial a_{k}}{\\partial a_{j}} in the equation (4) we get \\displaystyle \\delta_{j}=h^{\\prime}(a_{j})\\sum_{k}w_{k j}\\delta_{k}\nIn short what we discussed till now  \\begin{align*}\n\\displaystyle  \\frac{\\partial E_{n}}{\\partial w_{j i}}&=\\frac{\\partial E_{n}}{\\partial a_{j}}\\frac{\\partial a_{j}}{\\partial w_{j i}}\\\\\n&= \\left(  \\sum_{k}{\\frac{\\partial E_{n}}{\\partial a_{k}}}{\\frac{\\partial a_{k}}{\\partial a_{j}}}  \\right)   \\frac{\\partial a_{j}}{\\partial w_{j i}}\\\\\n&= \\left(  \\sum_{k}{\\frac{\\partial E_{n}}{\\partial a_{k}}}\\left( w_{k j}h^{\\prime}(a_{j}) \\right)  \\right)   \\frac{\\partial a_{j}}{\\partial w_{j i}}\\\\\n&= h^{\\prime}(a_{j})\\left(  \\sum_{k}{\\frac{\\partial E_{n}}{\\partial a_{k}}}w_{k j}    \\right)   \\frac{\\partial a_{j}}{\\partial w_{j i}}\\\\\n&= h^{\\prime}(a_{j})\\left(  \\sum_{k}{\\frac{\\partial E_{n}}{\\partial a_{k}}}w_{k j}    \\right)  z_i\\\\\n&= h^{\\prime}(a_{j}) z_i \\left(  \\sum_{k}{\\frac{\\partial E_{n}}{\\partial a_{k}}}w_{k j}    \\right)\\\\\n\\end{align*}\nIn sort what we discussed till now with little more elaboration  \\displaystyle  y_{k}({\\bf x},{\\bf w})= \\overbrace{\\sigma \\left(  \\underbrace{ \\sum_{j=0}^{M}w_{k j}^{(2)} \\times  \\overbrace{h\\left(\\underbrace{\\sum_{i=0}^{D}w_{j i}^{(1)} \\times z_{i}}_{a_j} \\right)}^{z_j}}_{a_k} \\right)}^{z_k}\n\\displaystyle  y_{k}({\\bf x},{\\bf w})= \\overbrace{ \\underbrace{\\sigma}_{\\text{activation fucntion at output }}  \\left(  \\underbrace{ \\sum_{j=0}^{M}w_{k j}^{(2)} \\times  \\overbrace{ \\underbrace{h}_{\\text{activation fucntion at hidden}}  \\left(\\underbrace{\\sum_{i=0}^{D}w_{j i}^{(1)} \\times \\overbrace{z_{i}}^{\\text{from last layer or input }x_i} }_{a_j} \\right)}^{z_j}}_{a_k} \\right)}^{z_k}\n\\displaystyle E_{n}={\\frac{1}{2}}\\sum_{k}(y_{k}-t_{k})^{2} Till now we had below expression :  \\displaystyle  \\frac{\\partial E_{n}}{\\partial w_{j i}}= \\left(  \\sum_{k} \\frac{\\partial E_{n}}{\\partial a_{k}} {\\frac{\\partial a_{k}}{\\partial a_{j}}}  \\right)   \\frac{\\partial a_{j}}{\\partial w_{j i}} But if we consider activation function \\sigma at out layer and y_k = z_k = \\sigma (a_k) we get :  \\begin{align*}\n\\displaystyle  \\frac{\\partial E_{n}}{\\partial w_{j i}}&= \\left(\\sum_{k} \\frac{\\partial E_{n}}{\\partial z_{k}} {\\frac{\\partial z_{k}}{\\partial a_{k}}} {\\frac{\\partial a_{k}}{\\partial a_{j}}}\\right)\\frac{\\partial a_{j}}{\\partial w_{j i}}\\\\\n&= \\left(  \\sum_{k} \\frac{\\partial E_{n}}{\\partial z_{k}}  {\\frac{\\partial z_{k}}{\\partial a_{k}}} {\\frac{\\partial \\sum_{j}w_{k j}z_{j}}{\\partial a_{j}}}  \\right)   \\frac{\\partial a_{j}}{\\partial w_{j i}}\\\\\n&= \\left(  \\sum_{k} \\frac{\\partial E_{n}}{\\partial z_{k}}  {\\frac{\\partial z_{k}}{\\partial a_{k}}} {\\frac{\\partial \\sum_{j}w_{k j}h(a_{j})}{\\partial a_{j}}}  \\right)   \\frac{\\partial a_{j}}{\\partial w_{j i}}\\\\\n&= \\left(  \\sum_{k} \\frac{\\partial E_{n}}{\\partial z_{k}}  {\\frac{\\partial z_{k}}{\\partial a_{k}}} w_{kj}h^\\prime(a_j) \\right)   \\frac{\\partial a_{j}}{\\partial w_{j i}}\\\\\n&= \\left(  \\sum_{k} \\frac{\\partial E_{n}}{\\partial z_{k}}  {\\frac{\\partial z_{k}}{\\partial a_{k}}} w_{kj} \\right)  h^\\prime(a_j)  \\frac{\\partial a_{j}}{\\partial w_{j i}}\\\\\n&= \\left(  \\sum_{k} \\frac{\\partial E_{n}}{\\partial z_{k}}  {\\frac{\\partial z_{k}}{\\partial a_{k}}} w_{kj} \\right)  h^\\prime(a_j)  \\frac{\\partial \\sum_{i}w_{j i}z_{i}}{\\partial w_{j i}}\\\\\n&= \\left(\\sum_{k}\\frac{\\partial E_{n}}{\\partial z_{k}}{\\frac{\\partial z_{k}}{\\partial a_{k}}} w_{kj} \\right) h^\\prime(a_j)  z_i\\\\\n\\end{align*}\nSince z_k is same as y_k, we can replace z_k with y_k in above equation   \\displaystyle  \\frac{\\partial E_{n}}{\\partial w_{j i}}= \\left(  \\sum_{k} {\\frac{\\partial E_{n}}{\\partial y_{k}}}  {\\frac{\\partial y_{k}}{\\partial a_{k}}} w_{kj} \\right)  h^\\prime(a_j)  z_i we know y_k=z_k=\\sigma (a_k) so we get  \\begin{align*}\n\\displaystyle  \\frac{\\partial E_{n}}{\\partial w_{j i}}&= \\left(  \\sum_{k} {\\frac{\\partial E_{n}}{\\partial y_{k}}}  {\\frac{\\partial \\sigma (a_k)}{\\partial a_{k}}} w_{kj} \\right)  h^\\prime(a_j)  z_i\\\\\n&= \\underbrace{\\left(  \\sum_{k} \\underbrace{\\frac{\\partial E_{n}}{\\partial y_{k}}}_{(y_k-t_k)} \\times  \\overbrace{ \\frac{\\partial \\sigma (a_k)}{\\partial a_{k}}}^{z_k(1-z_k) \\text{ or }y_k(1-y_k)} \\times  w_{kj} \\right)}_{\\text{miss}}   \\underbrace{h^\\prime(a_j) }_{ z_j(1-z_j)}  z_i\\\\\n&=  \\left(  \\sum_{k}  (y_k-t_k) y_k (1-y_k)w_{kj} \\right) z_j(1-z_j)z_i\\\\  \n\\end{align*}\nAlso if we consider two layer network we can replace z_i with x_i, Hence we get \\boxed{\\frac{\\partial E_{n}}{\\partial w_{j i}} =\\left(  \\sum_{k}  (y_k-t_k) y_k (1-y_k)w_{kj} \\right) z_j(1-z_j)x_i}\nNotice\n\n\n\n\n\n\nNote\n\n\n\n\\begin{align*}\ny_{k}({\\bf x},{\\bf w}) &= \\overbrace{ \\underbrace{\\sigma}_{\\text{activation fucntion at output }}  \\left(  \\underbrace{ \\sum_{j=0}^{M}w_{k j}^{(2)} \\times  \\overbrace{ \\underbrace{h}_{\\text{activation fucntion at hidden}}  \\left(\\underbrace{\\sum_{i=0}^{D}w_{j i}^{(1)} \\times \\overbrace{z_{i}}^{\\text{from last layer or input }x_i} }_{a_j} \\right)}^{z_j}}_{a_k} \\right)}^{z_k}\\\\\n\\frac{\\partial E_{n}}{\\partial w_{j i}} &=  \\left(  \\sum_{k}  (y_k-t_k) y_k (1-y_k)w_{kj} \\right) z_j(1-z_j)z_i\n\\end{align*}\n\n\n\n\n\nError Backpropagation summery :\n\nApply an input vector x_n to the network and forward propagate through the network using below two equations to find the activations of all the hidden and output units.  \\displaystyle a_{j}=\\sum_{i}w_{j i}z_{i}  z_j = h(a_j)\nEvaluate the \\delta _k for all the output units using \\delta_K = y_k - t_k\nBackpropagete all the \\delta using below equation to find \\delta _j for each hidden unit in the network  \\displaystyle \\delta_{j}=h^{\\prime}(a_{j})\\sum_{k}w_{k j}\\delta_{k}\nUse below equation to evaluate the required derivatives \\displaystyle \\frac{\\partial E_{n}}{\\partial w_{j i}}=\\delta_{j}z_{i}\n\nFor batch methods, the derivative of the total error E can then be obtained by repeating the above steps for each pattern in the training set and then summing over all patterns: \\displaystyle \\frac{\\partial E}{\\partial w_{j i}}=\\sum_{n}\\frac{\\partial E_{n}}{\\partial w_{j i}}\n\n\n\n4.2 Backpropagation Form lecture PDF\nIt also has same points as per PRML but from different angle. But DIFFERENT TERMINOLOGY IS USED HERE SO BE CAREFUL\n\n  \n\\mathbf{\\text{Alert !!!}}\n\n\nSOME NOTATION IS WRONG IN THIS SECTION. DO NOT READ THIS SECTION, ALL CONCEPTS ARE ALREADY EXPLAINED \n\nTerminology  g is activation function  y=g(z)  E = (t_i-y_i)^2  z_i = a_j\\times w_{ij}\nAdd a dimension for the observed error\nTry to minimize your position on the “error surface”\nCompute :  \\text{Grad}_E = \\left[ \\frac{dE}{dW_1},\\frac{dE}{dW_2},\\dots , \\frac{dE}{dW_n} \\right]\nChange i_{th} weight by  \\Delta W_i = -\\alpha \\frac{dE}{dW_i}\nWe also use activation function at the end of every node.\nconsider g(z)=y where g is sigmoid activation function.\ng'(z)=g(z)\\times (1-g(z))=y(1-y)\nActivation function must be continuous, differential, non-decreasing, and easy to compute.\nWe want activation function to be non-decreasing because, so that it should not increase value in some reason and decrease it in some other reason.\n\n\n4.2.1 Updating Hidden-to-Output\n\nUpdating Hidden-to-Output \\frac{\\partial E}{\\partial W_ij}= \\frac{\\partial E}{\\partial y} \\times \\frac{\\partial y}{\\partial z} \\times \\frac{\\partial z}{\\partial W_ij} \\Delta W_{ij} = \\alpha \\times \\underbrace{(t_i -y_i)}_{\\frac{\\partial E}{\\partial y} } \\times \\underbrace{g'(z_i)}_{\\frac{\\partial y}{\\partial z} } \\times \\underbrace{a_j}_{\\frac{\\partial z}{\\partial W_ij}} \\Delta W_{ij} = \\overbrace{\\alpha}^{\\text{learning rate}} \\times \\underbrace{(\\overbrace{t_i}^{\\text{Teacher supplied}} -y_i)}_{\\text{miss}} \\times \\overbrace{g'(z_i)}^{\\text{derivatve of acitvation function}} \\times \\underbrace{a_j}_{\\text{previous layer output}} \\Delta W_{ij} = \\alpha \\times (t_i -y_i) \\times y_i \\times (1-y_i)\\times a_j \n\n\n\n4.2.2 Updating interior weights\n\nUpdating interior weights Layer k units provide values to all layers k+1 units. “miss” is sum of the misses from all units on k+1 \\displaystyle \\text{miss}_j = \\sum \\left[ a_j(1-a_j)(t_i-a_j)w_{ji} \\right] \\displaystyle \\frac{\\partial E}{\\partial W_kj}= \\left( \\sum \\frac{\\partial E}{\\partial y_i} \\times \\frac{\\partial y_i}{\\partial z_i} \\times \\frac{\\partial z_i}{\\partial a_j} \\right) \\times \\frac{\\partial a_j}{\\partial l_j} \\times \\frac{\\partial l_j}{\\partial W_ij} \\displaystyle \\frac{\\partial E}{\\partial W_kj}= \\left( \\sum \\underbrace{\\frac{\\partial E}{\\partial y_i}}_{t_i-y_i} \\times \\overbrace{\\frac{\\partial y_i}{\\partial z_i}}^{y_i(1-y_i)} \\times \\underbrace{\\frac{\\partial z_i}{\\partial a_j}}_{w_ji} \\right) \\times \\underbrace{\\frac{\\partial a_j}{\\partial l_j}}_{a_j(1-a_j)} \\times \\overbrace{\\frac{\\partial l_j}{\\partial W_ij}}^{l_k} \\displaystyle \\frac{\\partial E}{\\partial W_kj} = \\left( \\sum y_i \\times (1-y_i) \\times (t_i-y_i) \\times w_{ji}\\right) \\times l_k \\times a_j \\times (1-a_j)\n\n \\tiny {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-10-15-CS5590-week7.html",
    "href": "Data_Science_Notes/Machine-Learning/2022-10-15-CS5590-week7.html",
    "title": "Machine Learning 7",
    "section": "",
    "text": "Numeric\n\nZero mean, unit variance: x’= (x-\\mu)/ \\sigma\nIn interval [0,1]: x'=(x -\\min)/(\\max - \\min)\n\nCategorical\n\nEncoded as number in such a way that there is no sense of ordering, for e.g. if there are 3 classes apple, orange and banana, and encoded as 1,2,3 respectively, it appears as apple comes first than orange, which is not correct. So the correct way to encode is one hot encoding.\nAlso here only equality testing is meaningful.\n\nOrdinal\n\nEncoded as numbers to preserve ordering\n\\le, \\ge operations meaningful"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-10-15-CS5590-week7.html#feature-extraction-from-data",
    "href": "Data_Science_Notes/Machine-Learning/2022-10-15-CS5590-week7.html#feature-extraction-from-data",
    "title": "Machine Learning 7",
    "section": "2 Feature Extraction from Data",
    "text": "2 Feature Extraction from Data\n\nImages\n\nPixel values, Segment and extract features, Handcrafted features: HOG, SIFT,etc\nDeep learned features!\n\nText\n\nBag of words, Ngrams\nDeep learned features!\n\nSpeech\n\nMel Frequency Cepstral Coefficients (MFCCs), Other frequency based features\nDeep learned features!\n\nTime varying sensor Data\n\nStatistical and moment based features (mean, variance) etc"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-10-15-CS5590-week7.html#challenges",
    "href": "Data_Science_Notes/Machine-Learning/2022-10-15-CS5590-week7.html#challenges",
    "title": "Machine Learning 7",
    "section": "3 Challenges",
    "text": "3 Challenges\n\nStructured input/Structured output\n\nOne fix: Attribute = root-to-leaf paths\n\nMissing data\n\nFix: Fill in the value, Introduce special label, remove instance, remove attribute, Use classifiers that can handle missing values\n\nOutliers\n\nFix: Remove, Threshold, Visualize!\n\nData assumptions\n\nGenerated how? Sources?\nSmooth? Linear? Noise?"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-10-15-CS5590-week7.html#class-imbalance",
    "href": "Data_Science_Notes/Machine-Learning/2022-10-15-CS5590-week7.html#class-imbalance",
    "title": "Machine Learning 7",
    "section": "4 Class Imbalance",
    "text": "4 Class Imbalance\nAlmost all classifiers attempt to reduce global quantities such as the error rate, not taking the data distribution into consideration.\nAs a result, examples from the overwhelming class are well classified whereas examples from the minority class tend to be misclassified.\n\nAre all classifiers sensitive to class imbalance?\n\nDecision Tree:Very sensitive to class imbalances. This is because the algorithm works globally, not paying attention to specific data points.\nMulti Layer perceptrons (MLPs): are less prone to the class imbalance problem. This is because of their flexibility: their solution gets adjusted by each data point in a bottom up manner as well as by the overall data set in a top down manner.\nSupport Vector Machines (SVMs) SVMs are even less prone to the class imbalance problem than MLPs because they are only concerned with a few support vectors, the data points located close to the boundaries.\n\n\n\n4.1 Solution\n\nCollect more data!\nChange your performance metric:\n\nConfusion Matrix, Precision Recall, F1 score, etc.\n\nResample dataset\nGenerate synthetic samples\nTry penalized models\nTry a different perspective anomaly/change detection \nAt the data Level: Re Sampling\n\nOversampling (Random or Directed)\nUnder-sampling (Random or Directed), (not good for model performance)\nActive Sampling\n\nAt the Algorithmic Level:\n\nAdjusting the Costs\nAdjusting the decision threshold / probabilistic estimate at the tree leaf \n\nUnder-sampling (random and directed) is not effective and can even hurt performance.\nRandom oversampling helps quite dramatically. Directed oversampling makes a bit of a difference by helping slightly more.\nCost adjusting is about as effective as Directed oversampling. Generally, however, it is found to be slightly more useful."
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-10-15-CS5590-week7.html#smote",
    "href": "Data_Science_Notes/Machine-Learning/2022-10-15-CS5590-week7.html#smote",
    "title": "Machine Learning 7",
    "section": "5 SMOTE",
    "text": "5 SMOTE\nSMOTE = Synthetic Minority Oversampling Technique\n\nFor each minority example k, compute nearest minority class examples (i,j,l,n,m)\nSynthetically generate event k_1 such that k_1 lies between k and i\nRandomly chose an example out of 5 closest points."
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-10-15-CS5590-week7.html#using-large-datasets",
    "href": "Data_Science_Notes/Machine-Learning/2022-10-15-CS5590-week7.html#using-large-datasets",
    "title": "Machine Learning 7",
    "section": "6 Using Large Datasets",
    "text": "6 Using Large Datasets\n\nAt large data scales, the performance of different algorithms converge such that performance differences virtually disappear.\nGiven a large enough data set, the algorithm you’d want to use is the one that is computationally less expensive.\nIt’s only at smaller data scales that the performance differences between algorithms matter.\nCPUs vs GPUs\n\nDeep learning has greatly benefited from GPUs\n\nMap Reduce/ Hadoop , Apache Spark, Vowpal Wabbit frameworks\n\nMany learning algorithms amenable to partitioning of computations"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-10-15-CS5590-week7.html#generalization-error",
    "href": "Data_Science_Notes/Machine-Learning/2022-10-15-CS5590-week7.html#generalization-error",
    "title": "Machine Learning 7",
    "section": "7 Generalization Error",
    "text": "7 Generalization Error\n\nComponents of generalization error\nBias: how much the average model over all training sets differ from the true model?\n\nError due to inaccurate assumptions/simplifications made by the model\n\nVariance: how much models estimated from different training sets differ from each other\nMSE in terms of bias and variance \\color{blue} \\text{MSE}=\\color{red} \\underbrace{\\text{Bias}^2}_{\\text{error due to incorrect assumption}} + \\color{green} \\underbrace{\\text{Variance}}_{\\text{error due to variance in training}} + \\color{purple} \\underbrace{\\text{Noise}}_{\\text{Unavoidable error}} \\tag{1} \nSuppose the ultimate true function is f, the one which we ideally want to learn. Our target is t. Relation between t and f is as follows t=f+\\epsilon where \\epsilon is noise and it’s expected value is considered to be zero i.e. \\mathbf{E}[\\epsilon]=0  We consider y_i to be predicted output by a Neural Network then MSE is given as \\mathrm{MSE}=\\frac{1}{N}\\sum_{i=1}^n {\\left(t_i -y_i \\right)}^2 Find expectation of MSE \\begin{align*}{}\n\\mathit{\\mathbf{E}}\\left\\lbrack \\mathrm{MSE}\\right\\rbrack &=\\mathit{\\mathbf{E}}\\left\\lbrack \\frac{1}{N}\\sum_{i=1}^n {\\left(t_i -y_i \\right)}^2 \\right\\rbrack \\\\\n&=\\frac{1}{N}\\sum_{i=1}^n \\mathit{\\mathbf{E}}\\left\\lbrack {\\left(t_i -y_i \\right)}^2 \\right\\rbrack\n\\end{align*} Now we examine \\mathit{\\mathbf{E}}\\left\\lbrack {\\left(t_i -y_i \\right)}^2 \\right\\rbrack \\begin{align*}{}\n\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(t_i -y_i \\right)}^2 \\right\\rbrack &=\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(\\left(t_i -f_i \\right)+\\left(f_i -y_i \\right)\\right)}^2 \\right\\rbrack \\\\\n&=\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(t_i -f_i \\right)}^2 +{\\left(f_i -y_i \\right)}^2 -2\\left(t_i -f_i \\right)\\left(f_i -y_i \\right)\\right\\rbrack \\\\\n&=\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(t_i -f_i \\right)}^2 \\right\\rbrack +\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(f_i -y_i \\right)}^2 \\right\\rbrack -\\mathit{\\mathbf{E}}\\left\\lbrack 2\\left(t_i -f_i \\right)\\left(f_i -y_i \\right)\\right\\rbrack \\\\\n&=\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(t_i -f_i \\right)}^2 \\right\\rbrack +\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(f_i -y_i \\right)}^2 \\right\\rbrack -2\\left(\\mathit{\\mathbf{E}}\\left\\lbrack t_i f_i \\right\\rbrack -\\mathit{\\mathbf{E}}\\left\\lbrack t_i y_i \\right\\rbrack -\\mathit{\\mathbf{E}}\\left\\lbrack f_i^2 \\right\\rbrack +\\mathit{\\mathbf{E}}\\left\\lbrack f_i y_i \\right\\rbrack \\right)\\\\\n&=\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(t_i -f_i \\right)}^2 \\right\\rbrack +\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(f_i -y_i \\right)}^2 \\right\\rbrack -2\\left(f_i^2 -\\mathit{\\mathbf{E}}\\left\\lbrack f_i y_i \\right\\rbrack -f_i^2 +\\mathit{\\mathbf{E}}\\left\\lbrack f_i y_i \\right\\rbrack \\right)\\\\\n&=\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(t_i -f_i \\right)}^2 \\right\\rbrack +\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(f_i -y_i \\right)}^2 \\right\\rbrack\n\\end{align*} Above we used the fact that\n\n\\mathit{\\mathbf{E}}\\left\\lbrack t_i f_i \\right\\rbrack =f_i^2 \\;\\mathrm{Since}\\;f\\;\\mathrm{is}\\;\\mathrm{deterministic}\\;\\mathrm{and}\\;\\mathit{\\mathbf{E}}\\left\\lbrack t_i \\right\\rbrack =f_i\n:\\mathit{\\mathbf{E}}\\left\\lbrack f_i^2 \\right\\rbrack =f^2 \\;\\mathrm{Since}\\;f\\;\\mathrm{is}\\;\\mathrm{deterministic}\n:\\mathit{\\mathbf{E}}\\left\\lbrack t_i y_i \\right\\rbrack =\\mathit{\\mathbf{E}}\\left\\lbrack \\left(f_i +\\epsilon \\right)y_i \\right\\rbrack =\\mathit{\\mathbf{E}}\\left\\lbrack f_i y_i \\right\\rbrack +\\mathit{\\mathbf{E}}\\left\\lbrack \\epsilon y_i \\right\\rbrack =\\mathit{\\mathbf{E}}\\left\\lbrack f_i y_i \\right\\rbrack +0=\\mathit{\\mathbf{E}}\\left\\lbrack f_i y_i \\right\\rbrack, Here \\mathit{\\mathbf{E}}\\left\\lbrack \\epsilon y_i \\right\\rbrack is zero because the noise in the infinite test set over which we take the expectation is probabilistically independent of the NN prediction\n\nSo we got: \\mathit{\\mathbf{E}}\\left\\lbrack {\\left(t_i -y_i \\right)}^2 \\right\\rbrack =\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(t_i -f_i \\right)}^2 \\right\\rbrack +\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(f_i -y_i \\right)}^2 \\right\\rbrack \\tag{2} Thus the MSE can be decomposed in expectation into the variance of the noise and the MSE between the true function and the predicted values\nWe can apply same trick on last term of equation (2)\n\\begin{align*}{}\n\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(f_i -y_i \\right)}^2 \\right\\rbrack &=\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(\\left(f_i -\\mathit{\\mathbf{E}}\\left\\lbrack y_i \\right\\rbrack \\right)+\\left(\\mathit{\\mathbf{E}}\\left\\lbrack y_i \\right\\rbrack -y_i \\right)\\right)}^2 \\right\\rbrack \\\\\n&=\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(f_i -\\mathit{\\mathbf{E}}\\left\\lbrack y_i \\right\\rbrack \\right)}^2 +{\\left(\\mathit{\\mathbf{E}}\\left\\lbrack y_i \\right\\rbrack -y_i \\right)}^2 -2\\left(f_i -\\mathit{\\mathbf{E}}\\left\\lbrack y_i \\right\\rbrack \\right)\\left(\\mathit{\\mathbf{E}}\\left\\lbrack y_i \\right\\rbrack -y_i \\right)\\right\\rbrack \\\\\n&=\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(f_i -\\mathit{\\mathbf{E}}\\left\\lbrack y_i \\right\\rbrack \\right)}^2 \\right\\rbrack +\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(\\mathit{\\mathbf{E}}\\left\\lbrack y_i \\right\\rbrack -y_i \\right)}^2 \\right\\rbrack -\\mathit{\\mathbf{E}}\\left\\lbrack 2\\left(f_i -\\mathit{\\mathbf{E}}\\left\\lbrack y_i \\right\\rbrack \\right)\\left(\\mathit{\\mathbf{E}}\\left\\lbrack y_i \\right\\rbrack -y_i \\right)\\right\\rbrack \\\\\n&=\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(f_i -\\mathit{\\mathbf{E}}\\left\\lbrack y_i \\right\\rbrack \\right)}^2 \\right\\rbrack +\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(\\mathit{\\mathbf{E}}\\left\\lbrack y_i \\right\\rbrack -y_i \\right)}^2 \\right\\rbrack -2\\left(\\mathit{\\mathbf{E}}\\left\\lbrack f_i \\times \\mathit{\\mathbf{E}}\\left\\lbrack y_i \\right\\rbrack \\right\\rbrack -\\mathit{\\mathbf{E}}\\left\\lbrack f_i y_i \\right\\rbrack -\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(\\mathit{\\mathbf{E}}\\left\\lbrack y_i \\right\\rbrack \\right)}^2 \\right\\rbrack +\\mathit{\\mathbf{E}}\\left\\lbrack \\mathit{\\mathbf{E}}\\left\\lbrack y_i \\right\\rbrack \\times y_i \\right\\rbrack \\right)\\\\\n&=\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(f_i -\\mathit{\\mathbf{E}}\\left\\lbrack y_i \\right\\rbrack \\right)}^2 \\right\\rbrack +\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(\\mathit{\\mathbf{E}}\\left\\lbrack y_i \\right\\rbrack -y_i \\right)}^2 \\right\\rbrack -2\\left(\\mathit{\\mathbf{E}}\\left\\lbrack f_i \\right\\rbrack \\mathit{\\mathbf{E}}\\left\\lbrack y_i \\right\\rbrack -\\mathit{\\mathbf{E}}\\left\\lbrack f_i \\right\\rbrack \\mathit{\\mathbf{E}}\\left\\lbrack y_i \\right\\rbrack -{\\left(\\mathit{\\mathbf{E}}\\left\\lbrack y_i \\right\\rbrack \\right)}^2 +{\\left(\\mathit{\\mathbf{E}}\\left\\lbrack y_i \\right\\rbrack \\right)}^2 \\right)\\\\\n&=\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(f_i -\\mathit{\\mathbf{E}}\\left\\lbrack y_i \\right\\rbrack \\right)}^2 \\right\\rbrack +\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(\\mathit{\\mathbf{E}}\\left\\lbrack y_i \\right\\rbrack -y_i \\right)}^2 \\right\\rbrack \\\\\n\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(f_i -y_i \\right)}^2 \\right\\rbrack &={\\mathrm{bias}}^2 +\\mathrm{Var}\\left(y_i \\right)\n\\end{align*}\nNow we can write equation (2) as \\mathit{\\mathbf{E}}\\left\\lbrack {\\left(t_i -y_i \\right)}^2 \\right\\rbrack =\\mathrm{Var}\\left(\\mathrm{Noise}\\right)+{\\mathrm{bias}}^2 +\\mathrm{Var}\\left(y_i \\right) Hence proved!\n\n\n7.1 Bias variance tradeoff\nFrom equation (1) we can see that when MSE is constant and if we try to reduce the variance Bias has to increase and vice versa.\n\nModels with too few parameters are inaccurate because of a large bias bias (not enough flexibility).\nModels with too many parameters are inaccurate because of a large variance (too much sensitivity to the sample).\nUnderfitting: Model is too “simple” to represent all the relevant class characteristics\n\nHigh bias and low variance\nHigh training error and high test error\n\nOverfitting: Model is too “complex” and fits irrelevant characteristics (noise) in the data\n\nLow bias and high variance\nLow training error and high test error\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn case of classification, variance dominates bias. Very roughly, this is because we only need to make a discrete decision rather than get an exact value."
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-10-15-CS5590-week7.html#measuring-bias-and-variance",
    "href": "Data_Science_Notes/Machine-Learning/2022-10-15-CS5590-week7.html#measuring-bias-and-variance",
    "title": "Machine Learning 7",
    "section": "8 Measuring Bias and Variance",
    "text": "8 Measuring Bias and Variance\n\nCreate multiple training set using bootstrap replicates.\nApply learning algorithm on each replicates to obtain hypothesis.\ncompute predicted value for each hypothesis on the data which did not appear on the bootstrap replicate the hypothesis was trained on.\ncompute the average prediction\nEstimate bias\nEstimate variance.\nAssume noise is 0\n\nIf we have multiple data points with the same x value, then we can estimate the noise not generally available in machine learning"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-10-15-CS5590-week7.html#some-inferences",
    "href": "Data_Science_Notes/Machine-Learning/2022-10-15-CS5590-week7.html#some-inferences",
    "title": "Machine Learning 7",
    "section": "9 Some Inferences",
    "text": "9 Some Inferences\n\nHow to reduce variance of classifier\n\nChoose a simpler classifier\nRegularize the parameters\nGet more training data\n\nTraining Error and Cross Validation\n\nSuppose we use the training error to estimate the difference between the true model prediction and the learned model prediction.\nThe training error is downward biased: on average it underestimates the generalization error.\nCross validation is nearly unbiased; it slightly overestimates the generalization error."
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-10-15-CS5590-week7.html#regularizers",
    "href": "Data_Science_Notes/Machine-Learning/2022-10-15-CS5590-week7.html#regularizers",
    "title": "Machine Learning 7",
    "section": "10 Regularizers",
    "text": "10 Regularizers\n\nKNN\n\nChoose higher k\n\nDecision Trees\n\nPruning\n\nNaïve Bayes\n\nParametric models automatically act as regularizers\n\nSVMs\n\nControl c value\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nIf the wight value is high the model is likely to over fit, so we want weight to be small. Consider a dataset in 2-D, Hiving large variance across y-axis, now consider a model which overfits to these data, to do so W has to be larger because for very small change in x,\\;y will have to change by very high amount (due to such data distribution) which can be achieved only if W is large. It is shown in the below pic."
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-10-15-CS5590-week7.html#model-based-machine-learning",
    "href": "Data_Science_Notes/Machine-Learning/2022-10-15-CS5590-week7.html#model-based-machine-learning",
    "title": "Machine Learning 7",
    "section": "11 Model-based Machine Learning",
    "text": "11 Model-based Machine Learning\n\npick a model\npick a criteria to optimize (aka objective function)\ndevelop a learning algorithm (aka Find W and b that minimizes the loss)\nGenerally, we don’t want huge weights\n\nIf weights are large, a small change in a feature can result in a large change in the prediction\nAlso, can give too much weight to any one feature\n\n\n\n11.1 Regularization in Model based ML\n\nA regularizer is an additional criteria to the loss function to make sure that we don’t overfit\nIt’s called a regularizer since it tries to keep the parameters more normal/regular\nIt is a bias (inductive bias) on the model that forces the learning to prefer certain types (smaller) of weights over others (larger). \\argmin_{w,b} \\sum_{i=1}^n \\mathrm{loss}\\left(y,y^{\\prime } \\right)+\\lambda \\times \\boxed{\\mathrm{regulizer}\\left(w,b\\right)}\nType of norm regularizer\n\n1-norm ( sum of weights ) r(w,b)=\\sum_{w_j} \\left\\lvert w_j \\right\\rvert \n2-norm ( sum of squared weights ) r(w,b)=\\sum_{w_j} \\sqrt{\\left\\lvert w_j \\right\\rvert^2} \n\np-norm ( sum of squared weights ) r(w,b)=\\sum_{w_j} \\sqrt[p]{\\left\\lvert w_j \\right\\rvert^p}=\\left\\lVert w\\right\\rVert ^p\n\n\n\n\n\n\n\nTip\n\n\n\n\nSmaller values of p, (p < 2) encourage sparser vectors\nLarger values of p discourage large weights more.\nAll p norms penalize larger weights.\np < 2 tends to create sparse i.e. lots of 0 weights\n\n\n\n\nL1 is popular because it tends to result in sparse solutions (i.e. lots of zero weights)\nHowever, it is not differentiable, so it only works for gradient descent solvers\nL2 is also popular because for some loss functions, it can be solved directly (no gradient descent required, though often iterative solvers still)\nLp is less popular since they don’t tend to shrink the weights enough"
  },
  {
    "objectID": "Data_Science_Notes/Machine-Learning/2022-10-15-CS5590-week7.html#introduction-to-learning-theory",
    "href": "Data_Science_Notes/Machine-Learning/2022-10-15-CS5590-week7.html#introduction-to-learning-theory",
    "title": "Machine Learning 7",
    "section": "12 Introduction to Learning Theory",
    "text": "12 Introduction to Learning Theory\n\n12.1 Optimality of Bayes Decision Rule\n\nLet X be a random variable over a space \\Omega\nTwo category decision problem: H_1:X \\in \\omega_1 H_2:X \\in \\omega_2\noptimal decision is \n\nChose H_1 when p(x \\mid \\omega_1)p(\\omega_1)\\ge p(x \\mid \\omega_2)p(\\omega_2)\nChose H_2 when p(x \\mid \\omega_1)p(\\omega_1) < p(x \\mid \\omega_2)p(\\omega_2)\n\nConsider the partition of reason \\Omega as shown below, \\mathcal{R}_1,\\mathcal{R}_2,\\Omega_1,\\Omega_2 are partitions of \\Omega. Every thing outside of \\mathcal{R}_1 is \\mathcal{R}_2 and everything outside of \\Omega_1 is \\Omega_2    \n\nConsider arbitrary decision rule: - partition \\Omega into two disjoint regions: \\mathcal{R}_1 and \\mathcal{R}_2 choose H_1 if X \\in \\mathcal{R}_1 choose H_2 if X \\in \\mathcal{R}_2\nConsider bayesian decision rule:\n\npartition \\Omega into two disjoint regions: \\Omega_1 and \\Omega_2 choose H_1 if X \\in \\Omega_1 choose H_2 if X \\in \\Omega_2\nwhere  \\Omega_1 = \\{x \\in \\Omega : p(x \\mid \\omega_1)p(\\omega_1)\\ge p(x \\mid \\omega_2)p(\\omega_2)\\} \\Omega_2 = \\{x \\in \\Omega : p(x \\mid \\omega_1)p(\\omega_1)< p(x \\mid \\omega_2)p(\\omega_2)\\}\n\nNow, For the arbitrary decision rule:\n\\begin{align*}\np(\\text{error})&=p(X\\in \\mathcal{R}_1,\\omega_2)+p(X\\in \\mathcal{R}_2,\\omega_1) \\\\\n&= p(X\\in \\mathcal{R}_1\\mid \\omega_2)p(\\omega_2)+p(X\\in \\mathcal{R}_2\\mid \\omega_1)p(\\omega_1) \\\\\n&= \\int_{\\mathcal{R}_1} p(x\\mid \\omega_2)p(\\omega_2)dx + \\int_{\\mathcal{R}_2} p(x\\mid \\omega_1)p(\\omega_1)dx \\\\\n\\end{align*}\nSimilarly we can write for Bayesian decision rule:\n\\begin{align*}\np(\\text{error}_\\text{Bayes})&=p(X\\in \\Omega_1,\\omega_2)+p(X\\in \\Omega_2,\\omega_1) \\\\\n&= p(X\\in \\Omega_1\\mid \\omega_2)p(\\omega_2)+p(X\\in \\Omega_2\\mid \\omega_1)p(\\omega_1) \\\\\n&= \\int_{\\Omega_1} p(x\\mid \\omega_2)p(\\omega_2)dx + \\int_{\\Omega_2} p(x\\mid \\omega_1)p(\\omega_1)dx \\\\\n\\end{align*}\nLet \\Delta(\\text{error})=p(\\text{error})-p(\\text{error}_\\text{Bayes}) \\tag{2}  If we can prove that \\Delta(\\text{error}) is always positive, it means p(\\text{error}_\\text{Bayes}) is the optimal error (least error we can get).\n\nWith reference of the above figure we can write: \\mathcal{R}_1=(\\mathcal{R}_1\\cap \\Omega_1)\\cup (\\mathcal{R}_1 \\cap \\Omega_2) \\mathcal{R}_2=(\\mathcal{R}_2\\cap \\Omega_1)\\cup (\\mathcal{R}_2 \\cap \\Omega_2)\n\\Omega_1=( \\Omega_1 \\cap \\mathcal{R}_1)\\cup (\\Omega_1 \\cap \\mathcal{R}_2) \\Omega_2=( \\Omega_2 \\cap \\mathcal{R}_1)\\cup (\\Omega_2 \\cap \\mathcal{R}_2)\n\nFrom above points we can write: \\mathcal{R}_1-\\Omega_1 = (\\mathcal{R}_1 \\cap \\Omega_2)-(\\Omega_1 \\cap \\mathcal{R}_2) \\tag{3} \\mathcal{R}_2-\\Omega_2=(\\mathcal{R}_2\\cap \\Omega_1)-( \\Omega_2 \\cap \\mathcal{R}_1) \\tag{4}\n\nsubstituting values in equation (2)\n\\begin{align*}\n\\Delta(\\text{error})&=\\int_{\\mathcal{R}_1} p(x\\mid \\omega_2)p(\\omega_2)dx + \\int_{\\mathcal{R}_2} p(x\\mid \\omega_1)p(\\omega_1)dx - \\int_{\\Omega_1} p(x\\mid \\omega_2)p(\\omega_2)dx - \\int_{\\Omega_2} p(x\\mid \\omega_1)p(\\omega_1)dx \\\\\n&=p(\\omega_2)\\left [\\int_{\\mathcal{R}_1} p(x\\mid \\omega_2)dx- \\int_{\\Omega_1} p(x\\mid \\omega_2)dx \\right ]  + p(\\omega_1)\\left [\\int_{\\mathcal{R}_2} p(x\\mid \\omega_1)dx  - \\int_{\\Omega_2} p(x\\mid \\omega_1)dx \\right] \\\\\n&=p(\\omega_2)\\left [\\int_{ (\\mathcal{R}_1 \\cap \\Omega_2)} p(x\\mid \\omega_2)dx- \\int_{(\\Omega_1 \\cap \\mathcal{R}_2)} p(x\\mid \\omega_2)dx \\right ]  + p(\\omega_1)\\left [\\int_{(\\mathcal{R}_2\\cap \\Omega_1)} p(x\\mid \\omega_1)dx  - \\int_{( \\Omega_2 \\cap \\mathcal{R}_1)} p(x\\mid \\omega_1)dx \\right] \\;\\;\\text{Using eq } (3) \\text{ and } (4)\\\\\n&=\\left\\lbrack \\int_{({\\mathcal{R}}_1 \\cap \\Omega_2 )} p(x\\mid \\omega_2 )p(\\omega_2 )dx-\\int_{({\\mathcal{R}}_1 \\cap \\Omega_2 )} p(x\\mid \\omega_1 )p(\\omega_1 )dx\\right\\rbrack +\\left\\lbrack \\int_{(\\Omega_1 \\cap {\\mathcal{R}}_2 )} p(x\\mid \\omega_1 )p(\\omega_1 )dx-\\int_{(\\Omega_1 \\cap {\\mathcal{R}}_2 )} p(x\\mid \\omega_2 )p(\\omega_2 )dx\\right\\rbrack\\\\\n&=\\underbrace{\\int_{({\\mathcal{R}}_1 \\cap \\Omega_2 )} \\left\\lbrack p(x\\mid \\omega_2 )p(\\omega_2 )-p(x\\mid \\omega_1 )p(\\omega_1 )\\right\\rbrack dx}_{\\ge0}  +  \\underbrace{\\int_{(\\Omega_1 \\cap {\\mathcal{R}}_2 )} \\left\\lbrack p(x\\mid \\omega_1 )p(\\omega_1 )-p(x\\mid \\omega_2 )p(\\omega_2 )\\right\\rbrack dx}_{\\ge0} \\\\\n\\Delta(\\text{error})&\\ge 0\n\\end{align*}\nIn above two terms are zero due to below reason \\begin{align*}\n  \\Omega_1 = \\{x \\in \\Omega : p(x \\mid \\omega_1)p(\\omega_1)\\ge p(x \\mid \\omega_2)p(\\omega_2)\\} \\Rightarrow \\int_{(\\Omega_1 \\cap {\\mathcal{R}}_2 )} \\left\\lbrack p(x\\mid \\omega_1 )p(\\omega_1 )-p(x\\mid \\omega_2 )p(\\omega_2 )\\right\\rbrack dx \\ge 0 \\\\\n  \\Omega_2 = \\{x \\in \\Omega : p(x \\mid \\omega_1)p(\\omega_1)< p(x \\mid \\omega_2)p(\\omega_2)\\} \\Rightarrow \\int_{({\\mathcal{R}}_1 \\cap \\Omega_2 )} \\left\\lbrack p(x\\mid \\omega_2 )p(\\omega_2 )-p(x\\mid \\omega_1 )p(\\omega_1 )\\right\\rbrack dx \\ge 0\n  \\end{align*}\n\\begin{align*}\n  &&\\Delta(\\text{error})&\\ge0\\\\\n  &\\Rightarrow &p(\\text{error})-p(\\text{error}_\\text{Bayes}) &\\ge 0 \\\\\n  &\\Rightarrow &p(\\text{error}) &\\ge p(\\text{error}_\\text{Bayes})\n  \\end{align*}\n \\tiny {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}"
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-08-06-CS6660-week1.html",
    "href": "Data_Science_Notes/Mathematics/2022-08-06-CS6660-week1.html",
    "title": "Probability Theory 1",
    "section": "",
    "text": "The union E\\cup F\\; of events E and F always means E OR F , The intersection E\\cap F of events E and F always means E AND F\n\n\n\n\n\n\nTip\n\n\n\nThe union \\bigcup_i E_{i\\;} of events E_{i\\;\\;} always means at least one of the E_i’s, The intersection \\bigcap_i E_i of events E_{i\\;} always means each of the E_i’s\n\n\n\n\n\nThe complement of an event is E^c =\\bar{E} =E^* :=\\Omega -E\n\n\n\n\ncommutativity:  \\begin{array}{l} E\\cup F=F\\cup E\\\\ E\\cap F=F\\cap E \\end{array}\nAssociativity:  \\begin{array}{l} E\\cup \\left(F\\cup G\\right)=\\left(E\\cup F\\right)\\cup G=E\\cup F\\cup G\\\\ E\\cap \\left(F\\cap G\\right)=\\left(E\\cap F\\right)\\cap G=E\\cap F\\cap G \\end{array}\nDistributivity:  \\begin{array}{l} \\left(E\\cup F\\right)\\cap G=\\left(E\\cap G\\right)\\cup \\left(F\\cap G\\right)\\\\ \\left(E\\cap F\\right)\\cup G=\\left(E\\cup G\\right)\\cap \\left(F\\cup G\\right)\\; \\end{array}\nDe Morgan’s Law:  \\begin{array}{l} {\\left(E\\cup F\\right)}^c =E^{c\\;} \\cap F^{c\\;} \\\\ {\\left(E\\cap F\\right)}^{c\\;} =E^c \\cup F^c \\end{array} Similarly \\begin{array}{l} {\\left({\\bigcup_{\\;\\;} }_i E_{i\\;} \\right)}^c ={\\bigcap_{\\;} }_i E_{i\\;}^{c\\;} \\\\ {\\left({\\bigcap_{\\;} }_i E_{i\\;} \\right)}^{c\\;} ={\\bigcup_{\\;} }_i E_i^{c\\;} \\end{array}\n\n\n\n\nThe probability P on a sample space \\Omega assigns numbers to events \\Omega of in such a way that 1. The probability of any event is non-negative : P\\left\\lbrace E\\right\\rbrace \\ge 0 2. The probability if the sample space is one : P\\left\\lbrace \\Omega \\;\\right\\rbrace =1 3. For any finitely or countably infinitely many manually exclusive events E_{1,} E_2 ,\\ldotp \\ldotp \\ldotp , P\\left\\lbrace {\\bigcup_{i\\;} E_{i\\;} }_{\\;} \\right\\rbrace =\\sum_i P\\left\\lbrace E_i \\right\\rbrace \\;\n\n\n\nInclusion-exclusion principle:  - For any events E and F, P\\left\\lbrace E\\cup F\\right\\rbrace =P\\left\\lbrace E\\right\\rbrace +P\\left\\lbrace F\\right\\rbrace -P\\left\\lbrace E\\cap F\\right\\rbrace  - For any events E, F and G: P\\left\\lbrace E\\cup F\\cup G\\right\\rbrace =P\\left\\lbrace E\\right\\rbrace +P\\left\\lbrace F\\right\\rbrace +P\\left\\lbrace G\\right\\rbrace -P\\left\\lbrace E\\cap F\\right\\rbrace -P\\left\\lbrace E\\cap G\\right\\rbrace -P\\left\\lbrace F\\cap G\\right\\rbrace +P\\left\\lbrace E\\cap F\\cap G\\right\\rbrace - Generally: p\\left\\lbrace E_1 \\cup E_2 \\cup E_3 \\cup \\ldotp \\ldotp \\ldotp \\cup E_n \\right\\rbrace =\\sum_{1\\le i\\le n} P\\left\\lbrace E_i \\right\\rbrace -\\sum_{1\\le i_1 \\le i_2 \\le n} P\\left\\lbrace E_{i_1 } \\cap E_{i_2 } \\right\\rbrace +\\sum_{1\\le i_1 \\le i_2 \\le i_{3\\;} \\le n} \\left\\lbrace P\\left\\lbrace E_{i_1 } \\cap E_{i_2 } \\cap E_{i_3 } \\right\\rbrace \\right\\rbrace -\\ldotp \\ldotp \\ldotp \\;+{\\left(-1\\right)}^{n+1} P\\left\\lbrace E_1 \\cap E_2 \\cap E_3 \\right\\rbrace\n\n\n\n\nFor any events E_1 ,E_2 ,\\ldotp \\ldotp \\ldotp E_n P\\left\\lbrace \\bigcup_{i=1}^n E_i \\right\\rbrace \\le \\sum_{i=1}^n P\\left\\lbrace E_i \\right\\rbrace \n\n\n\n\nOut of n people, what is the probability that there are no coinciding birthdays? |\\Omega |={365}^n |E|=365\\ldotp 364\\ldotp \\ldotp \\ldotp \\left(365-n+1\\right)=\\frac{365!}{\\left(365-n\\right)!}  P\\left\\lbrace E\\right\\rbrace =\\frac{|E|}{|\\Omega |}=\\frac{365!}{\\left(365-n\\right)!{365}^n }\n\n\n\nLet F be an Event with P\\left\\lbrace F\\right\\rbrace >0 . then the conditional probability E of given F is defined as: P\\left\\lbrace E|F\\right\\rbrace :=\\frac{P\\left\\lbrace E\\cap F\\right\\rbrace }{P\\left\\lbrace F\\right\\rbrace } \n\n\n\n\n\n\nNote\n\n\n\nConditional Probability can be interpreted as:“In what proportion of case in F will also E occur?” or “How does the probability of both E and F compare to the probability of F only?”\n\n\n conditional probability is a proper probability and it satisfies the axioms:\n\nThe conditional probability of any event is non-negative :P\\left\\lbrace E|F\\right\\rbrace \\ge 0\nThe conditional probability if the sample space is one :P\\left\\lbrace \\Omega |F\\;\\right\\rbrace =1\nFor any finitely or countably infinitely many manually exclusive events E_{1,} E_2 ,\\ldotp \\ldotp \\ldotp , P\\left\\lbrace {\\bigcup_{i\\;} E_{i\\;} |F}_{\\;} \\right\\rbrace =\\sum_i P\\left\\lbrace E_i |F\\right\\rbrace \\;\n\n\n\n\n\nP\\left\\lbrace E^c |F\\right\\rbrace =1-P\\left\\lbrace E|F\\right\\rbrace\nP\\left\\lbrace \\phi |F\\right\\rbrace =0\nP\\left\\lbrace E|F\\right\\rbrace =1-P\\left\\lbrace E^c |F\\right\\rbrace \\le 1\nP\\left\\lbrace \\left(E\\cup G\\right)|F\\right\\rbrace =P\\left\\lbrace E|F\\right\\rbrace +P\\left\\lbrace G|F\\right\\rbrace -P\\left\\lbrace E\\cap F|F\\right\\rbrace proof: \\begin{array}{l} P\\left\\lbrace \\left(E\\cup G\\right)|F\\right\\rbrace =\\frac{P\\left\\lbrace \\left(E\\cup G\\right)\\cap F\\right\\rbrace }{P\\left\\lbrace F\\right\\rbrace }=\\frac{P\\left\\lbrace \\left(E\\cap F\\right)\\cup \\left(G\\cap F\\right)\\right\\rbrace }{P\\left\\lbrace F\\right\\rbrace }\\\\ =\\frac{P\\left(E\\cap F\\right)+P\\left(G\\cap F\\right)-P\\left\\lbrace \\left(E\\cap F\\right)\\cap \\;\\left(G\\cap F\\right)\\right\\rbrace }{P\\left\\lbrace F\\right\\rbrace }=\\frac{P\\left(E\\cap F\\right)+P\\left(G\\cap F\\right)-P\\left\\lbrace E\\cap G\\cap F\\right\\rbrace }{P\\left\\lbrace F\\right\\rbrace }\\\\ =P\\left\\lbrace E|F\\right\\rbrace +P\\left\\lbrace G|F\\right\\rbrace -P\\left\\lbrace E\\cap F|F\\right\\rbrace \\end{array}\nif E\\subseteq G then P\\left\\lbrace \\left(G-E\\right)|F\\right\\rbrace =P\\left\\lbrace G|F\\right\\rbrace -P\\left\\lbrace E|F\\right\\rbrace proof: \\begin{array}{l} P\\left\\lbrace G|F\\right\\rbrace -P\\left\\lbrace E|F\\right\\rbrace =\\frac{\\;P\\left\\lbrace G\\cap F\\right\\rbrace }{P\\left\\lbrace \\mathrm{F}\\right\\rbrace }-\\frac{\\;P\\left\\lbrace E\\cap F\\right\\rbrace }{P\\left\\lbrace \\mathrm{F}\\right\\rbrace }\\\\ =\\frac{P\\left\\lbrace \\left(G\\cap F\\right)-\\left(E\\cap F\\right)\\right\\rbrace \\;}{P\\left\\lbrace \\mathrm{F}\\right\\rbrace }=\\frac{P\\left\\lbrace \\left(G\\cap F\\right)\\cap {\\left(E\\cap F\\right)}^c \\right\\rbrace \\;}{P\\left\\lbrace \\mathrm{F}\\right\\rbrace }\\\\ =\\frac{P\\left\\lbrace \\left(G\\cap F\\right)\\cap {\\left(E^{c\\;} \\cup F^{c\\;} \\right)}^{\\;} \\right\\rbrace \\;}{P\\left\\lbrace \\mathrm{F}\\right\\rbrace }=\\frac{P\\left\\lbrace G\\cap \\left(F\\cap {\\left(E^{c\\;} \\cup F^{c\\;} \\right)}^{\\;} \\right)\\right\\rbrace \\;}{P\\left\\lbrace \\mathrm{F}\\right\\rbrace }\\\\ =\\frac{P\\left\\lbrace G\\cap \\left({\\left({F\\cap \\;E}^{c\\;} \\right)\\cup \\left({F\\cap \\;F}^{c\\;} \\right)}^{\\;} \\right)\\right\\rbrace \\;}{P\\left\\lbrace \\mathrm{F}\\right\\rbrace }=\\frac{P\\left\\lbrace G\\cap {F\\cap \\;E}^{c\\;} \\right\\rbrace \\;}{P\\left\\lbrace \\mathrm{F}\\right\\rbrace }\\\\ =\\frac{P\\left\\lbrace G\\cap {\\;E}^{c\\;} \\cap F\\right\\rbrace \\;}{P\\left\\lbrace \\mathrm{F}\\right\\rbrace }=\\frac{P\\left\\lbrace \\left(G-{\\;E}^{\\;} \\right)\\cap F\\right\\rbrace \\;}{P\\left\\lbrace \\mathrm{F}\\right\\rbrace }=P\\left\\lbrace \\left(G-E\\right)|F\\right\\rbrace \\end{array} if A\\subseteq B then P\\left(B\\right)-P\\left(A\\right)=P\\left(B_{\\;} -A\\right) Here as E\\subseteq G so \\left(E\\cap F\\right)\\subseteq \\left(G\\cap F\\right) so we can write P\\left\\lbrace G\\cap F\\right\\rbrace -\\;P\\left\\lbrace E\\cap F\\right\\rbrace=P\\left\\lbrace \\left(G\\cap F\\right)-\\left(E\\cap F\\right)\\right\\rbrace\n\n\n\nif E\\subseteq G then P\\left\\lbrace E|F\\right\\rbrace \\le P\\left\\lbrace G|F\\right\\rbrace\n\n\n\n\nFor E_1 ,E_2 ,\\ldotp \\ldotp \\ldotp E_n events: P\\left\\lbrace E_1 \\cap E_2 \\cap \\ldotp \\ldotp \\ldotp \\cap E_n \\right\\rbrace =P\\left\\lbrace E_1 \\right\\rbrace \\ldotp P\\left\\lbrace E_2 |E_1 \\right\\rbrace \\ldotp P\\left\\lbrace E_3 |E_1 \\cap E_2 \\right\\rbrace \\ldotp \\ldotp \\ldotp \\ldotp P\\left\\lbrace E_n |E_1 \\cap E_2 \\cap \\ldotp \\ldotp \\ldotp \\cap E_{n-1} \\right\\rbrace\n\n\n\nThis is also known as partition theorem For any events E and F  P\\left\\lbrace E\\right\\rbrace =P\\left\\lbrace E|F\\right\\rbrace \\ldotp P\\left\\lbrace E\\right\\rbrace +P\\left\\lbrace E|F^c \\right\\rbrace \\ldotp P\\left\\lbrace F^c \\right\\rbrace P\\left\\lbrace E\\right\\rbrace =\\sum_i P\\left\\lbrace E|F_i \\right\\rbrace \\ldotp P\\left\\lbrace F_i \\right\\rbrace\n\n\n\nFor any events E and F  P\\left\\lbrace F|E\\right\\rbrace =\\frac{P\\left\\lbrace E|F\\right\\rbrace \\ldotp P\\left\\lbrace F\\right\\rbrace }{P\\left\\lbrace E|F\\right\\rbrace \\ldotp P\\left\\lbrace F\\right\\rbrace +P\\left\\lbrace E|F^c \\right\\rbrace \\ldotp P\\left\\lbrace F^c \\right\\rbrace }\n\n\n\n\n\n\nImportant\n\n\n\nif {\\left\\lbrace F_i \\right\\rbrace }_i is a complete system of events, then P\\left\\lbrace F_i |E\\right\\rbrace =\\frac{\\;P\\left\\lbrace E|F_i \\right\\rbrace \\ldotp P\\left\\lbrace F_i \\right\\rbrace }{\\sum_j \\;P\\left\\lbrace E|F_j \\right\\rbrace \\ldotp P\\left\\lbrace F_j \\right\\rbrace }\n\n\n\n\n\nEvent E and F are independent if P\\left\\lbrace E|F\\right\\rbrace =P\\left\\lbrace E\\right\\rbrace or P\\left\\lbrace E\\cap F\\right\\rbrace =P\\left\\lbrace E\\right\\rbrace \\cdot P\\left\\lbrace F\\right\\rbrace\n\n\n\n\n\n\nImportant\n\n\n\nMutually exclusive events are necessarily also dependent events because one’s existence depends on the other’s non-existence.Dependent events are not necessarily mutually exclusive\n\n\n\n\nIf A and B are independent then A^c and B are also also independent Proof: P\\left(A^c |B\\right)=\\frac{P\\left(A^c \\cap B\\right)}{P\\left(B\\right)}=\\frac{P\\left(B\\right)-P\\left(A^{\\;} \\cap \\;\\;B\\right)}{P\\left(B\\right)}=1-P\\left(A|B\\right)=1-P\\left(A\\right)=P\\left(A^c \\right) \n\nThree events E, F and G are (mutually) independent if - P\\left\\lbrace E\\cap F\\right\\rbrace =P\\left\\lbrace E\\right\\rbrace \\ldotp P\\left\\lbrace F\\right\\rbrace - P\\left\\lbrace E\\cap G\\right\\rbrace =P\\left\\lbrace E\\right\\rbrace \\ldotp P\\left\\lbrace G\\right\\rbrace - P\\left\\lbrace F\\cap G\\right\\rbrace =P\\left\\lbrace F\\right\\rbrace \\ldotp P\\left\\lbrace G\\right\\rbrace - P\\left\\lbrace E\\cap F\\cap G\\right\\rbrace =P\\left\\lbrace E\\right\\rbrace \\ldotp P\\left\\lbrace F\\right\\rbrace \\ldotp P\\left\\lbrace G\\right\\rbrace\n \\tiny {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Subruk, IIT Hyderabad }}}}"
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-09-17-CS6660-week4_2.html",
    "href": "Data_Science_Notes/Mathematics/2022-09-17-CS6660-week4_2.html",
    "title": "Linear Algebra 2",
    "section": "",
    "text": "Gaussian Elimination are elementary transformation of subsystems fo linear equations, which transforms the equation systems into a simple form.\nElementary transformation that keep the solution set the same, but that transform the equation system.\nProcess\n\nExchange fo two equations (rows in the matrix representing the system of equations)\nMultiplication of an equation (row) with a constant \\lambda \\in \\mathbb{R}-\\{0\\}\nAddition of two equations (rows)\n\nExample :  \\begin{array}{r r r r r r r r r r} -2x_1 & + & 4x_2 & - & 2x_3 & - & x_4 & + & 4x_5 & = & -3 \\\\ 4x_1 & - & 8x_2 & + & 3x_3 & - & 3x_4 & + & x_5 & = & 2 \\\\ x_1 & - & 2x_2 & + & x_3 & - & x_4 & + & x_5 & = & 0 \\\\ x_1 & - & 2x_2 & & & - & 3x_4 & + & 4x_5 & = & a \\\\ \\end{array}  Build the _augmented matrix (in the form of \\left[A \\mid b\\right] )  \\left[ \\begin{array}{r r r r r | r} -2 & 4 & -2 & -1 & 4 & -3 \\\\  4 &-8 & 3 & -3 & 1 & 2 \\\\  1 &-2 & 1 & -1 & 1 & 0 \\\\  1 &-2 & 0 & -3 & 4 & a \\\\ \\end{array}\\right]  Exchange row 3 and row 1 for simplicity  \\left[ \\begin{array}{r r r r r | r}  1 &-2 & 1 & -1 & 1 & 0 \\\\  4 &-8 & 3 & -3 & 1 & 2 \\\\ -2 & 4 & -2 & -1 & 4 & -3 \\\\  1 &-2 & 0 & -3 & 4 & a \\\\ \\end{array}\\right]  Perform operations \\{ R_2 \\leftarrow R_2 - 4R_1, R_3 \\leftarrow R_3+2R_1, R_4 \\leftarrow R_4-R_1 \\} \\left[ \\begin{array}{r r r r r | r}  1 &-2 & 1 & -1 & 1 & 0 \\\\  0 & 0 & -1 & 1 & -3 & 2 \\\\  0 & 0 & 0 & -3 & 6 & -3 \\\\  0 & 0 & -1 & -2 & 3 & a \\\\ \\end{array}\\right]  Perform operations \\{ R_4 \\leftarrow R_4 - R_2 -R_3 \\} \\left[ \\begin{array}{r r r r r | r}  1 &-2 & 1 & -1 & 1 & 0 \\\\  0 & 0 & -1 & 1 & -3 & 2 \\\\  0 & 0 & 0 & -3 & 6 & -3 \\\\  0 & 0 & 0 & 0 & 0 & a \\\\ \\end{array}\\right]  Perform operations \\{ R_2 \\leftarrow -R_2 , R_3 \\leftarrow -\\frac{1}{3}R_3 \\} \\left[ \\begin{array}{r r r r r | r}  1 &-2 & 1 & -1 & 1 & 0 \\\\  0 & 0 & 1 & -1 & 3 & -2 \\\\  0 & 0 & 0 & 1 & -2 & 1 \\\\  0 & 0 & 0 & 0 & 0 & a+1 \\\\ \\end{array}\\right]  Above matrix is in row-echelon from Convert above matrix to normal equation form  \\begin{array}{l l l l l l r}  x_1 &-2x_2 & +x_3 & -x_4 & +x_5 & = & 0 \\\\  & & +x_3 & -x_4 & +3x_5 & = & -2 \\\\  & & & +x_4 & -2x_5 & = & 1 \\\\  & & & & +0 & = & a+1 \\\\ \\end{array}  Only for a=1 this system can be solved, and can be solved using back substitution  \\left[ \\begin{array}{r} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ \\end{array}\\right] = \\left[ \\begin{array}{r}  2 \\\\  0\\\\ -1 \\\\ 1 \\\\ 0 \\\\ \\end{array}\\right]  General solution of the above equation can be found as explained earlier, and is given as below   \\left\\{x\\in\\mathbb{R}^{5}:x= {\\left[\\begin{array}{r}{2}\\\\ {0}\\\\ {-1}\\\\ {1} \\\\0 \\end{array}\\right]}    + \\lambda_{1}{\\left[\\begin{array}{r}{2}\\\\ {1}\\\\ {0}\\\\ {0} \\\\0 \\end{array}\\right]}   + \\lambda_{2}{\\left[\\begin{array}{r}{2}\\\\ {0}\\\\ {-1}\\\\ {2} \\\\1\\end{array}\\right]},\\;\\;\\ \\lambda_{1},\\lambda_{2}\\in\\mathbb{R}\\right\\}"
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-09-17-CS6660-week4_2.html#row-echelon-form",
    "href": "Data_Science_Notes/Mathematics/2022-09-17-CS6660-week4_2.html#row-echelon-form",
    "title": "Linear Algebra 2",
    "section": "2 Row-Echelon Form",
    "text": "2 Row-Echelon Form\n\nExample matrix  \\left[ \\begin{array}{r r r r r | r}  \\boxed 1 &-2 & 1 & -1 & 1 & 0 \\\\  0 & 0 & \\boxed 1 & -1 & 3 & -2 \\\\  0 & 0 & 0 & \\boxed 1 & -2 & 1 \\\\  0 & 0 & 0 & 0 & 0 & a+1 \\\\ \\end{array}\\right] \nDefinition  A matrix is in row-echelon form if\n\nAll rows that contains only zeros are at the bottom ot the matrix; correspondingly, all rows that contains at least one nonzero element are on top of rows that contain only zeros.\nLooking at nonzero rows only, the first nonzero number from the left ( also called the pivot or the leading coefficient ) is always strictly to the right of the pivot of the row above it.\n\nBasic and free variables : The variables corresponding to the pivots in the row-echelon form are called basic variables and the other variables are free variables. For example, x_1; x_3; x_4 are basic variables, whereas x_2;x_5 are free variables in above example matrix."
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-09-17-CS6660-week4_2.html#reduced-row-echelon-forms",
    "href": "Data_Science_Notes/Mathematics/2022-09-17-CS6660-week4_2.html#reduced-row-echelon-forms",
    "title": "Linear Algebra 2",
    "section": "3 Reduced row Echelon Forms",
    "text": "3 Reduced row Echelon Forms\n\nDefinition  An equation system is in reduced row echelon form ( also: rwo reduced echelon form or row canonical from ) if\n\nIt is in row echelon from.\nEvery pivot is 1.\nThe pivot is the only nonzero entry in its column."
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-09-17-CS6660-week4_2.html#solving-ax0-minus-1-trick",
    "href": "Data_Science_Notes/Mathematics/2022-09-17-CS6660-week4_2.html#solving-ax0-minus-1-trick",
    "title": "Linear Algebra 2",
    "section": "4 Solving AX=0 (Minus 1 Trick)",
    "text": "4 Solving AX=0 (Minus 1 Trick)\n\nExample Matrix  \\left[ \\begin{array}{r r r r r} 1 & 3 & 0 & 0 & 3 \\\\ 0 & 0 & 1 & 0 & 9 \\\\ 0 & 0 & 0 & 1 & -4 \\\\ \\end{array}\\right] For finding the solutions of AX=0 is to look at the non-pivot columns, which we will need to express as a (linear) combination of the pivot columns.  \\left\\{x\\in\\mathbb{R}^{5}:x= \\lambda_{1}{\\left[\\begin{array}{r}3\\\\ -1\\\\ 0\\\\ 0 \\\\0 \\end{array}\\right]} + \\lambda_{2}{\\left[\\begin{array}{r}3\\\\ 0\\\\ 9\\\\ -4 \\\\ -1\\end{array}\\right]},\\;\\;\\ \\lambda_{1},\\lambda_{2}\\in\\mathbb{R}\\right\\}  Insert row in place of free variable ( 2^{\\text{nd}} and 5^{\\text{th}}) with -1 value in place of dependent variable as shown below  \\tilde{A} = \\left[ \\begin{array}{r r r r r} 1 & 3 & 0 & 0 & 3 \\\\ \\boxed 0 & \\boxed { \\mathbf {-1}} & \\boxed 0 & \\boxed 0 & \\boxed 0 \\\\ 0 & 0 & 1 & 0 & 9 \\\\ 0 & 0 & 0 & 1 & -4 \\\\ \\boxed 0 & \\boxed 0 & \\boxed 0 & \\boxed 0 & \\boxed {\\mathbf {-1}} \\\\ \\end{array}\\right] Now the solution for AX=0 can be found by the columns of the the free variable which is 2^{\\text{nd}} and 5^{\\text{th}} column."
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-09-17-CS6660-week4_2.html#solving-a-system-of-liner-equation",
    "href": "Data_Science_Notes/Mathematics/2022-09-17-CS6660-week4_2.html#solving-a-system-of-liner-equation",
    "title": "Linear Algebra 2",
    "section": "5 Solving a System of Liner Equation",
    "text": "5 Solving a System of Liner Equation\n\nInversion of matrices which are not square and non-invertible. Ax=b is given as x=A^{-1}b  Ax=b \\Longleftrightarrow A^TAx=A^Tb \\Longleftrightarrow x= (A^TA)^{-1}A^Tb This is called Moore-Penrose pseudo-inverse  The disadvantage of this method is that it requires a lot of computation.\nGaussian Elimination plays a key role in\n\nComputing determinants\nChecking whether a set of vectors is linearly independent\nComputing the rank of the matrix\nDetermining a basis of the vector space."
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-09-17-CS6660-week4_2.html#vector-space",
    "href": "Data_Science_Notes/Mathematics/2022-09-17-CS6660-week4_2.html#vector-space",
    "title": "Linear Algebra 2",
    "section": "6 Vector Space",
    "text": "6 Vector Space\n\nA vector space V is a set that is closed under finite vector addition and scalar multiplication.\n\nwe two vectors are added or multiplied it should stay in the same vector space.\n\nVector space properties  In order for V to be a vector space, the following condition must hold for all elements X,Y,Z \\in V and any scalars r,s \\in F:\n\nCommutativity: X+Y=Y+X\nAssociativity of vector addition: (X+Y)+Z = X+(Y+Z)\nAdditive Identity: For all X, \\;\\; 0+X = X+0=X\nExistence of additive inverse: For any X, there exists a -X such that X+(-X) =0\nAssociativity of vector multiplication: r(sX) = (rs)X\nDistributivity of scalar sums: (r+s)X = rX+sX\nScalar multiplication identity: 1X=X"
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-09-17-CS6660-week4_2.html#subspace",
    "href": "Data_Science_Notes/Mathematics/2022-09-17-CS6660-week4_2.html#subspace",
    "title": "Linear Algebra 2",
    "section": "7 Subspace",
    "text": "7 Subspace\n\nLet V be a vector space, and let W be the subspace of V, if W is a vector space with respect to the operation in V, then W is called a subspace of V\nLet V be the vector space, with operations + and \\;\\cdot \\; and let W be subset of V. Then W is subspace of V if and only if the following conditions hold.\n\nW is non-empty: The zero vector belongs to W.\nClosure under +: If u and v are any vectors in W, the u+v is in W.\nClosure under \\cdot \\;:if v is any vector in W, and c is nay real number, then c \\cdot v si in W."
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-09-17-CS6660-week4_2.html#liner-combination",
    "href": "Data_Science_Notes/Mathematics/2022-09-17-CS6660-week4_2.html#liner-combination",
    "title": "Linear Algebra 2",
    "section": "8 Liner Combination",
    "text": "8 Liner Combination\n\nConsider a vector space V and a finite number of vectors  x_1, \\dots , x_k \\in V, then every v \\in V of the form  \\displaystyle v=\\lambda_1 x_1+ \\dots + \\lambda_kx_k = \\sum_{i=1}^k \\lambda_i x_i \\in V  \\lambda_1,\\dots , \\lambda_k \\in \\mathbb{R} is a linear combination fo the vectors x_1, \\dots ,x_k \\displaystyle 0=\\sum_{i=1}^k 0 x_i"
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-09-17-CS6660-week4_2.html#linear-independence",
    "href": "Data_Science_Notes/Mathematics/2022-09-17-CS6660-week4_2.html#linear-independence",
    "title": "Linear Algebra 2",
    "section": "9 Linear Independence",
    "text": "9 Linear Independence\n\nLet us consider a vector space V with k \\in \\mathbb{N} and x_1, \\dots ,x_k \\in V. If there is a non trivial linear combination, such that 0=\\sum_{i=1}^k \\lambda_i x_i with at least one \\lambda _i \\ne 0, the vectors x_1, \\dots , x_k are linearly independent. If only the trivial solution exists, i.e., \\lambda_1 =\\dots=\\lambda_k=0 the vectors x_1, \\dots , x_k are linearly independent.\nIf at-least one of the vectors x_1, \\dots , x_k is 0 then they are linearly dependent. The same holds if two vectors are identical.\nTo find if a system is linear independent, perform gaussian elimination, if there is no non-pivot column then the vectors are independent.\n\n \\tiny {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Srijith, IIT Hyderabad }}}}"
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-09-03-CS6660-week3.html",
    "href": "Data_Science_Notes/Mathematics/2022-09-03-CS6660-week3.html",
    "title": "Linear Algebra 1",
    "section": "",
    "text": "Let us see some Systems of linear equations\n\nA company produce products N_1, \\dots ,N_n for which resources R_1, \\dots ,R_m are required. To produce a unit of porduct N_j, a_{ij} units of resources R_i are needed, where i=1,\\dots ,m and j=1, \\dots ,n.  The objective is to find an optimal production plan, i.e., a plan of how many units x_j of product N_j should be produced if a total of b_i units of resource R_i are available and (ideally) no resources are left over. write system of equation of the same \\left\\lbrace \\begin{array}{c}\n  a_{11}x_1 + \\dots +a_{1n}x_n =b_1\\\\\n  \\vdots \\\\\n  a_{m1}x_1+\\dots +a_{mn}x_n=b_n\n  \\end{array}\\right.\nJohn received an inheritance of \\$12000 that he divided in three parts and invested in three ways: in a money market fund paying 3\\% annual interest; in municipal bonds paying 4\\% of annual interest; and in mutual fund paying 7\\% of interest, john invested \\$4,000 more in mutual funds than in municipal bonds. He earned \\$670 in interest the first year.How much did john invest in each type of found \\left\\lbrace \\begin{array}{c}\n  x+y+z=12000\\\\\n  0.03x+0.04y+0.07z=670 \\\\\n  -x+z=4000\n  \\end{array}\\right.\\\\\n   \nsolution of above question: x=3500,y=1000,z=7500\n\n\n\n\nIf there are more variables than equations than there might be infinitely many solutions.\nIf AX=b has infinitely many solutions then to find other solutions we find Y such that AY=b, then we can write A(X+Y)=b as A(X+Y)=AX+AY=AX because AY=0 here, X+Y is another solution here. If AX=b and AY=0 then A(X+Y)=b\nParticular and general solutions\n\n1. Find a particular solution to AX=b\n2. Find all the solutions to AX=0\n3. Combine the solution from step 1 and 2 to get the general solution\n\nFor Example, consider  {\\left[\\begin{array}{l l l}{1}&{0}&{8}&{-4}\\\\ {0}&{1}&{2}&{12}\\end{array}\\right]} {\\left[\\begin{array}{l}{x_1}\\\\ {x_2}\\end{array}\\right]}={\\left[\\begin{array}{l}{42}\\\\ {8}\\end{array}\\right]} solution for AX=b (particular solution)  {\\left[\\begin{array}{l l l}{1}&{0}&{8}&{-4}\\\\ {0}&{1}&{2}&{12}\\end{array}\\right]} {\\left[\\begin{array}{l}{x_1}\\\\ {x_2}\\end{array}\\right]}={\\left[\\begin{array}{l}{48}\\\\ {8}\\end{array}\\right]} is given by  {\\left[\\begin{array}{l}{x_1}\\\\ {x_2}\\end{array}\\right]}={\\left[\\begin{array}{l}{42}\\\\ {8}\\end{array}\\right]} Now we find solution for AX=0 (general solution ) {\\left[\\begin{array}{l l l}{1}&{0}&{8}&{-4}\\\\ {0}&{1}&{2}&{12}\\end{array}\\right]} {\\left[\\begin{array}{l}{x_1}\\\\ {x_2}\\end{array}\\right]}={\\left[\\begin{array}{l}{0}\\\\ {0}\\end{array}\\right]} We can find that there are two vectors which satisfy it  {\\left[\\begin{array}{l}{x_1}\\\\ {x_2}\\\\ {x_3}\\\\ {x_4} \\end{array}\\right]}={\\left[\\begin{array}{l}{8}\\\\ {2}\\\\ {-1}\\\\ {0}\\end{array}\\right]} {\\left[\\begin{array}{l}{x_1}\\\\ {x_2}\\\\ {x_3}\\\\ {x_4} \\end{array}\\right]}={\\left[\\begin{array}{l}{-4}\\\\ {12}\\\\ {0}\\\\ {-1}\\end{array}\\right]} We can represent the final solution by combining particular and general solution \\left\\{x\\in\\mathbb{R}^{4}:x= {\\left[\\begin{array}{r}{42}\\\\ {8}\\\\ {0}\\\\ {0} \\end{array}\\right]}    + \\lambda_{1}{\\left[\\begin{array}{r}{8}\\\\ {2}\\\\ {-1}\\\\ {0} \\end{array}\\right]}   + \\lambda_{2}{\\left[\\begin{array}{r}{-4}\\\\ {12}\\\\ {0}\\\\ {-1} \\end{array}\\right]},\\;\\;\\ \\lambda_{1},\\lambda_{2}\\in\\mathbb{R}\\right\\}\n\n \\tiny {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Srijith, IIT Hyderabad }}}}"
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-10-01-CS6660-week6.html",
    "href": "Data_Science_Notes/Mathematics/2022-10-01-CS6660-week6.html",
    "title": "Linear Algebra 3",
    "section": "",
    "text": "How to find if vectors are linearly independent\n\nwrite all the vectors as a column of the the matrix.\nPerform Gaussian elimination until the matrix is in row echelon form\nThe reduced row echelon form is not necessary here.\nThe pivot columns indicate the vectors, which are linearly independent of the vectors on the left, Note that there is an ordering of vectors when the matrix is built.\nThe non pivot columns can be expressed as linear combinations of the pivot columns on their left.\n\nIf we have a matrix B of all independent column vectors b_i, and we multiply it with with j independent column vectors \\lambda_j, \\;\\;j=1,\\dots ,j then we get new set of independent columns vectors x_j, \\;\\;j=1, \\dots ,j\nExample  consider a set of linearly independent vectors b_1,b_2,b_3,b_4 \\in \\mathbb{R}^n \\begin{array}{l l l l l l}  x_1 & = & +b_1 & -2b_2 & + b_3 & -b_4 \\\\  x_2 & = & -4b_1 & -2b_2 & & +4b_4 \\\\  x_3 & = & +2b_1 & +3b_2 & - b_3 & -3b_4 \\\\  x_4 & = & +17b_1 &-102b_2 & + 11b_3 &+b_4 \\\\  \\end{array} Are vectors x_1, \\dots , x_4 \\in \\mathbb{R}^n linearly independent?  As the matrix B has all independent vectors, x_1, \\dots , x_4 will be independent if the vectors formed by coefficients of equation are independent, so we create the coefficient matrix A and perform the gaussian elimination, if we find no non-pivot column we can say x_1, \\dots , x_4 are independent. \\left\\{ \\left[ \\begin{array}{r}1\\\\-2\\\\1\\\\-1\\end{array}\\right],  \\left[ \\begin{array}{r}4\\\\-2\\\\0\\\\4\\end{array}\\right],  \\left[ \\begin{array}{r}2\\\\3\\\\-1\\\\-3\\end{array}\\right],  \\left[ \\begin{array}{r}17\\\\-10\\\\11\\\\1\\end{array}\\right] \\right\\} A = \\left[ \\begin{array}{r r r r }  1 & 4 & 2 & 17 \\\\  -2 & -2 & 3 & -10 \\\\  1 & 0 & -1 & 11 \\\\  -1 & 4 & -3 & 1 \\end{array}\\right] After performing gaussian elimination we get row echelon form  A = \\left[ \\begin{array}{r r r r }  1 & 0 & 0 & -7 \\\\  0 & 1 & 0 & -15 \\\\  0 & 0 & 1 & -18 \\\\  0 & 0 & 0 & 0 \\end{array}\\right] Here we got last column as non pivot column, hence we can say that x_1, \\dots , x_4 are not linearly independent."
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-10-01-CS6660-week6.html#generating-set-and-basis",
    "href": "Data_Science_Notes/Mathematics/2022-10-01-CS6660-week6.html#generating-set-and-basis",
    "title": "Linear Algebra 3",
    "section": "2 Generating Set and Basis",
    "text": "2 Generating Set and Basis\n\nDefinition (Generating Set and Span): Consider a vector space V = (V,+,\\cdot ) and a set of vectors A =\\{ x_1, \\dots , x_k\\}, If every vector v \\in V can be expressed as a linear combination of x_1, \\dots , x_k, A is called a generating set of V . The set of all linear combinations of vectors in A is called the span of A. If A spans the vector space V, we write V = \\text{span}[A] or V = \\text{span}[ x_1, \\dots , x_k]\nDefinition (Basis): Consider a vector space V = (V,+,\\cdot ) and A \\subseteq V. A generating set A of V is called minimal if there exists no smaller set A \\subsetneq A \\subseteq V that spans V. Every linearly independent generating set of V is minimal and is called a basis of V.\nBasis need not be orthogonal,also there can be multiple basis of a given vector space.\nLet V = (V,+,\\cdot ) be a vector space and B \\subseteq V, B \\ne \\phi, Then the following statements are equivalent:\n\nB is a basis of V.\nB is a minimal generating set.\nB is a maximal linearly independent set of vectors in V, i.e., adding any other vector to this set will make it linearly dependent.\nEvery vector x \\in V is a linear combination of vectors from B, and every linear combination is unique.\n\nEvery vector space V possesses a basis B. There can be many bases of a vector space V, i.e. there is no unique basis."
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-10-01-CS6660-week6.html#basis-and-dimension",
    "href": "Data_Science_Notes/Mathematics/2022-10-01-CS6660-week6.html#basis-and-dimension",
    "title": "Linear Algebra 3",
    "section": "3 Basis and Dimension",
    "text": "3 Basis and Dimension\n\nAny vector space V has a basis.\nAll bases for V are of the same cardinality.\nA set of m vectors from \\mathbb{R}^n, with m<n can not span \\mathbb{R}^n, and hence can not be a basis for \\mathbb{R}^n.\nA set of m vectors from \\mathbb{R}^n, with m>n is not linearly independent set, and hence can not be a basis for \\mathbb{R}^n.\nEvery basis of the vector space \\mathbb{R}^n consists of n vectors.\nThe dimension of a vector space V, denoted \\text{dim}V, is the cardinality of its bases.\n\n\n3.1 How to find Basis\n\nA basis of a subspace U = \\text{span}[x_1, \\dots , x_m] \\subseteq \\mathbb{R}^n can be found by executing the following steps:\n\nWrite the spanning vectors as columns of a matrix A\nDetermine the row echelon form of A.\nThe spanning vectors associated with the pivot columns are basis of U.\n\nRemarks : The dimension of a vector space is not necessarily the number of elements in a vector, For instance, the vector space V = \\text{span}\\left[ \\begin{array}{c} 0\\\\1 \\end{array}\\right] is one dimensional, although the basis vector possesses two elements."
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-10-01-CS6660-week6.html#null-space-row-space-column-space",
    "href": "Data_Science_Notes/Mathematics/2022-10-01-CS6660-week6.html#null-space-row-space-column-space",
    "title": "Linear Algebra 3",
    "section": "4 Null space, Row Space, Column Space",
    "text": "4 Null space, Row Space, Column Space\n\nLet A be an m \\times n matrix.\n\nThe subspace of \\mathbb{R}^n spanned by the row vectors of A is called the row space of A.\nThe subspace of \\mathbb{R}^n spanned by column vectors of A is called the column space of A\n\nRecall: Determine whether Ax=b has a solution thus amounts to testing whether b is in the span of the columns of A ( called the column space of A)\n\nThe Null space of A denoted \\text{Null} \\;A is the set of all solutions to homogenous equation Ax=0, written in set notation, we have \\text{Null} \\;A = \\{ x:x \\in \\mathbb{R}^n \\text{ and } Ax=0\\}"
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-10-01-CS6660-week6.html#rank",
    "href": "Data_Science_Notes/Mathematics/2022-10-01-CS6660-week6.html#rank",
    "title": "Linear Algebra 3",
    "section": "5 Rank",
    "text": "5 Rank\n\nTh rank of a matrix is size of the largest subset of columns of A that constitute linearly independent set.\nOften referred to as simply the number of linearly independent columns of A\nNote that rank(A) = rank(A^T). This means that column rank = row rank.\nFor A \\in \\mathbb{R}^{m\\times n}, rank (A) \\le\\min(m,n). If rank (A)=\\min(m,n), then A is said to be full rank.\nThe column of A \\in \\mathbb{R}^{m\\times n} span a subspace U \\subseteq \\mathbb{R}^m with \\text{dim}(U)=rk(A). Later we will call this subspace the image or range. A basis of U can be found by applying gaussian elimination to A to identify the pivot columns.\nThe row of A \\in \\mathbb{R}^{m\\times n} span a subspace W \\subseteq \\mathbb{R}^n with \\text{dim}(W)=rk(A). A bais of W can be found by applying gaussian elimination to A.\nFor all A \\in \\mathbb{R}^{m\\times n} it holds that A is regular (invertible) if and only if rk(A)=n.\n\n \\tiny {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Srijith, IIT Hyderabad }}}}"
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-09-24-CS6660-week5.html",
    "href": "Data_Science_Notes/Mathematics/2022-09-24-CS6660-week5.html",
    "title": "Probability Theory 4",
    "section": "",
    "text": "The cumulative distribution function (CDF) of a random variable X is given by  F : \\mathbb{R} \\rightarrow [0,1], \\;\\;\\;\\; x \\rightarrow F(x) = p\\{ X \\le x\\}  Notice that the function is well defined for any random variable. CDF is defined as capital F or sometimes as capital F_X\nRemark: Ther distribution function contains all relevant information about the distribution of our random variable. E.g., for any fixed a<b  \\mathsf{P}\\{a<X\\leq b\\}=\\mathsf{P}\\{X\\leq b\\}-\\mathsf{P}\\{X\\leq a\\}=F(b)-\\mathsf{P}(a) \\mathsf{P}\\{a\\leq X<b\\}=\\mathsf{P}\\{a<X\\leq b\\} -\\mathsf{P}\\{X\\leq b\\} +\\mathsf{P}\\{X\\leq a\\}\nProbability at jump is given by difference between jump points, e.g.   p(X = 2) = \\frac{7}{8} - \\frac{4}{8} =\\frac{3}{8}\nA random variable with piecewise constant distribution function is called discrete. It’s mass function values equal to the jump size in the distribution function.\nProposition  A cumulative distribution function F \n\nIs non-decreasing\nhas limit \\lim_{x \\rightarrow - \\infty} F(X) =0 on the left;\nhas limit \\lim_{x \\rightarrow + \\infty} F(X) =1 on the right;\nIs continuous form the right.\n\nAny function F with the above properties is a cumulative distribution function. There is a sample space and a random variable on it that realizes this distribution function.\nOne Use case of CDF  Suppose we have a tool that can generate only uniform random variable. But we want to generate the data which follows a specific probability distribution f whose CDF is F_X.  To achieve thais we generate random number using uniform distribution in range [0,1], say a. Now look at CDF F_X and find A such that F_X(S)=a  Output A. The output will be distributed as per f"
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-09-24-CS6660-week5.html#density-function",
    "href": "Data_Science_Notes/Mathematics/2022-09-24-CS6660-week5.html#density-function",
    "title": "Probability Theory 4",
    "section": "2 Density function",
    "text": "2 Density function\n\nSuppose that a random variable has it’s distribution function in the form of  \\displaystyle F(a) = \\int _{- \\infty}^a f(X) dX, \\;\\;\\;\\; \\forall a \\in \\mathbb{R}  with a function f \\ge 0. Then the distribution is called (absolutely) continuous, and f is the probability density function (pdf).\nProposition  A probability density function f\n\nIs non-negative\nHas total integral \\displaystyle \\int _{- \\infty} ^ \\infty f(x) dx =1\n\nAny function f with the above properties is a probability density function. There is a sample space and a continuous random variable on it that realizes this density.\n\n\n2.1 Properties of the density function\n\nProposition  For any subset B \\subseteq \\mathbb{R},  \\displaystyle P\\{X \\in B \\} = \\int _B f(x) dx\nCorollary  Indeed, for a continuous random variable X, \\displaystyle P\\{X \\in a \\} = \\int _{\\{a\\}} f(x) dx = 0 \\;\\;\\;\\;\\; \\forall a \\in \\mathbb{R}\nCorollary  For a small \\varepsilon ,  \\displaystyle P\\{X \\in (a,a+\\varepsilon] \\} = \\int _a ^{a + \\varepsilon} f(x) dx \\simeq f(a) \\cdot \\varepsilon\nThere is no particular value that X can take on with positive chance. We can only talk about intervals, and density tells us the likelihood that X is around a point a.\nTo get to the density from an absolutely continuous distribution function,  \\displaystyle f(a) = \\frac{dF(a)}{da}\\;\\;\\;\\; a \\in \\mathbb{R}"
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-09-24-CS6660-week5.html#expectation-variance",
    "href": "Data_Science_Notes/Mathematics/2022-09-24-CS6660-week5.html#expectation-variance",
    "title": "Probability Theory 4",
    "section": "3 Expectation, Variance",
    "text": "3 Expectation, Variance\n\nThe way of defining the expectation will be no surprise for anyone (c.f the discrete case): \nDefinition:  The expected value of a continuous random variable X is defined by  \\displaystyle EX = \\int_{- \\infty}^ \\infty X \\cdot f(X)dX,  if the integral exists.\nProposition  Let X be a continuous random variable, and g an \\mathbb{R } \\rightarrow \\mathbb{R } function then  \\displaystyle Eg(X)= \\int _{- \\infty} ^{\\infty} g(X) \\cdot f(X) dX, \\;\\;\\;\\;\\; if exists.\nWe can define  Moments:  \\displaystyle EX^n= \\int _{- \\infty} ^{\\infty} X^n \\cdot f(X) dX, Absolute moments:  \\displaystyle E|X|^n= \\int _{- \\infty} ^{\\infty} |X| ^n \\cdot f(X) dX,  variance \\text{Var}X = E(X-EX)^2=EX^2-(EX)^2 and standard deviation \\text{SD}X= \\sqrt{\\text{Var}X } as in the discrete case. These enjoy the same properties as before."
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-09-24-CS6660-week5.html#uniform",
    "href": "Data_Science_Notes/Mathematics/2022-09-24-CS6660-week5.html#uniform",
    "title": "Probability Theory 4",
    "section": "4 Uniform",
    "text": "4 Uniform\n\nDefinition  Fix \\alpha < \\beta reals. We say that X has the uniform distribution over the interval (\\alpha , \\beta ), in short, X \\sim U(\\alpha , \\beta), if it’s density is given by  f(X) = \\left\\lbrace \\begin{array}{c c} \\displaystyle \\frac{1}{\\beta -\\alpha} & \\text{if } X \\in (\\alpha,\\beta),\\\\ 0, & \\text{otherwise} \\end{array}\\right . \nNotice that this is exactly the value of the constant that makes this a density.\nIntegrating the density  F(X) = \\left\\lbrace \\begin{array}{l l} 0,&\\text{if } X \\le \\alpha, \\\\ \\displaystyle \\frac{X-\\alpha}{\\beta -\\alpha}, & \\text{if } X \\in (\\alpha,\\beta),\\\\ 1, & \\text{otherwise} \\end{array}\\right .  \nRemarks:  If X \\sim U (\\alpha , \\beta), and \\alpha < a <b < \\beta then  \\displaystyle P\\{a < X \\le b\\} = \\int_a^b f(X)dX=\\frac{b-a}{\\beta - \\alpha } Probabilities are computed by proportions of lengths.\n\n\n4.1 Expectation, Variance of uniform\n\nFor X \\sim U(\\alpha , \\beta ),  \\boxed{ \\displaystyle EX =\\frac{\\alpha + \\beta }{2}, \\;\\;\\;\\; \\text{Var}X = \\frac{(\\beta - \\alpha)^2}{12}}\nproof of EX  \\displaystyle EX= \\int_{- \\infty}^{\\infty} Xf(X)dX= \\int_{\\alpha}^{\\beta} \\frac{X}{\\beta - \\alpha}dX=\\frac{\\frac{\\beta ^2}{2} - \\frac{\\alpha ^2}{2}}{\\beta - \\alpha}=\\frac{\\alpha + \\beta}{2}\nproof of \\text{Var}X  First find EX^2 \\displaystyle EX^2= \\int_{- \\infty}^{\\infty} X^2f(X)dX= \\int_{\\alpha}^{\\beta} \\frac{X^2}{\\beta - \\alpha}dX=\\frac{\\frac{\\beta ^3}{3} - \\frac{\\alpha ^3}{3}}{3(\\beta - \\alpha)}=\\frac{\\alpha^2+\\alpha \\beta + \\beta^2}{3} \\displaystyle \\text{Var}X = EX^2-(EX)^2 = \\frac{\\alpha^2+\\alpha \\beta + \\beta^2}{3}- \\frac{(\\alpha + \\beta)^2}{4}=\\frac{ \\beta^2 -2\\alpha \\beta + \\alpha^2}{12} \\displaystyle \\text{Var}X = EX^2-(EX)^2 = \\frac{(\\beta-\\alpha )^2}{12}"
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-09-24-CS6660-week5.html#exponential",
    "href": "Data_Science_Notes/Mathematics/2022-09-24-CS6660-week5.html#exponential",
    "title": "Probability Theory 4",
    "section": "5 Exponential",
    "text": "5 Exponential\n\nThe Exponential is a very special distribution because of its memoryless property. It is often considered as a waiting time, and is widely used in the theory of stochastic processes.\nDefinition  Fix a positive parameter \\lambda. X is said to have the Exponential distribution with parameter \\lambda or, in short, X \\sim \\text{Exp}(\\lambda ), if its density is given by \\displaystyle f(x) = \\left\\lbrace \\begin{array}{l l} 0, & \\text{if }x\\le 0 \\\\ \\lambda e^{-\\lambda x}, & \\text{if }x\\ge 0 \\\\ \\end{array}\\right .\nIntegration of density exponential density function  \\displaystyle F(X) = \\int _{-\\infty}^0 f(z)dz = \\int _{0}^x f(z)dz+ \\int _{0}^x f(z)dz \\displaystyle F(X) = \\int _{-\\infty}^0 0dz + \\int _{0}^x \\lambda e^{-\\lambda z} dz \\displaystyle F(X) =\\int _{0}^x \\lambda e^{-\\lambda z} = \\lambda \\frac{e^{- \\lambda z}}{- \\lambda} \\bigg|_{0}^x = 1-e^{-\\lambda x} \\displaystyle F(x) = \\begin{cases} 0, & \\text{if }x\\le 0 \\\\ 1-e^{-\\lambda x}, & \\text{if }x\\ge 0 \\\\ \\end{cases} \n\n\n5.1 Expectation, Variance of Exponential\n\nFor X \\sim \\text{Exp}(\\alpha , \\beta ),  \\boxed{ \\displaystyle EX =\\frac{1 }{\\lambda}, \\;\\;\\;\\; \\text{Var}X = \\frac{1}{\\lambda^2}}\nProof of EX  \\displaystyle EX = \\int _0 ^\\infty X \\lambda e^{-\\lambda X}dX Tips integration by parts:  d(uv) = udv+vdu  udv = d(uv) - vdu  \\int udv = uv - \\int vdu  In our case u=X \\;\\;\\; v=-e^{-\\lambda X} \\Rightarrow dv= \\lambda e^{-\\lambda X} \\displaystyle \\int _0 ^\\infty udv = -\\left(X\\cdot e^{-\\lambda X} \\right)\\big| _0 ^\\infty - \\int _0 ^\\infty \\left( -e^{-\\lambda X}\\right) dX \\displaystyle \\int _0 ^\\infty udv = 0- \\frac{e^{-\\lambda X}}{\\lambda} \\Bigg|_0 ^\\infty = \\frac{1}{\\lambda} \\displaystyle EX = \\frac{1}{\\lambda}\nProof of \\text{Var}X First we find E(X^2) \\displaystyle E(X^2) = \\int _0 ^\\infty X^2 \\lambda e^{-\\lambda X}dX In this case u=X^2 \\;\\;\\; v=-e^{-\\lambda X} \\Rightarrow dv= \\lambda e^{-\\lambda X} \\displaystyle \\int _0 ^\\infty udv = -\\left(X^2\\cdot e^{-\\lambda X} \\right)\\big| _0 ^\\infty - 2\\int _0 ^\\infty \\left( -e^{-\\lambda X}\\right) XdX \\displaystyle \\int _0 ^\\infty udv = 0+ 2\\int _0 ^\\infty \\left( e^{-\\lambda X}\\right) XdX \\displaystyle \\int _0 ^\\infty udv = 0+ 2\\frac{1}{\\lambda} \\int _0 ^\\infty \\lambda\\left( e^{-\\lambda X}\\right) XdX \\displaystyle \\int _0 ^\\infty udv = 0+ \\frac{2}{\\lambda}\\underbrace{\\int _0 ^\\infty \\lambda \\left( e^{-\\lambda X}\\right) XdX}_{\\text{This is same as }EX=\\frac{1}{\\lambda}} \\displaystyle \\int _0 ^\\infty udv =\\frac{2}{\\lambda^2}=E(X^2) Now, \\displaystyle \\text{Var}X = E(X^2)-(EX)^2=\\frac{2}{\\lambda^2} - \\left(\\frac{1}{\\lambda}\\right)^2=\\frac{1}{\\lambda^2}\nThinking about X as a waiting time, we now see that \\lambda describes how fast the event we wait for, happens. Therefore \\lambda is also called the rate of the exponential waiting time.\n\n\n\n5.2 The memoryless property\n\nThe exponential is the only continuous non-negative memory less distribution. That is, the only distribution with X \\ge 0 and P\\{X > t + s \\mid X > t\\} = P\\{X > s\\} \\;\\;\\;\\;\\;(\\forall\\; t,\\;\\; s \\ge 0)\n\nSuppose we have waited for time t. The chance of waiting an additional time s is the same as if we would start waiting anew. The distribution does not remember its past.\n\nProof  \\displaystyle P \\left\\{ X > t + s \\mid X>t \\right\\}= \\frac {P\\left\\{ X > t + s \\right\\} \\cap P\\left\\{X>t \\right\\}}{P\\left\\{X>t \\right\\}} if X > t+s then X is also greater than t  \\displaystyle P \\left\\{ X \\ge t + s \\mid X>t \\right\\}= \\frac {P\\left\\{ X \\ge t + s \\right\\} }{P\\left\\{X>t \\right\\}} \\displaystyle P \\left\\{ X \\ge t + s \\mid X>t \\right\\}= \\frac {\\lambda e^{-\\lambda (t+s)} }{\\lambda e^{-\\lambda t}}= e^{-\\lambda s}=P \\left\\{ X>s \\right\\}"
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-09-24-CS6660-week5.html#normal",
    "href": "Data_Science_Notes/Mathematics/2022-09-24-CS6660-week5.html#normal",
    "title": "Probability Theory 4",
    "section": "6 Normal",
    "text": "6 Normal\n\nDefinition  Let \\mu \\in \\mathbb{R}, \\sigma > 0 be ral parameters, X has the normal distribution with parameters \\mu and \\sigma^2 or in short X \\sim \\mathcal{N}(\\mu,\\sigma ^2), if it’s density is given by  \\displaystyle f(x)=\\frac{1}{\\sqrt{2\\pi}\\cdot\\sigma}\\cdot e^{-\\frac{1}{2} \\left(\\frac{X-\\mu}{\\sigma}\\right)^2}\\;\\;\\;\\;X\\in \\mathbb{R} \nThe case \\mu =0, \\sigma^2=1 is called standard normal distribution X \\sim \\mathcal{N}(0,1), Its density is denoted by \\phi and it’s distribution is denoted by \\Phi \\displaystyle \\phi(x)=\\frac{1}{\\sqrt{2\\pi}\\cdot\\sigma}\\cdot e^{-\\frac{ X^2}{2}} \\displaystyle \\Phi(x)=\\int_{-\\infty }^X \\frac{1}{\\sqrt{2\\pi}\\cdot\\sigma}\\cdot e^{-\\frac{ y^2}{2}}dy\\;\\;\\;\\;X\\in \\mathbb{R}\nRemarks : The standard normal distribution \\phi(x)=\\int_{-\\infty }^X \\frac{1}{\\sqrt{2\\pi}\\cdot\\sigma}\\cdot e^{-\\frac{ y^2}{2}}dy has no closed from, its values wil be looked up in tables.\n\n\n6.1 Symmetry\n\nFor any z \\in \\mathbb{R}, \\;\\;\\Phi (-Z ) = 1- \\Phi(Z) Proof:  The standard normal distribution is symmetric: if X \\sim \\mathcal{N}(0,1) and also -X \\sim \\mathcal{N}(0,1), Therefore  \\Phi(-Z)=P\\{ X<-Z\\} = P\\{-X > Z \\} = P\\{ X >Z\\} = 1 - \\Phi(Z)\n\n\n\n6.2 Linear transformations\n\nLet X \\sim \\mathcal{N}(\\mu,\\sigma ^2) and \\alpha , \\beta \\in \\mathbb{R}, Then \\alpha X + \\beta \\sim \\mathcal{N}(\\alpha \\mu + \\beta , \\alpha^2 \\sigma^2) proof: we prove for positive \\alpha, for negative it’s similar. Start with the distribution function Y = \\alpha X +\\beta \\displaystyle F_Y(y)=P\\{ Y <y\\} = P\\{ \\alpha X + \\beta <y\\} = P\\left\\{ X < \\frac{y-\\beta}{\\alpha}\\right\\} \\displaystyle F_Y(y)= P\\left\\{ X < \\frac{y-\\beta}{\\alpha}\\right\\}=F_X\\left(\\frac{y-\\beta}{\\alpha}\\right) \\displaystyle f_y \\left(y\\right)=\\frac{d}{\\textrm{d}y}F_y \\left(y\\right)=\\frac{d}{\\textrm{d}y}F_X \\left(\\frac{y-\\beta }{\\alpha }\\right)=f_X \\left(\\frac{y-\\beta }{\\alpha }\\right)\\frac{1}{\\alpha } \\displaystyle f_y \\left(y\\right)=\\frac{1}{\\sqrt{2\\pi }\\sigma }\\mathrm{Exp}\\left(-\\frac{1}{2}{\\left(\\frac{\\left(\\frac{y-\\beta }{\\alpha }\\right)-\\mu }{\\sigma }\\right)}^2 \\right)\\frac{1}{\\alpha } \\displaystyle f_y \\left(y\\right)=\\frac{1}{\\sqrt{2\\pi }\\sigma }\\mathrm{Exp}\\left(-\\frac{1}{2}{\\left(\\frac{y-\\beta -\\mu \\alpha }{\\sigma \\alpha \\;}\\right)}^2 \\right)\\frac{1}{\\alpha } \\displaystyle f_y \\left(y\\right)=\\frac{1}{\\sqrt{2\\pi }\\sigma }\\mathrm{Exp}\\left(-\\frac{\\left(y-\\beta -\\mu \\alpha \\right)^2 }{2{\\left(\\sigma \\alpha \\right)}^2 \\;}\\right)\\frac{1}{\\alpha } \\displaystyle f_y \\left(y\\right)=\\frac{1}{\\sqrt{2\\pi }\\sigma \\alpha }\\mathrm{Exp}\\left(-\\frac{\\left(y-\\left(\\mu \\alpha +\\beta \\right)\\right)^2 }{2{\\left(\\sigma \\alpha \\right)}^2 \\;}\\right) which implies the statement Y \\sim \\mathcal{N}(\\alpha \\mu + \\beta , \\alpha^2 \\sigma^2)\nIf X \\sim \\mathcal{N}(\\mu, \\sigma ^2), then its standardized version \\frac{X-\\mu}{\\sigma} \\sim \\mathcal{N}(0, 1)  Just use \\alpha = \\frac{1}{\\sigma} and \\beta = -\\frac{\\mu}{\\sigma}\nIf X \\sim \\mathcal{N}(0, 1) is standard normal, then its mean is 0 and its variance is 1. Proof:  That the mean is zero follows from symmetry. For the variance we need to calculate  \\displaystyle EX^{2}=\\int_{-\\infty}^{\\infty}\\frac{x^{2}}{\\sqrt{2\\pi}}\\cdot\\mathrm{e}^{-x^{2}/2}\\,\\mathrm{d}x using intergration by parts. TODO : complete the integration\nIf X \\sim \\mathcal{N}(\\mu,\\sigma ^2) then its mean is \\mu and its variance is \\sigma ^2 Proof:  \\displaystyle \\mathrm{EX}=\\sigma \\cdot E\\left(\\frac{X-\\mu }{\\sigma }\\right)+\\mu =0+\\mu =\\mu  \\displaystyle \\mathrm{VarX}=\\sigma^2 \\mathrm{Var}\\left(\\frac{X-\\mu }{\\sigma }\\right)=\\sigma^2 \\cdot 1=\\sigma^2  X \\sim \\mathcal{N}(\\mu,\\sigma ^2) is also said to be normal distribution with mean \\mu and variance \\sigma^2\n\n\n\n6.3 Why normal\n\nTheorem (DeMoivre-Laplace) Fix p and let X_n \\sim \\text{Binom}(n,p). Then for every fixed a<b reals, \\displaystyle \\lim_{n\\to \\infty } P\\left\\lbrace a<\\frac{X_n -\\mathrm{np}}{\\sqrt{\\mathrm{np}\\left(1-\\mathrm{np}\\right)}}\\le b\\right\\rbrace =\\Phi \\left(b\\right)-\\Phi \\left(a\\right)  That is, take X_n \\sim \\text{Binom}(n,p) with large n, fixed (not small) p. then \\frac{X_n -\\mathrm{np}}{\\sqrt{\\mathrm{np}\\left(1-\\mathrm{np}\\right)}} is approximately \\mathcal{N}(0,1) distributed. This will be a special case in the Central Limit Theorem, In fact, Normal will appear in many similar scenarios. Measured quantities, heights or people, length of these lectures, etc.\n\n \\tiny {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Subruk, IIT Hyderabad }}}}"
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-09-17-CS6660-week4_1.html",
    "href": "Data_Science_Notes/Mathematics/2022-09-17-CS6660-week4_1.html",
    "title": "Probability Theory 3",
    "section": "",
    "text": "Definition:  Suppose that n independent trails are performed, each succeeding with probability p. Let X count the number of success within the n trails. Then X has the Binomial distribution with parameters n and p of, in short X~\\mathrm{Binom}\\left(n,p\\right) \\displaystyle p\\left(i\\right)=P\\left\\lbrace X=i\\right\\rbrace = {n \\choose i} \\times p^i \\times {\\left(1-p\\right)}^{n-i} ,\\;\\;\\;\\;i=0,1,\\ldotp \\ldotp \\ldotp \\ldotp ,n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport math\ndef get_Binom(n,p):\n    P=[]\n    for i in range(n+1):\n        P.append( math.comb(n,i) * math.pow(p,i) * math.pow(1-p,n-i))  \n    return P\ndef plot_dist(n,p):\n    P =get_Binom(n=n,p=p)  \n    plt.plot(range(len(P)),P,linestyle='--', marker='o',label=\"p={:.2f}\".format(p))     \n\nplot_dist(n=30,p=0.5)\nplot_dist(n=30,p=0.3)\nplot_dist(n=30,p=0.7)\nplt.title(\"Binomial Distribution\")\nplt.xlabel(\"$i$\")\nplt.ylabel(\"$p(x=i)$\");\nplt.legend();\n\n\n\n\n\n\nBinomial :  \\boxed{\\displaystyle p\\left(i\\right)=P\\left\\lbrace X=i\\right\\rbrace = {n \\choose i} \\times p^i \\times {\\left(1-p\\right)}^{n-i}} \\;\\;\\;\\;i=0,1,\\ldotp \\ldotp \\ldotp \\ldotp ,n  \\displaystyle \\mathrm{EX}=\\mathrm{np}, and \\mathrm{VarX}=\\mathrm{np}\\left(1-p\\right)\nBinomial mass function : \\displaystyle \\sum_{i=0}^n p\\left(i\\right)=\\sum_{i=0}^n P\\left\\lbrace X=i\\right\\rbrace = \\sum_{i=0}^n {n \\choose i} \\times p^i \\times {\\left(1-p\\right)}^{n-i}\nIf we consider just one trail (event), it is called bernoulli, if we consider more trails (events) it is called binomial.\nIn particular, the Bernoulli (\\rho) variable can take on values 0 or 1, with respective probabilities.  \\rho(1) = p and \\rho(0) = 1-p\nRemarks :  \\displaystyle \\sum_{i=0}^n p\\left(i\\right)=\\sum_{i=0}^n P\\left\\lbrace X=i\\right\\rbrace = \\sum_{i=0}^n {n \\choose i} \\times p^i \\times {\\left(1-p\\right)}^{n-i} =[p+(1-p)]^n=1\n\n\n\n\n\\boxed {EX = np}\n\nproof (using trick):  Trick : \\displaystyle i=\\frac{d}{dt}{{t}}^{i}{\\Big|}_{t=1}\\,  \\displaystyle EX =\\sum_{i=0}^n {n \\choose i} \\times i \\times p^i \\times (1-p)^{n-i}  using the trick we get  \\displaystyle EX =\\frac{d}{dt} \\left( \\sum_{i=0}^n {n \\choose i} \\times t^{i} \\times p^i \\times (1-p)^{n-i} \\right) {\\Big|}_{t=1}  \\displaystyle EX =\\frac{d}{dt} \\left( \\sum_{i=0}^n {n \\choose i} \\times {(tp)}^{i} \\times (1-p)^{n-i} \\right) {\\Big|}_{t=1}  \\displaystyle EX =\\frac{d}{dt} \\left( tp+1-p \\right)^n {\\Big|}_{t=1}  \\displaystyle EX =n \\left( tp+1-p \\right)^{n-1}\\cdot p {\\Big|}_{t=1}  \\displaystyle EX =n p  \n\nProof (normal way) :  \\displaystyle EX =\\sum_{i=0}^n {n \\choose i} \\times i \\times p^i \\times (1-p)^{n-i}  we can start the sum from 1 as 0^{\\text{th}} term will become zero  \\displaystyle EX =\\sum_{i=1}^n {n \\choose i} \\times i \\times p^i \\times (1-p)^{n-i}  we know that  \\displaystyle i \\cdot {n \\choose i} = n \\cdot {n-1 \\choose i-1} so we get  \\displaystyle EX =\\sum_{i=1}^n n \\times {n-1 \\choose i-1} \\times p^i \\times (1-p)^{n-i}  \\displaystyle EX =np \\sum_{i=1}^n {n-1 \\choose i-1} \\times p^{i-1} \\times (1-p)^{n-i}  \\displaystyle EX =np \\sum_{i=1}^n {n-1 \\choose i-1} \\times p^{i-1} \\times (1-p)^{(n-1)-(i-1)}  Consider n-1=m and i-1=j, we get  \\displaystyle EX =np \\sum_{j=0}^n {m \\choose j} \\times p^{j} \\times (1-p)^{m-j}  \\displaystyle EX =np \\underbrace{\\sum_{j=0}^n {m \\choose j} \\times p^{j} \\times (1-p)^{m-j}}_{=1}  \\displaystyle EX =np\n\n\n\n\n\n\n\\boxed {\\mathrm{Var} X = np(1-p)} \n\nproof (using trick):  Trick : \\displaystyle i(i-1)=\\frac{d}{dt^2}{{t}}^{i}{\\Big|}_{t=1}\\,  First find E[X(X-1)] \\displaystyle E[X(X-1)] =\\sum_{i=0}^n {n \\choose i} \\times i\\times(i-1) \\times p^i \\times (1-p)^{n-i}  \\displaystyle E[X(X-1)] =\\sum_{i=0}^n {n \\choose i} \\times \\frac{d}{dt^2}{{t}}^{i}{\\Big|}_{t=1} \\times p^i \\times (1-p)^{n-i} \\;\\;\\; We got this using the trick  \\displaystyle E[X(X-1)] =\\frac{d}{dt^2} \\left( \\sum_{i=0}^n {n \\choose i} \\times {{t}}^{i} \\times p^i \\times (1-p)^{n-i} \\right) {\\Big|}_{t=1}  \\displaystyle E[X(X-1)] =\\frac{d}{dt^2} \\left( tp + (1-p) \\right)^n {\\Big|}_{t=1}  \\displaystyle E[X(X-1)] = n(n-1) \\left( tp + 1-p \\right)^{n-2}p\\cdot p {\\Big|}_{t=1}  \\displaystyle E[X(X-1)] = n(n-1)p^2 \nNow,  \\displaystyle \\mathrm{Var} X = E(X^2)-(EX)^2  \\displaystyle \\mathrm{Var} X = \\left( E(X^2)-E(X) \\right)+ \\left(E(X)- (EX)^2 \\right)  \\displaystyle \\mathrm{Var} X = E\\left[X^2-X\\right] + \\left(E(X)- (EX)^2 \\right)  \\displaystyle \\mathrm{Var} X = E\\left[ X(X-1) \\right] + \\left(E(X)- (EX)^2 \\right)  \\displaystyle \\mathrm{Var} X = n(n-1)p^2 + np- (np)^2  \\displaystyle \\mathrm{Var} X = (np)^2-np^2 + np- (np)^2  \\displaystyle \\mathrm{Var} X = -np^2 + np  \\displaystyle \\mathrm{Var} X = np(1-p) \nProof (normal way) :  \\displaystyle \\mathrm{Var} X = E(X^2)-(EX)^2  First we find E(X^2) \\displaystyle E(X^2) =\\sum_{i=0}^n {n \\choose i} \\times i^2 \\times p^i \\times (1-p)^{n-i}  we can start the sum from 1 as 0^{\\text{th}} term will become zero  \\displaystyle E(X^2) =\\sum_{i=1}^n {n \\choose i} \\times i^2 \\times p^i \\times (1-p)^{n-i}  we know that  \\displaystyle i \\cdot i \\cdot {n \\choose i} = n \\cdot i \\cdot{n-1 \\choose i-1}\nso we get  \\displaystyle E(X^2) =\\sum_{i=1}^n n \\cdot i \\cdot{n-1 \\choose i-1} \\times p^i \\times (1-p)^{n-i}  \\displaystyle E(X^2) =n \\cdot p \\sum_{i=1}^n i \\cdot{n-1 \\choose i-1} \\times p^{i-1} \\times (1-p)^{n-i}  \\displaystyle E(X^2) =n \\cdot p \\sum_{i=1}^n i \\cdot{n-1 \\choose i-1} \\times p^{i-1} \\times (1-p)^{(n-1)-(i-1)}  Consider n-1=m and i-1=j, we get  \\displaystyle E(X^2) =n \\cdot p \\sum_{j=0}^n \\left( j+1 \\right) \\cdot{m \\choose j} \\times p^{j} \\times (1-p)^{m-j}  \\displaystyle E(X^2) =n \\cdot p \\underbrace{ \\sum_{j=0}^n j \\cdot{m \\choose j} \\times p^{j} \\times (1-p)^{m-j} }_{\\text{same as we did in expectation,so it's }mp} \\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; +n \\cdot p \\underbrace{\\sum_{j=0}^n {m \\choose j} \\times p^{j} \\times (1-p)^{m-j}}_{\\text{sum of all probability  }=1}  \\displaystyle E(X^2) =n \\cdot p \\times m\\cdot p+n \\cdot p \\displaystyle E(X^2) =n \\cdot p \\times (n-1)\\cdot p+ n \\cdot p \\displaystyle E(X^2) =(np)^2-np^2+np \\displaystyle E(X^2) =(np)^2+np(1-p)\nNow,  \\displaystyle \\mathrm{Var} X = E(X^2) - (EX)^2 \\displaystyle \\mathrm{Var} X = (np)^2+np(1-p) -(np)^2  \\displaystyle \\mathrm{Var} X = np(1-p) \nProof (yet another way, considering events are independent): if Y is just a Bernoulli trail,  Y=\\left\\lbrace \\begin{array}{ll} 1, & \\mathrm{with}\\;\\mathrm{probability}\\;P\\\\ 0, & \\mathrm{with}\\;\\mathrm{probability}\\;1-P \\end{array}\\right.\n\\mathrm{VarY}={\\mathrm{EY}}^2 -{\\left(\\mathrm{EY}\\right)}^2 =\\left(1^2 \\cdot P +0^2 \\cdot (1-P) \\right)-P^2 \\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=P \\cdot \\left(1-P \\right)\nLet X be multiple copies of independent Y If all Y are independent the we can say  \\displaystyle \\mathrm{Var}\\left( \\sum_{i=1}^n Y_i \\right) = \\sum_{i=1}^n \\mathrm{Var}\\left( Y_i \\right) Proof of above statement: Consider only 2 Events Y_1 and Y_2 for simplicity \\displaystyle \\mathrm{Var}(Y_1+Y_2) = E\\left[(Y_1+Y_2)^2 \\right] - \\left(E[Y_1+Y_2]\\right)^2 \\displaystyle = E\\left[Y_1^2 + 2Y_1Y_2 + Y_2^2 \\right] - \\left((EY_1)^2+2(EY_1)(EY_2)+(EY_2)^2\\right) \\displaystyle = E(Y_1^2) + 2E(Y_1Y_2) + E(Y_2^2) -(EY_1)^2-2(EY_1)(EY_2)-(EY_2)^2 \\displaystyle = E(Y_1^2) -(EY_1)^2 + E(Y_2^2)-(EY_2)^2 + 2(EY_1)(EY_2) -2(EY_1)(EY_2) \\displaystyle =\\mathrm{Var}Y_1+\\mathrm{Var}Y_2 In Above proof we used 2E(Y_1Y_2)=2(EY_1)(EY_2) We can do that if the 2 events are independent Proof of the same: \\displaystyle E(Y_1Y_2)= \\sum_{Y_1}\\sum_{Y_2} Y_1 Y_2 P(Y_1=y_1,Y_2=y_2) \\displaystyle E(Y_1Y_2)= \\sum_{Y_1}\\sum_{Y_2} Y_1 Y_2 P(Y_1=y_1)P(Y_2=y_2)\\;\\; as Y_1 and Y2 are independent  \\displaystyle E(Y_1Y_2)= \\sum_{Y_1}Y_1 P(Y_1=y_1) \\sum_{Y_2} Y_2 P(Y_2=y_2)  \\displaystyle E(Y_1Y_2)= E(Y_1) E(Y_2)  Hence we can say, \\displaystyle \\mathrm{Var}X = \\mathrm{Var}\\left( \\sum_{i=1}^n Y_i \\right) = \\sum_{i=1}^n \\mathrm{Var}\\left( Y_i \\right) = np(1-p) \\;\\;\\; Considering all Y_i has variance of p(1-p)"
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-09-17-CS6660-week4_1.html#poisson",
    "href": "Data_Science_Notes/Mathematics/2022-09-17-CS6660-week4_1.html#poisson",
    "title": "Probability Theory 3",
    "section": "2 Poisson",
    "text": "2 Poisson\n\nDefinition  Fix a positive real number \\lambda. The random variable X is Poisson distributed with parameter \\lambda, in short X ∼ \\mathrm{Poi}(\\lambda), if it is non-negative integer valued, and its mass function is  \\displaystyle \\rho (i) = P\\left\\{ X=i \\right\\} = \\frac{\\lambda^i}{i!} e^{-\\lambda}\\;\\;\\;\\;\\;i=0,1,2,\\dots \n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport math\ndef get_poisson(_lambda):\n    P=[]\n    for i in range(30):\n        P.append((math.pow(_lambda,i)/math.factorial(i))* math.exp(-_lambda))\n    return P\ndef plot_dist(_lambda):\n    P =get_poisson(_lambda=_lambda) \n    plt.plot(range(len(P)),P,linestyle='--', marker='o',label='lambda = '+str(_lambda))     \n\nplot_dist(_lambda=1)\nplot_dist(_lambda=4)\nplot_dist(_lambda=10)\nplot_dist(_lambda=15)\nplt.title(\"poisson Distribution\")\nplt.xlabel(\"$i$\")\nplt.ylabel(\"$p(x=i)$\");\nplt.legend();\n\n\n\n\n\n\nTake Y \\sim \\mathrm{Binom}(n, p) with large n, small p, such that np \\simeq \\lambda. Then Y is approximately \\mathrm{Poisson}(\\lambda) distributed.\nProof of Poisson formula  Binomial formula: \\displaystyle p\\left(i\\right)=P\\left\\lbrace X=i\\right\\rbrace = {n \\choose i} \\times p^i \\times {\\left(1-p\\right)}^{n-i} we say that poisson is approximation of binomial when n \\rightarrow \\infty, and \\lambda =np \\Rightarrow p=\\frac{\\lambda}{n} so we get \\displaystyle \\rho \\left(i\\right) = \\lim_{n \\rightarrow \\infty}{n \\choose i} \\times \\left( \\frac{\\lambda}{n} \\right)^i \\times {\\left( 1-\\frac{\\lambda}{n} \\right)}^{n-i} \\displaystyle \\rho \\left(i\\right) = \\lim_{n \\rightarrow \\infty} \\frac{n!}{i!(n-i)!}\\times \\left( \\frac{\\lambda}{n} \\right)^i \\times {\\left( 1-\\frac{\\lambda}{n} \\right)}^{n-i} \\displaystyle \\rho \\left(i\\right) = \\lim_{n \\rightarrow \\infty} \\frac{n \\cdot (n-1)\\cdot (n-2) \\dots \\cdot (n-i+1)!}{i!} \\times \\left( \\frac{\\lambda}{n} \\right)^i \\times {\\left( 1-\\frac{\\lambda}{n} \\right)}^{n-i} \\displaystyle \\rho \\left(i\\right) = \\lim_{n \\rightarrow \\infty} \\frac{n \\cdot (n-1)\\cdot (n-2) \\dots \\cdot (n-i+1)!}{i!} \\times \\left( \\frac{\\lambda}{n} \\right)^i \\times {\\left( 1-\\frac{\\lambda}{n} \\right)}^{n} {\\left( 1-\\frac{\\lambda}{n} \\right)}^{i} \\displaystyle \\rho \\left(i\\right) = \\frac{\\lambda^i}{i!} \\lim_{n \\rightarrow \\infty} \\frac{\\overbrace{n \\cdot (n-1)\\cdot (n-2) \\dots \\cdot (n-i+1)!}^{i\\;\\;\\text{ terms}} }{n^i} \\times {\\left( 1-\\frac{\\lambda}{n} \\right)}^{n} {\\left( 1-\\frac{\\lambda}{n} \\right)}^{i} \\displaystyle \\rho \\left(i\\right) = \\frac{\\lambda^i}{i!} \\lim_{n \\rightarrow \\infty} \\frac{n^i +\\dots}{n^i} \\times \\lim_{n \\rightarrow \\infty}{\\left( 1-\\frac{\\lambda}{n} \\right)}^{n} \\times \\lim_{n \\rightarrow \\infty}{\\left( 1-\\frac{\\lambda}{n} \\right)}^{i} We know that \\displaystyle \\;\\; \\lim_{n \\rightarrow \\infty}{\\left( 1+\\frac{a}{x} \\right)}^{x} = e^a \\displaystyle \\rho \\left(i\\right) = \\frac{\\lambda^i}{i!} \\underbrace{\\lim_{n \\rightarrow \\infty} \\frac{n^i +\\dots}{n^i}}_{=1} \\times \\underbrace{\\lim_{n \\rightarrow \\infty}{\\left( 1-\\frac{\\lambda}{n} \\right)}^{n}}_{e^{-\\lambda}} \\times \\underbrace{\\lim_{n \\rightarrow \\infty}{\\left( 1-\\frac{\\lambda}{n} \\right)}^{i}}_{=1,\\;i \\text{ is very small } } \\boxed{\\displaystyle \\rho \\left(i\\right) = \\frac{\\lambda^i}{i!} e^{-\\lambda}}\n\n\n2.1 Expectation \\displaystyle EX =\\lambda\n\n\\boxed {EX = \\lambda} EX = \\displaystyle \\sum_{i=0}^\\infty i \\times \\rho(i) = \\sum_{i=0}^\\infty i \\times \\frac{\\lambda^i}{i!} e^{-\\lambda} EX = \\displaystyle \\sum_{i=1}^\\infty i \\times \\frac{\\lambda^i}{i!} e^{-\\lambda} EX = \\displaystyle \\lambda \\sum_{i=1}^\\infty i \\times \\frac{\\lambda^{i-1}}{i!} e^{-\\lambda} EX = \\displaystyle \\lambda \\sum_{i=1}^\\infty \\frac{\\lambda^{i-1}}{(i-1)!} e^{-\\lambda} consider i-1=j  EX = \\displaystyle \\lambda \\underbrace{\\sum_{j=0}^\\infty \\frac{\\lambda^{j}}{j!} e^{-\\lambda}}_{=1}\nEX = \\displaystyle \\lambda\n\n\n\n2.2 Variance \\displaystyle \\mathrm{Var} X = \\lambda\n\n\\boxed {\\mathrm{Var} X = \\lambda}  \\displaystyle E\\left[X(X-1)\\right]=\\sum_{i=0}^n i(i-1)\\rho(i)  \\displaystyle E\\left[X(X-1)\\right]=\\sum_{i=0}^n i(i-1) \\frac{\\lambda ^i}{i!}e^{-\\lambda}  \\displaystyle E\\left[X(X-1)\\right]=\\sum_{i=2}^n i(i-1) \\frac{\\lambda ^i}{i!}e^{-\\lambda}  \\displaystyle E\\left[X(X-1)\\right]=\\sum_{i=2}^n \\frac{\\lambda ^i}{(i-2)!}e^{-\\lambda}  \\displaystyle E\\left[X(X-1)\\right]= \\lambda ^2\\sum_{i=2}^n \\frac{\\lambda ^{i-2}}{(i-2)!}e^{-\\lambda}  Consder i-2=j  \\displaystyle E\\left[X(X-1)\\right]= \\lambda ^2\\underbrace{\\sum_{j=0}^n \\frac{\\lambda ^{j}}{j!}e^{-\\lambda}}_{=1}  \\displaystyle E\\left[X(X-1)\\right]= \\lambda ^2  Now,  \\displaystyle \\mathrm{Var}X = E(X^2)-(EX)^2 \\displaystyle \\mathrm{Var}X = E\\left[X(X-1)\\right] +EX-(EX)^2 \\displaystyle \\mathrm{Var}X = \\lambda^2 +\\lambda-\\lambda^2 \\displaystyle \\mathrm{Var}X = \\lambda"
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-09-17-CS6660-week4_1.html#geometric",
    "href": "Data_Science_Notes/Mathematics/2022-09-17-CS6660-week4_1.html#geometric",
    "title": "Probability Theory 3",
    "section": "3 Geometric",
    "text": "3 Geometric\n\nDefinition  Suppose that independent trials, each succeeding with probability p, are repeated until the first success. The total number X of trials made has the \\mathrm{Geometric}(p) distribution (in short, X \\sim \\mathrm{Geom}(p)).  X can take on positive integers, with probabilities  \\boxed{ \\displaystyle \\rho(i) = (1-p)^{i-1} \\cdot p} \\;\\;\\;\\;\\; i=1,2,3,\\dots That is a function, we verify by \\rho(i) \\ge 0 and  \\displaystyle \\sum_{i=1}^\\infty \\rho(i) = \\sum_{i=1}^\\infty (1-p)^{i-1} \\cdot p = \\frac{p}{1-(1-p)}=1\nRemarks  For a Geometric(p) random variable and any k \\ge 1 we have  P \\left\\{ X \\ge k \\right\\}=(1-p)^{k-1} (We have at least k-1 failures).\nCorollary  The Geometric random variable is (discrete) memory-less :for every k \\ge 1 , n \\ge 0 \\displaystyle P \\left\\{ X \\ge n + k \\mid X>n \\right\\}=P \\left\\{ X \\ge k \\right\\} Proof :  \\displaystyle P \\left\\{ X \\ge n + k \\mid X>n \\right\\}= \\frac {P\\left\\{ X \\ge n + k \\right\\} \\cap P\\left\\{X>n \\right\\}}{P\\left\\{X>n \\right\\}} if X \\ge n+k then X is also greater than n as k is at least 1 \\displaystyle P \\left\\{ X \\ge n + k \\mid X>n \\right\\}= \\frac {P\\left\\{ X \\ge n + k \\right\\} }{P\\left\\{X>n \\right\\}} \\displaystyle P \\left\\{ X \\ge n + k \\mid X>n \\right\\}= \\frac {(1-p)^{n+k-1} }{(1-p)^n}= (1-p)^{k-1}=P \\left\\{ X>k \\right\\}\n\n\n3.1 Expectation \\displaystyle EX =\\frac{1}{p}\n\n\\boxed {EX = \\frac{1}{p}}\n\nproof (using trick):  Trick : \\displaystyle i=\\frac{d}{dt}{{t}}^{i}{\\Big|}_{t=1}\\,  \\displaystyle EX = \\sum_{i=1}^\\infty i \\cdot (1-p)^{i-1} \\cdot p  \\displaystyle EX = \\sum_{i=0}^\\infty \\frac{d}{dt}{t}^{i}{\\Big|}_{t=1} (1-p)^{i-1} \\cdot p  \\displaystyle EX = \\frac{d}{dt} \\left( \\sum_{i=0}^\\infty {t}^{i} (1-p)^{i-1} \\cdot p \\right) {\\Bigg|}_{t=1} \\displaystyle EX = \\frac{d}{dt} \\left( \\frac{p}{1-p}\\sum_{i=0}^\\infty {t}^{i} (1-p)^{i} \\right) {\\Bigg|}_{t=1} \\displaystyle EX = \\frac{p}{1-p} \\cdot \\frac{d}{dt} \\left( \\frac{1}{1-t(1-p)} \\right) {\\Bigg|}_{t=1} \\displaystyle EX = \\frac{p}{1-p} \\cdot \\left( \\frac{1-p}{\\left(1-t(1-p)\\right)^2} \\right) {\\Bigg|}_{t=1} \\displaystyle EX = \\frac{1}{p}\nProof (normal way): \\displaystyle EX = \\sum_{i=1}^\\infty i \\cdot (1-p)^{i-1} \\cdot p \n\n\\begin{align*}{}\nEX &=   p + 2 \\cdot (1-p)\\cdot p + 3 \\cdot (1-p)^2 \\cdot p+\\cdots  &   \\qquad (1) \\\\\nEX(1-p) &=  \\qquad\\;\\;\\; (1-p)\\cdot p + 2 \\cdot (1-p)^2\\cdot p + 3 \\cdot (1-p)^3 \\cdot p +\\cdots &  \\qquad (2)\n\\end{align*}\n\nSubtracting equation (1) from (2) we get  \\displaystyle p \\cdot EX = p + (1-p)\\cdot p + (1-p)^2 \\cdot p +\\cdots \\displaystyle EX = 1 + (1-p) + (1-p)^2 +\\cdots \\displaystyle EX = \\frac{1}{1-(1-p)} \\displaystyle EX = \\frac{1}{p}\n\n\n\n\n3.2 Variance \\displaystyle \\mathrm{Var} X = \\frac{1-p}{p^2}\n\n\\boxed {\\mathrm{Var} X = \\frac{1-p}{p^2}} \n\nProof (using trick); Trick : \\displaystyle i(i-1)=\\frac{d}{dt^2}{{t}}^{i}{\\Big|}_{t=1}\\,  First find E[X(X-1)] \\displaystyle E[X(X-1)] =\\sum_{i=1}^\\infty i\\times(i-1) \\times (1-p)^{i-1} \\times p \n\\displaystyle E[X(X-1)] =\\sum_{i=1}^\\infty \\frac{d}{dt^2}{t}^{i}{\\Big|}_{t=1} \\times (1-p)^{i-1} \\times p \n\\displaystyle E[X(X-1)] =p \\frac{d}{dt^2} \\left( \\sum_{i=1}^\\infty{t}^{i}(1-p)^{i-1} \\right) {\\Bigg|}_{t=1} \n\\displaystyle E[X(X-1)] = \\frac{p}{1-p} \\cdot \\frac{d}{dt^2} \\left( \\sum_{i=1}^\\infty{t}^{i}(1-p)^{i} \\right) {\\Bigg|}_{t=1} \n\\displaystyle E[X(X-1)] = \\frac{p}{1-p} \\cdot \\frac{d}{dt^2} \\left( \\frac{1}{1-t(1-p)} \\right) {\\Bigg|}_{t=1}  \\displaystyle E[X(X-1)] = \\frac{p}{1-p} \\cdot \\left( \\frac{2(1-p)(1-p)}{(\\left( 1-t(1-p) \\right)^3 } \\right) {\\Bigg|}_{t=1}  \\displaystyle E[X(X-1)] = \\frac{p}{1-p} \\cdot \\left( \\frac{2(1-p)(1-p)}{p^3 } \\right)  \\displaystyle E[X(X-1)] = \\left( \\frac{2(1-p)}{p^2 } \\right)  Now,  \\displaystyle \\mathrm{Var}X = E(X^2)-(EX)^2 \\displaystyle \\mathrm{Var}X = E\\left[X(X-1)\\right] +EX-(EX)^2\n\\displaystyle \\mathrm{Var}X = \\left( \\frac{2(1-p)}{p^2 } \\right) + \\frac{1}{p} -\\left( \\frac{1}{p} \\right)^2\n\\displaystyle \\mathrm{Var}X = \\left( \\frac{2-2p}{p^2 } \\right) + \\frac{1}{p} -\\left( \\frac{1}{p} \\right)^2 \\displaystyle \\mathrm{Var}X = \\frac{2-2p+p-1}{p^2} \\displaystyle \\mathrm{Var}X = \\frac{1-p}{p^2}\nproof (normal way): we first solve for E(X^2) \\displaystyle E(X^2) = \\left( \\sum_{i=1}^\\infty i^2 \\times (1-p)^{i-1} \\times p \\right) \n\n\\begin{align*}{}\n  E(X^2) &=   p + 4 \\cdot (1-p) \\cdot p + 9 \\cdot (1-p)^2 \\cdot p + \\cdots  &     (3) \\\\\n  (1-p) E(X^2) &=  \\qquad\\;\\;\\; \\;(1-p) \\cdot p + 4 \\cdot (1-p)^2 \\cdot p + 9 \\cdot (1-p)^3 \\cdot p + \\cdots  &    (4)\n\\end{align*}\n\n subtracting eqution (4) from equation (3) \n\\displaystyle E(X^2) = p+3 \\cdot (1-p) + 5 \\cdot (1-p)^2 + 7 \\cdot (1-p)^3 + \\cdots This is AGP, the sum is given by s_{\\infty}=\\frac{a}{1-r}+\\frac{dr}{(1-r)^2}, \\;\\;\\;\\; for r<1 In our case r=(1-p),\\;a=1,\\;d=2\\; so we get,  \\displaystyle E(X^2) =\\frac{1}{1-(1-p)}+\\frac{2\\cdot (1-p)}{(1-(1-p))^2} \\displaystyle E(X^2) =\\frac{1}{p}+\\frac{2-2p}{p^2} \\displaystyle E(X^2) =\\frac{p+2-2p}{p^2} \\displaystyle E(X^2) =\\frac{2-p}{p^2} \\displaystyle \\mathrm{Var}X = E(X^2) - (EX)^2  \\displaystyle \\mathrm{Var}X = \\frac{2-p}{p^2} - \\left(\\frac{1}{p}\\right)^2  \\displaystyle \\mathrm{Var}X = \\frac{1-p}{p^2} \n\n\n \\tiny {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Subruk, IIT Hyderabad }}}}"
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-08-13-CS6660-week2.html",
    "href": "Data_Science_Notes/Mathematics/2022-08-13-CS6660-week2.html",
    "title": "Probability Theory 2",
    "section": "",
    "text": "A random variable is a function from a sample space \\Omega to the real numbers \\mathbb{R} A random variable X that can take on finitely or countably infinitely many possible values is called discrete."
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-08-13-CS6660-week2.html#indicator-random-variable",
    "href": "Data_Science_Notes/Mathematics/2022-08-13-CS6660-week2.html#indicator-random-variable",
    "title": "Probability Theory 2",
    "section": "1 Indicator random variable",
    "text": "1 Indicator random variable\nX=\\left\\lbrace \\begin{array}{ll} 1, & \\mathrm{if}\\;\\mathrm{event}\\;E\\;\\mathrm{occurs}\\\\ 0, & \\mathrm{if}\\;\\mathrm{event}\\;E^c \\;\\mathrm{occurs} \\end{array}\\right. \\mathrm{EX}=0\\cdot p\\left(0\\right)+1\\cdot p\\left(1\\right)=P\\left\\lbrace E\\right\\rbrace \\mathrm{VarX}={\\mathrm{EX}}^2 -{\\left(\\mathrm{EX}\\right)}^2 =\\left(1^2 \\cdot P\\left\\lbrace E\\right\\rbrace +0^2 \\cdot P\\left\\lbrace E^c \\right\\rbrace \\right)-{\\left(P\\left\\lbrace E\\right\\rbrace \\right)}^2 =P\\left\\lbrace E\\right\\rbrace \\cdot \\left(1-P\\left\\lbrace E\\right\\rbrace \\right) \\mathrm{SD}\\;X=\\sqrt{P\\left\\lbrace E\\right\\rbrace \\cdot \\left(1-P\\left\\lbrace E\\right\\rbrace \\right)}"
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-08-13-CS6660-week2.html#mass-function",
    "href": "Data_Science_Notes/Mathematics/2022-08-13-CS6660-week2.html#mass-function",
    "title": "Probability Theory 2",
    "section": "2 Mass function",
    "text": "2 Mass function\nLet X be a discrete random variable with possible values X_1 ,X_2 ,\\ldotp \\ldotp \\ldotp The probability mass function (pmf), or distribution of a random variable tells us the probabilities of these possible values:  p_X \\left(x_i \\right)=P\\left\\lbrace X=x_i \\right\\rbrace For any discrete random variable X  p\\left(X_i \\right)\\ge 0, and \\sum_i p\\left(X_i \\right)=1"
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-08-13-CS6660-week2.html#expectation",
    "href": "Data_Science_Notes/Mathematics/2022-08-13-CS6660-week2.html#expectation",
    "title": "Probability Theory 2",
    "section": "3 Expectation",
    "text": "3 Expectation\n\nThe expection, or mean, or expected value of a discrete random variable X is defined as EX=\\sum_i X_i \\cdot p\\left(X_i \\right), provided that this sum exists\nExpected value is not necessarily a possible value\nExpected value can be infinity\nExpected value many not exist\nE\\left(\\mathrm{aX}+b\\right)=a\\cdot \\mathrm{EX}+b\n\nproof: E\\left(\\mathrm{aX}+b\\right)=\\sum_i \\left(\\mathrm{aX}+b\\right)\\cdot p\\left(i\\right)=a\\sum_i X\\cdot p\\left(i\\right)+b\\sum_i p\\left(i\\right)=a\\cdot \\mathrm{EX}+b\n\n{\\mathrm{EX}}^n =E\\left(X^n \\right)\\not= {\\left(\\mathrm{EX}\\right)}^n"
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-08-13-CS6660-week2.html#variance",
    "href": "Data_Science_Notes/Mathematics/2022-08-13-CS6660-week2.html#variance",
    "title": "Probability Theory 2",
    "section": "4 Variance",
    "text": "4 Variance\nThe variance and the standard deviation of a random variable are defined as \\mathrm{VarX}:={E\\left(X-\\mathrm{EX}\\right)}^2 and \\mathrm{SDX}:=\\sqrt{\\;\\mathrm{VarX}}\n\n4.1 Properties of the Variance\n\n\\mathrm{VarX}={\\mathrm{EX}}^2 -{\\left(\\mathrm{EX}\\right)}^2\n\nproof :\n\n\\begin{align*}{}\n  \\mathrm{VarX}&:={E\\left(X-\\mathrm{EX}\\right)}^2 \\\\\n  &=E\\left(\\left(X^2 -2\\cdot X\\cdot \\mathrm{EX}+{\\left(\\mathrm{EX}\\right)}^2 \\right)\\right)\\\\\n  &=E\\left(X^2 \\right)-E\\left(2\\cdot X\\cdot \\mathrm{EX}\\right)+{E\\left({\\left(\\mathrm{EX}\\right)}^2 \\right)}^{\\;} \\\\\n  &=E\\left(X^2 \\right)-2\\cdot \\mathrm{EX}\\cdot \\mathrm{EX}+{\\left(\\mathrm{EX}\\right)}^2 ={E\\left({\\mathrm{X}}^2 \\right)}^{\\;} -{\\left(\\mathrm{EX}\\right)}^2\n  \\end{align*}\n\nHere EX is a constant so can come out of E\\left(2\\cdot X\\cdot \\mathrm{EX}\\right) and it becomes 2\\cdot \\mathrm{EX}\\cdot \\mathrm{EX} also expectation has no effect on {E\\left({\\left(\\mathrm{EX}\\right)}^2 \\right)}^{\\;} for the same reason so it becomes {\\left(\\mathrm{EX}\\right)}^2\n\ncorollary: for any X,{\\mathrm{EX}}^2 \\ge {\\left(\\mathrm{EX}\\right)}^2 here equality hold only if X=constant a.s. (almost always means probability one)\n\n\\mathrm{Var}\\left(\\mathrm{aX}+b\\right)=a^2 \\cdot \\mathrm{VarX}\n\nproof \\begin{align*}{}\n  \\mathrm{Var}\\left(\\mathrm{aX}+b\\right)&={E\\left(\\mathrm{aX}+b\\right)}^2 -{\\left(E\\left(\\mathrm{aX}+b\\right)\\right)}^2 \\\\\n  &=E\\left(a^2 X^2 +2\\mathrm{abX}+b^2 \\right)-{\\left(\\mathrm{aEX}+b\\right)}^2 \\\\\n  &=a^2 {\\cdot \\mathrm{EX}}^2 +2\\mathrm{ab}\\cdot \\mathrm{EX}+b^2 -a^2 \\cdot {\\left(\\mathrm{EX}\\right)}^2 -2\\mathrm{ab}\\cdot \\mathrm{EX}-b^2 \\\\\n  &=a^2 \\left({\\mathrm{EX}}^2 -{\\left(\\mathrm{EX}\\right)}^2 \\right)=a^2 \\mathrm{VarX}\n  \\end{align*}\n\n\\mathrm{Var}\\left(X+b\\right)=\\mathrm{VarX}=\\mathrm{Var}\\left(-X\\right)\n\n\n\n\n\n\n\nTip\n\n\n\n\\mathrm{VarX}={\\mathrm{EX}}^2 -{\\left(\\mathrm{EX}\\right)}^2\\mathrm{Var}\\left(\\mathrm{aX}+b\\right)=a^2 \\cdot \\mathrm{VarX}\\mathrm{Var}\\left(X+b\\right)=\\mathrm{VarX}=\\mathrm{Var}\\left(-X\\right)"
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-08-13-CS6660-week2.html#bernoulli-binomial",
    "href": "Data_Science_Notes/Mathematics/2022-08-13-CS6660-week2.html#bernoulli-binomial",
    "title": "Probability Theory 2",
    "section": "5 Bernoulli, Binomial",
    "text": "5 Bernoulli, Binomial\nSuppose that n independent trails are performed, each succeeding with probability p. Let X count the number of success within the n trails. Then X has the Binomial distribution with parameters n and p of, in short X~\\mathrm{Binom}\\left(n,p\\right)\n\n5.1 Binomial Function\np\\left(i\\right)=P\\left\\lbrace X=i\\right\\rbrace = {n \\choose i} p^i {\\left(1-p\\right)}^{n-i} ,\\;\\;\\;\\;i=0,1,\\ldotp \\ldotp \\ldotp \\ldotp ,n \\mathrm{EX}=\\mathrm{np}, and \\mathrm{VarX}=\\mathrm{np}\\left(1-p\\right)\n \\tiny {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Subruk, IIT Hyderabad }}}}"
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-10-15-CS6660-week7.html",
    "href": "Data_Science_Notes/Mathematics/2022-10-15-CS6660-week7.html",
    "title": "Probability Theory 5",
    "section": "",
    "text": "Note\n\n\n\nMost examples will involve two random variables, but everything can be generalized for more of them.\n\n\n\n\nSuppose two discrete random variables X and Y are defined on a common probability space, and can take on values x_1, x_2, \\dots and y_1, y_2, \\dots, respectively. The joint probability mass function of them is defined as  p(x_i , y_j ) = P{X = x_i , Y = y_j}, i = 1, 2, \\dots , j = 1, 2, \\dots . This function contains all information about the joint distribution of X and Y. Any joint mass function satisfies : p(x,y)\\ge 0, \\; \\forall x,y \\in\\mathbb{R} \\sum_{i,j}p(x_i,j_j)=1 Any function with above properties is a joint probability mass function.\n\n\n\nMarginal mass functions are  p_X(x_i):=P\\{X=x_i\\} and p_Y(y_i):=P\\{Y=y_i\\} Also p_X(x_i)=\\sum_jp(x_i,y_j) and p_Y(y_j)=\\sum_ip(x_i,y_j)\n\n\n\nAn urn has 3 red, 4 white, 5 black balls. Drawing 3 at once, let X be the number of red, Y the number of white balls drawn. The joint mass function is:\n\nTable shows Joint probability distribution.\n\n\n\n\n\n\n\n\n\n\nY \\downarrow X \\rightarrow\n0\n1\n2\n3\np_Y(\\cdot)\n\n\n\n\n0\n\\displaystyle \\frac{\\binom{5}{3}}{\\binom{12}{3}}\n\\displaystyle \\frac{\\binom{3}{1}\\binom{5}{2}}{\\binom{12}{3}}\n\\displaystyle \\frac{\\binom{3}{2}\\binom{5}{1}}{\\binom{12}{3}}\n\\displaystyle \\frac{\\binom{3}{3}\\binom{5}{0}}{\\binom{12}{3}}\n\\displaystyle \\frac{\\binom{8}{3}}{\\binom{12}{3}}\n\n\n1\n\\displaystyle \\frac{\\binom{4}{1}\\binom{5}{2}}{\\binom{12}{3}}\n\\displaystyle \\frac{\\binom{4}{1}\\binom{3}{1}\\binom{5}{1}}{\\binom{12}{3}}\n\\displaystyle \\frac{\\binom{4}{1}\\binom{3}{2}}{\\binom{12}{3}}\n0\n\\displaystyle \\frac{\\binom{4}{1}\\binom{8}{2}}{\\binom{12}{3}}\n\n\n2\n\\displaystyle \\frac{\\binom{4}{2}\\binom{5}{1}}{\\binom{12}{3}}\n\\displaystyle \\frac{\\binom{4}{2}\\binom{3}{1}}{\\binom{12}{3}}\n0\n0\n\\displaystyle \\frac{\\binom{4}{2}\\binom{8}{1}}{\\binom{12}{3}}\n\n\n3\n\\displaystyle \\frac{\\binom{4}{3}}{\\binom{12}{3}}\n0\n0\n0\n\\displaystyle \\frac{\\binom{4}{3}}{\\binom{12}{3}}\n\n\np_X(\\cdot)\n\\displaystyle \\frac{\\binom{9}{3}}{\\binom{12}{3}}\n\\displaystyle \\frac{\\binom{3}{1}\\binom{9}{2}}{\\binom{12}{3}}\n\\displaystyle \\frac{\\binom{3}{2}\\binom{9}{1}}{\\binom{12}{3}}\n\\displaystyle \\frac{\\binom{3}{3}}{\\binom{12}{3}}\n1"
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-10-15-CS6660-week7.html#conditional-mass-function",
    "href": "Data_Science_Notes/Mathematics/2022-10-15-CS6660-week7.html#conditional-mass-function",
    "title": "Probability Theory 5",
    "section": "2 Conditional mass function",
    "text": "2 Conditional mass function\n\n2.1 Definition\nsuppose p_Y(y_j)>0. The conditional mass function of X given Y=y_j is defined by p_{X \\mid Y}(x \\mid y_i):= P\\{X=x \\mid Y=y_j\\}=\\frac{\\overbrace{p(x,y_i)}^{\\text{joint}} }{\\underbrace{p_Y(y_i)}_{\\text{marginal}} } As the conditional probability was a proper probability, this is a proper mass function: \\forall x,y_i p_{X \\mid Y}(x \\mid y_j) \\ge 0, \\qquad \\sum_i p_{X|Y}(x_i \\mid y_j)=1\n\n2.1.1 Example\nLet X and Y have joint mass function\n\njoint distribution\n\n\nX \\downarrow Y \\rightarrow\n0\n1\n\n\n\n\n0\n0.4\n0.2\n\n\n1\n0.1\n0.3\n\n\n\nThe conditional distribution of X given Y=0 is p_{X\\mid Y}(0 \\mid 0)= \\frac{p(0,0)}{p_Y(0)}= \\frac{p(0,0)}{p(0,0)+p(1,0)}=\\frac{0.4}{0.4+0.1}=\\frac{4}{5} p_{X\\mid Y}(1 \\mid 0)= \\frac{p(1,0)}{p_Y(0)}= \\frac{p(1,0)}{p(0,0)+p(1,0)}=\\frac{0.1}{0.4+0.1}=\\frac{1}{5}"
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-10-15-CS6660-week7.html#independent-random-variable",
    "href": "Data_Science_Notes/Mathematics/2022-10-15-CS6660-week7.html#independent-random-variable",
    "title": "Probability Theory 5",
    "section": "3 Independent Random Variable",
    "text": "3 Independent Random Variable\n\n3.1 Definition\nRandom variables X and Y are independent, if events formulated with them are so, That is, if for every A,B\\subseteq \\mathbb{R} P\\{X\\in A,Y\\in B\\}=P\\{X\\in A\\} \\cdot P \\{ Y \\in B\\} \n\n\n\n\n\n\nTip\n\n\n\nThe abbreviation i.i.d. is used for independent and identically distributed random variables.\n\n\nTwo random variables X and Y are independent if and only if their joint mass function factorizes into the product of the marginals : p(x_i,y_i)=p_X(x_i)\\cdot p_Y(y_i), \\qquad \\forall x_i,y_i\n\n\n3.2 Discrete Convolution\nLet X and Y be independent, integer valued random variables with respective mass functions p_X and p_Y . Then p_{X+Y}(k) = \\sum_{i=-\\infty}^\\infty p_X(k − i) \\cdot p_Y (i), \\qquad (\\forall k \\in \\mathbb{Z}) This formula is called discrete convolution of the mass function p_X and p_Y\n\nProof \\begin{align*}\np_{X+Y}(K)&=P\\{X+Y=K\\}\\\\\n&=\\sum_{i=-\\infty}^\\infty P\\{X=k-i,Y=i\\}\\\\\n&=\\sum_{i=-\\infty}^\\infty P_X(k-i)\\cdot p_Y(i)\n\\end{align*}\n\nLet X \\sim \\textrm{Poi}(\\lambda) and Y \\sim \\textrm{Poi}(\\mu) be independent than:  X+Y \\sim \\textrm{Poi}(\\lambda+\\mu)\n\nproof \\begin{align*}\np_{X+Y}(K)&=\\sum_{i=-\\infty}^\\infty P_X(k-i)\\cdot p_Y(i) \\\\\n&= \\sum_{i=-\\infty}^\\infty \\frac{\\lambda^{(k-i)}}{(k-i)!}e^{-\\lambda}\\cdot \\frac{\\mu^i}{i!}e^{-\\mu} \\\\\n&= e^{-\\lambda - \\mu}\\frac{1}{k!}\\sum_{i=-\\infty}^\\infty \\frac{k!}{(k-i)!\\cdot i!}\\lambda^{(k-i)} \\cdot \\mu^i \\\\\n&= e^{-\\lambda - \\mu}\\frac{1}{k!}\\sum_{i=-\\infty}^\\infty \\binom{k}{i} \\lambda^{(k-i)} \\cdot \\mu^i \\\\\n&= e^{-(\\lambda + \\mu)}\\frac{1}{k!}(\\lambda + \\mu)^k \\\\\n&=\\textrm{Poi}(\\lambda+\\mu)\n\\end{align*}\n\nLet X,Y be i.i.d. \\textrm{Geom}(p) variables then:  X+Y is not geometric.\n\nproof \\begin{align*}\np_{X+Y}(K)&=\\sum_{i=-\\infty}^\\infty P_X(k-i)\\cdot p_Y(i) \\\\\n&=\\sum_{i=1}^{k-1}(1-p)^{k-i-1}p\\cdot (1-p)^{i-1}p \\\\\n&=(k-1)(1-p)^{k-2}p^2\n\\end{align*}\nHence X+Y is not Geometric, it’s actually called Negative Binomial.\n\nLet X \\sim \\textrm{Binom}(n,p) and Y \\sim \\textrm{Binom}(m,p) be independent (notice the same \\mathbf p) then : X+Y \\sim \\textrm{Binom}(n+m,p)\n\nproof \\begin{align*}\np_{X+Y}(K)&=\\sum_{i=-\\infty}^\\infty P_X(k-i)\\cdot p_Y(i) \\\\\n&=\\sum_{i=0}^k \\binom{n}{k-i}p^{k-i}(1-p)^{n-k+i}\\cdot \\binom{m}{i}p^i(1-p)^{m-i} \\\\\n&=p^k(1-p)^{m+n-k}\\sum_{i=0}^k \\binom{n}{k-i} \\binom{m}{i}  \\\\\n&=\\binom{m+n}{k}  p^k(1-p)^{m+n-k} \\\\\n&=\\textrm{Binom}(n+m,p)\n\\end{align*}\nTo prove above equation we used the fact that \\sum_{i=0}^k \\binom{n}{k-i} \\binom{m}{i}=\\binom{m+n}{k}\n\n\n\n3.3 Continuous convolution\nSuppose X and Y are independent continuous random variables with respective densities f_X and f_Y. Then their sum is a continuous random variable with density f_{X+Y}(a)=\\int_{- \\infty} ^ \\infty f_X(a-y)\\cdot f_Y(y)dy, \\qquad (\\forall a \\in \\mathbb{R})\n\n\n3.4 Gamma distribution\nLet X and Y be i.i.d. \\mathrm{Exp}(\\lambda), and the density of their sum (a\\ge 0)\n\\begin{align*}\nf_{X+Y}(a)&=\\int_{-\\infty}^\\infty f_X(a-y) \\cdot f_Y(y)dy \\\\\n&=\\int_{0}^a \\lambda e^{-\\lambda (a-y)}\\cdot \\lambda e^{-\\lambda y}dy \\\\\n&=\\lambda^2 e^{-\\lambda a}\\cdot y \\Big |_0^a \\\\\n&= \\lambda^2 a \\cdot e^{-\\lambda a}\n\\end{align*} This density is called \\mathrm{Gamma}(2,\\lambda)\n\nLet X \\sim \\mathrm{Exp}(\\lambda) and Y \\sim \\mathrm{Gamma}(2,\\lambda) be i.i.d. again\n\\begin{align*}\nf_{X+Y}(a)&=\\int_{-\\infty}^\\infty f_X(a-y) \\cdot f_Y(y)dy \\\\\n&=\\int_{0}^a \\lambda e^{-\\lambda (a-y)}\\cdot \\lambda^2 y \\cdot e^{-\\lambda y}dy \\\\\n&=\\lambda^3 e^{-\\lambda a}\\cdot \\frac{y^2}{2} \\Big |_0^a \\\\\n&= \\frac{\\lambda^3 a^2 \\cdot e^{-\\lambda a}}{2}\n\\end{align*} This density is called \\mathrm{Gamma}(3,\\lambda)\n\nLet X \\sim \\mathrm{Exp}(\\lambda) and Y \\sim \\mathrm{Gamma}(3,\\lambda) be i.i.d. again\n\\begin{align*}\nf_{X+Y}(a)&=\\int_{-\\infty}^\\infty f_X(a-y) \\cdot f_Y(y)dy \\\\\n&=\\int_{0}^a \\lambda e^{-\\lambda (a-y)}\\cdot\\frac{\\lambda^3 y^2 \\cdot e^{-\\lambda y}}{2}dy \\\\\n&=\\lambda^3 e^{-\\lambda a}\\cdot \\frac{y^3}{2\\cdot 3} \\Big |_0^a \\\\\n&= \\frac{\\lambda^4 a^3 \\cdot e^{-\\lambda a}}{2\\cdot 3} \\\\\n&= \\frac{\\lambda^4 a^3 \\cdot e^{-\\lambda a}}{3!}\n\\end{align*} This density is called \\mathrm{Gamma}(4,\\lambda)\n The convolution of n i.i.d. \\mathrm{Exp}(\\lambda) distributions results in the \\mathrm{Gamma}(n,\\lambda) density:\nf(X)=\\frac{\\lambda^n X^{n-1} \\cdot e^{-\\lambda X}}{(n-1)!},\\qquad \\forall X\\ge 0 \\tag{1} and zero otherwise.  This is the density of the sum of n i.i.d. \\mathrm{Exp}(\\lambda) random variables. In particular, \\mathrm{Gamma}(1,\\lambda) \\equiv \\mathrm{Exp}(\\lambda) \nNow if we integrate f(X) it should equal to 1 \\begin{align*}\n\\int_{-\\infty}^\\infty f(x) &= \\int_{-\\infty}^\\infty \\frac{\\lambda^n X^{n-1} \\cdot e^{-\\lambda X}}{(n-1)!} dx \\\\\n&=  \\int_{-\\infty}^\\infty \\frac{ (\\lambda X)^{n-1} \\cdot e^{-\\lambda X}}{(n-1)!} \\lambda dx \\\\\n&=1\n\\end{align*} Now we write Z= \\lambda X, \\;dZ=\\lambda dX,from above equation we get: (n-1)! = \\int_{-\\infty}^\\infty  (Z)^{n-1} \\cdot e^{-Z}  dZ \nThe Gamma function is defined for every \\alpha > 0 real numbers, by \\Gamma(\\alpha) :=\\int_{-\\infty}^\\infty  Z^{\\alpha-1} \\cdot e^{-Z}  dZ In particular, \\Gamma(n)=(n-1)! for positive integer n  Using equation (1) we can write Gamma distribution\nf(X)=\\frac{\\lambda^n X^{n-1} \\cdot e^{-\\lambda X}}{\\Gamma(n)}, \\qquad \\forall X\\ge 0 and zero otherwise.\n If X \\sim \\mathrm{Gamma}(\\alpha , \\lambda), then\nEX= \\frac{\\alpha}{\\lambda}, \\qquad \\mathrm{Var}X=\\frac{\\alpha}{\\lambda ^2}"
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-10-15-CS6660-week7.html#expectation-covariance",
    "href": "Data_Science_Notes/Mathematics/2022-10-15-CS6660-week7.html#expectation-covariance",
    "title": "Probability Theory 5",
    "section": "4 Expectation, covariance",
    "text": "4 Expectation, covariance\n\n4.1 Expectation\nExpectation is defined as EX = \\sum_i X_i\\cdot p(X_i), \\qquad EX=\\int_{-\\infty}^\\infty Xf(X)dX\n\n4.1.1 Properties of Expectation\n\nSimple monotonicity property : If a \\le X \\le b then, a \\le EX \\le b  Proof: \\begin{align*}\n& a =a\\cdot 1 =a \\sum_i p(X_i) \\\\\n& \\le\\sum_i X_i p(X_i) \\le \\\\\n& b \\sum_i p(X_i) =b \\cdot 1=b\n\\end{align*}\nExpectation of functions of variables Let X and Y be the random variables and g: \\mathbb{R}\\times \\mathbb{R} \\rightarrow \\mathbb{R} function then \\mathbf{E}g(X,Y)=\\sum_{i,j}g(X_i,Y_j)\\cdot p(X_i,Y_j)\nExpectation of sums and differences:\n\nLet X and Y be the random variables then E(X+Y)=EX+EY and E(X-Y)=EX-EY Proof: \\begin{align*}\nE(X\\pm Y) &= \\sum _{i,j}(X_i \\pm Y_j)\\cdot P(X_i, Y_j) \\\\\n&=\\sum_i \\sum_j X_i \\cdot P(X_i, Y_j) + \\sum_i \\sum_j Y_j \\cdot P(X_i, Y_j) \\\\\n&=\\sum_i X_i\\cdot p_X(X_i) \\pm \\sum_j Y_j\\cdot p_Y(Y_j)\\\\\n&=EX \\pm EY\n\\end{align*}\nLet X and Y be the random variable such that X \\le Y, then EX \\le EY Proof:  The difference Y-X is non negative and difference of it’s expectation is also non negative \\begin{align*}\n& &E(Y-X) &\\ge0 \\\\\n& \\Rightarrow & EY-EX &\\ge 0 \\\\\n& \\Rightarrow & -EX &\\ge -EY \\\\\n&\\Rightarrow & EX &\\le EY \\\\\n\\end{align*}\nExample (sample mean)  Let X_1,X_2, \\dots , X_n be identically distributed random variables with mean \\mu. Their sample mean is \\bar{X}:=\\frac{1}{n}\\sum_{i=1}^nX_i It’s expectation is \\begin{align*}\nE\\bar{X}&=E\\left(\\frac{1}{n}\\sum_{i=1}^nX_i\\right)\\\\\n&=\\frac{1}{n}E\\left(\\sum_{i=1}^nX_i\\right)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^nE\\left(X_i\\right)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n\\mu\\\\\n&=\\mu\n\\end{align*}\n\n\n\n\n\n4.2 Covariance\nIndependence Let X and Y be independent random variables, and g,h be functions. Then \\mathbf{E}(g(X)\\cdot h(Y))= \\mathbf{E}g(X)\\cdot \\mathbf{E}h(Y) \\tag{2} proof:  \\begin{align*}\n\\mathbf{E}(g(X)\\cdot h(Y))&=\\int \\int g(X)h(Y)p_{XY}(X,Y)dXdY \\\\\n&=\\int \\int g(X)h(Y)p_{X}(X)p_{Y}(Y)dXdY \\\\\n&=\\int  g(X)p_{X}(X)dX\\int h(Y)p_{Y}(Y)dXdY \\\\\n&=\\mathbf{E}g(X)\\cdot \\mathbf{E}h(Y)\n\\end{align*} Here we used the fact that the joint probability distribution factorizes (p_{XY}(x,y)=p_{X}(x)p_{Y}(y)) as X and Y are independent\n Covariance  The covariance of the random variable X and Y is \\mathbf{Cov}(X,Y)=\\mathbf{E}[(X-\\mathbf{E}X)\\cdot (Y-\\mathbf{E}Y)] Another form of the above formula \\mathbf{Cov}(X,Y)=\\mathbf{E}(XY)-\\mathbf{E}X\\mathbf{E}Y \\tag{3} Proof: \\begin{align*}\n\\mathbf{E}[(X-\\mathbf{E}X)\\cdot (Y-\\mathbf{E}Y)]&=\\mathbf{E}[XY-Y\\mathbf{E}X-X\\mathbf{E}Y+\\mathbf{E}X\\mathbf{E}Y]\\\\\n&=\\mathbf{E}(XY)-\\mathbf{E}Y\\mathbf{E}X-\\mathbf{E}X\\mathbf{E}Y+\\mathbf{E}X\\mathbf{E}Y \\\\\n&= \\mathbf{E}(XY)-\\mathbf{E}X\\mathbf{E}Y\n\\end{align*}\nNow we know that \\mathbf{Cov}(X,Y)=\\mathbf{E}(XY)-\\mathbf{E}X\\mathbf{E}Y Using equation (2) and (3) we can say for independent random variables X and Y,\\; \\mathbf{Cov}(X,Y)=0, as \\mathbf{E}(XY)=\\mathbf{E}X\\mathbf{E}Y \\mathbf{Cov}(X,Y)=\\mathbf{E}(XY)-\\mathbf{E}X\\mathbf{E}Y=0 But this is not true other way around. covariance zero doesn’t necessarily mean random variables are independent.\n\n\n\n\n\n\nTip\n\n\n\n\nIf random variables X and Y are independent, then \\mathbf{Cov}(X,Y)=0 \nBut if \\mathbf{Cov}(X,Y)=0, doesn’t necessarily mean X and Y are independent.\n\n\n\n\n\nProperties \n\nCovariance is positive semidefinite: \\mathbf{Cov}(X,X)=\\mathbf{Var}(X)\\ge0,\nCovariance is symmetric: \\mathbf{Cov}(X,Y)=\\mathbf{Cov}(Y,X)\nAlmost bilinear: Fix a_i , b, c_j , d real numbers. Covariance is \\mathbf{Cov}\\left(\\sum_ia_iX_i+b,\\sum_jc_jY_j+d \\right)=\\sum_{i,j}a_ic_j\\mathbf{Cov}(X_i,Y_j)\n\n\n\n\n4.3 Variance\nLet X_1, X_2, \\dots , X_n be random variables. Then \\mathbf{Var}\\sum_{i=1}^n X_i =\\sum_{i=1}^n \\mathbf{Var}\\left(X_i \\right)+2\\sum_{1\\le i\\le j\\le n} \\mathbf{Cov}\\left(X_i ,X_j \\right) In particular, variances of independent random variables are additive.  Proof: \\begin{align*}{}\n\\mathbf{Var}\\sum_{i=1}^n X_i &=\\mathbf{Cov}\\left(\\sum_{i=1}^n X_i ,\\sum_{j=1}^n X_j \\right)\\\\\n&=\\sum_{i=1,j=1}^n \\mathbf{Cov}\\left(X_i ,X_j \\right)\\\\\n&=\\sum_{i=1}^n \\mathbf{Cov}\\left(X_i ,X_i \\right)+\\sum_{i\\not= j} \\mathbf{Cov}\\left(X_i ,X_j \\right)\\\\\n&=\\sum_{i=1}^n \\mathbf{Var}\\left(X_i \\right)+\\sum_{i<j} \\mathbf{Cov}\\left(X_i ,X_j \\right)+\\sum_{i>j} \\mathbf{Cov}\\left(X_i ,X_j \\right)\\\\\n&=\\sum_{i=1}^n \\mathbf{Var}\\left(X_i \\right)+2\\sum_{1\\le i\\le j\\le n} \\mathbf{Cov}\\left(X_i ,X_j \\right)\n\\end{align*}\nNotice that for independent variables. \\begin{align*}{}\n\\mathbf{Var}\\left(X-Y\\right)&=\\mathbf{Var}\\left(X+-\\left(Y\\right)\\right)\\\\\n&=\\mathbf{Var}\\left(X\\right)+\\mathbf{Var}\\left(-Y\\right)+2\\mathbf{Cov}\\left(X,-Y\\right)\\\\\n&=\\mathbf{Var}\\left(X\\right)+\\mathbf{Var}\\left(Y\\right)-2\\mathbf{Cov}\\left(X,Y\\right)\\\\\n&=\\mathbf{Var}\\left(X\\right)+\\mathbf{Var}\\left(Y\\right)\n\\end{align*}\nAbove used the fact that  \\mathbf{Var}\\left(X+Y\\right)=\\mathbf{Var}\\left(X\\right)+\\mathbf{Var}\\left(Y\\right)+2\\mathbf{Cov}\\left(X,Y\\right), Here is the proof:\n\\begin{align*}{}\n\\mathbf{Var}\\left(X+Y\\right)&=\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(X+Y\\right)}^2 \\right\\rbrack -{\\left(\\mathit{\\mathbf{E}}\\left\\lbrack X+Y\\right\\rbrack \\right)}^2 \\\\\n&=\\mathit{\\mathbf{E}}\\left\\lbrack X^2 +Y^2 +2\\mathrm{XY}\\right\\rbrack -{\\left(\\mathit{\\mathbf{E}}\\left\\lbrack X\\right\\rbrack +\\mathit{\\mathbf{E}}\\left\\lbrack Y\\right\\rbrack \\right)}^2 \\\\\n&=\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack +\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack +2\\mathit{\\mathbf{E}}\\left\\lbrack X Y\\right\\rbrack -{\\left(\\mathit{\\mathbf{E}}\\left\\lbrack X\\right\\rbrack \\right)}^2 -{\\left(\\mathit{\\mathbf{E}}\\left\\lbrack Y\\right\\rbrack \\right)}^2 -2\\mathit{\\mathbf{E}}\\left\\lbrack X\\right\\rbrack \\mathit{\\mathbf{E}}\\left\\lbrack Y\\right\\rbrack \\\\\n&=\\left(\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack -{\\left(\\mathit{\\mathbf{E}}\\left\\lbrack X\\right\\rbrack \\right)}^2 \\right)+\\left(\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack -{\\left(\\mathit{\\mathbf{E}}\\left\\lbrack Y\\right\\rbrack \\right)}^2 \\right)+2\\left(\\mathit{\\mathbf{E}}\\left\\lbrack XY\\right\\rbrack -\\mathit{\\mathbf{E}}\\left\\lbrack X\\right\\rbrack \\mathit{\\mathbf{E}}\\left\\lbrack Y\\right\\rbrack \\right)\\\\\n&=\\mathbf{Var}\\left(X\\right)+\\mathbf{Var}\\left(Y\\right)+2\\mathbf{Cov}\\left(X,Y\\right)\n\\end{align*}\n\nExample (variance of the sample mean) Suppose that X_j’s are i.i.d. each with variance \\sigma^2, The sample mean is  \\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i It’s Variance is \\begin{align*}{}\n\\mathbf{Var}\\left(\\bar{X} \\right)&=\\mathbf{Var}\\left(\\frac{1}{n}\\sum_{i=1}^n X_i \\right)\\\\\n&=\\frac{1}{n^2 }\\mathbf{Var}\\left(\\sum_{i=1}^n X_i \\right)\\\\\n&=\\frac{1}{n^2 }\\left(\\sum_{i=1}^n \\mathbf{Var}\\left(X_i \\right)\\right)\\\\\n&=\\frac{1}{n^2 }n\\sigma^2 \\\\\n&=\\frac{\\sigma^2 }{n}\n\\end{align*} Variance of the sample mean decreases with n, that’s why we like sample averages.\nExample (unbiased sample variance)  Suppose we are given the values of X_1,X_2,\\dots X_n of an i.i.d. sequence of random variables with mean \\mu and variance \\sigma^2. We know that the sample mean \\bar{X}  has mean \\mu and  small variance \\frac{\\sigma^2}{n}  Therefore it serves as a good estimator for the value of \\mu. But what should we use to estimate the variance \\sigma^2? This quantity is the unbiased sample variance: S^2 :=\\frac{1}{n-1}\\sum_{i=1}^n {\\left(X-\\bar{X} \\right)}^2 Now we compute the expected value of sample variance \\begin{align*}{}\n{\\mathrm{ES}}^2 &=\\mathit{\\mathbf{E}}\\left\\lbrack \\frac{1}{n-1}\\sum_{i=1}^n {\\left(X-\\bar{X} \\right)}^2 \\right\\rbrack \\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n \\mathit{\\mathbf{E}}\\left\\lbrack {\\left(X-\\bar{X} \\right)}^2 \\right\\rbrack \\\\\n&=\\frac{n}{n-1}\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(X-\\bar{X} \\right)}^2 \\right\\rbrack\n\\end{align*} we get {\\mathrm{ES}}^2=\\frac{n}{n-1}\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(X-\\bar{X} \\right)}^2 \\right\\rbrack \\tag{4} Next notice that \\mathit{\\mathbf{E}}\\left\\lbrack X-\\bar{X} \\right\\rbrack =\\mathit{\\mathbf{E}}\\left\\lbrack X\\right\\rbrack -\\mathit{\\mathbf{E}}\\left\\lbrack \\bar{X} \\right\\rbrack =\\mu -\\mu =0 also \\begin{align*}{}\n\\mathbf{Var}\\left(X-\\bar{X} \\right)&=\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(X-\\bar{X} \\right)}^2 \\right\\rbrack -{\\left(\\mathit{\\mathbf{E}}\\left\\lbrack X-\\bar{X} \\right\\rbrack \\right)}^2 \\\\\n\\mathbf{Var}\\left(X-\\bar{X} \\right)&=\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(X-\\bar{X} \\right)}^2 \\right\\rbrack \\\\\n\\Rightarrow \\mathit{\\mathbf{E}}\\left\\lbrack {\\left(X-\\bar{X} \\right)}^2 \\right\\rbrack &=\\mathbf{Var}\\left(X-\\bar{X} \\right)\\\\\n&=\\mathbf{Var}\\left(X\\right)+\\mathbf{Var}\\left(\\bar{X} \\right)-2\\mathbf{Cov}\\left(X,\\bar{X} \\right)\n\\end{align*} Now , \\begin{align*}{}\n\\mathbf{Cov}\\left(X,\\bar{X} \\right)&=\\mathbf{Cov}\\left(X,\\frac{1}{n}\\sum_j X_j \\right)\\\\\n&=\\frac{1}{n}\\sum_j \\mathbf{Cov}\\left(X,X_j \\right)\\\\\n&=\\frac{1}{n}\\mathbf{Cov}\\left(X,X\\right)\\\\\n&=\\frac{\\sigma^2 }{n}\n\\end{align*} we have now \\mathbf{Var}\\left(X\\right)=\\sigma^2 ,\\mathbf{Var}\\left(\\bar{X} \\right)=\\frac{\\sigma^2 }{n},\\mathbf{Cov}\\left(X,\\bar{X} \\right)=\\frac{\\sigma^2 }{n}\nPutting it all back in equation (4) we get \\begin{align*}{}\n\\mathit{\\mathbf{E}}\\left\\lbrack S^2 \\right\\rbrack &=\\frac{n}{n-1}\\left(\\sigma^2 +\\frac{\\sigma^2 }{n}-2\\frac{\\sigma^2 }{n}\\right)\\\\\n&=\\frac{n}{n-1}\\left(\\sigma^2 -\\frac{\\sigma^2 }{n}\\right)\\\\\n&=\\frac{n}{n-1}\\frac{\\left(n-1\\right)\\sigma^2 }{n}\\\\\n&=\\sigma^2\n\\end{align*}\nExample (Binomal distribution) Suppose that n independent trails are made, each succeeding with probability p. Define X_i as the indicator of success in the i^{\\text{th}} trail, i=1,2,\\dots ,n. Then X=\\sum_{i=1}^n X_i Counts the total number of success, therefore X \\sim \\mathrm{Binom}(n,p). It’s variance is\n\\begin{align*}{}\n\\mathbf{Var}\\left(X\\right)&=\\mathbf{Var}\\left(\\sum_{i=1}^n X_i \\right)\\\\\n&=\\sum_{i=1}^n \\mathbf{Var}\\left(X_i \\right)\\\\\n&=\\sum_{i=1}^n p\\left(1-p\\right)\\\\\n&=n\\cdot p\\left(1-p\\right)\n\\end{align*}\nExample (Gamma distribution)  Let n be a positive integer, \\lambda > 0 real, and X \\sim \\mathrm{Gamma}(n,\\lambda), Then we know  X\\overset{d}{=} \\sum_{i=1}^n X_i where X_1,X_2,\\dots, X_n are i.i.d. \\mathrm{Exp}(\\lambda). Therefore \\begin{align*}{}\n\\mathbf{Var}\\left(X\\right)&=\\mathbf{Var}\\left(\\sum_{i=1}^n X_i \\right)\\\\\n&=\\sum_{i=1}^n \\mathbf{Var}\\left(X_i \\right)\\\\\n&=\\sum_{i=1}^n \\frac{1}{\\lambda^2 }\\\\\n&=\\frac{n}{\\lambda^2 }\n\\end{align*} Here \\overset{d}{=} means “equal in distribution”.\n\n\n\n4.4 Cauchy-Schwarz inequality\nFor every X and Y |\\mathit{\\mathbf{E}}\\left\\lbrack \\mathrm{XY}\\right\\rbrack |\\le \\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack }\\cdot \\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack } with equality iff Y=\\text{Const}. \\cdot X \\text{a.s.}\nProof:\n\\begin{align*}{}\n0 &\\le {\\mathit{\\mathbf{E}}\\left\\lbrack \\frac{X}{\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack }}\\pm \\frac{Y}{\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack }}\\right\\rbrack }^2 \\\\\n&=\\mathit{\\mathbf{E}}\\left\\lbrack \\frac{X^2 }{\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack }\\right\\rbrack +\\mathit{\\mathbf{E}}\\left\\lbrack \\frac{Y^2 }{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack }\\right\\rbrack \\pm 2\\mathit{\\mathbf{E}}\\left\\lbrack \\frac{\\mathrm{XY}}{\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack \\;}\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack }}\\right\\rbrack \\\\\n&=2\\pm 2\\mathit{\\mathbf{E}}\\left\\lbrack \\frac{\\mathrm{XY}}{\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack \\;}\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack }}\\right\\rbrack\n\\end{align*}\nFor - case:\n\\begin{align*}{}\n&&0 &\\le 2-2\\mathit{\\mathbf{E}}\\left\\lbrack \\frac{\\mathrm{XY}}{\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack \\;}\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack }}\\right\\rbrack \\\\\n&\\Rightarrow & -2 &\\le -2\\mathit{\\mathbf{E}}\\left\\lbrack \\frac{\\mathrm{XY}}{\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack \\;}\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack }}\\right\\rbrack \\\\\n&\\Rightarrow & -\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack \\;}\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack } & \\le -\\mathit{\\mathbf{E}}\\left\\lbrack \\mathrm{XY}\\right\\rbrack \\\\\n&\\Rightarrow & \\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack \\;}\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack }& \\ge \\mathit{\\mathbf{E}}\\left\\lbrack \\mathrm{XY}\\right\\rbrack \\\\\n&\\Rightarrow & \\mathit{\\mathbf{E}}\\left\\lbrack \\mathrm{XY}\\right\\rbrack &\\le \\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack \\;}\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack }\n\\end{align*}\nFor + case: \\begin{align*}{}\n&&0 &\\le 2+2\\mathit{\\mathbf{E}}\\left\\lbrack \\frac{\\mathrm{XY}}{\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack \\;}\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack }}\\right\\rbrack \\\\\n&\\Rightarrow & -2 &\\le +2\\mathit{\\mathbf{E}}\\left\\lbrack \\frac{\\mathrm{XY}}{\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack \\;}\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack }}\\right\\rbrack \\\\\n&\\Rightarrow & -\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack \\;}\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack } &\\le \\mathit{\\mathbf{E}}\\left\\lbrack \\mathrm{XY}\\right\\rbrack \\\\\n&\\Rightarrow & \\mathit{\\mathbf{E}}\\left\\lbrack \\mathrm{XY}\\right\\rbrack &\\ge -\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack \\;}\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack }\n\\end{align*}\nUsing Cauchy-Schwarz inequality we can get below important relations:\n\n\\mathit{\\mathbf{E}}\\left\\lbrack |\\mathrm{XY}|\\right\\rbrack \\le \\sqrt {\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack }\\cdot \\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack } Proof:\n\\begin{align*}{}\n\\mathit{\\mathbf{E}}\\left\\lbrack |\\mathrm{XY}|\\right\\rbrack &=\\mathit{\\mathbf{E}}\\left\\lbrack |X|\\cdot |Y|\\right\\rbrack \\\\\n&\\le \\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack {|X|}^2 \\right\\rbrack }\\cdot \\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack {|Y|}^2 \\right\\rbrack }\\\\\n&=\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack }\\cdot \\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack }\n\\end{align*}\n\\big|\\mathbf{Cov}\\left(X,Y\\right)\\big|\\le \\mathrm{SD}\\;X\\cdot \\mathrm{SD}\\;Y  Proof: \\begin{align*}{}\n\\big|\\mathbf{Cov}\\left(X,Y\\right)\\big|&=\\big|\\mathit{\\mathbf{E}}\\left\\lbrack \\left(X-\\mathit{\\mathbf{E}}\\left\\lbrack X\\right\\rbrack \\right)\\left(Y-\\mathit{\\mathbf{E}}\\left\\lbrack Y\\right\\rbrack \\right)\\right\\rbrack \\big|\\\\\n&\\le \\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(X-\\mathit{\\mathbf{E}}\\left\\lbrack X\\right\\rbrack \\right)}^2 \\right\\rbrack }\\cdot \\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(Y-\\mathit{\\mathbf{E}}\\left\\lbrack Y\\right\\rbrack \\right)}^2 \\right\\rbrack }\\\\\n&=\\mathrm{SD}\\;X\\cdot \\mathrm{SD}\\;Y\n\\end{align*}"
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-10-15-CS6660-week7.html#correlation",
    "href": "Data_Science_Notes/Mathematics/2022-10-15-CS6660-week7.html#correlation",
    "title": "Probability Theory 5",
    "section": "5 Correlation",
    "text": "5 Correlation\nThe correlation coefficient of random variables X and Y is \\rho \\left(X,Y\\right):=\\frac{\\mathbf{Cov}\\left(X,Y\\right)}{\\mathrm{SD}\\;X\\cdot \\mathrm{SD}\\;Y}\n\n-1 \\le \\rho(X,Y) \\le 1 Proof : \\begin{align*}{}\n&&\\big|\\mathbf{Cov}\\left(X,Y\\right)\\big| &\\le \\mathrm{SD}\\;X\\cdot \\mathrm{SD}\\;Y\\\\\n\\Rightarrow &&\\frac{\\big|\\mathbf{Cov}\\left(X,Y\\right)\\big|}{\\mathrm{SD}\\;X\\cdot \\mathrm{SD}\\;Y}&\\le 1\\\\\n\\Rightarrow &&-1\\le \\frac{\\mathbf{Cov}\\left(X,Y\\right)}{\\mathrm{SD}\\;X\\cdot \\mathrm{SD}\\;Y}&\\le \\\\\n\\Rightarrow &&-1 \\le \\rho(X,Y) &\\le 1\n\\end{align*} and the “equality iff” part of Cauchy-Schwarz implies that we have equality iff Y = aX, that is, Y = aX +b for some fixed a, b."
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-10-15-CS6660-week7.html#conditional-expectation",
    "href": "Data_Science_Notes/Mathematics/2022-10-15-CS6660-week7.html#conditional-expectation",
    "title": "Probability Theory 5",
    "section": "6 Conditional expectation",
    "text": "6 Conditional expectation\nThe conditional expectation of X, given Y = y_j is \\mathbf{E}(X \\mid Y=y_j):=\\sum_i X_i \\times \\underbrace{ p_{X\\mid Y}(x_i\\mid y_j)}_{\\text{conditional mass function}}\n\nExample : Let X and Y be independent \\text{Poi}(\\lambda) and \\text{Poi}(\\mu) variables, and Z = X + Y. Find the conditional expectation \\mathbf {E}(X \\mid Z = k). Conditional mass function for 0\\le i\\le k is :  p_{X\\mid Z} \\left(i\\mid k\\right)=\\frac{p\\left(i,k\\right)}{p_Z \\left(k\\right)} where p(i, k) is the joint mass function of X and Z at (i, k). Now, \\begin{align*}{}\np\\left(i,k\\right)&=P\\left\\lbrace X=i,Z=k\\right\\rbrace \\\\\n&=P\\left\\lbrace X=i,X+Y=k\\right\\rbrace \\\\\n&=P\\left\\lbrace X=i,Y=k-i\\right\\rbrace \\\\\n&=e^{-\\lambda } \\frac{\\lambda^i }{i!}\\cdot e^{-\\mu } \\frac{\\mu^{\\left(k-i\\right)} }{\\left(k-i\\right)!}\n\\end{align*} We know that Z=X+Y\\sim \\mathrm{Poi}\\left(\\lambda +\\mu \\right) so, p_Z \\left(k\\right)=e^{-\\left(\\lambda +\\mu \\right)} \\frac{{\\left(\\lambda +\\mu \\right)}^k }{k!} Now using these values we can compute conditional mass function \\begin{align*}{}\np_{X|Z} \\left(i|k\\right)&=\\frac{p\\left(i,k\\right)}{p_Z \\left(k\\right)}\\\\\n&=e^{-\\lambda } \\frac{\\lambda^i }{i!}\\cdot e^{-\\mu } \\frac{\\mu^{\\left(k-i\\right)} }{\\left(k-i\\right)!}\\times \\frac{1}{e^{-\\left(\\lambda +\\mu \\right)} \\frac{{\\left(\\lambda +\\mu \\right)}^k }{k!}}\\\\\n&=e^{-\\lambda } \\frac{\\lambda^i }{i!}\\cdot e^{-\\mu } \\frac{\\mu^{\\left(k-i\\right)} }{\\left(k-i\\right)!}\\times \\frac{k!}{e^{-\\left(\\lambda +\\mu \\right)} {\\left(\\lambda +\\mu \\right)}^k }\\\\\n&=\\frac{k!}{\\left(k-i\\right)!\\cdot i!}e^{-\\left(\\lambda +\\mu \\right)} \\times \\frac{{\\lambda^i \\mu }^{\\left(k-i\\right)} }{e^{-\\left(\\lambda +\\mu \\right)} {\\left(\\lambda +\\mu \\right)}^k }\\\\\n&={\\left({{k}\\atop{i}}\\right)}\\times \\frac{{\\lambda^i \\mu }^{\\left(k-i\\right)} }{{\\left(\\lambda +\\mu \\right)}^k }\\\\\n&={\\left({{k}\\atop{i}}\\right)}\\times \\frac{{\\lambda^i \\mu }^{\\left(k-i\\right)} }{{\\left(\\lambda +\\mu \\right)}^k }\\\\\n&={\\left({{k}\\atop{i}}\\right)}\\times \\frac{{\\lambda^i \\mu }^{\\left(k-i\\right)} }{{\\left(\\lambda +\\mu \\right)}^i {\\left(\\lambda +\\mu \\right)}^{k-i} }\\\\\n&={\\left({{k}\\atop{i}}\\right)}\\times {\\left(\\frac{\\lambda }{\\lambda +\\mu }\\right)}^i \\times {\\left(\\frac{\\mu }{\\lambda +\\mu }\\right)}^{k-1} \\\\\n&={\\left({{k}\\atop{i}}\\right)}\\times p^i \\times \\left(1-p\\right)^{k-i}\n\\end{align*} where p=\\left(\\frac{\\lambda }{\\lambda +\\mu }\\right) We conclude (X \\mid Z = k) \\sim \\text{Binom}(k, p), Therefore \\mathbf {E}(X \\mid Z = k)=kp=k\\cdot \\frac{\\lambda }{\\lambda +\\mu } We abbreviate above expression as \\mathbf {E}(X \\mid Z )=Z\\cdot \\frac{\\lambda }{\\lambda +\\mu } Notice \\mathbf {E}(X \\mid Z ) dependts only on Z not on X"
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-10-15-CS6660-week7.html#tower-rule",
    "href": "Data_Science_Notes/Mathematics/2022-10-15-CS6660-week7.html#tower-rule",
    "title": "Probability Theory 5",
    "section": "7 Tower rule",
    "text": "7 Tower rule\n\\mathit{\\mathbf{E}}\\left\\lbrack \\mathit{\\mathbf{E}}\\left\\lbrack X\\mid Y\\right\\rbrack \\right\\rbrack =\\mathit{\\mathbf{E}}\\left\\lbrack X\\right\\rbrack Intution on stack overflow. Proof:\n\\begin{align*}{}\n\\mathit{\\mathbf{E}}\\left\\lbrack \\mathit{\\mathbf{E}}\\left\\lbrack X\\mid Y\\right\\rbrack \\right\\rbrack &=\\sum_j E\\left\\lbrack X\\mid Y=y_j \\right\\rbrack {\\cdot p}_Y \\left(y_j \\right)\\\\\n&=\\sum_j \\left(\\sum_i x_i \\cdot p_{x\\mid y} \\left(x_i ,y_j \\right)\\right){\\cdot p}_Y \\left(y_j \\right)\\\\\n&=\\sum_j \\sum_i x\\cdot p_{x\\mid y} \\left(x_i ,y_j \\right){\\cdot p}_Y \\left(y_j \\right)\\\\\n&=\\sum_j \\sum_i x_i p\\left(x_i ,y_j \\right)\\\\\n&=\\sum_i x_i \\sum_j p\\left(x_i ,y_j \\right)\\\\\n&=\\sum_i x_i p_X \\left(x_i \\right)\\\\\n&=\\mathit{\\mathbf{E}}\\left\\lbrack X\\right\\rbrack\n\\end{align*}"
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-10-15-CS6660-week7.html#todo",
    "href": "Data_Science_Notes/Mathematics/2022-10-15-CS6660-week7.html#todo",
    "title": "Probability Theory 5",
    "section": "8 TODO",
    "text": "8 TODO\nBelow topics are skipped, complete it later:\n\nConditional variance\nRandom sums\nMoment generating functions\nIndependent sums"
  },
  {
    "objectID": "Data_Science_Notes/Mathematics/2022-10-15-CS6660-week7.html#concentration-bounds",
    "href": "Data_Science_Notes/Mathematics/2022-10-15-CS6660-week7.html#concentration-bounds",
    "title": "Probability Theory 5",
    "section": "9 Concentration Bounds",
    "text": "9 Concentration Bounds\n\n9.1 Markov’s inequality\nLet X be a non-negative random variable. Then for all a > 0 reals, \\mathit{\\mathbf{P}}\\left\\lbrace X\\ge a\\right\\rbrace \\le \\frac{\\mathit{\\mathbf{E}}\\left\\lbrack X\\right\\rbrack }{a} Of course this inequality is useless for $a ≤ X$. Proof:  Let indicator random variable I be defined as I = \\begin{cases}\n   1 &\\text{if } X\\ge a \\\\\n   0 &\\text{if } X<a\n\\end{cases} Then \\begin{align*}{}\n\\mathit{\\mathbf{E}}\\left\\lbrack I\\right\\rbrack &=1\\cdot \\mathit{\\mathbf{P}}\\left(X\\ge a\\right)+0\\cdot P\\left(X<a\\right)\\\\\n&=\\mathit{\\mathbf{P}}\\left(X\\ge a\\right)\n\\end{align*} Also if we look at Indicator random variable we can say that I\\le \\frac{X}{a} Because if X\\ge a then \\frac{X}{a}\\ge 1, when 0\\le X<a then 0\\le\\frac{X}{a}<1\nHence\n\\begin{align*}{}\n\\mathit{\\mathbf{E}}\\left\\lbrack \\mathrm{I}\\right\\rbrack &\\le \\frac{\\mathit{\\mathbf{E}}\\left\\lbrack X\\right\\rbrack }{a}\\\\\n\\Rightarrow P\\left\\lbrace X\\ge a\\right\\rbrace &\\le \\frac{\\mathit{\\mathbf{E}}\\left\\lbrack X\\right\\rbrack }{a}\n\\end{align*}\n\nExample  A coin is flipped n times what is the probability of getting 90\\% heads. Using Markov’s inequality P\\left\\lbrace X\\ge 0\\ldotp 9\\right\\rbrace \\le \\frac{\\mathit{\\mathbf{E}}\\left\\lbrack X\\right\\rbrack }{a}=\\frac{\\frac{n}{2}}{0\\ldotp 9n}=\\frac{5}{9}\n\n\n\n9.2 Chebyshev’s inequality\nLet X be a random variable with mean \\mu and variance \\sigma^2 both finite. Then for all b > 0 reals, \\mathit{\\mathbf{P}}\\left\\lbrace |X-\\mu |\\ge b\\right\\rbrace \\le \\frac{\\mathbf{Var}\\left(X\\right)}{b^2 } Of course this inequality is useless for b ≤ \\mathbf{SD}X. Proof: Apply Markov’s inequality on the random variable (X − \\mu)^2 \\ge 0\n\\begin{align*}{}\n\\mathit{\\mathbf{P}}\\left\\lbrace |X-\\mu |\\ge b\\right\\rbrace &=\\mathit{\\mathbf{P}}\\left\\lbrace {\\left(X-\\mu \\right)}^2 \\ge b^2 \\right\\rbrace \\\\\n&\\le \\frac{\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(X-\\mu \\right)}^2 \\right\\rbrack }{b^2 }=\\frac{\\mathbf{Var}\\left(X\\right)}{b^2 }\n\\end{align*}\n\nExample  A coin is flipped n times what is the probability of getting 90\\% heads. Chebyshev’s inequality Consider X_1,X_2,\\dots,X_n, Each X_i=1 if the i^{\\text{th}} toss is head and X_i=0 if i^{\\text{th}} toss is tail. \\mathit{\\mathbf{E}}\\left\\lbrack X_i \\right\\rbrack =\\frac{1}{2} \\mathit{\\mathbf{E}}\\left\\lbrack X_i^2 \\right\\rbrack =\\frac{1}{2} Now we find variance \\mathbf{Var}\\left(X_i \\right)=\\mathit{\\mathbf{E}}\\left\\lbrack X_i^2 \\right\\rbrack -{\\left(\\mathit{\\mathbf{E}}\\left\\lbrack X_i \\right\\rbrack \\right)}^2 =\\frac{1}{2}-\\frac{1}{4}=\\frac{1}{4} Now, \\begin{align*}{}\n|X| &\\ge 0\\ldotp 9\\\\\n\\Rightarrow |X-\\mu |&\\ge 0\\ldotp 9-\\mu \\\\\n\\Rightarrow |X-\\mu |&\\ge 0\\ldotp 9-0\\ldotp 5\\\\\n\\Rightarrow |X-\\mu |&\\ge 0\\ldotp 4\n\\end{align*} Now can use Chebyshev’s inequality\n\\begin{align*}{}\n\\mathit{\\mathbf{P}}\\left\\lbrace |X-\\mu |  \\ge b\\right\\rbrace &\\le \\frac{\\mathbf{Var}\\left(X\\right)}{b^2 }\\\\\n\\mathit{\\mathbf{P}}\\left\\lbrace |X-\\mu | \\ge 0\\ldotp 4\\right\\rbrace &\\le \\frac{\\frac{1}{4}n}{{\\left(0\\ldotp 4n\\right)}^2 }\\\\\n&=\\frac{\\frac{1}{4}n}{0\\ldotp 16n^2 }=\\frac{25}{16n}\n\\end{align*} So |X|\\ge0.9=\\frac{25}{16n} We can see that Chebyshev’s inequality provides better bound than Markov’s inequality and the bound becomes better as n increases.\n\n \\tiny {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Subruk, IIT Hyderabad }}}}"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week6.html",
    "href": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week6.html",
    "title": "Deep Learning 6",
    "section": "",
    "text": "Do you know the deeper the model better it performs.\nAdding layer improves the performance.\nIf we continue stacking deeper layers on a plane convolutional network.The training loss keeps decreasing.But the testing loss does not decrease as much.\nDeeper the network, gradients vanish quickly, thereby slowing the rate of change in initial layers."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week6.html#reidual-nets",
    "href": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week6.html#reidual-nets",
    "title": "Deep Learning 6",
    "section": "1 Reidual Nets",
    "text": "1 Reidual Nets\n\n\nThe deeper model should be able to perform at least as well as the shallower model.\nSolution is to use the network layer to fit a residual mapping instead of directly trying to fit a desired underlying nothing.\nuse skip connection. Such a block is called residual block.\nBypass some layers.\nStack residual blocks, every residual block has 2, 3 x 3 convolution layers.\nNo fully connected layer at the end.\nadditional convolutional layer at the beginning\nPeriodically double the filter.down sample reaching stride of two.\nAlex net had fully connected network at last.\nFor deeper network use bottleneck layer to improve efficiency.\nShort circuit path provides a path for the gradient to flow deeper.\nAble to train very deep neural network\ndeeper layer networks now achieve lower training error as expected."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week6.html#other-networks",
    "href": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week6.html#other-networks",
    "title": "Deep Learning 6",
    "section": "2 Other Networks",
    "text": "2 Other Networks\nNetwork in network.\n\nBetween every convolution, there has MLP in between.\nthese micro network within each convolutional layer helps to compute more abstract feature for local batches.\nmicron network uses multilayer perceptron fully connected That is 1 x 1 convolutional layer\nthis network was not so famous\n\nWide residual network.\n\nit argues that residuals are important factor not the depth.\nuse wide residual block of F x K filters instead of F filter in each layer.\n50 layer wide residual network outperforms 152 layer original residual network.\nIncreasing width at instead of depth is more computationally efficient because it helps in parallelization of the computation\n\nDeep Networks with Stochastic Depth\n\nIt reduces vanishing gradient and training time through short network during training.\nRandomly drop subset of layer during each training pass.\nIt’s like dropout, but drops out the residual block.\nIn every Iteration randomly drop one residual block.\n\nDens net.\n\nDense blocks where each layer is connected to every layer in feet forward fashion.\nSimply connect every layer with every layer. Let the neural network figure out.\nAlleviates Vanishing gradient, strengthens feature propagation,Encourages Feature Reuse.\n\n\nTop five means check if prediction is among the top five."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week6.html#understanding-and-visualizing-cnns",
    "href": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week6.html#understanding-and-visualizing-cnns",
    "title": "Deep Learning 6",
    "section": "3 Understanding and Visualizing CNNs",
    "text": "3 Understanding and Visualizing CNNs\nCNN Features are Generic\n\nCNN features are generic, so we can reuse the CNN feature for the small problem.\nWe can reuse the feature.That led to the notion of transfer learning.\nInitially train a CNN or any new network.On large database like Image net. Now use the network, for different problem.It is called transfer learning.\nDepending on the data, decide how many layer of the neural network we want to fine tune.\nThis works because features are generic in earlier network.\nExtend to more number of losses just by fine tuning.\nExtend to new task. Extend from object classification to scene classification.\nAlso can be used to extend to new datasets, if the data set is similar."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week6.html#understanding-convolutional-neural-network.",
    "href": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week6.html#understanding-convolutional-neural-network.",
    "title": "Deep Learning 6",
    "section": "4 Understanding convolutional neural network.",
    "text": "4 Understanding convolutional neural network.\n\nVisualize patches that maximally activate neurons.\nvisualize the weights.For example, by looking at the filter, we can say whether its edge detector.\nvisualize the representation space. using t-SNE,\nOcclusion experiment.\nDeconvolution based approaches.\nOptimization over image approaches.\nInitial layer of neural network Weight looks like gabor-like filters. \n\nOcclusion experiment.\n\nHere we black out some of the area of the image and study the effect of this, on the output. \n\nDeConv approach.\n\nfeed image to the network.\nPick a layer set the gradient there to be all zero, except for one some neuron of interest.\nBack propagate to the image. \nIn guided bank propagation, we guide the gradient.So that negative gradient doesn’t flow.An image looks little better. \nIn case of guided back propagation, we first zero out all those activation, which was negative just before rectified linear unit in case of forward pass.\nThen we zero out all those gradients which are negative while doing backward pass.\nthis is explained in below picture:."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week6.html#optimization-to-image",
    "href": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week6.html#optimization-to-image",
    "title": "Deep Learning 6",
    "section": "5 optimization to image",
    "text": "5 optimization to image\n\ncan we find an image that maximizes some class score  \nScore of a class c before softmax \\arg \\max_I S_c(I)-\\lambda \\vert| I \\vert | ^2_2\n\nI start with black image are you using gradient descent on that image we maximize the score on output neuron\n\nfeed in zero\nSet the gradient of all vectors zero except the one which we are interested in and perform the back propagation.\ndo the small input update\ngo back to step 2\n\n\n\nvisualize the data gradient  \nnow we can use grab cut using the same approach for segmentation. \nwe can use this approach even for the object detection.\nwe can minimize this code: \\mathbf{x}^{*}=\\operatorname*{argmin}_{\\mathbf{x}\\in\\mathbb{R}^{H}\\times W\\times C}\\ell(\\Phi(\\mathbf{x}),\\Phi_{0})+\\lambda\\mathcal{R}(\\mathbf{x}) where \\ell(\\Phi({\\bf x}),\\Phi_{0})=\\|\\Phi({\\bf x})-\\Phi_{0}\\|^{2}"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week6.html#class-activation-mapping-cam",
    "href": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week6.html#class-activation-mapping-cam",
    "title": "Deep Learning 6",
    "section": "6 Class Activation Mapping (CAM)",
    "text": "6 Class Activation Mapping (CAM)\n\nin this method we train a linear regression model for each and every class at the output layer\nwe take the global average pooling from the layer which comes before the classification layer.\nnow from this average pooling we construct a linear regression for each class.\nonce we obtain the weight of each class we multiply the weight of each linear regression with the feature which we used for global leverage pooling.\nthen we project this image what we obtain after multiplication to the original image to obtain class activation map."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week6.html#grad-cam-and-guided-grad-cam",
    "href": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week6.html#grad-cam-and-guided-grad-cam",
    "title": "Deep Learning 6",
    "section": "7 Grad CAM (and Guided Grad CAM)",
    "text": "7 Grad CAM (and Guided Grad CAM)\n\nthe class activation map requires to train a linear regression model can we avoid this.\nthis issue is addressed by grad CAM.\nwe use the weight which is already there in the network and we do the global average pulling of rectified convolutional layer, we multiply this with the weight of the fully connected layer and we do the same process as we did in class activation map to obtain the Heat map."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week6.html#grad-cam-for-image-captioning",
    "href": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week6.html#grad-cam-for-image-captioning",
    "title": "Deep Learning 6",
    "section": "8 Grad CAM for Image Captioning",
    "text": "8 Grad CAM for Image Captioning\n\nOther methods\n\nGrad CAM++\nIntegrated Gradients\nSmoothGrad\nLIME\nLRP\nDeepSHAP\nDeepLIFT\n\n \\tiny {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-03-04-CS5480-week5.html",
    "href": "Data_Science_Notes/Deep-Learning/2023-03-04-CS5480-week5.html",
    "title": "Deep Learning 5",
    "section": "",
    "text": "What is the problem in using feed-forward networks for image data?\n\nWe do rasterization of image, so we lose special context in case of feed-forward image.\nWhen we flatten the image in case of feed forward network, we will have humongous number of inputs data, so we will have much more parameter.\n\nConvolutional Neural Networks\n\nMust deal with very high dimensional inputs\n\n150 x 150 pixels = 22500 inputs, or 3 x 22500 if RGB pixels\n\nNeed to look at the 2D topology of pixels (or 3D for video data)\nNeed invariance to certain variations we can expect\n\nTranslations, illumination, etc.\n\n\nConvolutional Neural Networks\n\nObject can appear either in the left image or in the right image\nOutput indicates presence of object regardless of position\nWhat do we know about the weights?\nEach possible location the object can appear in has its own set of hidden units  \nEach set detects the same features except in a different location.\nLocations can overlap\n\nOriginally\n\nLocal Connectivity\nParameter Sharing\nPooling\n\nMore recently\n\nNormalization\nLast layer customization\nLoss functions\nIf the cat is there always only to left at the time of training, and cat is it the time of testing at down it will not work,\nso what we can do that we can divide the network in small patches, and we can have different set of weights\nBut we also want the weight to be shared so can we have just one kernal for all the pathches\nwe want to give focus on high activation on one local area so we can use pooling\nusing all these we get the notion of convolution network\n\nLocal Connectivity\n\nEach hidden unit is connected only to a subregion (patch) of the input image\nIt is connected to all channels -1 if greyscale image\n\n3 (R, G, B) for color image\n\nSolves the following problems:\n\nFully connected hidden layer would have an unmanageable number of parameters\nComputing the linear activations of the hidden units would be very expensive\n\nUnits are connected to all channels:\n\n1 channel if grayscale image, 3 channels (R, G, B) if color image  \nUnits organized into the same ‘‘feature map’’ share parameters\nHidden units within a feature map cover different positions in the image \n\n\nParameter Sharing\n\nUnits organized into the same ‘‘feature map’’ share parameters\nHidden units within a feature map cover different positions in the image\nReduces the number of parameters further\nWill extract the same features at every position\nfeatures are equivariant or invariant\n\nwe want invariance here, invariant\nequivalence means if we change the input buy 10 output will change by 10, but in our chase we want even if position change same output\nLocal Connectivity and Parameter Sharing\n\nFully connected layer\n\n\nHidden Units: 120,000\nParams : 14.4 billion\n200x200x3\n\nLocally connected layer\n\n\nHidden Units: 120,000\nParams : 3.2 Million\n\nThe convolution of an image X with a kernel k is computed as: (x*k)_{ij}=\\,\\sum_{p q}\\,x_{i+p,j+q}\\,\\,\\,k_{r-p,r-q}\n\n\nConvolutional layer with single feature map.\n\nSharing parameters\nExploiting the stationarity property and preserves locality of pixel dependencies\n\nConvolutional Layer (with Stride)\n\n\nImage size: W_1\\times H_1\\times D_1\nReceptive field size: F\\times F\n#Feature maps: K\nIt is also better to do zero padding to preserve input size spatially.\nW_2=(W_1-F)/S+1\nH_2=(H_1-F)/S+1\nD_2=K\nstride reduces the computation\naggregating could help in getting better solution\n\nConvolutional Layer\n\ny_{j}^{n}\\,=\\,f(\\sum_{k=1}^{F}\\,x_{k}^{n-1}\\,*\\,w_{k j}^{n})\n\nHere f is a non linear activation function.\nF = no. of feature maps\nn= layer index\n* represents element by element multiplication\n\nA typical deep convolutional network\n\nOther layers\n\nPooling ( Next key idea)\nNormalization\nFully connected\n\nPooling Layer\n\nRole of an aggregator.\nInvariance to image transformation and increases compactness to representation.\nPooling types: Max, Average, L2 etc.\nEvery patch is non-overlapping.\nTake max value from a non-overlapping patch\npooling is done on each channel individually\nIt serves the purpose of aggregator\nIt also servers the purpose of little bit of local in-variance.\nBecause we are taking best in a local patch.\nwe can do max agv etc, but max works the best.\nThere are no weight on the pooling layer, nothing to learn\nThe subsequent layer gets even lesser work to do due to max pool\n\nNormalization Layer\n\nLocal contrast normalization (Jarrett\n\nImproves invariances\nImproves sparsity\n\nLocal response normalization\n\nKind of “lateral inhibition” and performed across the channels\n\nBatch normalization\n\nActivation of the mini batch is centered to zero mean and unit variance to prevent internal covariate shifts.\n\n\nInitially CNN had no normalization layer\n\nNow we have batch norm\nwe can control the varience using batch norm\nthere are other norm\nlocal contrast norm, normalize the contrast in a patch, locally for every patch\nlocal response normalization\n\ntake one top left pixel across all the feature map, ie across channel and normalize it, kind of lateral inhibition and performed across the channels\n\n\ntwo successive 3x3 filter is same as 5x5 filter\nFully Connected Layer\n\nMulti layer perceptron\nRole of a classifier\nGenerally used in final layers to classify the object represented in terms of discriminative parts and higher semantic entities.\nSoftMax\n\nNormalizes the output. z_{n}=\\frac{e^{x_{n}}}{\\sum_{i=1}^{K}e^{x_{i}}}\n\n\nCase Study: Alex Net\n\nWinner of ImageNet LSVRC 2012.\nTrained over 1.2M images using SGD with regularization.\nDeep architecture (60M parameters)\nOptimized GPU implementation (cuda convnet)\ninput to Alex net is fromm image net, image size 224 x 224, filter size is 11x11x3 and stride of 4\nimage itself was padded with 3 pix to make 227 x 227 (227-11)/4+1= 55\n\nAlexNet Architecture\n\n8 Layers in total (5 convolutional layers, 3 fully connected layers)\nTrained on ImageNet Dataset\nResponse normalization layers follow the first and second convolutional layers.\nMax pooling follow first, second and the fifth convolutional layers.\nThe ReLU non linearity is applied to the output of every layer\n\n\nParameter Calculation\n\nW_2=[\\ (\\left.W_1-F+2P\\right)/S]+1 \\text{ and } D_2=K\nS = 4, W_1 = 227, F =11, P = 0, D_2 = 96\nW_2 = (227 - 11 )/4 + 1 = 55\nOutput Size: 55 \\times 55 \\times 96\nConvolutional layers cumulatively contain about 90 95% of computation, only about 5% of the parameters\nFully connected layers contain about 95% of parameters. \nTrained with stochastic gradient descent\n\non two NVIDIA GTX 580 3GB GPUs, for about a week\n\n650,000 neurons, 60 M parameters, 630 M connections, Final feature layer: 4096 - dimensional\nLearning: Minimizing the loss function (incl. regularization) w.r.t . parameters of the network \\theta^{\\ast}=a r g m i n_{\\theta}\\sum_{n=1}^{N}L(x^{n},y^{n},\\theta)"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-03-04-CS5480-week5.html#backpropagation-in-cnns",
    "href": "Data_Science_Notes/Deep-Learning/2023-03-04-CS5480-week5.html#backpropagation-in-cnns",
    "title": "Deep Learning 5",
    "section": "2 Backpropagation in CNNs",
    "text": "2 Backpropagation in CNNs\nAs we know that we have modular designs, so we will focus on a backward pass on just one module. Let the points on X, w and y be X(r,c), w(a,b) and y(r,c). Consider the below diagram.\n\nX\\left\\lbrack r+a,c+b\\right\\rbrack Can be considered as a cropped part of the image where filter was applied in a particular stride. So now we can write the below equation.\ny\\left\\lbrack r,c\\right\\rbrack =\\sum_{a=0}^{k_1 -1} \\sum_{b=0}^{k_2 -1} X\\left\\lbrack r+a,c+b\\right\\rbrack w\\left\\lbrack a,b\\right\\rbrack\nThis equation denotes the convolution of the cropped part of the image.\nNow we want to calculate partial derivative of L with respect to W at the point a', b'. Since the filter w[a',b'] effects every parts of y so we need to take a summation over y. Since y has a dimension of r\\times c. So we can write the equation as shown below. \\frac{\\partial L}{\\partial w\\left\\lbrack a^{\\prime } ,b^{\\prime } \\right\\rbrack }=\\sum_{r=0}^{N_1 -1} \\sum_{c=0}^{N_2 -1} \\frac{\\partial L}{\\partial y\\left\\lbrack r,c\\right\\rbrack }\\frac{\\partial y\\left\\lbrack r,c\\right\\rbrack }{\\partial w\\left\\lbrack a^{\\prime } ,b^{\\prime } \\right\\rbrack }\nThe quantity \\frac{\\partial L}{\\partial y\\left\\lbrack r,c\\right\\rbrack } is known as it is incoming gradient from the upper layer in general, so we need to calculate only \\frac{\\partial y\\left\\lbrack r,c\\right\\rbrack }{\\partial w\\left\\lbrack a^{\\prime } ,b^{\\prime } \\right\\rbrack }, This equation denotes the partial derivative of y with respect to a particular point of w,Which depends on only the same particular points in corresponding to X. So we can write. \\frac{\\partial y\\left\\lbrack r,c\\right\\rbrack }{\\partial w\\left\\lbrack a^{\\prime } ,b^{\\prime } \\right\\rbrack }=X\\left\\lbrack r+a^{\\prime } ,c+b^{\\prime } \\right\\rbrack So finally we can write as below. \\frac{\\partial L}{\\partial w\\left\\lbrack a^{\\prime } ,b^{\\prime } \\right\\rbrack }=\\sum_{r=0}^{N_1 -1} \\sum_{c=0}^{N_2 -1} \\frac{\\partial L}{\\partial y\\left\\lbrack r,c\\right\\rbrack }X\\left\\lbrack r+a^{\\prime } ,c+b^{\\prime } \\right\\rbrack\nNow we are going to find the gradient with respect to X.Here we need to understand that a particular point in X affects an area (patch) in y Let’s call this patch as P_Y. \\frac{\\partial L}{\\partial X\\left\\lbrack r^{\\prime } ,c^{\\prime } \\right\\rbrack }=\\sum_{P_y }^{\\;} \\frac{\\partial L}{\\partial y_{P_y } }\\frac{\\partial y_{P_y } }{\\partial X\\left\\lbrack r^{\\prime } ,c^{\\prime } \\right\\rbrack }\nWhich ca be written as :\n\\frac{\\partial L}{\\partial X\\left\\lbrack r^{\\prime } ,c^{\\prime } \\right\\rbrack }=\\sum_{a=0}^{k_1 -1} \\sum_{b=0}^{k_2 -1} \\frac{\\partial L}{\\partial y\\left\\lbrack r^{\\prime } -a,c^{\\prime } -b\\right\\rbrack }\\frac{\\partial y\\left\\lbrack r^{\\prime } -a,c^{\\prime } -b\\right\\rbrack }{\\partial X\\left\\lbrack r^{\\prime } ,c^{\\prime } \\right\\rbrack }\nsame as earlier we already know \\frac{\\partial L}{\\partial y\\left\\lbrack r^{\\prime } -a,c^{\\prime } -b\\right\\rbrack } we only need to compute \\frac{\\partial y\\left\\lbrack r^{\\prime } -a,c^{\\prime } -b\\right\\rbrack }{\\partial X\\left\\lbrack r^{\\prime } ,c^{\\prime } \\right\\rbrack }\nTo find this now we first look at the conv operation itself:\ny\\left\\lbrack r^{\\prime } ,c^{\\prime } \\right\\rbrack =\\sum_{a^{\\prime } =0}^{k_1 -1} \\sum_{b^{\\prime } =0}^{k_2 -1} X\\left\\lbrack r^{\\prime } +a^{\\prime } ,c^{\\prime } +b^{\\prime } \\right\\rbrack w\\left\\lbrack a^{\\prime } ,b^{\\prime } \\right\\rbrack\nHence\ny\\left\\lbrack r^{\\prime } -a,c^{\\prime } -b\\right\\rbrack =\\sum_{a^{\\prime } =0}^{k_1 -1} \\sum_{b^{\\prime } =0}^{k_2 -1} X\\left\\lbrack r^{\\prime } -a+a^{\\prime } ,c^{\\prime } -b+b^{\\prime } \\right\\rbrack w\\left\\lbrack a^{\\prime } ,b^{\\prime } \\right\\rbrack\nIn the above equation which w correspond to \\partial X\\left\\lbrack r^{\\prime } ,c^{\\prime } \\right\\rbrack, the answer is w[a,b] because in the above equation when a^\\prime=a and b^\\prime=b it becomes X\\left\\lbrack r^{\\prime } ,c^{\\prime } \\right\\rbrack In short we just want to know the coefficient of X\\left\\lbrack r^{\\prime } ,c^{\\prime } \\right\\rbrack which is w\\left\\lbrack a,b\\right\\rbrack\nHence\n\\frac{\\partial y\\left\\lbrack r^{\\prime } -a,c^{\\prime } -b\\right\\rbrack }{\\partial X\\left\\lbrack r^{\\prime } ,c^{\\prime } \\right\\rbrack }=w\\left\\lbrack a,b\\right\\rbrack\nFinally we get\n\\frac{\\partial L}{\\partial X\\left\\lbrack r^{\\prime } ,c^{\\prime } \\right\\rbrack }=\\sum_{a=0}^{k_1 -1} \\sum_{b=0}^{k_2 -1} \\frac{\\partial L}{\\partial y\\left\\lbrack r^{\\prime } -a,c^{\\prime } -b\\right\\rbrack }w\\left\\lbrack a,b\\right\\rbrack\nPooling Layer:\nConsider Max pooling.Now consider 2 cross 2 grid, It has total 4 value.Among all these 4 value only one value goes in forward direction.It means that only one has contributed.So the, one who contributes gets rewarded.It means that while doing back propagation, only one will receive the gradient. Other 3 will not receive any gradient.\nx = \\begin{cases}\n    {\\frac{\\sum_{k=1}x_{k}}{m}},\\,{\\frac{\\partial g}{\\partial x}}={\\frac{1}{m}} &\\text{mean pooling }  \\\\\n    {\\mathrm{max}}(x),{\\frac{\\partial g}{\\partial x_i}} = \\begin{cases}\n        1 &\\text{if } x_i=\\max(x) \\\\\n        c &\\text{if } 0, \\text{otherwise}\n    \\end{cases} &\\text{Max pooling } \\\\\n    \\|x\\|_{p}=\\left(\\sum_{k=1}|x_{k}|^{p}\\right)^{1/p}\\ \\ ,{\\frac{\\partial g}{\\partial x_i}}=\\left(\\sum_{k=1}|x_{k}|^{p}\\right)^{1/p-1}|x_{i}|^{p-1}& L^P \\text{ pooling } \\\\\n\\end{cases}\nIn max pooling, the unit that contributed (won) gets all the gradient."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-03-04-CS5480-week5.html#architectures-of-cnns-over-time",
    "href": "Data_Science_Notes/Deep-Learning/2023-03-04-CS5480-week5.html#architectures-of-cnns-over-time",
    "title": "Deep Learning 5",
    "section": "3 Architectures of CNNs over time",
    "text": "3 Architectures of CNNs over time\nLeNet\n\n\nConv filters were 5 cross 5 and were applied with stride one.\nPooling layer where 2 cross 2 and were applied with with stride two.\nShow the architecture was Conv Pool Conv Pool FC FC.\n\nZFNet\n\n\n\nCONV1 : Change form (11 x 11 stride 4) to (7 x 7 stride 2)\nCONV3,4,5: instead of 384, 384, 256 filters use 512, 1024, 512\nBy doing this they brought ImageNet top 5 error from 16.4% to 11.7%\n\nVGG Net\n\n\nMore layers lead to more nonlinearities\nOnly 3 x 3 CONV stride 1, pad 1 and 2 x 2 MAXPOOL stride 2\nSmaller receptive fields:\n\nless parameters; faster\ntwo 3 X 3 leads to 5 X 5; three 3 x 3 leads to 7 x 7, so we can have only 3 x 3 receptive field and should be enough\nFewer parameters\n\nThree 3 ^2C^2 (VS) 7 ^2C^2\n\n\nThere was a performance improvement from VGG 11 to 13 and also from 13 to 16. But there was no significant performance improvement from VGG 16 to 19. In fact, the error increased. This can be attributed to vanishing gradient problem.\n\n\nGoogLeNet\nThis was a deeper network with more computational efficiency.\n\nIt has total 22 layers.\nIt has efficient inception modules.\nIt doesn’t have any fully connected layers.\nIt has only 5,000,000 parameters, which is 12 times lesser than the alexnet.\nILSVRC’s 14 classification winner with 6.7% top error 5 error.\n\nWhat is an inception module?\n\nThe idea here is to design a good local network topology and then stack these modules on top of each other.\nApply combination of many parallel kernel operations like 1 cross 1 convolution 3 cross 3.convolution operations.\nCan also apply the polling operation in parallel.\nInstead of choosing just one kernel size, we can choose many kernel size and use all together in parallel.\nBut now there are 2 problems. The first problem is the computational complexity and the second problem is the size of the output. So to address this issue we use one cross one convolution before others convolutions.\nThe notion of one cross one convolution is also called as the bottleneck layer.  \nThis network doesn’t use any fully connected layer at the output.\nBefore stacking all the inception module it usages Stem network.\nIt also uses auxiliary classification output to inject additional gradients at lower layer.It helps improving the gradient to flow at the lower layers. \nIt has 22 total layer with weights including each parallel layer in the inception module.\n\n \\tiny {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html",
    "href": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html",
    "title": "Deep Learning 2",
    "section": "",
    "text": "set \\Delta W^{(l)}:=0,\\Delta b^{(l)}:=0 (matrix/vector of zeros) for all l\nFor i = 1 to m\n\nUse backpropagation to compute \\nabla_{\\theta^{(l)}}J(\\theta;X,y)\nSet \\Delta\\theta^{(l)}:=\\Delta\\theta^{(l)}+\\nabla_{\\theta^{(l)}}J(\\theta;X,y)\n\nUpdate the parameters: \n\\begin{align*}\nW^{(l)}&=W^{(l)}-\\alpha\\left[\\left(\\frac{1}{m}\\Delta W^{(l)}\\right)+\\lambda W^{(l)}\\right]\\\\\nb^{(l)}&=b^{(l)}-\\alpha\\left[\\frac{1}{m}\\Delta b^{(l)}\\right]\n\\end{align*}\n\nRepeat for all data points until convergence.\n\n\nIteration: When weight is updated then it is called one iteration.\nEpoch : When training is done at least once on the whole data set then it is called one epoch.\nConvex objective function has One global minima but Non convex objective function has many local minima.\n\n\n\n\n\n\nNote\n\n\n\nThe neural network with more than one layer is almost always non convex, even if there is no activation function on any layer\n\nConsider a neural network with some hidden layer producing some output.Now if we permute the neurons of the hidden layer the output will still remain the same.It means for many possible coordinates there are only one output. Now if we plot hidden layer coordinates with respect to the output, we can see that the graph will not look like a linear function."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#local-minima",
    "href": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#local-minima",
    "title": "Deep Learning 2",
    "section": "2 Local Minima",
    "text": "2 Local Minima\n\nThe non-identifiability problem results in multiple equivalent local minima → Not problematic though, since cost function value is the same\nIf there are local minima with significant different cost value then that could be a problem.\nIt’s not required to find the true global minimum, recent work shows that even finding the parameter space which has low but not global minimum cost is enough.\n\n\n\n\n\n\nNote\n\n\n\nPlot the norm of the gradient over time. If the norm is very small, it is likely to be a local minimum (or a critical point)."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#saddle-points-and-plateaus",
    "href": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#saddle-points-and-plateaus",
    "title": "Deep Learning 2",
    "section": "3 Saddle Points and Plateaus",
    "text": "3 Saddle Points and Plateaus\n\nIf there is local minima across one cross section of cost function and local maxima across another cross section of the cost function then it is called a saddle point. \nIn higher dimension space there are more saddle points than local minima.\nIf there is a steep decline in the cost then it is called Cliff area. Such types of clips are very much possible in RNN because in such networks there are a lot of multiplication (over time) and when small terms multiplies many times then it becomes even smaller which causes cliffs."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#vanishingexploding-gradient",
    "href": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#vanishingexploding-gradient",
    "title": "Deep Learning 2",
    "section": "4 Vanishing/Exploding Gradient",
    "text": "4 Vanishing/Exploding Gradient\n\nWhile doing gradient descent if the smaller number multiplies many times it becomes even smaller and it causes vanishing gradient problem in very deep neural network.\nIf the activation function has more steep slope or it has narrow range then it can cause vanishing gradient problem, One such example is sigmoid activation function.\nIn the same way there may happen exploding gradient issue if the number is greater than one while doing gradient descent."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#challenges",
    "href": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#challenges",
    "title": "Deep Learning 2",
    "section": "5 Challenges",
    "text": "5 Challenges\n\nCost functions are often high dimensional non quadratic non convex,There can be many minima shadow points and also many flat regions,\nThere is no guarantee that a network will converge to an optimal point or it will converge at all.\nMost of the time problems are ill conditioned,\nThe gradient can be inexact.\nPoor correspondence between local and global structure.\nWe need to hyper tune learning rate and many other parameters."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#how-to-address",
    "href": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#how-to-address",
    "title": "Deep Learning 2",
    "section": "6 How to address",
    "text": "6 How to address\nThere are many techniques which can help address this issue up to some extent.\n\nAlgorithmic approach.\n\nBatch gradient descent, stochastic gradient descent, mini batch gradient descent.\nMomentum based methods Nestrov momentum.\nAdagrad, Adadelta, RMSprobe, Adam.\nOther advanced optimization methods.\n\nPractical tricks.\n\nRegularization method such as dropout.\nData manipulation methods.\nParameter in slicing method.\n\nBatch gradient descent: In this method we compute the gradient for entire training set and then we update the parameters or weights.\nStochastic gradient descent: In this method we first randomly shuffle the training set and then we compute the gradient for each and every training example and update the parameter or weights.\nMini batch Stochastic gradient descent: In this method we first draw a mini batch from randomly shuffled data set. And we compute the gradient for this mini batch and update the parameter or weights.\nAdvantage of Stochastic gradient descent:\n\nIt is faster than batch gradient descent because there is redundancy in the batch.\nOften results in more better than generalised solution because of the noisy update of the weight.\nThis method can be useful where we need to track the system overtime. We can update the weight stochastically.\n\nIssue with Stochastic gradient descent:\n\nSometimes noise in stochastic gradient descent may lead to no convergence at all.\nEquivalent to use of “mini-batches” in SGD (Start with a small batch size and increase size as training proceeds.\nCan be controlled using learning rate.\n\nAdvantages of Batch GD\n\nConditions of convergence are well understood.\nMany acceleration techniques (e.g. conjugate gradient) only operate in batch learning.\nTheoretical analysis of the weight dynamics and convergence rates are simpler.\n\n\n\n\n\n\nTip\n\n\n\nMini-batch SGD is the most commonly used method, with a mini-batch size of 20 or so (can be higher depending on dataset size)."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#momentum",
    "href": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#momentum",
    "title": "Deep Learning 2",
    "section": "7 Momentum",
    "text": "7 Momentum\nWeight update is given by  \\Delta\\theta_{t+1}=\\alpha\\nabla_{\\theta}J(\\theta_{t};x^{(i)},y)^{(i)}+ \\overbrace{\\boxed{\\gamma \\Delta \\theta _t}}^{\\text{Momentum term}}   \n\nCan increase speed when the cost surface is highly non-spherical.\nDamps step sizes along directions of high curvature, yielding a larger effective learning rate along the directions of low curvature.\n\n\n\n\n\n\nNote\n\n\n\nHere the idea is that, if we are going in one direction, we build a momentum to that direction, so if there is a sudden change in the direction, we do not suddenly change the direction but go for a average as per the above equation.\n\n\nLarger the \\gamma, more the previous gradients affect the current step.\nGenerally \\gamma is set to 0.5 until initial learning stabilizes and then increased to 0.9 or higher."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#sgd-with-momentum",
    "href": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#sgd-with-momentum",
    "title": "Deep Learning 2",
    "section": "8 SGD with Momentum",
    "text": "8 SGD with Momentum\nAssume: \n\nLearning rate \\alpha  momentum parameter \\gamma  minibatch size m  Initial weights \\theta_t\n\n\n\nwhile stopping criterion not met do\nSample a minibatch of m examples from the training set.\nCompute gradient estimate {\\nabla}_{\\theta}{\\sum}_{i=1}^{m}J({\\theta}_{t};X^{(i)},y^{(i)})\nCompute update \\Delta\\theta_{t+1}=\\alpha\\nabla_{\\theta}J+\\gamma\\Delta\\theta_{t}\nApply update \\theta_{t+1}=\\theta_{t}-\\Delta\\theta_{t+1}\nEnd while\n\n\nAlternate view of momentum update\n\\overbrace{\\mathrm{v}_{t+1}}^{\\text{Velocity }} =\\gamma \\times \\overbrace{\\mathrm{v}_{t}}^{\\text{past velocity}} +\\alpha\\nabla_{\\theta}J(\\theta_{t};X^{(i)},y^{(i)})"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#nesterov-accelerated-momentum",
    "href": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#nesterov-accelerated-momentum",
    "title": "Deep Learning 2",
    "section": "9 Nesterov Accelerated Momentum",
    "text": "9 Nesterov Accelerated Momentum\n\nIn this method weight update is given as below. \n\\begin{align*}\n  \\mathrm{v}_{t+1}&=\\gamma\\mathrm{v}_{t}+\\alpha\\nabla_{\\theta}J(\\boxed{\\theta_{t}-\\gamma\\mathrm{v}_{t}} ;X^{(i)},y^{(i)})\\\\\n  \\theta_{t+1}&=\\theta_{t}-\\mathrm{v}_{t+1}\n\\end{align*}\n\n\n\nHere intuition behind the term \\boxed{\\theta_{t}-\\gamma\\mathrm{v}_{t}} is that, If you know the stock market price for tomorrow, we can do much better on stocks. In the same way we take a step forward and find the cost and then compute derivative, so that we can do better.\n\n\nIn general, this method performs better."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#sgd-with-nesterov-momentum",
    "href": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#sgd-with-nesterov-momentum",
    "title": "Deep Learning 2",
    "section": "10 SGD with Nesterov Momentum",
    "text": "10 SGD with Nesterov Momentum\n\nLearning rate \\alpha  momentum parameter \\gamma  minibatch size m  Initial weights \\theta_t Initial velocity v_t\n\n\n\nwhile stopping criterion not met do\nSample a minibatch of m examples from the training set.\nApply interim update \\tilde{\\theta}_{t}=\\theta_{t}-\\gamma\\bf{v}_{t}\nCompute gradient estimate {\\nabla}_{\\theta}{\\sum}_{i=1}^{m}J(\\tilde{\\theta}_{t};X^{(i)},y^{(i)})\nCompute update {\\bf v_{t+1}}=\\gamma{\\bf v}_{t}+\\alpha\\nabla_{\\theta}\\Sigma_{i=1}^{m}J(\\tilde{\\theta}_{t};x^{(i)},y^{(i)})\nApply update \\theta_{t+1}=\\theta_{t}-\\bf v_{t+1}\nEnd while\n\n\n\nMomentum parameter (\\gamma): Weightage given to earlier steps taken in the process of gradient descent."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#adaptive-learning-rate-methods-adagrad",
    "href": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#adaptive-learning-rate-methods-adagrad",
    "title": "Deep Learning 2",
    "section": "11 Adaptive Learning Rate Methods: Adagrad",
    "text": "11 Adaptive Learning Rate Methods: Adagrad\n\nGlobal Learning rate \\alpha  minibatch size m  Initial weights \\theta_t Small constant \\delta near to 10^{-7} for numerical stability\n\n\n\nInitialize gradient accumulation variable \\mathbf{r=0}\nwhile stopping criterion not met do\nSample a minibatch of m examples from the training set.\nCompute gradient estimate {\\nabla}_{\\theta}{\\sum}_{i=1}^{m}J({\\theta}_{t};X^{(i)},y^{(i)})\n\n\n\nCompute squared gradient {\\bf r}={\\bf r}+(\\nabla_{\\theta}J\\;\\odot\\;\\nabla_{\\theta}J)\nCompute update \\displaystyle \\Delta\\theta_{t+1}={\\frac{\\alpha}{\\delta+\\sqrt{\\bf r}}}\\ \\odot \\ \\nabla_\\theta J (division and square root computed elementwise)\n\n\n\nApply update \\theta_{t+1}=\\theta_{t}-\\Delta\\theta_{t+1}\nEnd while\n\n\n\nIntuition\n\nCalculates a different learning rate for each feature.\nSparse features have higher learning rate."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#rmsprop",
    "href": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#rmsprop",
    "title": "Deep Learning 2",
    "section": "12 RMSProp",
    "text": "12 RMSProp\n\nGlobal Learning rate \\alpha  Decay rate \\rho  minibatch size m  Initial weights \\theta_t Small constant \\delta near to 10^{-6} for numerical stability\n\n\n\nInitialize gradient accumulation variable \\mathbf{r=0}\nwhile stopping criterion not met do\nSample a minibatch of m examples from the training set.\nCompute gradient estimate {\\nabla}_{\\theta}{\\sum}_{i=1}^{m}J({\\theta}_{t};X^{(i)},y^{(i)})\n\n\n\nAccumulate squared gradient {\\bf r}=\\rho{\\bf r}+(1-\\rho)(\\nabla_{\\theta}J\\;\\odot\\;\\nabla_{\\theta}J)\nCompute update \\displaystyle \\Delta\\theta_{t+1}={\\frac{\\alpha}{\\delta+\\sqrt{\\bf r}}}\\ \\odot \\ \\nabla_\\theta J (division and square root computed elementwise)\n\n\n\nApply update \\theta_{t+1}=\\theta_{t}-\\Delta\\theta_{t+1}\nEnd while\n\n\n\nIntuition\n\nCalculates a different learning rate for each feature.\nSparse features have higher learning rate.\nNow we can control the accumulation using \\rho."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#rmsprop-with-nesterov-momentum",
    "href": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#rmsprop-with-nesterov-momentum",
    "title": "Deep Learning 2",
    "section": "13 RMSProp with Nesterov Momentum",
    "text": "13 RMSProp with Nesterov Momentum\n\nGlobal Learning rate \\alpha  Decay rate \\rho  Momentum co-efficient \\gamma Inital velocity \\bf v minibatch size m  Initial weights \\theta_t Small constant \\delta near to 10^{-6} for numerical stability\n\n\n\nInitialize gradient accumulation variable \\mathbf{r=0}\nwhile stopping criterion not met do\nSample a minibatch of m examples from the training set.\n\n\n\nApply interim update \\tilde{\\theta}_{t}=\\theta_{t}-\\gamma\\bf{v}_{t}\nCompute gradient estimate {\\nabla}_{\\theta}{\\sum}_{i=1}^{m}J(\\tilde{\\theta}_{t};X^{(i)},y^{(i)})\n\n\n\n\nAccumulate squared gradient {\\bf r}=\\rho{\\bf r}+(1-\\rho)(\\nabla_{\\theta}J\\;\\odot\\;\\nabla_{\\theta}J)\nCompute update \\displaystyle \\Delta\\theta_{t+1}={\\frac{\\alpha}{\\delta+\\sqrt{\\bf r}}}\\ \\odot \\ \\nabla_\\theta J (division and square root computed elementwise)\n\n\n\nApply update \\theta_{t+1}=\\theta_{t}-\\Delta\\theta_{t+1}\nEnd while"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#adam",
    "href": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#adam",
    "title": "Deep Learning 2",
    "section": "14 Adam",
    "text": "14 Adam\n\nGlobal Learning rate \\alpha  Decay rates for moment estimate \\rho_1 and \\rho_2  Momentum co-efficient \\gamma minibatch size m  Initial weights \\theta_t Small constant \\delta near to 10^{-8} for numerical stability\n\n\n\nInitialize first and second moment variables \\mathbf{r=0} and \\mathbf{s=0}\nwhile stopping criterion not met do\nSample a minibatch of m examples from the training set.\n\n\n\nCompute gradient estimate {\\nabla}_{\\theta}{\\sum}_{i=1}^{m}J({\\theta}_{t};X^{(i)},y^{(i)})\nUpdate biased first moment estimate {\\bf s}=\\rho_1{\\bf s}+(1-\\rho_1)\\nabla_{\\theta}J\nUpdate biased second moment estimate {\\bf r}=\\rho_2{\\bf r}+(1-\\rho_2)(\\nabla_{\\theta}J\\;\\odot\\;\\nabla_{\\theta}J)\nCorrect bias in first moment \\displaystyle \\tilde{\\bf s}=\\frac{\\bf s}{1-\\rho_1^t}\nNow Correct bias in second moment \\displaystyle \\tilde{\\bf r}=\\frac{\\bf r}{1-\\rho_2^t}\nCompute update \\displaystyle \\Delta\\theta_{t+1}={\\alpha\\frac{\\tilde{\\bf s}}{\\delta+\\sqrt{\\tilde{\\bf r}}}} (division and square root computed elementwise)\n\n\n\nApply update \\theta_{t+1}=\\theta_{t}-\\Delta\\theta_{t+1}\nEnd while\n\n\n\nIntuition\n\nSimilar to RMSProp with momentum\nUses the idea of momentum, as well as having a different learning rate for each dimension (which is automatically adjusted, as in Adagrad, Adadelta or RMS)"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#interesting-facts",
    "href": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#interesting-facts",
    "title": "Deep Learning 2",
    "section": "15 Interesting Facts",
    "text": "15 Interesting Facts\n\n\n\n\n\n\n\nInteresting Facts\n\n\n\n\nIf input data is sparse, adaptive learning-rate methods may be best.No need to tune learning rate\nLearning rates diminish fast in Adagrad, RMSProp addresses this issue.\nAdam adds bias-correction and momentum to RMSprop.\nRMSprop, Adadelta, and Adam are similar algorithms, bias-correction helps Adam slightly outperform RMSprop towards the end of optimization as gradients become sparser\nAdam might be the best overall choice (May not be always true!)\nVanilla SGD depends on a robust initialization and annealing schedule, and may get stuck in saddle points rather than local minima.\nMany recent papers use vanilla SGD without momentum, but with a simple learning rate annealing schedule\nSome standard choices for training deep networks: SGD + Nesterov momentum, SGD with Adagrad/RMSProp/Adam.\nReLUs, Leaky ReLUs and MaxOut are the best bets for activation functions"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#activation-functions",
    "href": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#activation-functions",
    "title": "Deep Learning 2",
    "section": "16 Activation Functions",
    "text": "16 Activation Functions\n\nSigmoid\n\nThis function takes any real value as input and outputs values in the range of 0 to 1.\nFormulae : f(x)=\\frac{1}{1+e^{-x}} \nIt is commonly used for models where we have to predict the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice because of its range.\n\nTanh Function\n\nIt’s output range is -1 to 1.\nFormulae : f(x)=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} \n\nThe output of the tanh activation function is Zero centered; hence we can easily map the output values as strongly negative, neutral, or strongly positive.\nUsually used in hidden layers of a neural network as its values lie between -1 to 1; therefore, the mean for the hidden layer comes out to be 0 or very close to it. It helps in centering the data and makes learning for the next layer much easier.\n\n\n\n\n\n\nNote\n\n\n\nAlthough both sigmoid and tanh face vanishing gradient issue, tanh is zero centered, and the gradients are not restricted to move in a certain direction. Therefore, in practice, tanh nonlinearity is always preferred to sigmoid nonlinearity.\n\n\n\nReLU\n\nThe neurons will only be deactivated if the output of the linear transformation is less than 0.\nFormulae :  f(x)=\\max(0,x)\nSince only a certain number of neurons are activated, the ReLU function is far more computationally efficient when compared to the sigmoid and tanh functions.\nReLU accelerates the convergence of gradient descent towards the global minimum of the loss function due to its linear, non-saturating property.\nFound to accelerate convergence of SGD compared to sigmoid/tanh functions (a factor of 6) in AlexNet\nIt has the The Dying ReLU problem\n\nThe negative side of the graph makes the gradient value zero. Due to this reason, during the backpropagation process, the weights and biases for some neurons are not updated. This can create dead neurons which never get activated.\nAll the negative input values become zero immediately, which decreases the model’s ability to fit or train from the data properly.\n\n\nLeaky ReLU\n\nLeaky ReLU is an improved version of ReLU function to solve the Dying ReLU problem as it has a small positive slope in the negative area.\nformula: f(x)=\\max(0.1x,x)\nThe predictions may not be consistent for negative input values.\nThe gradient for negative values is a small value that makes the learning of model parameters time-consuming.\n\nParametric ReLU\n\nformula: f(x)=\\max(ax,x)\nParametric ReLU is another variant of ReLU that aims to solve the problem of gradient’s becoming zero for the left half of the axis.\nThis function provides the slope of the negative part of the function as an argument a. By performing backpropagation, the most appropriate value of a is learnt.\nThe parameterized ReLU function is used when the leaky ReLU function still fails at solving the problem of dead neurons, and the relevant information is not successfully passed to the next layer.\n\nExponential Linear Units (ELUs)\n\nELU uses a log curve to define the negativ values unlike the leaky ReLU and Parametric ReLU functions with a straight line.\nformula: f(x) = \\begin{cases}\n  x &\\text{if } x \\ge 0 \\\\\n  \\alpha(e^x-1) &\\text{if } x<0\n  \\end{cases}\nELU becomes smooth slowly until its output equal to \\alpha whereas RELU sharply smoothes.\nAvoids dead ReLU problem by introducing log curve for negative values of input. It helps the network nudge weights and biases in the right direction.\nNo learning of the \\alpha value takes place\n\nSoftmax\n\nIt is used for multiclass classification.\nformula: f(x_i)=\\frac{e^{x_i}}{\\sum_{j}e^{x_j} } \n\nswish\n\nFormulae : f(x)=\\frac{x}{1+e^{-x}} \nSwish is a smooth function that means that it does not abruptly change direction like ReLU does near x = 0. Rather, it smoothly bends from 0 towards values < 0 and then upwards again.\nThe swish function being non-monotonous enhances the expression of input data and weight to be learnt.\n\nMax Out\n\nIt is generalization of ReLU.\nformula: f(x)=\\max(w_1^Tx+b_1,w_2^Tx+b_2)\n\n\n\nGuide-Line to Chose activation Function\n\n\nRegression — Linear Activation Function\nBinary Classification — Sigmoid/Logistic Activation Function\nMulticlass Classification — Softmax\nMultilabel Classification — Sigmoid\nReLU activation function should only be used in the hidden layers.\nSigmoid/Logistic and Tanh functions should not be used in hidden layers as they make the model more susceptible to problems during training (due to vanishing gradients)\nSwish function is used in neural networks having a depth greater than 40 layers.\nTry tanh, but expect it to work worse than ReLU/Maxout.\n\n\n\n\n\nCode\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\npio.renderers.default = \"plotly_mimetype+notebook_connected\" # this is reqired only to render html not for notebook\n\nfig = make_subplots(rows=3, cols=2,subplot_titles=(\"Sigmoid\", \"tanh\", \"ReLU\", \"Leaky ReLU\", \"ELU\",\"swish\"))\nx = np.linspace(-5,5,50)\nsigmoid = 1/(1+np.exp(-x))\nfig.append_trace(go.Line(x=x,y=1/(1+np.exp(-x))),row=1,col=1)\nfig.append_trace(go.Line(x=x,y=np.tanh(x)),row=1,col=2)\nfig.append_trace(go.Line(x=x,y=np.where(x>0,x,0)),row=2,col=1)\nfig.append_trace(go.Line(x=x,y=np.where(x>0,x,0.1*x)),row=2,col=2)\nfig.append_trace(go.Line(x=x,y=np.where(x>0,x,5*(np.exp(x)-1))),row=3,col=1)\nfig.append_trace(go.Line(x=x,y=x/(1+np.exp(-x))),row=3,col=2)\nfig.update(layout_showlegend=False)\nfig.show()"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#loss-functions",
    "href": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#loss-functions",
    "title": "Deep Learning 2",
    "section": "17 Loss Functions",
    "text": "17 Loss Functions\n\nCross-Entropy Loss Function\n\nFormula J=-\\frac{1}{m}\\sum_{i=1}^{m}y_{i}\\log\\hat{y_{i}}+(1-y_{i})\\log(1-\\hat{y_{i}})\nIf the activation function is sigmoid \\left(\\sigma(z)=\\,\\frac{1}{1+e^{-z}}\\right) the darivative of cross-entropy is: so,\n\\hat y=\\sigma(w^Tz) \\text{ and } \\sigma(z)=\\frac{1}{1+e^{-z}} so we can simplify it as below considering just one record J = -\\frac{1}{m}\\sum_{i=1}^{m} \\left(y\\log(\\sigma(z))+(1-y)\\log(1-\\sigma(z))\\right) Now we find derivative \\begin{align*}\n\\frac{\\partial J}{\\partial w_j} &= -\\frac{1}{m}\\sum_{i=1}^{m} \\left(y \\frac{1}{\\sigma(z)}\\sigma(z)'x_j+(1-y) \\left( \\frac{1}{1-\\sigma(z)}\\right) (-\\sigma(z)'x_j)\\right) \\\\\n&=-\\frac{1}{m}\\sum_{i=1}^{m}\\left(  \\frac{y}{\\sigma(z)}-  \\frac{1-y}{1-\\sigma(z)}\\right) \\sigma(z)'x_j \\\\\n&= -\\frac{1}{m}\\sum_{i=1}^{m}\\frac{y(1-\\sigma(z))-\\sigma(z)(1-y)}{\\sigma(z)(1-\\sigma(z))}\\sigma(z)'x_j  \\\\\n&= -\\frac{1}{m}\\sum_{i=1}^{m}\\frac{y-y*\\sigma(z)-\\sigma(z)+\\sigma(z)*y}{\\sigma(z)(1-\\sigma(z))}\\sigma(z)'x_j  \\\\\n&= -\\frac{1}{m}\\sum_{i=1}^{m}\\frac{y-\\sigma(z)}{\\sigma(z)(1-\\sigma(z))}\\sigma(z)(1-\\sigma(z))*x_j  \\\\\n&=-\\frac{1}{m}\\sum_{i=1}^{m}(y-\\sigma(z))*x_j\\\\\n&=\\frac{1}{m}\\sum_{i=1}^{m}(\\sigma(z)-y)*x_j\n\\end{align*}\nEven if we consider softmax as activation function \\left(a_{j}^{L}=\\frac{e^{z_{j}^{L}}}{\\sum_{k}e^{z_{k}^{L}}}\\right) we get similar derivative \\frac{\\partial J}{\\partial w_{j k}^{L}}=\\overbrace{\\boxed{a_{k}^{L-1}(a_{j}^{L}-y_{j})}}^{\\text{similar to cross entropy}}"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#torch-loss-function",
    "href": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#torch-loss-function",
    "title": "Deep Learning 2",
    "section": "18 Torch Loss function",
    "text": "18 Torch Loss function\n\nClassification\n\nBCECriterion: binary cross-entropy for Sigmoid (two-class version)\nClassNLLCriterion: negative log-likelihood (multi-class)\nMarginCriterion: two class margin-based loss\n\nRegression\n\nAbsCriterion: measures the mean absolute value of the element wise difference between input\nMSECriterion: mean square error (a classic)\nDistKLDivCriterion: Kullback–Leibler divergence (for fitting continuous probability distributions)\n\nEmbedding (measuring whether two inputs are similar or dissimilar)\n\nL1HingeEmbeddingCriterion: L1 distance between two inputs\nCosineEmbeddingCriterion: cosine distance between two inputs"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#some-more-facts",
    "href": "Data_Science_Notes/Deep-Learning/2023-01-21-CS5480-week2.html#some-more-facts",
    "title": "Deep Learning 2",
    "section": "19 Some more facts",
    "text": "19 Some more facts\n\n\n\n\n\n\nImportant\n\n\n\nA multilayer network of perceptrons with a single hidden layer can be used to represent any boolean function precisely (no errors), in fact for n inputs, it needs {2}^n perceptron on hidden layer and one perceptron on output layer to represent any boolean function.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nA multilayer network of neurons with a single hidden layer can be used to approximate any continuous function to any desired precision. In other words, there is a guarantee that for any function f(x): \\mathbb{R}^n \\rightarrow \\mathbb{R}^m, we can always find a neural network (with 1 hidden layer containing enough neurons) whose output g(x) satisfies | g(x)-f(x)| < \\epsilon\n\n\n \\tiny {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-02-04-CS5480-week3.html",
    "href": "Data_Science_Notes/Deep-Learning/2023-02-04-CS5480-week3.html",
    "title": "Deep Learning 3",
    "section": "",
    "text": "why do we go for first derivative in gradient descent why not second derivative\nsecond derive is called hassian matrix\nwhy not use Hassian instead of jaccobian\nbecause it is expensive to compute, also it was proved that SGD works better than this,\nSGD is noisy which is an advantage, and it helps us generalize better. SGD is the true success for generalization of the deep learning"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-02-04-CS5480-week3.html#regularization-methods",
    "href": "Data_Science_Notes/Deep-Learning/2023-02-04-CS5480-week3.html#regularization-methods",
    "title": "Deep Learning 3",
    "section": "1 Regularization Methods",
    "text": "1 Regularization Methods\nDifference between Machine Learning and Optimization\n\nGeneralization, In general optimization we do not focus on generalization.\nIn DL we are interested in the data which will come, not in the data which is already there. In DL we are interested in the generalization error\nIn mainstream optimization, minimizing J is itself the goal; whereas in deep learning, minimizing J so as to minimize a generalizable out-of-sample performance measure is the goal\nEmpirical Risk Minimization (ERM): \\mathbb{E}_{\\mathbf{x,y}\\approx{\\widehat{p}}_{d a t a}(\\mathbf{x,y})}(J(\\theta;\\mathbf{x},y))={\\frac{1}{m}}\\sum_{i=1}^{m}J(\\theta;\\mathbf{x}_{i},y_{i})\nHowever, ERM can lead to overfitting. Avoiding overfitting is regularization."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-02-04-CS5480-week3.html#learning-and-generalization",
    "href": "Data_Science_Notes/Deep-Learning/2023-02-04-CS5480-week3.html#learning-and-generalization",
    "title": "Deep Learning 3",
    "section": "2 Learning and Generalization",
    "text": "2 Learning and Generalization\n\nSimple idea to keep monitoring the cost function, and not let it become too consistently low; stop at an earlier iteration \n\nAfter some time validation loss stops improving and we can do early stopping.\n\n2.1 When to stop\n\nTrain n epochs; lower learning rate; train m epochs : But it’s bad idea, can’t assume one-size-fits-all approach\nError-change criterion:\n\nStop when error isn’t dropping over a window of, say, 10 epochs\nTrain for a fixed number of epochs after criterion is reached (possibly with lower learning rate)\n\nWeight-change criterion:\n\nCompare weights at epochs t-10 and t and test: \\Pi\\mathrm{d}\\times_{i}\\left|\\left|W_{i}^{t}\\,-\\,W_{i}^{t-10}\\right|\\right|\\,<\\,\\rho\nDon’t base on length of overall weight change vector\nPossibly express as a percentage of the weight\n\n\n\n\n2.2 Weight Decay\n\nL1 regularization\nL2 regularization\n\n\n\n2.3 DropOut\n\nAnother standard approach to regularization in ML: Model Averaging\nDropOut → a very interesting way to perform model averaging in deep learning.\nTraining Phase: For each hidden layer, for each training sample, for each iteration, ignore (zero out) a random fraction, p, of nodes (and corresponding activations)\nTest Phase: Use all activations, but reduce them by a factor p (to account for the missing activations during training) \nWith H hidden units, each of which can be dropped, we have 2^H possible models\nEach of the 2^{H−1} models that include hidden unit h must share the same weights for the unit\n\nserves as a form of regularization\nmakes the models cooperate\n\nIncluding all hidden units at test with a scaling of 0.5 is equivalent to computing the geometric mean of all 2^H models\nIt’s like ensamble without doing it\nHere we are randomly dropping so that no neuron becomes specific to a task, but it learns to do all the task.\nDropConnect: An Extension \n\nUsing noise is another form of regularization; has shown some impressive results recently. Could be:\n\nData Noise\n\nHas been there for a while: add noise to data while training\nMinimization of sum-of-squares error with zero-mean gaussian noise(added to training data) is equivalent to minimization of sum-of-squares error without noise with an added regularized term\nVery similar to data augmentation that we will see later\n\nLabel Noise\nGradient Noise\nRegularization through Label Noise\n\nDisturb each training sample with the probability\n\nRegularization through Gradient Noise\n\nSimple idea: add noise to gradient g_{t}  \\leftarrow g_{t} +\\mathcal{N}(0,\\sigma_{t}^{2})\nAnnealed Gaussian noise by decaying the variance \\sigma_{t}^{2}\\,=\\,\\frac{\\eta}{(1\\,+\\,t)^{\\gamma}}\nShowed significant improvement in performance"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-02-04-CS5480-week3.html#data-manipulation-methods",
    "href": "Data_Science_Notes/Deep-Learning/2023-02-04-CS5480-week3.html#data-manipulation-methods",
    "title": "Deep Learning 3",
    "section": "3 Data Manipulation Methods",
    "text": "3 Data Manipulation Methods\n\n3.1 Normalize/standardize the inputs\n\nConvergence is faster if average input over the training set is close to zero. look at 8Le Cun et al, Efficient Backprop, 1998 and find answer\nScaled to have the same covariance - speeds learning\n\nIdeally, value of covariance should be matched with output of activation function (e.g. sigmoid)\n\nInitially we want to be in lierar reason, we can achieve\nAfter normalization, if the variance of the data is 2 we want the activation fucntion to have a linear range between 0 and 2\n\n\nPCA\n\nDecorrelate the inputs\nImagine one input is always twice the other, i.e. z_2 = 2z_1. Output y will be constant on lines w_2 + \\frac{1}{2}w_1= const. No use making weight changes on these lines. \n\n\n\n\n3.2 Batch Normalization\n\nChange in distributions of data inputs is a problem because the model needs to continuously adapt to the new distribution, called covariate shift\nThis is typically handled using domain adaptation\nEven while training minibatch distribution can change\nWhat if this happens in a subnetwork in DL? → called internal covariate shift. How to handle?\nBatch nor accelerate training and also regularize the network\n\nBN layer usually inserted before non-linearity layer (after FC or convolutional layer)\nAllows higher learning rates.\nReduces the strong dependence on initialization\nActs as a form of regularization too\nTake average of last 10 minibatch of gamma and beta and use in inferencing\nAlgo\n\n\\mu_{B}\\leftarrow{\\frac{1}{m}}\\sum_{i=1}^{m}x_{i}\n\\sigma_{B}^{2}\\leftarrow\\frac{1}{m}\\sum_{i=1}^{m}(x_{i}-\\mu_{B})^{2}\n\\widehat{x}_{i}\\leftarrow\\frac{x_{i}-\\mu_{B}}{\\sqrt{\\sigma_{B}^{2}+\\epsilon}}\ny_{i}\\leftarrow\\gamma\\widehat{\\alpha}_{i}\\leftarrow\\beta\\equiv\\mathrm{BN}_{\\gamma,\\beta}(x_{i})\n\n\n\n\n3.3 Shuffling Inputs\n\nRandom sample data and do SGD\nChoose examples with maximum information content\n\nShuffle the training set so that successive training examples never (rarely) belong to the same class.\nPresent input examples that produce a large error more frequently than examples that produce a small error.\n\nHelps take large steps in the gradient descent\n\n\n\n\n\n3.4 Curriculum Learning\n\nOld idea, proposed by Elman in 1993\nHumans and animals learn much better when examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones\nStart small, learn easier aspects of the task or easier sub-tasks, and then gradually increase the difficulty level\nBy choosing examples and their order, one can guide training and remarkably increase learning speed\nIntroduces the concept of a teacher who:\n\nhas prior knowledge about the training data to decide on a sequence of concepts that can more easily be learned when presented in that order\nmonitoring ’learner’s progress to decide when to move on to new material from the curriculum\n\n\n\n\n3.5 Data Augmentation\n\nData jittering\nRotations\nColor changes\nNoise injection\nMirroring\nHelps increase data; is useful when training data provided is less\nAlso acts as a regularizer (by avoiding overfitting to provided data)\nMixUP: if we have data x_1, y_1 and x_2,y_2, we can interpolate between x_1, x_2 and also for y_1,y_2 in this way we can create more data, and also it creates more labes, but that is ok, and it help in the genelarization of the models."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-02-04-CS5480-week3.html#parameter-choices-initialization-method",
    "href": "Data_Science_Notes/Deep-Learning/2023-02-04-CS5480-week3.html#parameter-choices-initialization-method",
    "title": "Deep Learning 3",
    "section": "4 Parameter Choices / Initialization method",
    "text": "4 Parameter Choices / Initialization method\n\nActivation Functions\nLoss Functions : should be differentiable\nLearning Rates : adaptive learning rates\nAll of them decrease it when weight vector “oscillates”, and increase it when the weight vector follows a relatively steady direction\nWorthwhile picking a different learning rate for each weight (e.g. based on curvature)\nAssuming a binary classification problem, what do you choose the target labels to be? +1 and -1?\nWhat if these are the sigmoid’s asymptotes?\n\nWeights will be increased continuously to very high values to match the target\nWeights multiplied by small sigmoid derivative → small weight updates → Stuck!\n\nChoose target values at the point of the maximum second derivative on the sigmoid so as to avoid saturating the output units.\n\nInitially we want activation to be in the linear region.\n\nTo be chosen randomly, but in such a way that the activation function is in its linear region\n\nBoth large and small weights can cause very low gradients (in case of sigmoid activation)\n\nXavier’s initialization \\mathrm{uniform}\\left(-{\\frac{\\sqrt{6}}{\\sqrt{\\mathrm{fan}_{\\mathrm{in}}+\\mathrm{fan}_{\\mathrm{out}}}}},\\;{\\frac{\\sqrt{6}}{\\sqrt{\\mathrm{fan}_{\\mathrm{in}}+\\mathrm{fan}_{\\mathrm{out}}}}}\\right)\nCaffe implements a simpler version of Xavier’s initialization as: \\mathrm{uniform}\\left(-{\\frac{2}{{\\mathrm{fan}_{\\mathrm{in}}+\\mathrm{fan}_{\\mathrm{out}}}}},\\;{\\frac{2}{{\\mathrm{fan}_{\\mathrm{in}}+\\mathrm{fan}_{\\mathrm{out}}}}}\\right)\nHe’s initialization \\mathrm{uniform}\\left(-{\\frac{4}{{\\mathrm{fan}_{\\mathrm{in}}+\\mathrm{fan}_{\\mathrm{out}}}}},\\;{\\frac{4}{{\\mathrm{fan}_{\\mathrm{in}}+\\mathrm{fan}_{\\mathrm{out}}}}}\\right)\n\nProof outline of Xavier’s initialization  We know  Y= w_1x_1+w_2x_2 + \\dots + w_nx_n then \\begin{align*}\n  \\mathrm{Var}(Y)&=\\sum_i\\mathrm{Var}(w_ix_i)\\\\\n  &=\\sum_i \\left( \\underbrace{\\mathbb{E}[x_i]^2\\mathrm{Var}(w_i)}_{\\;\\;0,\\text{ Since mean of input is 0}}  +\\underbrace{ \\mathbb{E}[w_i]^2\\mathrm{Var}(x_i)}_{\\;\\;0,\\text{ Since weight is normalized to 0}} +\\mathrm{Var}(w_i)\\mathrm{Var}(x_i) \\right) \\\\\n  &= \\sum_i  \\mathrm{Var}(w_i)\\mathrm{Var}(x_i) \\\\\n  &= n\\times \\mathrm{Var}(w_i)\\mathrm{Var}(x_i)\n\\end{align*}\nNow from above equation, if we want \\mathrm{Var}(Y) to be equal to \\mathrm{Var}(x_i) then \\mathrm{Var}(w_i) should be equal to \\frac{1}{n}, here n is fan in, similarly we can get something similar when can consider from backprop prospective and we get \\frac{1}{n}, here n is fan out, Putting the two together we want \\mathrm{Var}(w_i) = \\frac{2}{{\\mathrm{fan}_{\\mathrm{in}}+\\mathrm{fan}_{\\mathrm{out}}}} Which happens to be Caffe weight initialization method.\nNow if we consider uniform distribution to be in range of [-\\theta, \\theta] then  \\mathrm{Var} = \\frac{(\\theta-(-\\theta))^2}{12} = \\frac{\\theta^2}{3} This is the term which introduces \\sqrt{6} in Xavier’s initialization"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-02-04-CS5480-week3.html#takeaways",
    "href": "Data_Science_Notes/Deep-Learning/2023-02-04-CS5480-week3.html#takeaways",
    "title": "Deep Learning 3",
    "section": "5 Takeaways",
    "text": "5 Takeaways\n\nSome standard choices for training deep networks: SGD + Nesterov momentum, SGD with Adagrad/RMSProp/Adam\nReLUs, Leaky ReLUs and MaxOut are the best bets for activation functions\nBatch Normalization layers are here to stay (at least, for now)\nDropOut is an excellent regularizer\nData Augmentation is a must in vision applications\nWeight Initialization is very important while training a new network"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-02-04-CS5480-week3.html#computer-vision-basics",
    "href": "Data_Science_Notes/Deep-Learning/2023-02-04-CS5480-week3.html#computer-vision-basics",
    "title": "Deep Learning 3",
    "section": "6 Computer Vision Basics",
    "text": "6 Computer Vision Basics\n\nWe can think of a (grayscale) image as a function f: \\mathcal{R}^2 \\rightarrow \\mathcal{R} giving the intensity at position (x, y)\nA digital image is a discrete (sampled, quantized) version of this function\nAs with any function, we can apply operators to an image  \n\nImage Processing Operations\n\nPoint the output value at a specific coordinate is dependent only on the input value at that same coordinate.\nLocal the output value at a specific coordinate is dependent on the input values in the neighborhood of that same coordinate.\nGlobal the output value at a specific coordinate is dependent on all the values in the input image. eg ( fourier, wavelet)\n\nSimple Point Operations: Image Enhancement\n\nReversing the contrast new_pixel = max old_pixel + min\nContrast stretching\n\nNoise Reduction\n\nNoise Reduction using 2D Moving Average  \n\nLinear Filtering\n\nOne simple version: linear filtering\n\nReplace each pixel by a linear combination (a weighted sum) of its neighbors\n\nThe prescription for the linear combination is called the “kernel” (or “mask”, “filter”) \n\nLinear Filtering: Cross correlation\n\nSay the averaging window size is 2k+1 \\times 2k+1:\nG[i,j]=\\underbrace{\\frac{1}{(2k+1)^{2}}}_{\\text{Uniform weight to each pixel} } \\times \\overbrace{ \\sum_{u=-k}^{k}\\sum_{v=-k}^{k}F[i+u,j+v]}^{\\text{loop over all pixels in neighborhood}} \nNow generalize to allow different weights depending on neighboring pixel’s relative position: G[i,j]=\\sum_{u=-k}^{k}\\sum_{v=-k}^{k}\\underbrace{H[u,v]}_{\\text{Non-uniform weight}} F[i+u,j+v]\nThis is called cross correlation , denoted by G=H\\otimes F\nFiltering an image: replace each pixel with a linear combination of its neighbors\nCan think of as a “dot product” between local neighborhood and kernel for each pixel\nThe entries of the weight kernel or “mask” H[u,v] are called filter co-efficients."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-02-04-CS5480-week3.html#linear-filtering-correlation-example",
    "href": "Data_Science_Notes/Deep-Learning/2023-02-04-CS5480-week3.html#linear-filtering-correlation-example",
    "title": "Deep Learning 3",
    "section": "7 Linear Filtering: Correlation Example",
    "text": "7 Linear Filtering: Correlation Example\n\nConvolution operator G[i,j] =\\sum_{u=-k}^{k}\\sum_{v=-k}^{k}H[u,v]F[i-u,j-v] and H is then called the impulse response function.\nEquivalent to flip the filter in both dimensions (bottom to top, right to left) and apply cross correlation. G=H*F\n\n \\tiny {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html",
    "href": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html",
    "title": "Deep Learning 4",
    "section": "",
    "text": "Modify the pixels in an image based on some function of a local neighborhood of each pixel\n\nOne simple version: linear filtering\n\nReplace each pixel by a linear combination (a weighted sum) of its neighbors\n\nThe prescription for the linear combination is called the “kernel” (or “mask”, “\n\nLinear Filtering: Cross correlation - simple averge G[i,j]=\\underbrace{\\frac{1}{(2k+1)^{2}}}_{\\text{Uniform weight to each pixel} } \\times \\overbrace{ \\sum_{u=-k}^{k}\\sum_{v=-k}^{k}F[i+u,j+v]}^{\\text{loop over all pixels in neighborhood}}  - weighted average, G[i,j]=\\sum_{u=-k}^{k}\\sum_{v=-k}^{k}\\underbrace{H[u,v]}_{\\text{Non-uniform weight}} F[i+u,j+v]\n\nwhat we get in G depends on how we choose H\ndifferent H is required for different features\nTHis kind of linear filter is called cross corelation\nit is a measure of correlation between H and F, it can be also viewed as local operation\nH is called kernel as well\n\nSmoothing by Averaging Computer Vision\n\ntake 5 \\times 5 or 3 \\times 3 kernal and apply on image\n5 \\times 5 will blur more as compared to 3 \\times 3"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#gaussian-filter",
    "href": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#gaussian-filter",
    "title": "Deep Learning 4",
    "section": "2 Gaussian Filter",
    "text": "2 Gaussian Filter\nWhat if we want nearest neighboring pixels to have the most influence on the output? - Use Gaussian filter   - Removes high frequency components from the image (“low pass filter”). More on this later. - With Gaussian filter, we can preserve more structure, in mean (average) filter there is blockyness, but in Gaussian filter there is a fading kind of affect, no blockyness. - Smoothing with a Gaussian   - Mean vs Gaussian In below pic left is mean, right is Gaussian: \nSize of kernel or mask:\n\nGaussian function has infinite support, but discrete filters use finite kernels. \n\nWhat is the result of filtering the impulse signal (image) F with the arbitrary kernel H?\n\nit double filpps the image\nfor convolution we dobuble fipp the kernal and then do dot product"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#effect-of-simple-kernels-on-image",
    "href": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#effect-of-simple-kernels-on-image",
    "title": "Deep Learning 4",
    "section": "3 Effect of simple kernels on image",
    "text": "3 Effect of simple kernels on image\n\nDoes nothing \\left\\lbrack \\begin{array}{ccc}\n0 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 0\n\\end{array}\\right\\rbrack\nBlurs \\frac{1}{9}\\left\\lbrack \\begin{array}{ccc}\n1 & 1 & 1\\\\\n1 & 1 & 1\\\\\n1 & 1 & 1\n\\end{array}\\right\\rbrack\nShifts left by one pixel with correlation \\left\\lbrack \\begin{array}{ccc}\n0 & 0 & 0\\\\\n0 & 0 & 1\\\\\n0 & 0 & 0\n\\end{array}\\right\\rbrack"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#linear-filtering-correlation-example",
    "href": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#linear-filtering-correlation-example",
    "title": "Deep Learning 4",
    "section": "4 Linear Filtering: Correlation Example",
    "text": "4 Linear Filtering: Correlation Example\nWhat is the result of filtering the impulse signal (image) F with the arbitrary kernel H?\n\nIt double flips the kernel as shown below"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#convolution",
    "href": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#convolution",
    "title": "Deep Learning 4",
    "section": "5 Convolution",
    "text": "5 Convolution\n\nConvolution operator G[i,j]=\\sum_{u=-k}^{k}\\sum_{v=-k}^{k}H[u,v]F[i-u,j-v] and H is then called the impulse response function.\nEquivalent to flip the filter in both dimensions (bottom to top, right to left) and apply cross correlation. Notice minus sign in the above equation (F[i-u,j-v]) \nDenoted by G=H*F"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#correlation-vs-convolution",
    "href": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#correlation-vs-convolution",
    "title": "Deep Learning 4",
    "section": "6 Correlation vs Convolution",
    "text": "6 Correlation vs Convolution\n\nCorrelation : G=H\\otimes{\\cal F} G[i,j]=\\sum_{u=-k}^{k}\\sum_{v=-k}^{k}H[u,v]F[i+u,j+v]\nConvolution : G=H*F G[i,j]=\\sum_{u=-k}^{k}\\sum_{v=-k}^{k}H[u,v]F[i-u,j-v]\nFor a Gaussian or box filter, how will the outputs differ (among correlation and convolution )?\n\nit will not as filter is symmetric\n\nFor impulse image\n\nwe saw above"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#boundary-effects",
    "href": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#boundary-effects",
    "title": "Deep Learning 4",
    "section": "7 Boundary Effects",
    "text": "7 Boundary Effects\n\nvalid conv, we agree ouput will be smaller, we don’t touch the image\npadded conv, we pad the boundary and the conv,\n\nzero, wrap, clamp. mirror"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#correlation-vs-convolution-1",
    "href": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#correlation-vs-convolution-1",
    "title": "Deep Learning 4",
    "section": "8 Correlation vs Convolution",
    "text": "8 Correlation vs Convolution\n\nBoth correlation and convolution are linear shift invariant (LSI) operators , which obey both the superposition principle (Linearity) h\\circ(f_{0}+f_{1})=h\\circ f_{o}+h\\circ f_{1} and the shift invariance principle  \\text{If  \\;\\;}g(i,j)=f(i+k,j+l)\\leftrightarrow(h\\circ g)(i,j)=(h\\circ f)(i+k,j+l)\nwhich means that shifting a signal commutes with applying the operator.\nIs the same as saying that the effect of the operator is the same everywhere.\nWhat’s the difference?\n\nCommutativity?\n\nConvolution is commutative, but correlation is not.\n\nAssociativity?\n\nConvolution is associative, but correlation is not."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#convolution-a-linear-operator",
    "href": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#convolution-a-linear-operator",
    "title": "Deep Learning 4",
    "section": "9 Convolution: A Linear Operator",
    "text": "9 Convolution: A Linear Operator\n\nCommutative: a * b = b * a\n\nConceptually no difference between filter and signal\n\nAssociative: a * (b * c) = (a * b) * c\n\nOften apply several filters one after another: (((a * b1) * b2) * b3)\nThis is equivalent to applying one filter: a * (b1 * b2 * b3)\n\nDistributes over addition: a * (b + c) = (a * b) + (a * c)\nScalars factor out: ka * b = a * kb = k (a * b)\nIdentity: unit impulse e = [\\dots, 0, 0, 1, 0, 0, \\dots], a * e = a"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#separable-filters",
    "href": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#separable-filters",
    "title": "Deep Learning 4",
    "section": "10 Separable Filters",
    "text": "10 Separable Filters\n\nThe process of performing a convolution requires K ^2 operations per pixel, where K is the size (width or height) of the convolution kernel.\nIn many cases, this operation can be speed up by first performing a 1D horizontal convolution followed by a 1D vertical convolution, requiring 2K operations.\nIf this is possible, then the convolution kernel is called separable\nAnd it is the outer product of two kernels K = vh^T\nHow can we tell if a given kernel K is indeed separable?\n\nLook at the singular value decomposition (SVD) , and if only one singular value is non zero, then it is separable K=\\mathbf{U}\\Sigma\\mathbf{V}^{T}=\\sum_{i}\\sigma_{i}u_{i}v_{i}^{T} with \\Sigma=\\mathrm{diag}(\\sigma_{i}) {\\sqrt{\\sigma_{1}}}\\mathbf{u}_{1} and {\\sqrt{\\sigma_{1}}}V_{1}^{T} are the vertical and horizontal kernels."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#image-filtering-edge-detection",
    "href": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#image-filtering-edge-detection",
    "title": "Deep Learning 4",
    "section": "11 Image Filtering: Edge Detection",
    "text": "11 Image Filtering: Edge Detection\n\nMap image from 2d array of pixels to a set of curves or line segments or contours .\nMore compact than pixels.\nLook for strong gradients, post process.\nEdges look like steep cliffs\nAn edge is a place of rapid change in the image intensity function\n\nAn edge is a place of rapid change in the image intensity function"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#derivatives-with-convolution",
    "href": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#derivatives-with-convolution",
    "title": "Deep Learning 4",
    "section": "12 Derivatives with convolution",
    "text": "12 Derivatives with convolution\n\nFor 2D function, f(x,y), the partial derivative is: \\frac{\\partial f(x,y)}{\\partial x}=\\operatorname*{lim}_{\\varepsilon\\to0}\\frac{f(x+\\varepsilon,y)-f(x,y)}{\\varepsilon}\nFor discrete data, we can approximate using finite differences: {\\frac{\\partial f(x,y)}{\\partial x}}\\approx{\\frac{f(x+1,y)-f(x,y)}{1}}\nTo implement above as convolution, what would be the associated filter?\n\nSobel Edge Detection Filter"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#image-gradient",
    "href": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#image-gradient",
    "title": "Deep Learning 4",
    "section": "13 Image gradient",
    "text": "13 Image gradient\n\nThe gradient of an image: {\\nabla}f=\\left[{\\frac{\\partial f}{\\partial x}},{\\frac{\\partial f}{\\partial y}}\\right]\nThe gradient points in the direction of most rapid change in intensity   \nThe gradient direction orientation of edge normal) is given by: \\theta=\\tan^{-1}\\left(\\frac{\\partial f}{\\partial y}\\bigg/\\frac{\\partial f}{\\partial x}\\right)"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#effects-of-noise",
    "href": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#effects-of-noise",
    "title": "Deep Learning 4",
    "section": "14 Effects of Noise",
    "text": "14 Effects of Noise\nConsider a single row or column of a noisy image\n\nPlotting intensity as a function of position gives a signal  \nWe are not able to find the edge.So we need to smoothen it first.We can use blurring here for proper smoothing.Once smoothing is done, we can apply, gradient operation. Mathematically \\displaystyle\\frac{\\partial }{\\partial x}( h*f)  \nNotice.Nothing is in convolution operation here.So instead of doing convolution on the image we can First, find the gradient of the convolution filter.And keep it and now, convolve this on the image.As convolution follows associative property, so it will produce the same result as earlier.Mathematically, it can be express as: \\displaystyle\\left(\\frac{\\partial }{\\partial x} h\\right)*f"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#going-beyond-edges",
    "href": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#going-beyond-edges",
    "title": "Deep Learning 4",
    "section": "15 Going Beyond Edges",
    "text": "15 Going Beyond Edges\n\nHow to combine these two images to form a panorama? \nThe answer is feature extraction and matching, Image alignment\nDetection : Identify the interest\nDescription : Extract vector feature descriptor around each interest point\nMatching : Determine correspondence between descriptors in two \nto find 6 values of rotaition matrix we nee only 3 pair points."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#more-motivation",
    "href": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#more-motivation",
    "title": "Deep Learning 4",
    "section": "16 More motivation",
    "text": "16 More motivation\n\nFeature points are used for:\n\nImage alignment (e.g., mosaics)\n3D reconstruction\nMotion tracking\nObject recognition\nIndexing and database retrieval\nRobot navigation"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#what-point-to-choose-for-feature-extraction",
    "href": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#what-point-to-choose-for-feature-extraction",
    "title": "Deep Learning 4",
    "section": "17 What point to choose for feature extraction",
    "text": "17 What point to choose for feature extraction\nWant uniqueness\n\nLook for image regions that are unusual: lead to unambiguous matches in other images\nHow to define “unusual”?\n\nLocal measure of uniqueness\n\nSuppose we only consider a small window of pixels\n\nWhat defines whether a feature is a good or bad - candidate?  \nHow does the window change when we shift it?\nShifting the window in any direction causes a big change \n\n\nA simple matching criteria\n\nConsider the below image: \n\nConsider shifting the window W by (u,v)\ncompare each pixel before and after by summing up the squared differences (SSD)\nWe can notice that if the window moves on the edge, there will not be any significant squared differences (SSD) here.But if the window moves on the corner then there will be some squared differences (SSD).\nThe squared distance is given as below: E(u,v)=\\sum_{(x,y)\\in W}\\left[I(x+v,y+v)-I(x,y)\\right]^{2}\nCompare two image patches using (weighted) summed square difference (also, called auto correlation function E_{A C}(\\Delta\\mathbf{u})=\\sum_{\\cdot}{\\mathbf{}}w(\\mathbf{p}_{i})[I_{0}(\\mathbf{p}_{i}+\\Delta u)-I_{0}(\\mathbf{p}_{i})]^{2}"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#how-to-select-an-interest-point",
    "href": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#how-to-select-an-interest-point",
    "title": "Deep Learning 4",
    "section": "18 How to select an interest point",
    "text": "18 How to select an interest point\n\nSmall motion assumption\nUsing a Taylor Series expansion I_{0}({\\bf p}_{i}+\\Delta{\\bf u})\\approx I_{0}({\\bf p}_{i})+\\nabla I_{0}({\\bf p}_{i})\\Delta{\\bf u} where \\nabla I_{0}(\\mathbf{p}_{i})=\\left({\\frac{\\partial I_{0}}{\\partial x}},{\\frac{\\partial I_{0}}{\\partial y}}\\right)(\\mathbf{p}_{i})\nthe image gradient. We can approximate the autocorrelation as \n\\begin{align*}\n  E_{A C}(\\Delta\\mathbf{u})&=\\sum_{\\cdot}{\\mathbf{}}w(\\mathbf{p}_{i})[I_{0}(\\mathbf{p}_{i}+\\Delta u)-I_{0}(\\mathbf{p}_{i})]^{2}\\\\\n  &\\approx\\sum_{\\cdot}{\\mathbf{}}w(\\mathbf{p}_{i})[I_{0}({\\bf p}_{i})+\\nabla I_{0}({\\bf p}_{i})\\Delta{\\bf u}-I_{0}(\\mathbf{p}_{i})]^{2}\\\\\n  &=\\sum_{\\cdot}{\\mathbf{}}w(\\mathbf{p}_{i})[\\nabla I_{0}({\\bf p}_{i})\\Delta{\\bf u}]^{2}\\\\\n  &=\\Delta \\mathbf u^T \\mathbf A\\Delta \\mathbf u\n\\end{align*}\n\nGradient can be computed with the filtering techniques we saw, e.g., derivatives of Gaussians.\nThe autocorrelation is E_{A C}(\\Delta\\mathbf{u}) = \\Delta \\mathbf u^T \\mathbf A\\Delta \\mathbf u with \\mathbf{A}=\\sum_{u}\\sum_{\\nu}w(u,v)\\left[{\\begin{array}{l l}{I_{x}^{2}}&{I_{x}J_{y}}\\\\ {I_{y}I_{x}}&{I_{y}^{2}}\\end{array}}\\right]=w*\\left[{\\begin{array}{l l}{I_{x}^{2}}&{I_{x}I_{y}}\\\\ {I_{y}I_{x}}&{I_{y}^{2}}\\end{array}}\\right] where we have replaced the weighted summations with discrete convolutions with the weighting kernel w."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#using-eigenvalues",
    "href": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#using-eigenvalues",
    "title": "Deep Learning 4",
    "section": "19 Using eigenvalues",
    "text": "19 Using eigenvalues\n\nThe eigenvalues of \\mathbf A reveal the amount of intensity change in the two principal orthogonal gradient directions in the window. \\mathbf{A}=\\mathbf{U}\\left[\\begin{array}{c c}{{\\lambda_{0}}}&{{0}}\\\\ {{0}}&{{\\lambda_{1}}}\\end{array}\\right]\\mathbf{U}^T with \\mathbf{A}\\mathbf{u}_{i}=\\lambda_{i}\\mathbf{u}_{i}\nIn case of matrix:\n\nAn example, here \\lambda is an eigen value\n\nClassification of image points using eigenvalues of \\mathbf A\n\nType of responses\n\nBut here we still need to calculate the eigenvalues. There is a method where we can compare the eigenvalues without even calculating it.\nWe know that the trace of a matrix is the summation of the eigenvalues and the determinant of the matrix is the multiplication of the eigenvalues. We will use the same property."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#harris-corner-detector",
    "href": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#harris-corner-detector",
    "title": "Deep Learning 4",
    "section": "20 Harris corner detector",
    "text": "20 Harris corner detector\n\nFirst compute the gradient at each point of the image.\nCompute \\mathbf A for each image window to get its cornerness scores\nCompute the eigenvalues/compute the following function M_c M_{c}=\\lambda_{1}\\lambda_{2}-\\kappa\\left(\\lambda_{1}+\\lambda_{2}\\right)^{2}=\\operatorname*{det}(A)-\\kappa\\ \\mathrm{trace}^{2}(A)\nFind points whose surrounding window gave large corner response (M_c > threshold).\nTake the points of local maxima, i.e., perform non maximum suppression."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#a-lot-of-other-interest-point-detectors",
    "href": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#a-lot-of-other-interest-point-detectors",
    "title": "Deep Learning 4",
    "section": "21 A lot of other interest point detectors",
    "text": "21 A lot of other interest point detectors\n\nHessian\nLowe:DoG\nLindeberg: scale selection\nMikolajczyk& Schmid : Hessian/Harris Laplacian/Affine\nTuyttelaars & Van Gool: EBR and IBR\nMatas: MSER\nKadir& Brady: Salient Regions\nSpeededUp Robust Features (SURF) of Bay et al."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#sift",
    "href": "Data_Science_Notes/Deep-Learning/2023-02-11-CS5480-week4.html#sift",
    "title": "Deep Learning 4",
    "section": "22 SIFT",
    "text": "22 SIFT\n\nStep 1: Scales-pace extrema Detection: Detect interesting points (invariant to scale and orientation) using DOG.\nStep 2:Keypoint Localization: Determine location and scale at each candidate location, and select them based on stability.\nStep 3: Orientation Estimation: Use local image gradients to assigned orientation to each localized key point . Preserve theta, scale and location for each feature.\nStep 4:Keypoint Descriptor: Extract local image gradients at selected scale around keypoint and form a representation invariant to local shape distortion and representation invariant to local shape distortion and illumination them.\nSIFT: Scale space Extrema Detection\n\nConstructing scale space\n\nD(x,y,\\sigma)=L(x,y,k\\sigma)-L(x,y,\\sigma) where L(x,y,\\sigma)=G(x,y,\\sigma)*I(x,y)\n\nDetermine the location and scale of keypoints to sub-pixel and sub-scale accuracy by fitting a 3D quadratic polynomial:\n\nKey point location: X_i = (x_i,y_i,\\sigma_i) Note: We are finding key points for each and every scale. If there is a corner at one scale, tt can look like an edge, at another scale.\noffset : \\Delta X = (x-x_i,y-y_i,\\sigma-\\sigma_i) Map the image back to the original location.\nSub-Pixel, Sub-scale, estimated location: X_i=X_i+\\Delta X \n\n\nSIFT: Keypoint Localization\n\nUse Taylor expansion to locally approximate D(x,y, \\sigma ) (i.e., DoG function) and estimate \\Delta X D(\\Delta X)=D(X_{i})+{\\frac{\\partial D^{T}(X_{i})}{\\partial X}}\\Delta X+{\\frac{1}{2}}\\Delta X^{T}\\,{\\frac{\\partial^2 D(X_{i})}{\\partial X^2}}\\Delta X\nFind the extrema of D( \\Delta X): \\frac{\\partial D(X_{i})}{\\partial X}+\\frac{\\partial ^{2}D(X_{i})}{\\partial X^{2}}\\Delta X=0 From above we get \\Delta X=-\\frac{\\displaystyle{\\partial}^{2}D^{-1}(X_{i})}{\\displaystyle{\\partial}X^{2}}\\frac{\\partial D(X_{i})}{\\displaystyle{\\partial}X}\n\\Delta X can be computed by solving a 3x3 linear system \\displaystyle\n\\begin{bmatrix}\n  \\displaystyle\\frac{\\partial^2 D}{\\partial \\sigma^2} &\\displaystyle \\frac{\\partial^2 D}{\\partial \\sigma y} &\\displaystyle \\frac{\\partial^2 D}{\\partial \\sigma x}\\\\\n  \\displaystyle\\frac{\\partial^2 D}{\\partial \\sigma y} &\\displaystyle \\frac{\\partial^2 D}{\\partial  y^2} &\\displaystyle \\frac{\\partial^2 D}{y x} \\\\\n  \\displaystyle\\frac{\\partial^2 D}{\\partial \\sigma x} &\\displaystyle \\frac{\\partial^2 D}{\\partial  yx} &\\displaystyle \\frac{\\partial^2 D}{ \\partial x^2}\n\\end{bmatrix}\n\\begin{bmatrix}\n  \\Delta \\sigma\\\\\n  \\Delta y\\\\\n  \\Delta x\\\\\n\\end{bmatrix}=-\n\\begin{bmatrix}\n\\displaystyle \\frac{\\partial D}{ \\partial \\sigma} \\\\\n\\displaystyle \\frac{\\partial D}{ \\partial y}  \\\\\n\\displaystyle \\frac{\\partial D}{ \\partial x}\n\\end{bmatrix}\n where \\begin{align*}\n  {\\frac{\\partial D}{\\partial\\sigma}}&={\\frac{D_{k+1}^{i,j}-D_{k-1}^{i,j}}{2}}\\\\\n  \\frac{{\\partial}^{2}D}{{\\partial}\\sigma^{2}}&=\\frac{D_{k-1}^{i,j}-2D_{k}^{i,j}+D_{k+1}^{i,j}}{1}\\\\\n  \\frac{\\partial^{2}D}{\\partial\\sigma y}\\!&=\\!\\frac{({D}_{k+1}^{i+1,j}-{D}_{k-1}^{i+1,j})\\!-\\!({D}_{k+1}^{i-1,j}-{D}_{k-1}^{i-1,j})}{4}\\\\\n\\end{align*} if \\Delta X>0.5 in any dimension, repeat.\nNext, reject the low contrast points and the points that lie on the edge\nLow contrast points elimination:\n\nif |D(X_{i}+\\Delta X)|<0.03 reject keypoint.\n\nassumes that image values have been normalized in [0,1]\n\n\nEdge elimination\n\nSimilar to Harris corner detector!\nSIFT instead uses Hessian \\mathbf{H}={\\left[\\begin{array}{l l}{D_{x x}}&{D_{x y}}\\\\ {D_{x y}}&{D_{y y}}\\end{array}\\right]} \\begin{align*}\n  \\operatorname{Tr}(\\mathbf{H})&=D_{x x}+D_{y y}=\\alpha+\\beta,\\\\\n  \\operatorname{Det}({\\bf H})&=D_{x x}D_{y y}-(D_{x y})^{2}=\\alpha\\beta.\n\\end{align*} Hence {\\frac{\\operatorname{Tr}(\\mathbf{H})^{2}}{\\operatorname{Det}(\\mathbf{H})}}={\\frac{(\\alpha+\\beta)^{2}}{\\alpha\\beta}}={\\frac{(r\\beta+\\beta)^{2}}{r\\beta^{2}}}={\\frac{(r+1)^{2}}{r}}, Reject key point if \\frac{\\mathrm{Tr}({\\bf H})^{2}}{\\mathrm{Det}({\\bf H})}<\\frac{(r+1)^{2}}{r} (SIFT uses r = 10) (r=\\frac{\\alpha}{\\beta})\n\n\n\nSIFT: Orientation Assignment\n\nUse scale of point to choose correct image: L(x,y)=G(x,y,\\sigma)*I(x,y)\nCompute gradient magnitude and orientation using finite differences: \\begin{align*}\n  m\\bigl(x,y\\bigr)&=\\sqrt{\\bigl(L(x+1,y)-L\\bigl(x-1,y\\bigr)\\bigr)^{2}+\\bigl(L(x,y+1)-L\\bigl(x,y-1\\bigr)\\bigr)^{2}}\\\\\n  \\theta(x,y)&=\\tan^{-1}\\left(\\frac{\\left(L(x,y+1)-L(x,y-1)\\right)}{\\left(L(x+1,_{,}y)-L(x-1,_{,}y)\\right)}\\right)\n\\end{align*}\nUse this to compute final descriptor.\nCreate histogram of gradient directions, within a region around the keypoint, at selected scale: L(x,y,\\sigma)=G(x,y,\\sigma){*}\\,J(x,y) and m(x,y)={\\sqrt{(L(x+1,y)-L(x-1,y))^{2}+(L(x,y+1)-L(x,y-1))^{2}}} and \\theta(x,y)=a\\tan2{\\big(}(L(x,y+1)-L(x,y-1){\\big)}/(L(x+1,y)-L(x-1,y){\\big)})\n\nHistogram entries are weighted by ( i ) gradient magnitude and (ii) a Gaussian function with \\sigma equal to 1.5 times the scale of the keypoint\n\nSIFT: Feature descriptor\n\nCompute the gradient at each pixel in a 16 \\times 16 window around the detected keypoint, using the appropriate level of the Gaussian pyramid at which the keypoint was detected.\nDownweight gradients by a Gaussian fall off function (blue circle) to reduce the influence of gradients far from the center.\nIn each 4 x 4 quadrant, compute a gradient orientation histogram using 8 orientation histogram bins.\n\nThe resulting 128 nonnegative values form a raw version of the SIFT descriptor vector.\nTo reduce the effects of contrast or gain (additive variations arealready removed by the gradient), the 128 D vector is normalized tounit length.\nGreat engineering effort!\nExtraordinarily robust matching technique\n\nChanges in viewpoint: up to about 60 degree out of plane rotation\nChanges in illumination: sometimes even day vs. night\nFast and efficientcan run in real time\nLots of code available\n\n\nReview\n\nSURF = Speeded Up Robust Features\n\nUses box filters instead of Gaussians to approximate Laplacians\nUses wavelets to get keypoint orientations\nFew more changes (including indescriptor generation)\nLeads to 3x speedup over SIFT\n\nMore on image features: LBP Computer Vision\n\nLBP = Local Binary Patterns\n\n\n \\tiny {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week7.html",
    "href": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week7.html",
    "title": "Deep Learning 7",
    "section": "",
    "text": "Traditional feed-forward network assume that all inputs and outputs are independent of each other.\nCounterexample language/speech modeling\n\nPredicting the next word in a sentence depends on the entire sequence of words before the current word\nExample: “The man who wore a wig on his head went inside”\n\nWho went inside? Man or wig?\n\n\nUse the same weights/layers for every time step (hence, recurrent ), and pass on the output to the next time step Courtesy:  \nA recurrent neural network and the unfolding in time of the computation involved in its forward computation  \nWe can notice that \n\\begin{align*}\n  s_{t}&=\\operatorname{tanh}(U x_{t}+W s_{t-1})\\\\\n  \\hat{y}_{t}&=\\mathrm{softrnax}(Vs_{t})\\\\\n  E_{t}(y_{t},\\hat{y_{t}})&=-y_{t}\\log\\hat{y_{t}} \\\\\n  E(y,{\\hat{y}})&=\\sum_{t}E_{t}(y_{t},{\\hat{y}}_{t})\\\\\n  &=-\\sum_{t}y_{t}\\log\\;{\\hat{y}}_{t}\n\\end{align*}\n\nKinds of RNNs"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week7.html#backprop-through-time-bptt",
    "href": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week7.html#backprop-through-time-bptt",
    "title": "Deep Learning 7",
    "section": "2 Backprop Through Time (BPTT)",
    "text": "2 Backprop Through Time (BPTT)\n\nUnrolled RNN  \nGoal\n\nCalculate error gradients w.r.t U, V and W\nLearn weights using SGD\n\nJust like we sum up errors, we also sum up gradients at each time step for one training example {\\frac{\\partial E}{\\partial W}}=\\sum_{t}{\\frac{\\partial E_{t}}{\\partial W}}\n\n\\begin{align*}\n    \\frac{\\partial E_{3}}{\\partial V}&=\\frac{\\partial E_{3}}{\\partial\\hat{y}_{3}}\\frac{\\partial\\hat{y}_{3}}{\\partial V}\\\\\n    &=\\frac{\\partial E_{3}}{\\partial\\hat{y}_{3}}\\frac{\\partial\\hat{y}_{3}}{\\partial z_{3}}\\frac{\\partial z_{3}}{\\partial V}\\\\\n    &=(\\hat y_{3}-y_{3})\\otimes s_{3}\n\\end{align*}\nHere - \\otimes is outer product - \\frac{\\partial E_{3}}{\\partial\\hat{y}_{3}} = \\hat y_{3}-y_{3} (if we use mean square type of error) - \\frac{\\partial\\hat{y}_{3}}{\\partial z_{3}} is ignored, assuming we are not using the activation function - \\frac{\\partial z_{3}}{\\partial V}=s_{3}\n \n{\\frac{\\partial E_{3}}{\\partial W}}={\\frac{\\partial E_{3}}{\\partial{\\hat{y}}_{3}}}{\\frac{\\partial{\\hat{y}}_{3}}{\\partial s_{3}}}\\left[{\\frac{\\partial s_{3}}{\\partial W}}+{\\frac{\\partial s_{3}}{\\partial s_2}}{\\frac{\\partial s_{2}}{\\partial W}}+{\\frac{\\partial s_{3}}{\\partial s_2}}{\\frac{\\partial s_{2}}{\\partial s_1}}{\\frac{\\partial s_{1}}{\\partial W}}\\right]\nIt can be generalized days:\n{\\cfrac{\\partial E_{3}}{\\partial W}}=\\sum_{k=0}^{3}{\\cfrac{\\partial E_{3}}{\\partial{\\hat{y}}_{3}}}{\\cfrac{\\partial{\\hat{y}}_{3}}{\\partial s_{3}}}{\\cfrac{\\partial s_{3}}{\\partial s_{k}}}{\\cfrac{\\partial s_{k}}{\\partial W}}\n\\cfrac{\\partial E_{3}}{\\partial U} can be calculate in same was \\cfrac{\\partial E_{3}}{\\partial W}\ns_{3}=\\operatorname{tanh}(U x_{t}+W s_{2})\nSimilar to backprop , we can define: \\delta_{2}^{(3)}\\,=\\,\\frac{\\partial E_{3}}{\\partial z_{2}}\\,=\\frac{\\partial E_{3}}{\\partial s_{3}}\\frac{\\partial s_{3}}{\\partial s_{2}}\\frac{\\partial s_{2}}{\\partial z_{2}} here we assume z_{2}=U x_{2}+W s_{1} and s_2 is obtained by applying some activation function on z_2\n\\frac{\\partial E}{\\partial W}=\\sum_{i=0}^T \\frac{\\partial E_i }{\\partial W}\\propto\\sum_{i=0}^T \\left(\\prod_{i=k+1}^y \\frac{\\partial s_i }{\\partial s_{i-1} }\\right)\\frac{\\partial s_k }{\\partial W}\n\nProblems\n\nVanishing gradient \\left\\|{\\frac{\\partial s_{i}}{\\partial s_{i-1}}}\\right\\|_{2}<1\nExploding gradient \\left\\|{\\frac{\\partial s_{i}}{\\partial s_{i-1}}}\\right\\|_{2}>1\n\n\nvanishing gradient problem:\n{\\frac{\\partial E_{3}}{\\partial W}}=\\sum_{k=0}^{3}{\\frac{\\partial E_{3}}{\\partial{\\hat{y}}_{3}}}{\\frac{\\partial{\\hat{y}}_{3}}{\\partial s_{3}}}\\left(\\prod_{j=k+1}^{3}{\\frac{\\partial s_{j}}{\\partial s_{j-1}}}\\right){\\frac{\\partial s_{k}}{\\partial W}} - For sigmoid activation function gradient age upper bounded by 1. - it means that gradient will vanish overtime if there is a long range dependency - it can be solved using truncated gradient descent but this is inefficient. And this issue is solved by LSTM up to some extent\nexploding gradient problem\n\nif the weights are high the gradient will explode.\nbut this problem can easily be solved by clipping the gradient."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week7.html#long-short-term-memory-lstm",
    "href": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week7.html#long-short-term-memory-lstm",
    "title": "Deep Learning 7",
    "section": "3 Long Short Term Memory (LSTM)",
    "text": "3 Long Short Term Memory (LSTM)\n\n\nInput gate: scales input to cell (write)\nOutput gate: Scales output from cell (read)\nForget gate: Scales old cell value (reset)\n\n\\begin{array}{l}\ni=\\sigma (x_t U^i +s_{t-1} W^i )\\\\\nf=\\sigma (x_t U^f +s_{t-1} W^f )\\\\\no=\\sigma (x_t U^o +s_{t-1} W^g )\\\\\ng=\\tanh (x_t U^g +s_{t-1} W^g )\\\\\nc_t =c_{t-1} \\circ f+g\\circ i\\\\\ns_t =\\tanh \\left(c_t \\right)\\circ o\n\\end{array}\n\nhere bias is ignored.\nfor all the garing function sigmoid activation fucntion is used, and for others tanh is used. sigmoid is used for gating, becasue for gate we need values between 0 and 1.\n\n\n\nif he picks the input gate to always one and the forget gate to always zero, then we get the fucntion of almost RNN.\nalmost because tanh is added.\n\n\n\n\nnow if we have a closer look at c_t =c_{t-1} \\circ f+g\\circ i, we can see that the sales state doesnt only depend on the gated inputbut it also depends on the previous cell state. it means there is a skip connection between previous state to current state. it helps the gradient flow for long run"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week7.html#lstm-variant",
    "href": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week7.html#lstm-variant",
    "title": "Deep Learning 7",
    "section": "4 LSTM variant",
    "text": "4 LSTM variant\npeepholes LSTM\n\n\nLSTM with peephole connections \\begin{array}{l}{{f_{t}=\\sigma\\left(W_{f}\\cdot[C_{t-1},h_{t-1},x_{t}]\\right)}}\\\\\n{{{i}_{t}=\\sigma\\left(W_{i}\\cdot[C_{t-1},h_{t-1},x_{t}]\\ +\\ b_{i}\\right)}}\\\\  \n{{o_{t}=\\sigma\\left(W_{o}\\cdot[C_{t},h_{t-1},x_{t}]\\ +\\ b_{\\sigma}\\right)}}\\end{array}\nCoupled forget and input gates C_{t}=f_{t}\\star C_{t-1}+(1-f_{t})\\ast{\\bar{C}}_{t}"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week7.html#grus",
    "href": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week7.html#grus",
    "title": "Deep Learning 7",
    "section": "5 GRUs",
    "text": "5 GRUs\n\n\\begin{align*}\n    z&=\\sigma(x_{t}U^{z}+s_{t-1}W^{z})\\\\\n    r&=\\sigma(x_{t}U^{r}+s_{t-1}W^{r})\\\\\n    h&=\\tanh(x_{t}U^{h}+\\left(s_{t-1}\\circ r)W^{h}\\right)\\\\\n    s_{t}&=(1-z)\\circ h+z\\circ s_{t-1}\n\\end{align*}\n\nReset gate: defines how much of the previous memory to keep around.\nUpdate gate: determines how to combine the new input with the previous memory.\nA GRU has two gates, an LSTM has three gates.\nIn GRUs\n\nNo internal memory (c_t) different from the exposed hidden state.\nNo output gate as in LSTMs.\nThe input and forget gates of LSTMs are coupled by an update gate in GRUs, and the reset gate (GRUs) is applied directly to the previous hidden state.\n\n\n\n\nif the reset rate is set to all 1 and update get is set to all 0, then it behaves like RNN"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week7.html#example-applications",
    "href": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week7.html#example-applications",
    "title": "Deep Learning 7",
    "section": "6 Example Applications",
    "text": "6 Example Applications\nLanguage Modeling and Generating Text\n\nGiven a sequence of words we want to predict the probability of each word given the previous words\nCan also be looked at as a generative model = fun!\nTypical input is a sequence of words; output = sequence of predicted words\n\nDuring training, o_t=x_{t-1}\n\n\nMachine Translation\n\nSequence of words in source language \\Rightarrow Sequence of words in target language\n\nSpeech Recognition\n\nGiven an input sequence of acoustic signals from a sound wave, predict a sequence of phonetic segments together with their probabilities.\n\nCombining CNNs and RNNs: Image Captioning\n\nthe Image can be passed to a pre-trained network and the output of that network can be passed to RNN."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week7.html#attention-and-transformers",
    "href": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week7.html#attention-and-transformers",
    "title": "Deep Learning 7",
    "section": "7 Attention and Transformers",
    "text": "7 Attention and Transformers\nHistory:\nVisual attention computation model\n\nAttention modeled as a way to focus on parts of images   \nNotions of top-down and bottom-up attention\n\nbottom up attention: Suppose we are going somewhere, and something attractive passes in front of us, we pay attention to that, so here we pay attention to any content which is attractive we are not selective here.\ntop down attention: Now suppose we are searching for something very passionately, now we will not be attracted if other things comes in front of us, we will pay attention only if we see what we are looking for.\n\n\nSpatial Transformer Networks\n\nhere author introduced a kind of localization network and grid generator, which creates the attention mechanism and adds this to the original net of path. \n\nAttention in Image Captioning\n\nGenerate captions from an input image using the attention mechanism.\nVGGnet as encoder for extracting features from input image\nLSTM decoder generates captions by producing one word at each time step by using the context vector, previous hidden state and previously generated words. \nHere the network predicts attention and the distribution over vocab, the attention is then multiplied with the features as shown in the epoch figure.\nz_2 can get either a_1 attended feature or a_1 and a_2 attended feature. ( shown in the above figure )\nContext Vector : The context vector z_t is computed by a function \\phi using the attention vectors a_i and the positive weights \\alpha_i, i = 1, 2, \\dots,L correspond to different locations of the input image. \\begin{align*}\n  e_{t i}&=\\,f_{a t t}(\\mathrm{a}_{i},\\mathrm{h}_{t-1})\\\\\n  \\alpha_{t i}&={\\frac{e x p(e_{t i})}{\\sum_{k=1}^{L}e x p(e_{t k})}}\\\\\n  \\hat{z}_{t}&=\\phi(\\{\\mathbf{a}_{i}\\},\\{\\alpha_{i}\\})\n  \\end{align*}\nAt each location i, the weight \\alpha_i for the annotation vector a_i is calculated by an attention mechanism f_{att}.\n\\alpha_i can be interpreted in two ways based on the two versions of this attention mechanism, namely the stochastic Hard and the deterministic Soft\nHard attention:\n\nIn hard attention, to generate the word y_i, the weight \\alpha_i is seen as the probability that location i is the correct place to focus on.\ns_t is defined as the location variable that determines where the attention should be focused while producing the t^{th} word.\nThey assign a multinoulli distribution with weights \\alpha_i as the parameters and view \\hat z_t as a random variable. p(s_{t i}=1|s_{j<t},\\mathbf{a})=\\alpha_{t,i} \\hat{\\mathbf{z}}_{t}=\\sum_{t}s_{t,i}\\mathbf{a}_{i}\nWe can’t simply train the model with back propagation while using hard attention as we probabilistically sample from one of the attention vectors a_i and use that as the context vector \\hat z_t.\n\nSoft Attention:\n\nIn hard attention, to generate the word y_i, the weight \\alpha_i is seen as as the relative importance given to location i in combining the attention vectors a^′_i s.\nInstead of sampling the location vector st every time as in the hard attention, an expectation of the context vector \\hat z_t is taken and a deterministic attention model is formulated as: \\mathbb{E}_{p(s_{t}|a)}[{\\hat{\\mathbf{E}}}_{t}]=\\sum_{i=1}^{L}\\alpha_{t,i}\\mathbf{a}_{i} \\phi(\\{\\mathbf{a}_{i}\\},\\{\\alpha_{i}\\})=\\sum_{i}^{L}\\alpha_{i}\\mathbf{a}_{i}\nAs this model is differentiable under the deterministic attention, it can be trained end-to-end using back propagation.\n\nHard vs Soft Attention  \n\nDRAW: Deep Recurrent Attentive Writer\n\nhere they used variational autoencoder, it is a generative model.\nthey shown that image can be generated in recurrent way.\nattention was implemented using read and write module as shown in the figure. \nthe method was learning the grid, where to focus on the image  \nThe grid has central coordinate as (g_x,g_y) and \\delta is the gap between the grid locations,\nAt each grid location they implement a gaussian with the variance \\sigma\nNow the network can learn which area of the image to focus at, it can learn (g_x,g_y) adjust the \\delta and can manage to focus on small part or large part the image, and \\sigma tells how much of the local neighbor to take into account. (\\tilde{g}_{X},\\tilde{g}_{Y},\\log\\sigma^{2},\\log\\tilde{\\delta},\\log\\gamma)=\\,W(h^{d e c})\n\nself-attention\n\nLargely inspired by the need in machine translation\nEncoder-becoder DNN models (RNNs, LSTMs, GRUs) were used for neural machine translation then\n\nEncoder takes input sentence and creates a summary (last hidden layer also called Context Vector ).\nDecoder translates input sentence sequentially by processing the summary.\n\nProblems with this approach:\n\nTranslation quality depends on quality of summary.\nRNNs/LSTMs create bad summaries for longer sentences (long-range dependency problem).\nWe can’t give more importance to a set of words compared to others in the input sentence\n\n\nOrigin of Self-Attention and Transformers\n\nIntroduced by Bahdanau et al to improve the encoder-decoder based neural machine translation in NLP \nThey use a BiRNN as encoder and generate annotation h_j by concatenating forward and backward hidden states.\nEach annotation h_j contains information about the whole input sequence with more emphasis around the j^{th} word.\nTo focus on all input word embeddings while creating a context vector, they use a weighted sum of the hidden states rather than the final hidden state.\nFor output word y_i, context vector c_i is the weighted sum of annotations h_j c_{i}=\\sum_{j=1}^{T_{x}}\\alpha_{i j}h_{j}\nThe weight \\alpha_{ij} for a given annotation h_j is: \\alpha_{i j}=\\frac{\\exp(e_{i j})}{\\sum_{k=1}^{T_{x}}\\exp(e_{i k})} where e_{i j}=a(s_{i-1},h_{j}) is an alignment model that finds how well the j^{th} input matches with the i^{th} output."
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week7.html#motivation-for-transformers",
    "href": "Data_Science_Notes/Deep-Learning/2023-03-18-CS5480-week7.html#motivation-for-transformers",
    "title": "Deep Learning 7",
    "section": "8 Motivation for Transformers",
    "text": "8 Motivation for Transformers\n\nSequential computation prevents parallelization  \nDespite GRUs and LSTMs, RNNs still need attention mechanism to deal with long-range dependencies – path length for co-dependent computation between states grows with sequence length.\nBut if attention gives us access to any state, maybe we don’t need the RNN?!\n\n \\tiny {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-01-07-CS5480-week1.html",
    "href": "Data_Science_Notes/Deep-Learning/2023-01-07-CS5480-week1.html",
    "title": "Deep Learning 1",
    "section": "",
    "text": "Feed Forward McCulloch-Pitts net can compute any Boolean function f : \\{0,1\\}^n \\rightarrow \\{0,1\\} Recursive McCulloch-Pitts networks can simulate any Deterministic Finite Automaton (DFA). McCulloch-Pitts networks can be represented mathematically as shown below:\n\ny(x_1,x_2,\\dots,x_{n+m},\\theta) = \\begin{cases}\n   1 &\\displaystyle \\text{if } \\sum_{i=1}^n x_i \\ge \\theta  \\text{ and  } \\sum_{i=n+1}^m x_i =0\\\\\n   0 &\\displaystyle \\text{if } \\sum_{i=1}^n x_i < \\theta  \\text{ or  } \\sum_{i=n+1}^m x_i >0\n\\end{cases}\n\n\nHere all signals are binary\nThreshold \\theta \\in \\mathbb{N}\nn excitatory inputs x_1,\\dots,x_n\nm inhibitory inputs x_{n+1},\\dots,x_{n+m}\nOne output y\n\n\n\n\nRosenblatt’s Perceptron can be represented mathematically as shown below: \nf(x) = \\begin{cases}\n   1 &\\text{if } w\\cdot x+b>0 \\\\\n   0 &\\text{otherwise }\n\\end{cases}\n\nBoolean Gates using a Perceptron\n\n\nHebb, in his influential book The organization of Behavior (1949), claimed\n\nBehavior changes are primarily due to the changes of synaptic strengths (w_{ij}) between neurons i and j\nHebbian learning law: w_{ij} increases only when both i and j are “on”.\n“Neurons that fire together, wire together. Neurons that fire out of sync, fail to link”\nIn perceptron learning, Hebbian law can be restated as:\n\nw_{ij} increases only if the outputs of both units x_i and y have the same sign.\n\nIn our simple network (one output and n input units) \\nabla W_{ij}=\\nabla W_{ij}(\\text{new})-\\nabla W_{ij}(\\text{old})=X_iY Or \\nabla W_{ij}=\\nabla W_{ij}(\\text{new})-\\nabla W_{ij}(\\text{old})=\\alpha X_iY\n\n\n\n\n\nInitialization: b = 0, w_i = 0, i = 1 \\text{ to } n\nFor each of the training sample (x,y), do the following:\nUpdate weight and bias as : \\begin{align*}\nw_i&:=w_i+x_i\\times y_i, i=1 \\text{ to } n \\\\\nb&:=b+x_i \\times y\n\\end{align*}\n\nWe can obtain AND function using bipolar units (-1,-1). In this case a correct boundary -1 +x_1+x_2=0 shall we learned.  But we can not obtain AND function using Binary Unit (1,0) and \\alpha =1. In this case an incorrect boundary 1+x_1+x_2=0 is learned.\n\n\n\n\nInitialization: b = 0, w_i = 0, i = 1 \\text{ to } n\nWhile stop condition is false do step 2 to 4\nFor each of the training sample (x,y) do steps 3 to 4\ncompute output of perceptron, o\nif o\\ne y \\begin{align*}\nw_i&:=w_i+\\alpha \\times x_i \\times o ,\\quad i= 1 \\text{ to } n \\\\\nb&:=b+\\alpha \\times o\n\\end{align*}\n\n\n\nLearning occurs only when a sample has o\\ne y\nTwo loops, a completion of the inner loop (each sample is used once) is called an epoch\nStop when\n\nWhen no weight is changed in the current epoch, or\nWhen pre-determined number of epochs is reached\n\n\n\n\n\n\n\nIt is also called the delta learning rule.\nDeveloped for a perceptron.\nLearning algorithm: same as Perceptron learning, just change step 4 as below \\begin{align*}\nw_i&:=w_i+\\alpha \\times x_i \\times \\boxed{(o-y)} \\quad i= 1 \\text{ to } n \\\\\nb&:=b+\\alpha \\times \\boxed{(o-y)}\n\\end{align*}\n\n\n\n\n\nIf there exists an exact solution (if the training data set is linearly separable), then the perceptron learning algorithm is guaranteed to find an exact solution in a finite number of steps.\n\n\n\n\nConsider the above image\n\nPerceptron tries to find the boundary W^TX=0\nAngle between W and any point X which lies on that line will be 90^{\\circ}\nAngle between positive points (p_1,p_2,p_3,\\dots) and W shall be less than 90^{\\circ}\nAngle between negative points (n_1,n_2,n_3,\\dots) and W shall be greater than 90^{\\circ}\nIf for any positive point say p_3 the angle with W becomes more than 90^{\\circ}, it means perceptron has made a mistake\nBecause there is a mistake so the perceptron learning algorithm now tries to update the weight, with the rule \\overline {W}=W+ \\alpha X\\times Y, consider \\alpha and Y to be 1 for simplicity. So it becomes \\overline W=W+ X\nThe cos angle between p_3 and \\overline W is proportional to \\overline W^TX \\begin{align*}\n\\cos \\theta &\\propto \\overline W^TX \\\\\n&\\propto ( W+X)^TX \\\\\n&\\propto  W^TX+X^TX\n\\end{align*}\n\nSince X^TX is positive have we can say that the value of \\cos \\theta for \\overline W and X is more than that of W and X\nHence we can say that the value of angle \\theta for \\overline W and X is less than that of W and X\nwe can see that the angle between p_3 and W is decreased after weight update\nHence we can say that after certain number of iteration the angle between p_3 and W will become less than 90^{\\circ}\n\n\n\n\n\n\nLogistic regression and perceptron with sigmoid activation are the same.\nLinear SVMs and perceptrons are ‘almost’ the same. ‘Almost’ because SVM provides best margin but preceptron doesn’t have such notion.\n\n\n\n\nXOR can be solved by a more complex network with hidden units (Multi-Layer Perceptron). But it can not be solved using one layer perceptron.\nConsider the below picture:\n\nBelow is the output for above network considering inputs (x_1,x_2) as (-1,-1),(-1,1),(1,-1),(1,1)\n\n\nCode\nimport numpy as np\nimport plotly.express as px\nimport plotly.io as pio\nimport pandas as pd\npio.renderers.default = \"plotly_mimetype+notebook_connected\" # this is reqired only to render html not for notebook\n\nx_1=np.array([-1,-1,1,1])\nx_2=np.array([-1,1,-1,1])\nz_1=np.where((2*x_1-2*x_2)>=1,1,-1)\nz_2=np.where((-2*x_1+2*x_2)>=1,1,-1) \ny=np.where((2*z_1+2*z_2)>=0,1,-1) \ndf = pd.DataFrame(data=np.vstack([x_1.flatten(),x_2.flatten(),z_1.flatten(),z_2.flatten(),y.flatten()]).T,\ncolumns=['x_1','x_2','z_1','z_2','class'])\nfig = px.scatter( df,x='x_1', y='x_2', color='class')\nfig.update_layout(autosize=False,width=400,height=400,coloraxis_showscale=False)\nfig.show()\n\n\n\n                                                \n\n\nIn Depth analysis\nOutput of Layer one for z_1\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nnx, ny = (100, 100)\nx = np.linspace(-5, 5, nx)\ny = np.linspace(-5, 5, ny)\nx_1, x_2 = np.meshgrid(x, y)\nz_1=np.where((2*x_1-2*x_2)>=1,1,-1)\nz_2=np.where((-2*x_1+2*x_2)>=1,1,-1) \ny=np.where((2*z_1+2*z_2)>=0,1,-1) \ndf = pd.DataFrame(data=np.vstack([x_1.flatten(),x_2.flatten(),z_1.flatten(),z_2.flatten(),y.flatten()]).T,\ncolumns=['x_1','x_2','z_1','z_2','class'])\n\n\n\n\nCode\nfig = px.scatter( df,x='x_1', y='x_2', color='z_1')\nfig.update_layout(autosize=False,width=600,height=600,coloraxis_showscale=False)\nfig.show()\n\n\n\n                                                \n\n\nOutput of Layer one for z_2\n\n\nCode\nfig = px.scatter( df,x='x_1', y='x_2', color='z_2')\nfig.update_layout(autosize=False,width=600,height=600,coloraxis_showscale=False)\nfig.show()\n\n\n\n                                                \n\n\nFinal Layer output for y\n\n\nCode\nimport plotly.express as px\nfig = px.scatter( df,x='x_1', y='x_2', color='class')\nfig.update_layout(autosize=False,width=600,height=600,coloraxis_showscale=False)\nfig.show()\n\n\n\n                                                \n\n\nwe can see that the perceptron with two layers is able to classify the XOR data, it means it has non linearity, just to understand it more deeper if we remove thresholding at the neuron we get output as below:\nOutput of Layer one for z_1\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nnx, ny = (100, 100)\nx = np.linspace(-5, 5, nx)\ny = np.linspace(-5, 5, ny)\nx_1, x_2 = np.meshgrid(x, y)\nz_1=2*x_1-2*x_2 \nz_2=-2*x_1+2*x_2 \ny=2*z_1+2*z_2 \ndf = pd.DataFrame(data=np.vstack([x_1.flatten(),x_2.flatten(),z_1.flatten(),z_2.flatten(),y.flatten()]).T,\ncolumns=['x_1','x_2','z_1','z_2','class'])\n\n\n\n\nCode\nfig = px.scatter( df,x='x_1', y='x_2', color='z_1')\nfig.update_layout(autosize=False,width=600,height=600)\nfig.show()\n\n\n\n                                                \n\n\nOutput of Layer one for z_2\n\n\nCode\nfig = px.scatter( df,x='x_1', y='x_2', color='z_2')\nfig.update_layout(autosize=False,width=600,height=600)\nfig.show()\n\n\n\n                                                \n\n\nFinal Layer output for y\n\n\nCode\nimport plotly.express as px\nfig = px.scatter( df,x='x_1', y='x_2', color='class')\nfig.update_layout(autosize=False,width=600,height=600)\nfig.show()"
  },
  {
    "objectID": "Data_Science_Notes/Deep-Learning/2023-01-07-CS5480-week1.html#recall-training-a-neural-network",
    "href": "Data_Science_Notes/Deep-Learning/2023-01-07-CS5480-week1.html#recall-training-a-neural-network",
    "title": "Deep Learning 1",
    "section": "2 Recall: Training a Neural Network",
    "text": "2 Recall: Training a Neural Network\nIt was already discuss here in Machine Learning.\n \\tiny {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}"
  },
  {
    "objectID": "Data_Science_Notes/index.html",
    "href": "Data_Science_Notes/index.html",
    "title": "Data Science Notes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nDeep Learning 7\n\n\n\n\n\n\n\nDeep Learning\n\n\n\n\nRecurrent Neural Networks, Attention and Transformers\n\n\n\n\n\n\nMar 25, 2023\n\n\nAbhishek Kumar Dubey\n\n\n\n\n\n\n  \n\n\n\n\nDeep Learning 6\n\n\n\n\n\n\n\nDeep Learning\n\n\n\n\nresidual neural network,explainable AI\n\n\n\n\n\n\nMar 18, 2023\n\n\nAbhishek Kumar Dubey\n\n\n\n\n\n\n  \n\n\n\n\nDeep Learning 5\n\n\n\n\n\n\n\nDeep Learning\n\n\n\n\nConvolutional Neural Networks\n\n\n\n\n\n\nMar 4, 2023\n\n\nAbhishek Kumar Dubey\n\n\n\n\n\n\n  \n\n\n\n\nDeep Learning 4\n\n\n\n\n\n\n\nDeep Learning\n\n\n\n\nFiltering, correlation and convolution.\n\n\n\n\n\n\nFeb 11, 2023\n\n\nAbhishek Kumar Dubey\n\n\n\n\n\n\n  \n\n\n\n\nDeep Learning 3\n\n\n\n\n\n\n\nDeep Learning\n\n\n\n\nRegularization and Generalization.\n\n\n\n\n\n\nFeb 4, 2023\n\n\nAbhishek Kumar Dubey\n\n\n\n\n\n\n  \n\n\n\n\nDeep Learning 2\n\n\n\n\n\n\n\nDeep Learning\n\n\n\n\nOptimizers and activation functions\n\n\n\n\n\n\nJan 21, 2023\n\n\nAbhishek Kumar Dubey\n\n\n\n\n\n\n  \n\n\n\n\nDeep Learning 1\n\n\n\n\n\n\n\nDeep Learning\n\n\n\n\nHistory, Perceptrons, Neural Networks and Backpropagation\n\n\n\n\n\n\nJan 7, 2023\n\n\nAbhishek Kumar Dubey\n\n\n\n\n\n\n  \n\n\n\n\nMachine Learning 9\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\nSupport Vector Regression,Linear Regression\n\n\n\n\n\n\nNov 5, 2022\n\n\nAbhishek Kumar Dubey\n\n\n\n\n\n\n  \n\n\n\n\nMachine Learning 8\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\nIntroduction to Learning Theory, Regression Formulation\n\n\n\n\n\n\nOct 29, 2022\n\n\nAbhishek Kumar Dubey\n\n\n\n\n\n\n  \n\n\n\n\nMachine Learning 7\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\nClassifier System Design,Introduction to Learning Theory\n\n\n\n\n\n\nOct 15, 2022\n\n\nAbhishek Kumar Dubey\n\n\n\n\n\n\n  \n\n\n\n\nProbability Theory 5\n\n\n\n\n\n\n\nProbability Theory\n\n\n\n\nJoint, Conditional, Marginal, Discrete Convolution\n\n\n\n\n\n\nOct 15, 2022\n\n\nAbhishek Kumar Dubey\n\n\n\n\n\n\n  \n\n\n\n\nMachine Learning 6\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\nEnsemble Classifier\n\n\n\n\n\n\nOct 8, 2022\n\n\nAbhishek Kumar Dubey\n\n\n\n\n\n\n  \n\n\n\n\nLinear Algebra 3\n\n\n\n\n\n\n\nLinear Algebra\n\n\n\n\nIndependence, Basis, Space\n\n\n\n\n\n\nOct 1, 2022\n\n\nAbhishek Kumar Dubey\n\n\n\n\n\n\n  \n\n\n\n\nMachine Learning 5\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\nMulti Layer Perceptrons\n\n\n\n\n\n\nSep 24, 2022\n\n\nAbhishek Kumar Dubey\n\n\n\n\n\n\n  \n\n\n\n\nProbability Theory 4\n\n\n\n\n\n\n\nProbability Theory\n\n\n\n\nUniform, Exponential, Normal\n\n\n\n\n\n\nSep 24, 2022\n\n\nAbhishek Kumar Dubey\n\n\n\n\n\n\n  \n\n\n\n\nLinear Algebra 2\n\n\n\n\n\n\n\nLinear Algebra\n\n\n\n\nGaussian Elimination, Row-Echelon, Minus 1 Trick\n\n\n\n\n\n\nSep 17, 2022\n\n\nAbhishek Kumar Dubey\n\n\n\n\n\n\n  \n\n\n\n\nProbability Theory 3\n\n\n\n\n\n\n\nProbability Theory\n\n\n\n\nBinomial, Poisson\n\n\n\n\n\n\nSep 17, 2022\n\n\nAbhishek Kumar Dubey\n\n\n\n\n\n\n  \n\n\n\n\nMachine Learning 4\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\nSVM, Lagrange Multipliers, KKT condition, Mercer’s condition, Kernel Trick\n\n\n\n\n\n\nSep 10, 2022\n\n\nAbhishek Kumar Dubey\n\n\n\n\n\n\n  \n\n\n\n\nLinear Algebra 1\n\n\n\n\n\n\n\nLinear Algebra\n\n\n\n\nSystem of equation\n\n\n\n\n\n\nSep 3, 2022\n\n\nAbhishek Kumar Dubey\n\n\n\n\n\n\n  \n\n\n\n\nMachine Learning 3\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\nDecision Trees, Naive Bayes, Bayesian Belief network\n\n\n\n\n\n\nAug 27, 2022\n\n\nAbhishek Kumar Dubey\n\n\n\n\n\n\n  \n\n\n\n\nMachine Learning 2\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\nClassification Error, ROC/PR curve, KNN\n\n\n\n\n\n\nAug 20, 2022\n\n\nAbhishek Kumar Dubey\n\n\n\n\n\n\n  \n\n\n\n\nProbability Theory 2\n\n\n\n\n\n\n\nProbability Theory\n\n\n\n\nMass Function\n\n\n\n\n\n\nAug 13, 2022\n\n\nAbhishek Kumar Dubey\n\n\n\n\n\n\n  \n\n\n\n\nMachine Learning 1\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\nTraining Error, Generalization Error\n\n\n\n\n\n\nAug 6, 2022\n\n\nAbhishek Kumar Dubey\n\n\n\n\n\n\n  \n\n\n\n\nProbability Theory 1\n\n\n\n\n\n\n\nProbability Theory\n\n\n\n\nBasics, Bayes Theorem\n\n\n\n\n\n\nAug 6, 2022\n\n\nAbhishek Kumar Dubey\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Data_Science_Projects/index.html",
    "href": "Data_Science_Projects/index.html",
    "title": "Data Science Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nPoint cloud based 3d object detection\n\n\nY. Zhou and O. Tuzel,Voxelnet, IEEE Conference on CVPR, pp. 4490-4499, 2018.\n\n\n\n\n3D vehicle Detection\n\n\n\n\nVoxelNet- End-to-End Learning\n\n\n\n\n\n\nMar 12, 2023\n\n\nAbhishek Kumar Dubey  cs22mds15010\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Data_Science_Projects/CS6450_proposal.html#motivation",
    "href": "Data_Science_Projects/CS6450_proposal.html#motivation",
    "title": "Point cloud based 3d object detection",
    "section": "Motivation",
    "text": "Motivation\n\n\nThe motivations are:\n\n\nSafety:Improve safety on the road.\n\n\nTraffic Management: Manage traffic flow by identifying areas with high congestion.\n\n\nImproved Navigation: Better path planning, trajectory calculation.\n\n\nUse LiDAR : Compared to image based detection, LiDAR provides reliable depth information that can be used to accurately localize objects and characterize their shapes.\n\n\n\n\n\n\n\n\nFlexibility:can be used robotics, and augmented reality as well."
  },
  {
    "objectID": "Data_Science_Projects/CS6450_proposal.html#problem-statement",
    "href": "Data_Science_Projects/CS6450_proposal.html#problem-statement",
    "title": "Point cloud based 3d object detection",
    "section": "Problem Statement",
    "text": "Problem Statement\n\n\n\n\nDevelop a solution that can accurately locate a object in 3D space using LiDAR point Cloud.\n\n\nThe solution must not use any hand-crafted features or prior assumptions, which can limit accuracy and generalization.\n\n\n\n\n\n\n\n\nDirectly process raw LiDAR point clouds.\n\n\nDevelop an accurate, efficient, and robust model that can generalize to new environments and tasks."
  },
  {
    "objectID": "Data_Science_Projects/CS6450_proposal.html#challenges",
    "href": "Data_Science_Projects/CS6450_proposal.html#challenges",
    "title": "Point cloud based 3d object detection",
    "section": "Challenges",
    "text": "Challenges\n\n\n\n\n\n\nLiDAR point clouds are inherently sparse.\n\n\nThe point density of LiDAR point clouds can vary significantly depending on factors such as the sensor’s effective range, scanning pattern, and the relative pose of the object and the sensor.\n\n\n\n\n\nOcclusion can also result in missing or incomplete point cloud data, further exacerbating the sparsity of the data.\n\n\nVariations in the point density of LiDAR point clouds can make it difficult to design effective feature extraction and object detection algorithms.\n\n\nAddressing the sparsity and variability of LiDAR point clouds is a key challenge in developing accurate and robust 3D object detection models."
  },
  {
    "objectID": "Data_Science_Projects/CS6450_proposal.html#existing-methods-and-limitations",
    "href": "Data_Science_Projects/CS6450_proposal.html#existing-methods-and-limitations",
    "title": "Point cloud based 3d object detection",
    "section": "Existing methods and Limitations",
    "text": "Existing methods and Limitations\n\n\nUse pseudo Lidar instead of original Lidar and create the point cloud from the RGB image.However, these kind of method does not perform well, Because extracting point cloud from 2D images are not efficient. There are no depth information present in the 2D image.1\n\n\nSome methods for 3D object detection from LiDAR point clouds converts the point cloud data into a 2D image-like format, and then use techniques typically used for analyzing images to identify and classify objects.2 3\n\n\nAnother method for 3D object detection from LiDAR point clouds involves converting the point cloud data into a 3D grid of small cubes (called voxels), and then manually designing features to represent the objects within each voxel.4"
  },
  {
    "objectID": "Data_Science_Projects/CS6450_proposal.html#voxelnet-introduction",
    "href": "Data_Science_Projects/CS6450_proposal.html#voxelnet-introduction",
    "title": "Point cloud based 3d object detection",
    "section": "VoxelNet Introduction",
    "text": "VoxelNet Introduction\n\n\n\n\nit is a generic 3D detection framework that simultaneously learns a discriminative feature representation from point clouds and predicts accurate 3D bounding boxes\n\n\nVoxelNet directly operates on the raw point cloud."
  },
  {
    "objectID": "Data_Science_Projects/CS6450_proposal.html#voxelnet-architecture",
    "href": "Data_Science_Projects/CS6450_proposal.html#voxelnet-architecture",
    "title": "Point cloud based 3d object detection",
    "section": "VoxelNet Architecture",
    "text": "VoxelNet Architecture\n\n\n\n\nThe VoxelNet consists of three functional blocks\n\nFeature learning network\nConvolutional middle layers\nRegion proposal network"
  },
  {
    "objectID": "Data_Science_Projects/CS6450_proposal.html#feature-learning-network",
    "href": "Data_Science_Projects/CS6450_proposal.html#feature-learning-network",
    "title": "Point cloud based 3d object detection",
    "section": "Feature Learning Network",
    "text": "Feature Learning Network\n\n\nVoxel Partition\n\nSuppose the point cloud encompasses 3D space with range \\(D, H, W\\) along the \\(Z, Y, X\\) axes respectively.\nThe resulting 3D voxel grid is of size \\(D'=D/v_D, H'=H/v_H, W'=W/v_W\\) where \\(v_D,v_H,v_W\\) defines the voxel size accordingly\nPoint density in LiDAR point clouds is variable, resulting in varying numbers of points within each voxel."
  },
  {
    "objectID": "Data_Science_Projects/CS6450_proposal.html#section",
    "href": "Data_Science_Projects/CS6450_proposal.html#section",
    "title": "Point cloud based 3d object detection",
    "section": "",
    "text": "Feature Learning Network cont …\nRandom Sampling\n\nA high-definition LiDAR point cloud typically has around 100k points.\nDirectly processing all points can overload computing resources and create biased detection due to variable point density.\nTo avoid this, a fixed number of points (T) is randomly sampled from voxels that contain more than T points.\nThis sampling strategy has two purposes:\n\ncomputational savings and\ndecreases the imbalance of points between the voxels which reduces the sampling bias, and adds more variation to training."
  },
  {
    "objectID": "Data_Science_Projects/CS6450_proposal.html#section-1",
    "href": "Data_Science_Projects/CS6450_proposal.html#section-1",
    "title": "Point cloud based 3d object detection",
    "section": "",
    "text": "Feature Learning Network cont …\nStacked Voxel Feature Encoding\n\nlet \\({\\bf V}\\,=\\,\\{{\\bf p}_i\\,=\\,[x_{i},y_{i},z_{i},r_{i}]^{T}\\,\\in\\,\\mathbb{R}^{4}\\}_{i=1...t}\\) as a non-empty voxel containing \\(t <T\\) LiDAR points.\nHere \\({\\bf p}_i\\) contains \\(XYZ\\) coordinates for the \\(i^{\\text{th}}\\) point and \\(r_i\\) is the received reflectance.\nNow find \\({\\mathrm{\\bf V}}_{\\mathrm{in}}\\,=\\,\\{\\;\\mathrm{\\hat {\\bf p}_i}\\,=\\,[x_{i},y_{i},z_{i},r_{i},x_{i}-v_{x},y_{i}-v_{y},z_{i}-v_{z}]^{T}\\in\\mathbb{R}^{7}\\}_{i=1\\ldots t}.\\) Here \\((v_{x},v_{y},v_{z})\\) is the local mean as the centroid of all the points in \\({\\bf V}\\)\nNext, each \\(\\hat {\\bf p}_i\\) is transformed through the fully connected network (FCN) into a feature space."
  },
  {
    "objectID": "Data_Science_Projects/CS6450_proposal.html#section-2",
    "href": "Data_Science_Projects/CS6450_proposal.html#section-2",
    "title": "Point cloud based 3d object detection",
    "section": "",
    "text": "Feature Learning Network. VFE cont …\n\n\n\nInformation from point feature \\({\\bf f}_{i}\\ \\in\\ \\mathbb{R}^{m}\\) is aggregated to encode the shape of the surface contained within the voxel\nThe FCN is composed of a linear layer, a batch normalization (BN) layer, and a rectified linear unit (ReLU) layer\n\n\n\n\n\n\nElementwise MaxPooling across all \\(\\bf f_i\\) is used to get the locally aggregated feature \\(\\hat{\\mathbf{f}}\\ \\in\\ \\mathbb{R}^{m}\\)"
  },
  {
    "objectID": "Data_Science_Projects/CS6450_proposal.html#section-3",
    "href": "Data_Science_Projects/CS6450_proposal.html#section-3",
    "title": "Point cloud based 3d object detection",
    "section": "",
    "text": "Feature Learning Network. VFE cont …\n\n\n\nNow form the form the point-wise concatenated feature as \\(\\mathbf{f}_{i}^{o u t}\\,=\\,\\left[\\mathbf{f}_{i}^{T},{\\tilde{\\mathbf{f}}}^{T}\\right]^{T}\\,\\in\\,\\mathbb{R}^{2m}.\\)\nFinally we get \\({\\bf V_{\\ o u t}}\\ =\\ \\{{\\bf f}_{i}^{o u t}\\}_{i...t}.\\)\nThe output feature combines point-wise and locally aggregated features.\nStacking VFE layers allows encoding of point interactions within a voxel, enabling the final feature representation to learn descriptive shape information.\n\n\n\n\n\n\nVoxel features, uniquely associated with their spatial coordinates, are obtained by processing only non-empty voxels, resulting in a sparse 4D tensor representation."
  },
  {
    "objectID": "Data_Science_Projects/CS6450_proposal.html#convolutional-middle-layers",
    "href": "Data_Science_Projects/CS6450_proposal.html#convolutional-middle-layers",
    "title": "Point cloud based 3d object detection",
    "section": "Convolutional Middle Layers",
    "text": "Convolutional Middle Layers\n\nEach convolutional middle layer applies 3D convolution, BN layer, and ReLU layer sequentially.\nThe convolutional middle layers aggregate voxel-wise features within a progressively expanding receptive field, adding more context to the shape description\nDepending on the task this layer’ details can vary."
  },
  {
    "objectID": "Data_Science_Projects/CS6450_proposal.html#region-proposal-network",
    "href": "Data_Science_Projects/CS6450_proposal.html#region-proposal-network",
    "title": "Point cloud based 3d object detection",
    "section": "Region Proposal Network",
    "text": "Region Proposal Network\n\nRPN is combined with the feature learning network and convolutional middle layers to form an end-to-end trainable pipeline. \nThe first layer of each block downsamples the feature map by half. via convolution with a stride of 2\nFollowed by a sequence of convolutions of stride 1, BN and ReLU\nThe output of each block is upsampled to a fixed size and concatenated to construct a high-resolution feature map."
  },
  {
    "objectID": "Data_Science_Projects/CS6450_proposal.html#loss-function",
    "href": "Data_Science_Projects/CS6450_proposal.html#loss-function",
    "title": "Point cloud based 3d object detection",
    "section": "Loss Function",
    "text": "Loss Function\n\n3D ground truth box is parameterized as \\((x_{c}^{g},y_{c}^{g},z_{c}^{g},l^{g},w^{g},h^{g},\\theta^{g}),\\) where \\(x_{c}^{g},y_{c}^{g},z_{c}^{g}\\) represent the center location, \\(l^{g},w^{g},h^{g}\\) are length, width, height of the box,\\({\\theta}^{g}\\) is the yaw rotation.\nTo retrieve the ground truth box from a matching positive anchor parameterized as \\((x_{c}^{a},y_{c}^{a},z_{c}^{a},l^{a},w^{a},h^{a},\\theta^{a})\\) a vector \\(\\mathbf{u}^{*}\\in\\mathbb{R}^{7}\\) containing below 7 elements are calculated as below: \\[\\begin{align*}\n    &\\Delta x={\\frac{x_{c}^{g}-x_{c}^{a}}{d^{a}}},\\Delta y={\\frac{y_{c}^{g}-y_{c}^{a}}{d^{a}}},\\Delta z={\\frac{z_{c}^{g}-z_{c}^{a}}{h^{a}}},\\\\\n    &\\Delta l=\\log(\\frac{l^{g}}{l^{a}}),\\Delta w=\\log(\\frac{w^{g}}{w^{a}}),\\Delta h=\\log(\\frac{h^{g}}{h^{a}}), \\tag{1} \\\\\n    &\\Delta\\theta=\\theta^{g}-\\theta^{a}\n\\end{align*}\\] Where \\(\\;d^{a}\\,=\\,\\sqrt{(l^{a})^{2}+(w^{a})^{2}}\\)"
  },
  {
    "objectID": "Data_Science_Projects/CS6450_proposal.html#section-4",
    "href": "Data_Science_Projects/CS6450_proposal.html#section-4",
    "title": "Point cloud based 3d object detection",
    "section": "",
    "text": "Loss Function cont …\n\nNow we find loss as shown below \\[\\begin{align*}\n   L&=\\alpha{\\frac{1}{N_{\\mathrm{pos}}}}\\sum_{i}L_{\\mathrm{cls}}(p_{i}^{\\mathrm{pos}},1)\\\\\n   &+\\beta{\\frac{1}{N_{\\mathrm{neg}}}}\\sum_{j}L_{\\mathrm{cls}}(p_{j}^{\\mathrm{neg}},0)\\\\\n   &+\\frac{1}{N_{\\mathrm{pos}}}\\sum_{i}L_{\\mathrm{reg}}({\\bf u}_{i},{\\bf u}_{i}^{*}) \\tag{2}\n\\end{align*}\\]\nwhere \\(p_{i}^{\\mathrm{pos}}\\) and \\(p_{j}^{\\mathrm{neg}}\\) represent the softmax output for positive anchor \\(a_{i}^{\\mathrm{pos}}\\) and negative anchor \\(a_{j}^{\\mathrm{neg}}\\) respectively,\n\\(\\mathbf{u}_{i}\\ \\in\\ \\mathbb{R}^{7}\\) and \\(\\mathbf{u}_{i}^{*}\\ \\in\\ \\mathbb{R}^{7}\\) are the regression output and ground truth for positive anchor \\(a_{i}^{\\mathrm{pos}}\\)"
  },
  {
    "objectID": "Data_Science_Projects/CS6450_proposal.html#network-details",
    "href": "Data_Science_Projects/CS6450_proposal.html#network-details",
    "title": "Point cloud based 3d object detection",
    "section": "Network Details",
    "text": "Network Details\nCar Detection\n\nPoint clouds within the range of \\([-3,1]\\times[-40,40]\\times[0,70.4]\\) meters along \\(X,Y,Z\\) respectively is considered.\n\\(v_{D}\\,=\\,0.4,v_{H}\\,=\\,0.2,v_{W}\\,=\\,0.2\\)\n\\(D^{\\prime}\\;=\\;10,\\;H^{\\prime}\\;=\\;400,\\;W^{\\prime}\\;=\\;352\\)\nThe maximum number of randomly sampled points in each non-empty voxel \\(T\\,=\\,35\\)\nFor \\(\\mathrm{VFE}-1(7, 32)\\) and and \\(\\mathrm{VFE}-2(32, 128)\\) was used.\nThe final \\(\\mathrm{FCN}\\) maps \\(\\mathrm{VFE}\\) output to \\(\\mathbb{R}^{128}\\)\nThus the feature learning net generates a sparse tensor of shape \\(128\\times10\\times400\\times352\\)"
  },
  {
    "objectID": "Data_Science_Projects/CS6450_proposal.html#section-5",
    "href": "Data_Science_Projects/CS6450_proposal.html#section-5",
    "title": "Point cloud based 3d object detection",
    "section": "",
    "text": "Network Details, Car Detection cont …"
  },
  {
    "objectID": "Data_Science_Projects/CS6450_proposal.html#section-6",
    "href": "Data_Science_Projects/CS6450_proposal.html#section-6",
    "title": "Point cloud based 3d object detection",
    "section": "",
    "text": "Network Details, Car Detection cont …\n\nTo aggregate voxel-wise features, three convolution middle layers was employed sequentially as \\(\\mathrm{Conv3D}(128, 64, 3,(2,1,1), (1,1,1)),\\) \\(\\mathrm{Conv3D}(64, 64, 3, (1,1,1), (0,1,1)),\\) and \\(\\mathrm{Conv3D}(64, 64, 3, (2,1,1), (1,1,1)),\\)\nwhich yields a 4D tensor of size \\(64\\times2\\times400\\times352.\\)\nAfter reshaping, size becomes \\(128\\times400\\times352.\\) and is input to RPN.\nOnly one anchor size was used \\(l^{a}\\,=\\,3.9.\\,w^{a}\\,=\\,1.6.\\,h^{a}\\,=\\,1.56\\) meters centered at \\(z_{c}^{a}\\,=\\,-1.0\\) meters with two rotations, \\(0^\\circ\\) and \\(90^\\circ\\)\nPositive anchor: if it has the highest Intersection over Union (IoU) with a ground truth or its IoU with ground truth is above 0.6 (in bird’s eye view).\nPositive anchor: if the IoU between it and all ground truth boxes is less than 0.45.\nSet \\(\\alpha=1.5\\) and \\(\\beta=1\\) in equation \\((2)\\)"
  },
  {
    "objectID": "Data_Science_Projects/CS6450_proposal.html#training",
    "href": "Data_Science_Projects/CS6450_proposal.html#training",
    "title": "Point cloud based 3d object detection",
    "section": "Training",
    "text": "Training\n\nA similar network with different parameters, was used for Pedestrian and Cyclist Detection\n\n2 anchors, smaller anchors, small stride to capture fine details etc.\n\nDuring training, stochastic gradient descent (SGD) with learning rate 0.01 was used for the first 150 epochs and decreased the learning rate to 0.001 for the last 10 epochs.\nData Augmentation\n\nperturbation independently to each ground truth 3D bounding box together with those LiDAR points within the box.\nGlobal scaling."
  },
  {
    "objectID": "Data_Science_Projects/CS6450_proposal.html#experiments",
    "href": "Data_Science_Projects/CS6450_proposal.html#experiments",
    "title": "Point cloud based 3d object detection",
    "section": "Experiments",
    "text": "Experiments\n\nVoxelNet evaluated on KITTI 3D object detection benchmark\n7,481 training images/point clouds and 7,518 test images/point clouds\nThree categories: Car, Pedestrian, and Cyclist\nDetection outcomes evaluated based on three difficulty levels: easy, moderate, and hard\nEvaluation protocol based on object size, occlusion state, and truncation level\n3,712 data samples for training and 3,769 for validation\nHand-crafted baseline (HC-baseline) was implemented and trained to evaluate the importance of end-to-end learning.\nHC-baseline uses the bird’s eye view features described in Multi-view 3d object detection network for autonomous driving5 which are computed at 0.1 m resolution."
  },
  {
    "objectID": "Data_Science_Projects/CS6450_proposal.html#quantitative-results.",
    "href": "Data_Science_Projects/CS6450_proposal.html#quantitative-results.",
    "title": "Point cloud based 3d object detection",
    "section": "Quantitative results.",
    "text": "Quantitative results.\n\n\nPerformance comparison in 3D detection: average precision (in %) on KITTI validation set"
  },
  {
    "objectID": "Data_Science_Projects/CS6450_proposal.html#qualitative-results-for-car",
    "href": "Data_Science_Projects/CS6450_proposal.html#qualitative-results-for-car",
    "title": "Point cloud based 3d object detection",
    "section": "Qualitative results for car",
    "text": "Qualitative results for car"
  },
  {
    "objectID": "Data_Science_Projects/CS6450_proposal.html#qualitative-results-for-pedestrian",
    "href": "Data_Science_Projects/CS6450_proposal.html#qualitative-results-for-pedestrian",
    "title": "Point cloud based 3d object detection",
    "section": "Qualitative results for pedestrian",
    "text": "Qualitative results for pedestrian"
  },
  {
    "objectID": "Data_Science_Projects/CS6450_proposal.html#qualitative-results-for-cyclist",
    "href": "Data_Science_Projects/CS6450_proposal.html#qualitative-results-for-cyclist",
    "title": "Point cloud based 3d object detection",
    "section": "Qualitative results for cyclist",
    "text": "Qualitative results for cyclist"
  },
  {
    "objectID": "Data_Science_Projects/CS6450_proposal.html#conclusion",
    "href": "Data_Science_Projects/CS6450_proposal.html#conclusion",
    "title": "Point cloud based 3d object detection",
    "section": "Conclusion",
    "text": "Conclusion\n\nExisting LiDAR-based 3D detection methods rely on hand-crafted features\nVoxelNet proposes an end-to-end trainable deep architecture for point cloud-based 3D detection\nVoxelNet can operate directly on sparse 3D points and efficiently capture 3D shape information\nVoxelNet outperforms state-of-the-art LiDAR-based 3D detection methods on the KITTI car detection task"
  },
  {
    "objectID": "Data_Science_Projects/CS6450_proposal.html#novelty",
    "href": "Data_Science_Projects/CS6450_proposal.html#novelty",
    "title": "Point cloud based 3d object detection",
    "section": "Novelty",
    "text": "Novelty\n\n\nWe do not need to predict separately for positive and negative.\nExtend VoxelNet for joint LiDAR and image-based end-to-end 3D detection."
  },
  {
    "objectID": "Data_Science_Projects/CS6450_proposal.html#novelty-1",
    "href": "Data_Science_Projects/CS6450_proposal.html#novelty-1",
    "title": "Point cloud based 3d object detection",
    "section": "Novelty",
    "text": "Novelty\n\nPrevious time frames can be taken into account improve the detection. \nVFE can be improved with the help of transformer positional encoding.\nOne universal network instead of two."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "I work as a senior data scientist , and I am an mtech student  at IIT hyderabad, I love doing reseach , exploring new topics, my area of interest are deep learning, autonomous driving. I love conceptual thinking .\n    \n\n\n\n\n\n Resume\n\n\n\n\n\n\n\nIf resume doesn’t appear below, refresh the page or click on the above button to download it."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IITH-Data-Science",
    "section": "",
    "text": "This blog contains posts related to data science and mathematics behind it. Some of those are refered from the class notes of IIT hyderabad, and some are from my exprience.\n  \n  I work as Senior Data Scientist also I am mtech (Data science ) student at IIT hyderabad. Mostly I work on deep learning topic. I have exprience on Deep Learning domain of Autonomous Driving and  Medical Imaging. \n  \n    Learn more about me"
  },
  {
    "objectID": "index.html#hand-picked-blog-posts",
    "href": "index.html#hand-picked-blog-posts",
    "title": "IITH-Data-Science",
    "section": "Hand-picked Blog posts",
    "text": "Hand-picked Blog posts\n\n\n\n\n\n  \n    \n        \n             September 10, 2022\n            \n                 SVM in-depth\n                SVM, Lagrange Multipliers, KKT condition, Mercer’s condition, Kernel Trick\n            \n        \n    \n  \n  \n    \n        \n             September 24, 2022\n            \n                 Multi Layer Perceptrons\n                Forward pass, backward pass, erro calculation of multipalyer models.\n            \n        \n    \n  \n\n\nNo matching items\n\n\n\n\n\n\n\n  \n    \n        \n             October 15, 2022\n            \n                 Probability Theory\n                Joint mass function, Marginal mass functions,Conditional mass function, Concentration Bounds\n            \n        \n    \n  \n  \n    \n        \n             September 17, 2022\n            \n                 Linear Algebra\n                Gaussian Elimination, Row-Echelon, Minus 1 Trick\n            \n        \n    \n  \n\n\nNo matching items\n\n\n\n\n\nHome page contains only some Hand-picked Blog posts.\nPlease visit other pages like \n \n     \n   \nor \n \n\n   \nor \n \n \n \netc.\nto see other contents."
  },
  {
    "objectID": "Data_Science_Hacks/2022-11-22-sk-learn model search.html",
    "href": "Data_Science_Hacks/2022-11-22-sk-learn model search.html",
    "title": "sk-learn model search",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport time\n\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics"
  },
  {
    "objectID": "Data_Science_Hacks/2022-11-22-sk-learn model search.html#read-the-data",
    "href": "Data_Science_Hacks/2022-11-22-sk-learn model search.html#read-the-data",
    "title": "sk-learn model search",
    "section": "2 Read the data",
    "text": "2 Read the data\n\nmy_training_data = pd.read_csv('train.csv')\nmy_testing_data = pd.read_csv('test.csv')\ntest_id = my_testing_data['ID']"
  },
  {
    "objectID": "Data_Science_Hacks/2022-11-22-sk-learn model search.html#convert-to-onehot",
    "href": "Data_Science_Hacks/2022-11-22-sk-learn model search.html#convert-to-onehot",
    "title": "sk-learn model search",
    "section": "3 Convert to onehot",
    "text": "3 Convert to onehot\n\ncategorical_cols = ['Cat_1','Cat_2','Cat_3']\nmy_training_data = pd.get_dummies(my_training_data, columns = categorical_cols)\nmy_testing_data = pd.get_dummies(my_testing_data, columns = categorical_cols)"
  },
  {
    "objectID": "Data_Science_Hacks/2022-11-22-sk-learn model search.html#fill-missing-values",
    "href": "Data_Science_Hacks/2022-11-22-sk-learn model search.html#fill-missing-values",
    "title": "sk-learn model search",
    "section": "4 Fill missing values",
    "text": "4 Fill missing values\n\n\nmy_training_data['num_4'].fillna((my_training_data['num_4'].median()), inplace=True)\nmy_training_data['num_5'].fillna((my_training_data['num_5'].median()), inplace=True)\n\nmy_testing_data['num_4'].fillna((my_testing_data['num_4'].median()), inplace=True)\nmy_testing_data['num_5'].fillna((my_testing_data['num_5'].median()), inplace=True)"
  },
  {
    "objectID": "Data_Science_Hacks/2022-11-22-sk-learn model search.html#normalize",
    "href": "Data_Science_Hacks/2022-11-22-sk-learn model search.html#normalize",
    "title": "sk-learn model search",
    "section": "5 Normalize",
    "text": "5 Normalize\n\n\nscaled_columns=['num_1','num_2','num_3','num_4','num_5','num_6']\nfor column in scaled_columns:\n    my_training_data[column] = (my_training_data[column] - my_training_data[column].mean()) / my_training_data[column].std() \n    my_testing_data[column] = (my_testing_data[column] - my_testing_data[column].mean()) / my_testing_data[column].std()"
  },
  {
    "objectID": "Data_Science_Hacks/2022-11-22-sk-learn model search.html#remove-unwanted-features",
    "href": "Data_Science_Hacks/2022-11-22-sk-learn model search.html#remove-unwanted-features",
    "title": "sk-learn model search",
    "section": "6 Remove unwanted features",
    "text": "6 Remove unwanted features\n\n\nTarget =['target_class']\ncolumn_to_train = list(my_training_data.columns)\ncolumn_to_train.remove('target_class')\ncolumn_to_train.remove('ID')\ncolumn_to_train.remove('Cat_3_d1')\ncolumn_to_train.remove('Cat_3_d2')\ncolumn_to_train.remove('num_6')\ncolumn_to_train.remove('num_4')\ncolumn_to_train\n\n['num_3',\n 'num_1',\n 'num_5',\n 'num_2',\n 'Cat_1_d1',\n 'Cat_1_d2',\n 'Cat_1_d4',\n 'Cat_1_d5',\n 'Cat_1_d6',\n 'Cat_2_d1',\n 'Cat_2_d2',\n 'Cat_2_d3']\n\n\n\nmy_training_data.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      ID\n      num_3\n      num_4\n      num_1\n      num_5\n      num_6\n      num_2\n      target_class\n      Cat_1_d1\n      Cat_1_d2\n      Cat_1_d4\n      Cat_1_d5\n      Cat_1_d6\n      Cat_2_d1\n      Cat_2_d2\n      Cat_2_d3\n      Cat_3_d1\n      Cat_3_d2\n    \n  \n  \n    \n      0\n      1\n      -0.036001\n      -0.823512\n      -0.687687\n      -0.341508\n      1.882562\n      -1.471565\n      1\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n    \n    \n      1\n      2\n      -0.036001\n      1.626235\n      0.126056\n      -1.046135\n      2.806383\n      -0.338369\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      2\n      3\n      -1.793255\n      -0.823512\n      -0.562495\n      0.363119\n      2.128914\n      -0.163655\n      1\n      0\n      0\n      0\n      0\n      1\n      1\n      0\n      0\n      0\n      1\n    \n    \n      3\n      4\n      -0.914628\n      -0.006929\n      -0.708552\n      0.363119\n      -0.211433\n      -1.505775\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      4\n      5\n      -1.793255\n      -0.006929\n      -0.541630\n      -0.341508\n      2.005738\n      -0.707345\n      1\n      0\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\ncv_split = model_selection.StratifiedShuffleSplit(n_splits = 5, test_size = .1, random_state = 0 ) \n\n\n\nX = my_training_data[column_to_train]\n\ny = my_training_data[Target[0]]\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0,stratify=y)"
  },
  {
    "objectID": "Data_Science_Hacks/2022-11-22-sk-learn model search.html#test-performance-on-all-the-models",
    "href": "Data_Science_Hacks/2022-11-22-sk-learn model search.html#test-performance-on-all-the-models",
    "title": "sk-learn model search",
    "section": "7 Test performance on all the models",
    "text": "7 Test performance on all the models\n\n\nprint(\"\\n ada:\")\nada = ensemble.AdaBoostClassifier(learning_rate= 0.01,  n_estimators= 200,random_state=0)\ncv_results = model_selection.cross_validate(ada, my_training_data[column_to_train], my_training_data[Target[0]], cv  = cv_split,return_train_score=True)\nprint('test_score: ',cv_results['test_score'].mean())\nada.fit(X_train, y_train)\nscore = ada.score(X_test, y_test)\nprint('RealTest_score: ',score)\n\nprint(\"\\n bc:\")\nbc = ensemble.BaggingClassifier(max_samples= 0.01,n_estimators= 100,random_state=0)\ncv_results = model_selection.cross_validate(bc, my_training_data[column_to_train], my_training_data[Target[0]], cv  = cv_split,return_train_score=True)\nprint('test_score: ',cv_results['test_score'].mean())\nbc.fit(X_train, y_train)\nscore = bc.score(X_test, y_test)\nprint('RealTest_score: ',score)\n\nprint(\"\\n etc:\")\netc = ensemble.ExtraTreesClassifier(criterion= 'entropy', max_depth= 7, n_estimators= 300,random_state=0)\ncv_results = model_selection.cross_validate(etc, my_training_data[column_to_train], my_training_data[Target[0]], cv  = cv_split,return_train_score=True)\nprint('test_score: ',cv_results['test_score'].mean())\netc.fit(X_train, y_train)\nscore = etc.score(X_test, y_test)\nprint('RealTest_score: ',score)\n\nprint(\"\\n gbc:\")\ngbc = ensemble.GradientBoostingClassifier(learning_rate= 0.03, max_depth= 2, n_estimators= 200,random_state=0)\ncv_results = model_selection.cross_validate(gbc, my_training_data[column_to_train], my_training_data[Target[0]], cv  = cv_split,return_train_score=True)\nprint('test_score: ',cv_results['test_score'].mean())\ngbc.fit(X_train, y_train)\nscore = gbc.score(X_test, y_test)\nprint('RealTest_score: ',score)\n\nprint(\"\\n rfc:\")\nrfc = ensemble.RandomForestClassifier(bootstrap=True,criterion= 'entropy', max_depth= 4,max_features=3,min_samples_leaf=2,min_samples_split=12, n_estimators= 50,oob_score=True)\ncv_results = model_selection.cross_validate(rfc, my_training_data[column_to_train], my_training_data[Target[0]], cv  = cv_split,return_train_score=True)\nprint('test_score: ',cv_results['test_score'].mean())\nrfc.fit(X_train, y_train)\nscore = rfc.score(X_test, y_test)\nprint('RealTest_score: ',score)\n\nprint(\"\\n svc:\")\nsvc = svm.SVC(C= 1, decision_function_shape= 'ovo', gamma= 0.1,probability=True)\ncv_results = model_selection.cross_validate(svc, my_training_data[column_to_train], my_training_data[Target[0]], cv  = cv_split,return_train_score=True)\nprint('test_score: ',cv_results['test_score'].mean())\nsvc.fit(X_train, y_train)\nscore = svc.score(X_test, y_test)\nprint('RealTest_score: ',score)\n\n\n ada:\ntest_score:  0.6831515151515151\nRealTest_score:  0.6812121212121212\n\n bc:\ntest_score:  0.6688484848484848\nRealTest_score:  0.68\n\n etc:\ntest_score:  0.6172121212121212\nRealTest_score:  0.6436363636363637\n\n gbc:\ntest_score:  0.6860606060606059\nRealTest_score:  0.6787878787878788\n\n rfc:\ntest_score:  0.6535757575757575\nRealTest_score:  0.6751515151515152\n\n svc:\ntest_score:  0.6652121212121213\nRealTest_score:  0.6593939393939394\n\n\n\n\nfrom sklearn.neural_network import MLPClassifier\nmlp =  MLPClassifier(random_state=42,max_iter=1000,activation='logistic',hidden_layer_sizes= (100,))\ncv_results = model_selection.cross_validate(mlp, my_training_data[column_to_train], my_training_data[Target[0]], cv  = cv_split,return_train_score=True)\nprint('test_score: ',cv_results['test_score'].mean())\nmlp.fit(X_train, y_train)\nscore = mlp.score(X_test, y_test)\nprint('RealTest_score: ',score)\n\ntest_score:  0.6696969696969697\nRealTest_score:  0.6636363636363637\n\n\n\n\nimport xgboost as xgb\nclf_xgb=xgb.XGBClassifier(learning_rate=0.01,gamma=5,n_estimators=50,reg_alpha=0,reg_lambda  =0.9,subsample =0.8)\ncv_results = model_selection.cross_validate(clf_xgb, my_training_data[column_to_train], my_training_data[Target[0]], cv  = cv_split,return_train_score=True)\nprint('test_score: ',cv_results['test_score'].mean())\nclf_xgb.fit(X_train, y_train)\nscore = clf_xgb.score(X_test, y_test)\nprint('RealTest_score: ',score)\n\ntest_score:  0.6846060606060606\nRealTest_score:  0.6824242424242424\n\n\n\n7.1 Voting classifier tuning\n\n\nfrom sklearn.ensemble import VotingClassifier\nnamed_estimators = [ (\"ada\",ada),(\"bc\", bc), (\"gbc?\", gbc), (\"rfc\", rfc), (\"svc\", svc),(\"mlp\",mlp)]\nvoting_clf = VotingClassifier(named_estimators)\nvoting_clf.fit(X_train, y_train)\nvoting_clf.score(X_test, y_test)\n\n0.6824242424242424\n\n\n\nvoting_clf.fit(X,y)\nvoting_clf.score(X_test, y_test)\n\n0.6909090909090909\n\n\n\n\n# voting_clf.fit(X,y)\nresults = pd.DataFrame(data=[],columns=['ID','target_class'])\nresults['ID']=test_id\nresults['target_class'] =voting_clf.predict(my_testing_data[column_to_train])\nresults.to_csv(voting_clf.__class__.__name__+'score_'+str(0.6836363636363636_2)+'_tuned_ada_bc_gbc_rtc_svc_mlp.csv',index=False)\n\n\nresults = pd.DataFrame(data=[],columns=['ID','target_class'])\nresults['ID']=test_id\nresults['target_class'] =gbc.predict(my_testing_data[column_to_train])\nresults.to_csv(gbc.__class__.__name__+'score_'+str(0.6836363636363636)+'_tuned_gbc.csv',index=False)"
  },
  {
    "objectID": "Data_Science_Hacks/2022-11-22-sk-learn model search.html#grid-search",
    "href": "Data_Science_Hacks/2022-11-22-sk-learn model search.html#grid-search",
    "title": "sk-learn model search",
    "section": "8 Grid Search",
    "text": "8 Grid Search\n\n\n\nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [15,20,30],\n    'max_features': [ 3,4,5,6],\n    'min_samples_leaf': [2,3, 4],\n    'min_samples_split': [4,5,6,7],\n    'n_estimators': [50,100,150],\n    'criterion' :[ 'entropy', 'gini']\n}\n\n# Create a based model\nrf = ensemble.ExtraTreesClassifier()\n# Instantiate the grid search model\ngrid_search = model_selection.GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)\n\n# Fit the grid search to the data\ngrid_search.fit(X,y)\ngrid_search.best_score_ ,grid_search.best_params_\n\nFitting 3 folds for each of 864 candidates, totalling 2592 fits\n\n\n(0.5800500016534939,\n {'bootstrap': True,\n  'criterion': 'entropy',\n  'max_depth': 30,\n  'max_features': 6,\n  'min_samples_leaf': 2,\n  'min_samples_split': 4,\n  'n_estimators': 50})\n\n\n\n8.1 use best param to check the score\n\nrf.set_params(**grid_search.best_params_) \nrf.fit(X_train,y_train)\nclf_score = rf.score(X_test, y_test)\n\nprint('grid_search_std:',grid_search.cv_results_['std_test_score'][grid_search.best_index_]*100*3)  \nprint('grid_search_best_score:',grid_search.best_score_)\nprint('fitted_test_score',clf_score)\n\ngrid_search_std: 37.53328576919699\ngrid_search_best_score: 0.5800500016534939\nfitted_test_score 0.658989898989899\n\n\n\nrf.set_params(**grid_search.best_params_) \nrf.fit(X_train,y_train)\nclf_score = rf.score(X_test, y_test)\n\nprint('grid_search_std:',grid_search.cv_results_['std_test_score'][grid_search.best_index_]*100*3)  \nprint('grid_search_best_score:',grid_search.best_score_)\nprint('fitted_test_score',clf_score)\n\ngrid_search_std: 29.824640656735674\ngrid_search_best_score: 0.5514440292337709\nfitted_test_score 0.6505050505050505\n\n\n\n\n8.2 Save the prediction on test data\n\nrf.fit(X,y)\nresults = pd.DataFrame(data=[],columns=['ID','target_class'])\nresults['ID']=test_id\nresults['target_class'] =rf.predict(my_testing_data[column_to_train])\nresults.to_csv(rf.__class__.__name__+'score_'+str(clf_score)+'_tuned.csv',index=False)"
  },
  {
    "objectID": "Data_Science_Hacks/2022-11-22-sk-learn model search.html#random-search",
    "href": "Data_Science_Hacks/2022-11-22-sk-learn model search.html#random-search",
    "title": "sk-learn model search",
    "section": "9 Random search",
    "text": "9 Random search\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\nn_estimators = [int(x) for x in range(100,2000,200)]\n\nmax_features = ['d1', 'sqrt',2,3,4,5]\n\nmax_depth = [int(x) for x in np.linspace(2, 110, num = 5)]\nmax_depth.append(None)\n\nmin_samples_split = [2, 5, 10]\n\nmin_samples_leaf = [1, 2, 4]\n\nbootstrap = [True, False]\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)\n\n{'n_estimators': [100, 300, 500, 700, 900, 1100, 1300, 1500, 1700, 1900], 'max_features': ['d1', 'sqrt', 2, 3, 4, 5], 'max_depth': [2, 29, 56, 83, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n\n\n\n\nrf = ensemble.RandomForestClassifier()\n\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n\nrf_random.fit(X_train, y_train)\n\nFitting 3 folds for each of 100 candidates, totalling 300 fits\n\n\n/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:703: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n  \"timeout or by a memory leak.\", UserWarning\n\n\nRandomizedSearchCV(cv=3, estimator=RandomForestClassifier(), n_iter=100,\n                   n_jobs=-1,\n                   param_distributions={'bootstrap': [True, False],\n                                        'max_depth': [2, 29, 56, 83, 110, None],\n                                        'max_features': ['d1', 'sqrt', 2, 3,\n                                                         4, 5],\n                                        'min_samples_leaf': [1, 2, 4],\n                                        'min_samples_split': [2, 5, 10],\n                                        'n_estimators': [100, 300, 500, 700,\n                                                         900, 1100, 1300, 1500,\n                                                         1700, 1900]},\n                   random_state=42, verbose=2)\n\n\n\nrf_random.best_params_\n\n{'n_estimators': 1300,\n 'min_samples_split': 10,\n 'min_samples_leaf': 1,\n 'max_features': 5,\n 'max_depth': 2,\n 'bootstrap': True}"
  },
  {
    "objectID": "Data_Science_Hacks/2022-11-22-sk-learn model search.html#grid-seach-on-all-the-models",
    "href": "Data_Science_Hacks/2022-11-22-sk-learn model search.html#grid-seach-on-all-the-models",
    "title": "sk-learn model search",
    "section": "10 Grid seach on all the models",
    "text": "10 Grid seach on all the models\n\nvote_est = [\n    ('ada', ensemble.AdaBoostClassifier()),\n    ('bc', ensemble.BaggingClassifier()),\n    ('etc',ensemble.ExtraTreesClassifier()),\n    ('gbc', ensemble.GradientBoostingClassifier()),\n    ('rfc', ensemble.RandomForestClassifier()),\n    ('gpc', gaussian_process.GaussianProcessClassifier()),\n    ('lr', linear_model.LogisticRegressionCV()),\n    ('knn', neighbors.KNeighborsClassifier()),\n    ('svc', svm.SVC(probability=True)),\n   ('xgb', XGBClassifier())\n\n]\n\n\n\ngrid_n_estimator = [10, 50, 100, 300,400,500]\ngrid_ratio = [.1, .25, .5, .75, 1.0]\ngrid_learn = [.01, .03, .05, .1, .25]\ngrid_max_depth = [2, 4, 6, 8, 10, 50,80,100]\ngrid_min_samples = [5, 10, .03, .05, .10]\ngrid_criterion = ['gini', 'entropy']\ngrid_bool = [True, False]\ngrid_seed = [0]\n\n\ngrid_param = [\n            [{\n            'n_estimators': grid_n_estimator, \n            'learning_rate': grid_learn, \n            'random_state': grid_seed\n            }],\n    \n            [{\n            'n_estimators': grid_n_estimator, \n            'max_samples': grid_ratio,\n             }],\n    \n            [{\n            'n_estimators': grid_n_estimator, \n            'criterion': grid_criterion, \n            'max_depth': grid_max_depth, \n             }],\n\n            [{\n            'learning_rate': [0.01,0.02,0.03,0.05,0.1], \n            'n_estimators': [10,50,100,300,400], \n            'max_depth': grid_max_depth, \n             }],\n\n            [{\n      \n            'n_estimators': grid_n_estimator, \n            'criterion': grid_criterion,\n            'max_depth': grid_max_depth, \n            'max_features': [2, 3,4,5],\n            'min_samples_leaf': [3, 4, 5],\n            'min_samples_split': [8, 10, 12],\n            'bootstrap': [True],\n            'oob_score': [True], \n             }\n            ], \n    \n            [{    \n            'max_iter_predict': grid_n_estimator,\n            'random_state': grid_seed\n            }],\n        \n\n            [{\n            'fit_intercept': grid_bool, \n            'penalty': ['l1','l2'],\n            'solver': [ 'liblinear',  'saga'], \n            'random_state': grid_seed,\n            'max_iter':[5000]\n             },\n            {\n            'fit_intercept': grid_bool, \n            'penalty': ['l2'],\n            'solver': [ 'lbfgs','newton-cg','sag'],\n            'random_state': grid_seed,\n            'max_iter':[5000]\n             }],\n            \n            [{\n            'n_neighbors': [1,2,3,4,5,6,7],\n            'weights': ['uniform', 'distance'],\n            'algorithm': ['d1', 'ball_tree', 'kd_tree', 'brute']\n            }],\n            \n    \n            [{\n            'C': [1,2,3,4,5], \n            'gamma': grid_ratio, \n            'decision_function_shape': ['ovo', 'ovr'],\n            'probability': [True],\n            'random_state': grid_seed\n             }],\n\n    \n            [{\n            'learning_rate': grid_learn, \n            'max_depth': [1,2,4,6,8,10],\n            'n_estimators': grid_n_estimator, \n             }]   \n        ]\n\n\n\n\nall_models_results_col = [ 'Search Parameters', 'Search score', 'Search score 3*STD' ,'Time(s)','Clf_Score']\nall_models_results = pd.DataFrame(data=[],columns = all_models_results_col,index=[clf[1].__class__.__name__ for clf in vote_est])\nstart_total = time.perf_counter() \nfor clf, param in zip (vote_est, grid_param): \n  \n    \n    start = time.perf_counter()        \n    best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc')\n    best_search.fit(X_train, y_train)\n    run = time.perf_counter() - start\n\n    best_param = best_search.best_params_\n    clf[1].set_params(**best_param) \n    clf[1].fit(X_train,y_train)\n    clf_score = clf[1].score(x_test, y_test)\n\n\n    clf[1].fit(X,y)\n    results = pd.DataFrame(data=[],columns=['ID','target_class'])\n    results['ID']=test_id\n    results['target_class'] =clf[1].predict(my_testing_data[column_to_train])\n    results.to_csv(clf[1].__class__.__name__+'score_'+str(clf_score)+'_tuned.csv',index=False)\n    \n  \n\n    all_models_results.loc[clf[1].__class__.__name__]['Search Parameters']=best_search.best_params_\n    all_models_results.loc[clf[1].__class__.__name__]['Search score']=best_search.best_score_\n    all_models_results.loc[clf[1].__class__.__name__]['Search score 3*STD']=best_search.cv_results_['std_test_score'][best_search.best_index_]*100*3\n    all_models_results.loc[clf[1].__class__.__name__]['Time(s)']=run\n    all_models_results.loc[clf[1].__class__.__name__]['Clf_Score']=clf_score\n    \n    print('The best parameter for {} is {} with a search score of {:.2f} clf score of {:.2f} and runtime of {:.2f} seconds.'.format(clf[1].__class__.__name__, best_param,best_search.best_score_,clf_score, run))\n\nrun_total = time.perf_counter() - start_total\nprint('Total optimization time was {:.2f} minutes.'.format(run_total/60))\n\nprint('-'*10)\nall_models_results.sort_values(by = ['Search score'], ascending = False, inplace = True)\nall_models_results"
  },
  {
    "objectID": "Data_Science_Hacks/index.html",
    "href": "Data_Science_Hacks/index.html",
    "title": "Data Science Hacks",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nsk-learn model search\n\n\n\n\n\n\n\nsk-learn\n\n\n\n\nTrain multiple sk-learn models\n\n\n\n\n\n\nAug 26, 2022\n\n\nAbhishek Kumar Dubey\n\n\n\n\n\n\nNo matching items"
  }
]