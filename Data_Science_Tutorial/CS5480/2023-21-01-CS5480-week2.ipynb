{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "author: Abhishek Kumar Dubey\n",
    "badges: false\n",
    "categories:\n",
    "- Deep Learning\n",
    "date: '2023-21-01'\n",
    "description: History, Perceptrons, Neural Networks and Backpropagation\n",
    "image: CS5480_images/chrome_mmOWWTaZry.png\n",
    "title: Deep Learning 2\n",
    "toc: true\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent using Backpropagation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. set  $\\Delta W^{(l)}:=0,\\Delta b^{(l)}:=0$ (matrix/vector of zeros) for all $l$\n",
    "2. For $i = 1$ to $m$\n",
    "    1. Use backpropagation to compute $\\nabla_{\\theta^{(l)}}J(\\theta;X,y)$\n",
    "    2. Set $\\Delta\\theta^{(l)}:=\\Delta\\theta^{(l)}+\\nabla_{\\theta^{(l)}}J(\\theta;X,y)$\n",
    "3. Update the parameters:\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    W^{(l)}&=W^{(l)}-\\alpha\\left[\\left(\\frac{1}{m}\\Delta W^{(l)}\\right)+\\lambda W^{(l)}\\right]\\\\\n",
    "    b^{(l)}&=b^{(l)}-\\alpha\\left[\\frac{1}{m}\\Delta b^{(l)}\\right]\n",
    "    \\end{align*}\n",
    "   $$\n",
    "   \n",
    "4. Repeat for all data points until convergence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Iteration: When weight is updated then it is called one iteration.\n",
    "- Epoch : When training is done at least once on the whole data set  then it is called one epoch.\n",
    "- Convex objective function has One global minima but Non convex objective function has many local minima.\n",
    "- \n",
    "  ::: {.callout-note}\n",
    "  The neural network with more than one layer has cost function which  is almost always non convex, even if there is no activation function on any layer, it may sound non intuitive at first but it will become clear as we explore on this topic.\n",
    "  :::\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Minima"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The non-identifiability problem results in multiple equivalent local\n",
    "minima → Not problematic though, since cost function value is the\n",
    "same\n",
    "- If there are  local minima with  significant different cost value then that could be a problem.\n",
    "- It's not required to find the true global minimum, recent work shows that even finding the parameter space which has low but not global  minimum cost is enough.\n",
    "- \n",
    "  ::: {.callout-note}\n",
    "  Plot the norm of the gradient over time. If the norm is very small, it is likely to be a local minimum (or a critical point).\n",
    "  :::\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saddle Points and Plateaus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If there is local minima across one cross section of cost function and local maxima across another  cross section of the cost function then it is called a saddle point.<br>\n",
    "  ![](CS5480_images//Acrobat_2OJYOHomOb.png)<br>\n",
    "- In higher dimension space there are more saddle points than local minima.\n",
    "- If there is a steep decline in the cost then it is called Cliff area. Search types of clips are very much possible in RNN because in such networks there are a lot of multiplication (over time) and when small terms multiplies many times then it becomes even smaller which causes cliffs.<br>\n",
    "![](CS5480_images//Acrobat_5doamnxGE8.png)<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing/Exploding Gradient"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While doing gradient descent if the smaller number multiplies many times it becomes even smaller and it causes vanishing gradient problem in very deep neural network.\n",
    "- If the activation function has more steep slope or it has narrow range then it can cause vanishing gradient problem, One such example is sigmoid activation function.\n",
    "- In the same way there may happen exploding gradient issue if the number is greater than one while doing gradient descent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cost functions are often high dimensional non quadratic non convex,There can be many minima shadow points and also many flat regions,\n",
    "- There is no guarantee that a network will converge to an optimal point or it will converge at all.\n",
    "- Most of the time problems are ill conditioned,\n",
    "- The gradient can be inexact.\n",
    "- Power correspondence between local and global structure.\n",
    "- We need to hyper tune learning rate and many other parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to address"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many techniques which can help address this issue up to some extent.\n",
    "\n",
    "- Algorithmic approach.\n",
    "    - Batch gradient descent, stochastic gradient descent, mini batch gradient descent.\n",
    "    - Momentum based methods Nestrov momentum.\n",
    "    - Adagrad,  Adadelta, RMSprobe, Adam.\n",
    "    - Other advanced optimization methods.\n",
    "- Practical tricks.\n",
    "    - Regularization method such as dropout.\n",
    "    - Data manipulation methods.\n",
    "    - Parameter in slicing method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Batch gradient descent: In this method we compute the gradient for entire training set and then we update the parameters or weights.\n",
    "- Stochastic gradient descent: In this method we first randomly shuffle the training set and then we compute the gradient for each and every training example and  update the parameter or weights.\n",
    "- Mini batch Stochastic gradient descent: In this method we first draw a mini batch from randomly shuffled  data set. And we compute the gradient for this mini batch and update the parameter or weights."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Advantage of Stochastic gradient descent:\n",
    "    - It is faster than batch gradient descent because there is redundancy in the batch.\n",
    "    - Open results in more better than generalised solution because of the noisy  update of the weight.\n",
    "    - This method can be useful where we need to track the  system overtime. We can update the weight stochastically.\n",
    "- Issue with Stochastic gradient descent:\n",
    "    - Sometimes noise in stochastic gradient descent may lead to no convergence  at all.\n",
    "    - Equivalent to use of “mini-batches” in SGD (Start with a small batch size and increase size as training proceeds.\n",
    "    - Can be controlled using learning rate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Advantages of Batch GD\n",
    "    - Conditions of convergence are well understood.\n",
    "    - Many acceleration techniques (e.g. conjugate gradient) only operate in batch learning.\n",
    "    - Theoretical analysis of the weight dynamics and convergence rates are simpler.\n",
    "    - \n",
    "      ::: {.callout-tip}\n",
    "      Mini-batch SGD is the most commonly used method, with a mini-batch size of 20 or so (can be higher depending on dataset size).\n",
    "      :::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight update is given by <br>\n",
    "$$\\Delta\\theta_{t+1}=\\alpha\\nabla_{\\theta}J(\\theta_{t};x^{(i)},y)^{(i)}+ \\overbrace{\\boxed{\\gamma \\Delta \\theta _t}}^{\\text{Momentum term}}   $$\n",
    "\n",
    "- Can increase speed when the cost surface is highly non-spherical.\n",
    "- Damps step sizes along directions of high curvature, yielding a larger effective learning rate along the directions of low curvature.\n",
    "- \n",
    "  ::: {.callout-note}\n",
    "  Here the idea is that, if we are going in one direction, we build a momentum to that direction, so if there is a sudden change in the direction, we do not suddenly change the direction but go for a average  as per the above equation. \n",
    "  :::\n",
    "- Larger the $\\gamma$, more the previous gradients affect the current step.\n",
    "- Generally $\\gamma$ is set to $0.5$ until initial learning stabilizes and then increased to $0.9$ or higher."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD with Momentum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume: <br>\n",
    "\n",
    "> Learning rate $\\alpha$ <br>\n",
    "> momentum parameter $\\gamma$ <br>\n",
    "> minibatch size $m$ <br>\n",
    "> Initial weights $\\theta_t$\n",
    "\n",
    "::: {.alert .alert-dismissible .alert-success}\n",
    "1. _while_ stopping criterion not met _do_\n",
    "2. Sample a minibatch of $m$ examples from the training set.\n",
    "3. Compute gradient estimate ${\\nabla}_{\\theta}{\\sum}_{i=1}^{m}J({\\theta}_{t};X^{(i)},y^{(i)})$\n",
    "4. Compute update $\\Delta\\theta_{t+1}=\\alpha\\nabla_{\\theta}J+\\gamma\\Delta\\theta_{t}$\n",
    "5. Apply update $\\theta_{t+1}=\\theta_{t}-\\Delta\\theta_{t+1}$\n",
    "6. _End while_\n",
    ":::\n",
    "\n",
    "Alternate view of momentum update \n",
    "\n",
    "$$\\overbrace{\\mathrm{v}_{t+1}}^{\\text{Velocity }} =\\gamma \\times \\overbrace{\\mathrm{v}_{t}}^{\\text{past velocity}} +\\alpha\\nabla_{\\theta}J(\\theta_{t};X^{(i)},y^{(i)})$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov Accelerated Momentum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this method weight update is given as below.\n",
    "  $$\n",
    "  \\begin{align*}\n",
    "    \\mathrm{v}_{t+1}&=\\gamma\\mathrm{v}_{t}+\\alpha\\nabla_{\\theta}J(\\boxed{\\theta_{t}-\\gamma\\mathrm{v}_{t}} ;X^{(i)},y^{(i)})\\\\\n",
    "    \\theta_{t+1}&=\\theta_{t}-\\mathrm{v}_{t+1}\n",
    "  \\end{align*}\n",
    "  $$\n",
    "\n",
    "\n",
    "  ::: {.btn .btn-secondary .disabled }\n",
    "  <p align=\"justify\">\n",
    "\n",
    "  Here intuition  behind the term $\\boxed{\\theta_{t}-\\gamma\\mathrm{v}_{t}}$ is that, If you know the stock market price for tomorrow, we can do much better on stocks. In the same way we take a step forward and find the cost and then compute derivative, so that we can do better.\n",
    "  </p>\n",
    "  :::\n",
    "\n",
    "- In general, this method performs better.<br><br>\n",
    "  ![](CS5480_images//Acrobat_hqsl9yl8Vn.png) <br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD with Nesterov Momentum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Learning rate $\\alpha$ <br>\n",
    "> momentum parameter $\\gamma$ <br>\n",
    "> minibatch size $m$ <br>\n",
    "> Initial weights $\\theta_t$\n",
    "> Initial velocity $v_t$\n",
    "\n",
    "::: {.alert .alert-dismissible .alert-success}\n",
    "1. _while_ stopping criterion not met _do_\n",
    "2. Sample a minibatch of $m$ examples from the training set.\n",
    "3. Apply interim update $\\tilde{\\theta}_{t}=\\theta_{t}-\\gamma\\bf{v}_{t}$\n",
    "4. Compute gradient estimate ${\\nabla}_{\\theta}{\\sum}_{i=1}^{m}J(\\tilde{\\theta}_{t};X^{(i)},y^{(i)})$\n",
    "5. Compute update ${\\bf v_{t+1}}=\\gamma{\\bf v}_{t}+\\alpha\\nabla_{\\theta}\\Sigma_{i=1}^{m}J(\\tilde{\\theta}_{t};x^{(i)},y^{(i)})$\n",
    "6. Apply update $\\theta_{t+1}=\\theta_{t}-\\bf v_{t+1}$\n",
    "7. _End while_\n",
    ":::\n",
    "\n",
    "::: {.text-danger}\n",
    "Momentum parameter ($\\gamma$): Weightage given to earlier steps taken in the process of gradient descent.\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Learning Rate Methods: Adagrad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Global Learning rate $\\alpha$ <br>\n",
    "> minibatch size $m$ <br>\n",
    "> Initial weights $\\theta_t$<br>\n",
    "> Small constant $\\delta$ near to $10^{-7}$ for numerical stability\n",
    "\n",
    "::: {.alert .alert-dismissible .alert-success}\n",
    "1. Initialize gradient accumulation variable $\\mathbf{r=0}$\n",
    "2. _while_ stopping criterion not met _do_\n",
    "3. Sample a minibatch of $m$ examples from the training set.\n",
    "4. Compute gradient estimate ${\\nabla}_{\\theta}{\\sum}_{i=1}^{m}J({\\theta}_{t};X^{(i)},y^{(i)})$\n",
    "\n",
    "::: {.text-danger}\n",
    "\n",
    "5. Compute squared gradient ${\\bf r}={\\bf r}+(\\nabla_{\\theta}J\\;\\odot\\;\\nabla_{\\theta}J)$\n",
    "6. Compute update $\\displaystyle \\Delta\\theta_{t+1}={\\frac{\\alpha}{\\delta+\\sqrt{\\bf r}}}\\ \\odot \\ \\nabla_\\theta J$ (division and square root computed elementwise)\n",
    "\n",
    ":::\n",
    "\n",
    "7. Apply update $\\theta_{t+1}=\\theta_{t}-\\Delta\\theta_{t+1}$\n",
    "8. _End while_\n",
    "\n",
    ":::\n",
    "\n",
    "- Intuition \n",
    "    - Calculates a different learning rate for each feature.\n",
    "    - Sparse features have higher learning rate.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Global Learning rate $\\alpha$ <br>\n",
    "> Decay rate $\\rho$ <br>\n",
    "> minibatch size $m$ <br>\n",
    "> Initial weights $\\theta_t$<br>\n",
    "> Small constant $\\delta$ near to $10^{-6}$ for numerical stability\n",
    "\n",
    "::: {.alert .alert-dismissible .alert-success}\n",
    "1. Initialize gradient accumulation variable $\\mathbf{r=0}$\n",
    "2. _while_ stopping criterion not met _do_\n",
    "3. Sample a minibatch of $m$ examples from the training set.\n",
    "4. Compute gradient estimate ${\\nabla}_{\\theta}{\\sum}_{i=1}^{m}J({\\theta}_{t};X^{(i)},y^{(i)})$\n",
    "\n",
    "::: {.text-danger}\n",
    "\n",
    "5. Accumulate squared gradient ${\\bf r}=\\rho{\\bf r}+(1-\\rho)(\\nabla_{\\theta}J\\;\\odot\\;\\nabla_{\\theta}J)$\n",
    "6. Compute update $\\displaystyle \\Delta\\theta_{t+1}={\\frac{\\alpha}{\\delta+\\sqrt{\\bf r}}}\\ \\odot \\ \\nabla_\\theta J$ (division and square root computed elementwise)\n",
    "\n",
    ":::\n",
    "\n",
    "7. Apply update $\\theta_{t+1}=\\theta_{t}-\\Delta\\theta_{t+1}$\n",
    "8. _End while_\n",
    "\n",
    ":::\n",
    "\n",
    "- Intuition \n",
    "    - Calculates a different learning rate for each feature.\n",
    "    - Sparse features have higher learning rate.\n",
    "    - Now we can control the accumulation using $\\rho$.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp with Nesterov Momentum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Global Learning rate $\\alpha$ <br>\n",
    "> Decay rate $\\rho$ <br>\n",
    "> Momentum co-efficient $\\gamma$<br>\n",
    ">Inital velocity $\\bf v$<br>\n",
    "> minibatch size $m$ <br>\n",
    "> Initial weights $\\theta_t$<br>\n",
    "> Small constant $\\delta$ near to $10^{-6}$ for numerical stability\n",
    "\n",
    "::: {.alert .alert-dismissible .alert-success}\n",
    "1. Initialize gradient accumulation variable $\\mathbf{r=0}$\n",
    "2. _while_ stopping criterion not met _do_\n",
    "3. Sample a minibatch of $m$ examples from the training set.\n",
    "\n",
    "::: {.text-warning}\n",
    "\n",
    "4. Apply interim update $\\tilde{\\theta}_{t}=\\theta_{t}-\\gamma\\bf{v}_{t}$\n",
    "5. Compute gradient estimate ${\\nabla}_{\\theta}{\\sum}_{i=1}^{m}J(\\tilde{\\theta}_{t};X^{(i)},y^{(i)})$\n",
    "\n",
    ":::\n",
    "\n",
    "::: {.text-danger}\n",
    "\n",
    "6. Accumulate squared gradient ${\\bf r}=\\rho{\\bf r}+(1-\\rho)(\\nabla_{\\theta}J\\;\\odot\\;\\nabla_{\\theta}J)$\n",
    "7. Compute update $\\displaystyle \\Delta\\theta_{t+1}={\\frac{\\alpha}{\\delta+\\sqrt{\\bf r}}}\\ \\odot \\ \\nabla_\\theta J$ (division and square root computed elementwise)\n",
    "\n",
    ":::\n",
    "\n",
    "8. Apply update $\\theta_{t+1}=\\theta_{t}-\\Delta\\theta_{t+1}$\n",
    "9. _End while_\n",
    "\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Global Learning rate $\\alpha$ <br>\n",
    "> Decay rates for moment estimate $\\rho_1$ and $\\rho_2$ <br>\n",
    "> Momentum co-efficient $\\gamma$<br>\n",
    "> minibatch size $m$ <br>\n",
    "> Initial weights $\\theta_t$<br>\n",
    "> Small constant $\\delta$ near to $10^{-8}$ for numerical stability\n",
    "\n",
    "::: {.alert .alert-dismissible .alert-success}\n",
    "1. Initialize first and second moment variables  $\\mathbf{r=0}$ and $\\mathbf{s=0}$\n",
    "2. _while_ stopping criterion not met _do_\n",
    "3. Sample a minibatch of $m$ examples from the training set.\n",
    "\n",
    "::: {.text-danger}\n",
    "\n",
    "4. Compute gradient estimate ${\\nabla}_{\\theta}{\\sum}_{i=1}^{m}J({\\theta}_{t};X^{(i)},y^{(i)})$\n",
    "5. Update biased first moment estimate ${\\bf s}=\\rho_1{\\bf s}+(1-\\rho_1)\\nabla_{\\theta}J$\n",
    "6. Update biased second moment estimate ${\\bf r}=\\rho_2{\\bf r}+(1-\\rho_2)(\\nabla_{\\theta}J\\;\\odot\\;\\nabla_{\\theta}J)$\n",
    "7. Correct bias in first moment $\\displaystyle \\tilde{\\bf s}=\\frac{\\bf s}{1-\\rho_1^t}$\n",
    "8. Now Correct bias in second  moment $\\displaystyle \\tilde{\\bf r}=\\frac{\\bf r}{1-\\rho_2^t}$\n",
    "9. Compute update $\\displaystyle \\Delta\\theta_{t+1}={\\alpha\\frac{\\tilde{\\bf s}}{\\delta+\\sqrt{\\tilde{\\bf r}}}}\\ \\odot \\ \\nabla_\\theta J$ (division and square root computed elementwise)\n",
    "\n",
    ":::\n",
    "\n",
    "10. Apply update $\\theta_{t+1}=\\theta_{t}-\\Delta\\theta_{t+1}$\n",
    "11. _End while_\n",
    "\n",
    ":::\n",
    "\n",
    "- Intuition \n",
    "    - Similar to RMSProp with momentum\n",
    "    - Uses the idea of momentum, as well as having a different learning rate for each dimension (which is automatically adjusted, as in Adagrad, Adadelta or RMS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interesting Facts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.card  .bg-light .mb-3 }\n",
    "::: {.card-header}\n",
    "{{< fa bolt >}}\n",
    ":::\n",
    "::: {.card-body}\n",
    "::: {.card-title}\n",
    "<h4>Interesting Facts</h4>\n",
    ":::\n",
    "::: {.card-text}\n",
    "\n",
    "- If input data is sparse, adaptive learning-rate methods may be best.No need to tune learning rate\n",
    "- Learning rates diminish fast in Adagrad, RMSProp addresses this\n",
    "issue.\n",
    "- Adam adds bias-correction and momentum to RMSprop.\n",
    "- RMSprop, Adadelta, and Adam are similar algorithms, bias-correction helps Adam slightly outperform RMSprop towards the end of optimization as gradients become sparser\n",
    "- Adam might be the best overall choice (May not be always true!)\n",
    "- Vanilla SGD depends on a robust initialization and annealing schedule, and may get stuck in saddle points rather than local minima.\n",
    "- Many recent papers use vanilla SGD without momentum, but with a\n",
    "simple learning rate annealing schedule\n",
    "- Some standard choices for training deep networks: SGD + Nesterov\n",
    "momentum, SGD with Adagrad/RMSProp/Adam.\n",
    "- ReLUs, Leaky ReLUs and MaxOut are the best bets for activation\n",
    "functions\n",
    "\n",
    ":::\n",
    ":::\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sigmoid  \n",
    "    - This function takes any real value as input and outputs values in the range of 0 to 1. \n",
    "    - Formulae :\n",
    "      $$f(x)=\\frac{1}{1+e^{-x}} $$\n",
    "    - It is commonly used for models where we have to predict the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice because of its range.\n",
    "- Tanh Function\n",
    "    - It's output range is  -1 to 1.\n",
    "    - Formulae :\n",
    "      $$f(x)=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} $$    \n",
    "    - The output of the tanh activation function is Zero centered; hence we can easily map the output values as strongly negative, neutral, or strongly positive.\n",
    "    - Usually used in hidden layers of a neural network as its values lie between -1 to 1; therefore, the mean for the hidden layer comes out to be 0 or very close to it. It helps in centering the data and makes learning for the next layer much easier.\n",
    "    - \n",
    "      ::: {.callout-note}\n",
    "      Although both sigmoid and tanh face vanishing gradient issue, tanh is zero centered, and the gradients are not restricted to move in a certain direction. Therefore, in practice, tanh nonlinearity is always preferred to sigmoid nonlinearity.\n",
    "      :::\n",
    "- ReLU \n",
    "    - The neurons will only be deactivated if the output of the linear transformation is less than 0.\n",
    "    - Formulae :\n",
    "      $$ f(x)=\\max(0,x)$$\n",
    "    - Since only a certain number of neurons are activated, the ReLU function is far more computationally efficient when compared to the sigmoid and tanh functions.\n",
    "    - ReLU accelerates the convergence of gradient descent towards the global minimum of the loss function due to its linear, non-saturating property.\n",
    "    - Found to accelerate convergence of SGD compared to sigmoid/tanh functions (a factor of 6) in AlexNet\n",
    "    - It has the The Dying ReLU problem\n",
    "        - The negative side of the graph makes the gradient value zero. Due to this reason, during the backpropagation process, the weights and biases for some neurons are not updated. This can create dead neurons which never get activated.\n",
    "        - All the negative input values become zero immediately, which decreases the model’s ability to fit or train from the data properly. \n",
    "- Leaky ReLU\n",
    "    - Leaky ReLU is an improved version of ReLU function to solve the Dying ReLU problem as it has a small positive slope in the negative area.\n",
    "    - formula:\n",
    "      $$f(x)=\\max(0.1x,x)$$\n",
    "    - The predictions may not be consistent for negative input values. \n",
    "    - The gradient for negative values is a small value that makes the learning of model parameters time-consuming.\n",
    "- Parametric ReLU\n",
    "    - formula:\n",
    "      $$f(x)=\\max(ax,x)$$\n",
    "    - Parametric ReLU is another variant of ReLU that aims to solve the problem of gradient’s becoming zero for the left half of the axis.\n",
    "    - This function provides the slope of the negative part of the function as an argument $a$. By performing backpropagation, the most appropriate value of $a$ is learnt.\n",
    "    - The parameterized ReLU function is used when the leaky ReLU function still fails at solving the problem of dead neurons, and the relevant information is not successfully passed to the next layer. \n",
    "- Exponential Linear Units (ELUs) \n",
    "    - ELU uses a log curve to define the negativ values unlike the leaky ReLU and Parametric ReLU functions with a straight line.\n",
    "    - formula:\n",
    "      $$f(x) = \\begin{cases}\n",
    "        x &\\text{if } x \\ge 0 \\\\\n",
    "        \\alpha(e^x-1) &\\text{if } x<0\n",
    "        \\end{cases}$$\n",
    "    - ELU becomes smooth slowly until its output equal to $\\alpha$ whereas RELU sharply smoothes.\n",
    "    - Avoids dead ReLU problem by introducing log curve for negative values of input. It helps the network nudge weights and biases in the right direction.\n",
    "    - No learning of the $\\alpha$ value takes place\n",
    "- Softmax \n",
    "  - It is used for multiclass classification.\n",
    "  - formula:\n",
    "    $$f(x_i)=\\frac{e^{x_i}}{\\sum_{j}e^{x_j} } $$ \n",
    "- swish \n",
    "    - Formulae :\n",
    "      $$f(x)=\\frac{x}{1+e^{-x}} $$\n",
    "    - Swish is a smooth function that means that it does not abruptly change direction like ReLU does near x = 0. Rather, it smoothly bends from 0 towards values < 0 and then upwards again.\n",
    "    - The swish function being non-monotonous enhances the expression of input data and weight to be learnt.\n",
    "- Max Out\n",
    "    - It is generalization of ReLU.\n",
    "    - formula:\n",
    "      $$f(x)=\\max(w_1^Tx+b_1,w_2^Tx+b_2)$$\n",
    "- \n",
    "  ::: {.alert .alert-dismissible .alert-secondary} \n",
    "  <h5>Guide-Line to Chose activation Function</h5> \n",
    "    - Regression — Linear Activation Function\n",
    "    - Binary Classification — Sigmoid/Logistic Activation Function\n",
    "    - Multiclass Classification — Softmax\n",
    "    - Multilabel Classification — Sigmoid\n",
    "    - ReLU activation function should only be used in the hidden layers.\n",
    "    - Sigmoid/Logistic and Tanh functions should not be used in hidden layers as they make the model more susceptible to problems during training (due to vanishing gradients)\n",
    "    - Swish function is used in neural networks having a depth greater than 40 layers.\n",
    "    - Try tanh, but expect it to work worse than ReLU/Maxout.\n",
    "  :::\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross-Entropy Loss Function\n",
    "    - Formula\n",
    "      $$J=-\\frac{1}{m}\\sum_{i=1}^{m}y_{i}\\log\\hat{y_{i}}+(1-y_{i})\\log(1-\\hat{y_{i}})$$\n",
    "    - If the activation function is sigmoid $\\left(\\sigma(z)=\\,\\frac{1}{1+e^{-z}}\\right)$ the darivative of cross-entropy is:<br>\n",
    "    so,  \n",
    "      $$\\hat y=\\sigma(w^Tz) \\text{ and } \\sigma(z)=\\frac{1}{1+e^{-z}}$$\n",
    "      so we can simplify  it as below considering just one record\n",
    "      $$J = -\\frac{1}{m}\\sum_{i=1}^{m} \\left(y\\log(\\sigma(z))+(1-y)\\log(1-\\sigma(z))\\right)$$ \n",
    "      Now we find derivative \n",
    "      $$\\begin{align*}\n",
    "      \\frac{\\partial J}{\\partial w_j} &= -\\frac{1}{m}\\sum_{i=1}^{m} \\left(y \\frac{1}{\\sigma(z)}\\sigma(z)'x_j+(1-y) \\left( \\frac{1}{1-\\sigma(z)}\\right) (-\\sigma(z)'x_j)\\right) \\\\\n",
    "      &=-\\frac{1}{m}\\sum_{i=1}^{m}\\left(  \\frac{y}{\\sigma(z)}-  \\frac{1-y}{1-\\sigma(z)}\\right) \\sigma(z)'x_j \\\\\n",
    "      &= -\\frac{1}{m}\\sum_{i=1}^{m}\\frac{y(1-\\sigma(z))-\\sigma(z)(1-y)}{\\sigma(z)(1-\\sigma(z))}\\sigma(z)'x_j  \\\\\n",
    "      &= -\\frac{1}{m}\\sum_{i=1}^{m}\\frac{y-y*\\sigma(z)-\\sigma(z)+\\sigma(z)*y}{\\sigma(z)(1-\\sigma(z))}\\sigma(z)'x_j  \\\\\n",
    "      &= -\\frac{1}{m}\\sum_{i=1}^{m}\\frac{y-\\sigma(z)}{\\sigma(z)(1-\\sigma(z))}\\sigma(z)(1-\\sigma(z))*x_j  \\\\\n",
    "      &=-\\frac{1}{m}\\sum_{i=1}^{m}(y-\\sigma(z))*x_j\\\\\n",
    "      &=\\frac{1}{m}\\sum_{i=1}^{m}(\\sigma(z)-y)*x_j\n",
    "      \\end{align*}$$\n",
    "    - Even if we consider softmax as activation function\n",
    "      $$\\left(a_{j}^{L}=\\frac{e^{z_{j}^{L}}}{\\sum_{k}\\epsilon^{z_{k}^{L}}}\\right)$$ \n",
    "      we get similar derivative \n",
    "      $$\\frac{\\partial J}{\\partial w_{j k}^{L}}=\\overbrace{\\boxed{a_{k}^{L-1}(a_{j}^{L}-y_{j})}}^{\\text{similar to cross entropy}}  $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Loss function \n",
    "\n",
    "- Classification\n",
    "    - BCECriterion: binary cross-entropy for Sigmoid (two-class version)\n",
    "    - ClassNLLCriterion: negative log-likelihood (multi-class)\n",
    "    - MarginCriterion: two class margin-based loss\n",
    "- Regression\n",
    "    - AbsCriterion: measures the mean absolute value of the element wise difference between input\n",
    "    - MSECriterion: mean square error (a classic)\n",
    "    - DistKLDivCriterion: Kullback–Leibler divergence (for fitting continuous probability distributions)\n",
    "- Embedding (measuring whether two inputs are similar or dissimilar)\n",
    "    - L1HingeEmbeddingCriterion: L1 distance between two inputs\n",
    "    - CosineEmbeddingCriterion: cosine distance between two inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "$\\tiny  {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0e5e69b8442e8f020791fad6bfcef3777f0e89e0f1a2517b6b628b2eaf0fe66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
