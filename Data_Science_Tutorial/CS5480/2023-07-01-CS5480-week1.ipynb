{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "author: Abhishek Kumar Dubey\n",
    "badges: false\n",
    "categories:\n",
    "- Deep Learning\n",
    "date: '2023-07-01'\n",
    "description: Training Error, Generalization Error\n",
    "image: CS5480_images/chrome_mmOWWTaZry.png\n",
    "title: Deep Learning 1\n",
    "toc: true\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History of Deep Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "![](CS5480_images/chrome_np99k2xLxh.png)\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### McCulloch-Pitts Neuron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed Forward McCulloch-Pitts net can compute any Boolean function $f : \\{0,1\\}^n \\rightarrow \\{0,1\\}$<br>\n",
    "Recursive McCulloch-Pitts networks can simulate any Deterministic Finite Automaton (DFA).<br>\n",
    "McCulloch-Pitts networks can be represented mathematically as shown below:\n",
    "\n",
    "$$\n",
    "y(x_1,x_2,\\dots,x_{n+m},\\theta) = \\begin{cases}\n",
    "   1 &\\displaystyle \\text{if } \\sum_{i=1}^n x_i \\ge \\theta  \\text{ and  } \\sum_{i=n+1}^m x_i =0\\\\\n",
    "   0 &\\displaystyle \\text{if } \\sum_{i=1}^n x_i < \\theta  \\text{ or  } \\sum_{i=n+1}^m x_i >0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- Here all signals are binary \n",
    "- Threshold $\\theta \\in \\mathbb{N}$\n",
    "- $n$ excitatory inputs $x_1,\\dots,x_n$\n",
    "- $m$ inhibitory inputs $x_{n+1},\\dots,x_{n+m}$\n",
    "- One output $y$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rosenblatt’s Perceptron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rosenblatt’s Perceptron can be represented mathematically as shown below:\n",
    "$$\n",
    "f(x) = \\begin{cases}\n",
    "   1 &\\text{if } w\\cdot x+b>0 \\\\\n",
    "   0 &\\text{otherwise } \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Boolean Gates using a Perceptron\n",
    "\n",
    "![](CS5480_images/chrome_mmOWWTaZry.png)\n",
    "\n",
    "- Hebb, in his influential book The organization of Behavior (1949),\n",
    "claimed\n",
    "    - Behavior changes are primarily due to the changes of synaptic strengths $(w_{ij})$ between neurons $i$ and $j$\n",
    "    - Hebbian learning law: $w_{ij}$ increases only when both $i$ and $j$ are \"on\".\n",
    "    - _\"Neurons that fire together, wire together. Neurons that fire out of sync, fail to link\"_\n",
    "    - In perceptron learning, Hebbian law can be restated as:\n",
    "        - $w_{ij}$ increases only if the outputs of both units $x_i$\n",
    "and $y$ have the same sign.\n",
    "    - In our simple network (one output and n input units)\n",
    "      $$\\nabla W_{ij}=\\nabla W_{ij}(\\text{new})-\\nabla W_{ij}(\\text{old})=X_iY$$\n",
    "      Or\n",
    "      $$\\nabla W_{ij}=\\nabla W_{ij}(\\text{new})-\\nabla W_{ij}(\\text{old})=\\alpha X_iY$$\n",
    "\n",
    "#### Hebbian Learning:\n",
    "\n",
    "- Initialization: $b = 0, w_i = 0, i = 1 \\text{ to } n$\n",
    "- For each of the training sample $(x,y)$, do the following:\n",
    "- Update weight and bias as :\n",
    "  $$\\begin{align*}\n",
    "   w_i&:=w_i+x_i\\times y_i, i=1 \\text{ to } n \\\\\n",
    "   b&:=b+x_i \\times y\n",
    "  \\end{align*}$$\n",
    "\n",
    "We can obtain AND function using bipolar units $(-1,-1)$. In this case a correct boundary $-1 +x_1+x_2=0$ shall we learned. <br>\n",
    "But we can not obtain AND function using Binary Unit $(1,0)$ and $\\alpha =1$. In this case an incorrect boundary $1+x_1+x_2=0$ is learned. \n",
    "\n",
    "#### Perceptron Learning\n",
    "\n",
    "1. Initialization: $b = 0, w_i = 0, i = 1 \\text{ to } n$\n",
    "2. While stop condition is false do step 2 to 4\n",
    "3. For each of the training sample $(x,y)$ do steps 3 to 4\n",
    "4. compute output of perceptron, $o$\n",
    "5. if $o\\ne y$\n",
    "   $$\\begin{align*}\n",
    "   w_i&:=w_i+\\alpha \\times x_i \\times o ,\\quad i= 1 \\text{ to } n \\\\\n",
    "   b&:=b+\\alpha \\times o\n",
    "   \\end{align*}\n",
    "   $$\n",
    "\n",
    "- Learning occurs only when a sample has $o\\ne y$\n",
    "- Two loops, a completion of the inner loop (each sample is used once) is called an epoch\n",
    "- Stop when \n",
    "  - When no weight is changed in the current epoch, or\n",
    "  - When pre-determined number of epochs is reached\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Widrow-Hoff Learning Rule"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is also called  the delta learning rule.\n",
    "- Developed for a perceptron.\n",
    "- Learning algorithm: same as Perceptron learning, just change step 4 as below\n",
    "   $$\\begin{align*}\n",
    "   w_i&:=w_i+\\alpha \\times x_i \\times \\boxed{(o-y)} \\quad i= 1 \\text{ to } n \\\\\n",
    "   b&:=b+\\alpha \\times \\boxed{(o-y)}\n",
    "   \\end{align*}\n",
    "   $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron convergence theorem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there exists an exact solution (if the training data set is linearly separable), then the perceptron learning algorithm is guaranteed to find an exact solution in a finite number of steps.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proof\n",
    "\n",
    "<br>\n",
    "\n",
    "![](CS5480_images/chrome_Myg1q0YkBZ.png)\n",
    "\n",
    "Consider the above image \n",
    "\n",
    "- Perceptron tries to find the boundary $W^TX=0$\n",
    "- Angle between $W$ and any point $X$ which lies on that line will be $90^{\\circ}$\n",
    "- Angle between positive points $(p_1,p_2,p_3,\\dots)$ and $W$ shall be less than $90^{\\circ}$\n",
    "- Angle between negative  points $(n_1,n_2,n_3,\\dots)$ and $W$ shall be greater than $90^{\\circ}$\n",
    "- If for any positive point say $p_3$ the angle with $W$ becomes more than $90^{\\circ}$, it means perceptron has made a mistake\n",
    "- Because there is a mistake so the perceptron learning algorithm now tries to update the weight, with the rule $\\overline {W}=W+ \\alpha X\\times Y$, consider $\\alpha$ and $Y$ to be $1$ for simplicity. So it becomes $\\overline W=W+  X$ \n",
    "- The cos angle between $p_3$ and $\\overline W$ is proportional to $\\overline W^TX$\n",
    "  $$\\begin{align*}\n",
    "   \\cos \\theta &\\propto \\overline W^TX \\\\\n",
    "   &\\propto ( W+X)^TX \\\\\n",
    "   &\\propto  W^TX+X^TX \n",
    "  \\end{align*} \n",
    "  $$\n",
    "\n",
    "- Since $X^TX$ is positive have we can say that the value of $\\cos \\theta$ for  $\\overline  W$ and $X$ is more than that of $W$ and $X$ \n",
    "- Hence we can say that the value of angle $\\theta$ for  $\\overline  W$ and $X$ is less than that of $W$ and $X$ \n",
    "- we can see that the angle between $p_3$ and $W$ is decreased after weight update \n",
    "- Hence we can say that after certain number of iteration the angle between  $p_3$ and $W$ will become less than $90^{\\circ}$ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron and Other Linear Classifiers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic regression and perceptron with sigmoid activation\n",
    "are the same.\n",
    "- Linear SVMs and perceptrons are 'almost' the same. 'Almost' because SVM provides best margin but preceptron doesn't have such notion."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR Gate using a Perceptron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XOR can be solved by a more complex network with hidden units (Multi-Layer Perceptron)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall: Training a Neural Network\n",
    "\n",
    "It was already discuss  [here](../CS5590/2022-09-24-CS5590-week5.ipynb#multi-layer-perceptrons) in Machine Learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "$\\tiny  {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Aug 10 2022, 11:40:04) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
