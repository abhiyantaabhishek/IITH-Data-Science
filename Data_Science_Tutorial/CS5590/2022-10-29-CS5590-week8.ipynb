{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "author: Abhishek Kumar Dubey\n",
    "badges: true\n",
    "categories:\n",
    "- Machine Learning\n",
    "date: '2022-10-29'\n",
    "description: Introduction to Learning Theory, Regression Formulation\n",
    "image: CS5590_images/Acrobat_DOQ241iPvG.png\n",
    "title: Machine Learning 8\n",
    "toc: true\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Towards formalizing ‘learning’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The basic process of learning\n",
    "- Observe a phenomenon\n",
    "- Construct a model from observations\n",
    "- Use that model to make decisions/predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A statistical machinery for learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phenomenon of interest:\n",
    "\n",
    "- Input space: $X$; Output space: $Y$\n",
    "- There is an unknown distribution $D$ over $(X,Y)$\n",
    "- The learner observes $m$ examples $(x_1 ,y_1,\\dots, x_m ,y_m )$ drawn from $D$\n",
    "\n",
    "Construct a model:\n",
    "\n",
    "- Let $F$ be a collection of models, where each $f: X \\rightarrow Y$ predicts $y$ given $x$\n",
    "- From $m$ observations, select a model $f_m$  in $F$ which predicts well.\n",
    "- Generalization error of $f$:\n",
    "  $$\\mathrm{err}(f):=\\mathbb{P}_{(x,y)\\sim D}\\left[f(x)\\ne y\\right]$$\n",
    "  Notice this error is calculated on the whole distribution $D$\n",
    "- We can say that we have learned a phenomenon if \n",
    "  $$\\mathrm{err}(f_m)-\\mathrm{err}(f^*)\\le \\epsilon \\quad f^*:=\\argmin_{f \\in F}\\mathrm{err}(f)$$\n",
    "  for any tolerance level $\\epsilon$ of our choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all tolerance levels $\\epsilon > 0$, and all confidence levels $\\delta > 0$, if there exists some model selection algorithm $\\mathcal{A}$ that selects $f_m^\\mathcal{A} \\in \\mathcal{F}$ from $m$ observations i.e. \n",
    " \n",
    " - $$\\mathcal{A}:(x_i,y_i)_i^m \\mapsto f_m^\\mathcal{A}$$\n",
    " \n",
    " - And \n",
    "   $$\\mathrm{err}(f_m^\\mathcal{A})-\\mathrm{err}(f^*)\\le \\epsilon$$\n",
    "   with probability at least $1-\\delta$ over the draw of the sample.\n",
    "\n",
    "We call \n",
    "\n",
    "- The model class $\\mathcal{F}$ is __PAC-Learnable__. (Probably Approximate Correct)\n",
    "- If $m$ is polynomial in $\\frac{1}{\\epsilon}$ and $\\frac{1}{\\delta}$ then $\\mathcal{F}$ is __Efficiently PAC-Learnable__.\n",
    "\n",
    "A popular algorithm:\n",
    "\n",
    "- Empirical risk minimization (ERM) algorithm.\n",
    "  $$f_m^{\\text{ERM}}:=\\argmin_{f \\in \\mathcal{F}}\\frac{1}{m}\\sum_{i=1}^m\\mathbf{1}\\{f(x_i)\\ne y_i\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAC Learning Simple Model Classes\n",
    "\n",
    "__Theorem (finite seize $\\mathcal{F}$ ):__\n",
    "\n",
    "- Pick any tolerance level $\\epsilon > 0,$ and any confidence level $\\delta > 0$ let $(x_1,y_1),\\dots,(x_m,y_m)$ be $m$ examples drawn from an unknown $\\mathcal{D}$ if $\\displaystyle m \\ge C \\cdot \\frac{1}{\\epsilon^2}\\ln\\frac{\\lvert \\mathcal{F}\\rvert}{\\delta}$, then with the probability at least $1- \\delta$\n",
    "$$\\mathrm{err}(f_m^\\mathrm{ERM})-\\mathrm{err}(f^*)\\le \\epsilon$$\n",
    "$$\\boxed{\\mathcal{F}\\text{ is efficiently PAC-learnable}}$$\n",
    "\n",
    "- Proof Sketch\n",
    "  - Define Generalization error of $f$\n",
    "    $$\\text{err}(f):=\\mathbb{E}_{(x,y)\\sim \\mathcal{D}}\\left[\\mathbf{1}\\{f(x_i)\\ne y_i\\}\\right]$$\n",
    "  - Define sample error of $f$\n",
    "    $$\\text{err}_m(f):=\\frac{1}{m}\\sum_{i=1}^m\\left[\\mathbf{1}\\{f(x_i)\\ne y_i\\}\\right]$$\n",
    "  Fix any $f \\in \\mathcal{F}$ and sample $(x_i,y_i)$, define  random variable \n",
    "  $$\\mathbf{Z}_i^f=\\mathbf{1}\\{f(x_i)\\ne y_i\\}$$\n",
    "  Now we can re-write generalization error and sample error as below\n",
    "  - Generalization error of $f$\n",
    "    $$\\text{err}(f):=\\mathbb{E}_{(x,y)\\sim \\mathcal{D}}\\left[\\mathbf{Z}_1^f\\right]$$\n",
    "  - sample error of $f$\n",
    "    $$\\text{err}_m(f):=\\frac{1}{m}\\sum_{i=1}^m\\left[\\mathbf{Z}_i^f\\right]$$\n",
    "\n",
    "### __Lemma (Chernoff-Hoeffding bound'63)__<br>\n",
    "Let $\\mathbf{Z}_1,\\dots,\\mathbf{Z}_m$ be $m$ Bernouli r.v. drawn independently from $\\mathbf{B(p)}$, for any tolerance level $\\epsilon > 0$\n",
    "$${\\mathcal{P}}_{{\\mathit{\\mathbf{Z}}}_i } \\left\\lbrack \\left \\lvert \\frac{1}{m}\\sum_{i=m}^m \\left\\lbrack {\\mathit{\\mathbf{Z}}}_i \\right\\rbrack -\\mathbb{E}\\left\\lbrack \\mathbf{Z}_1 \\right\\rbrack \\right \\rvert\\ge \\epsilon \\right\\rbrack \\le 2e^{-2\\epsilon^2 m}$$\n",
    "\n",
    "Analyze\n",
    "$$\\begin{align*}{}\n",
    "&{\\mathcal{P}}_{\\left(x_i ,y_i \\right)} \\left\\lbrack \\mathrm{exists}\\;f\\in \\mathcal{F},\\left \\lvert \\frac{1}{m}\\sum_{i=m}^m \\left\\lbrack {\\mathit{\\mathbf{Z}}}_i^f \\right\\rbrack -\\mathbb{E}\\left\\lbrack {\\mathbf{Z}}_1^f \\right\\rbrack \\right \\rvert\\ge \\epsilon \\right\\rbrack \\\\\n",
    "&\\qquad \\quad \\le \\sum_{f\\in \\mathcal{F}} {\\mathcal{P}}_{\\left(x_i ,y_i \\right)} \\left\\lbrack \\left \\lvert \\frac{1}{m}\\sum_{i=m}^m \\left\\lbrack {\\mathit{\\mathbf{Z}}}_i^f \\right\\rbrack -\\mathbb{E}\\left\\lbrack {\\mathbf{Z}}_1^f \\right\\rbrack  \\right \\rvert\\ge \\epsilon \\right\\rbrack \\\\\n",
    "&\\qquad  \\quad \\le 2{\\left \\lvert \\;\\mathcal{F}\\right \\rvert e}^{-2\\epsilon^2 m} \\\\\n",
    "&\\qquad  \\quad \\le \\delta \n",
    "\\end{align*}$$\n",
    "\n",
    "Equivalently by choosing $\\displaystyle m \\ge \\frac{1}{2 \\epsilon ^2}\\ln\\frac{2\\mathcal{F}}{\\delta}$ with probability at least  \n",
    "$1-\\delta,$ for __all__ $f \\in \\mathcal{F}$\n",
    "$$\\left \\lvert \\frac{1}{m}\\sum_{i=m}^m \\left\\lbrack {\\mathit{\\mathbf{Z}}}_i^f \\right\\rbrack -\\mathbb{E}\\left\\lbrack {\\mathbf{Z}}_1^f \\right\\rbrack  \\right \\rvert=\\left \\lvert \\mathrm{err}_m(f)-\\mathrm{err}(f) \\right \\rvert \\le \\epsilon$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning general concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VC dimension "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VC dimension is also known as Vapnik-Chervonenkis dimension.\n",
    "\n",
    "- Definition <br>\n",
    "  We say that a model class $\\mathcal{F}$ has VC dimension $d,$ if $d$ is the largest set of points $x_1,\\dots,x_d \\subset X$ Such that for all possible labelling of  $x_1,\\dots,x_d$ there exists some $f \\in \\mathcal{F}$ that achieves that labelling.\n",
    "  - Example: $\\mathcal{F}=$ Linear classifier in $\\mathbb{R}^2$<br><br>\n",
    "    ![](CS5590_images/Acrobat_FqAMtysA4S.png)<br>\n",
    "    $$\\text{VC}(\\mathcal{F})=3$$\n",
    "    Notice that we can change the structure of the data, for e.g. on the left side, data is in the from of triangle, we can not change it to form a line, i.e. we can not change the position of the data, but we can change the label as we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VC Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Theorem (Vapnik-Chervonenkis'71)__<br>\n",
    "  Chose any tolerance level $\\epsilon >0,$ and any confidence level $\\delta>0$ let $(x_1,y_1),\\dots,(x_m,y_m)$ be $m$ examples drawn from an unknown $\\mathcal{D},$ <br>\n",
    "  if $\\displaystyle m>C.\\frac{\\text{VC}(\\mathcal{F})\\ln(1/\\delta)}{\\epsilon^2},$ then with probability at least $1-\\delta$\n",
    "  $$\\mathrm{err}(f_m^{\\mathrm{ERM}})-\\mathrm{err}(f^*)\\le\\epsilon$$\n",
    "  $$\\boxed{\\mathcal{F} \\text{ is efficiently PAC-learnable}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tightness of VC Bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Theorem (VC lower bound)__<br>\n",
    "Let $\\mathcal{A}$ be any model selection algorithm that given $m$ samples, returns  a model from $\\mathcal{F}$, that is $\\mathcal{A}:(x_i,y_i)_{i=1}^m \\mapsto f_m^\\mathcal{A}$<br>\n",
    "For all tolerance level $0<\\epsilon <1,$ and all confidence levels $0<\\delta<1/4,$ there exists a distribution $\\mathcal {D}$ such that if $\\displaystyle m \\leq C \\cdot \\frac{\\mathrm{VC}(\\mathcal{F})}{\\epsilon^2}$\n",
    "$$\\mathbb{P}_{(x_i,y_i)}\\left[ \\left \\lvert \\mathrm{err}(f_m^{\\mathcal{A}})-\\mathrm{err}(f^*) \\right \\rvert > \\epsilon \\right]> \\delta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facts of VC dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VC dimension:\n",
    "    - A combinatorial concept to capture the true richness of $\\mathcal{F}$\n",
    "    - Often (but not always!) proportional to the degrees of freedom or the number of independent parameters in $\\mathcal{F}$\n",
    "- Other Observations\n",
    "    - VC dimension of a model class fully characterizes its learning ability!\n",
    "    - Results are agnostic to the underlying distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ERM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the  discussion it may seem that ERM algorithm is universally consistent. Not really though! <br>\n",
    "Below is a theorem which shows that error will always greater than some amount no matter what we do<br>\n",
    "\n",
    "- __Theorem (no free lunch, Devroye'82):__<br>\n",
    "  Pick any sample size $m$, any algorithm $\\mathcal{A}$ any tolerance $\\epsilon>0$ there exists a distribution $\\mathcal {D}$ such that:\n",
    "  $$\\mathrm{err}(f_m^{\\mathcal{A}})A>1/2-\\epsilon$$\n",
    "  while base optimal error, $\\displaystyle \\min_f \\mathrm{err}(f)=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further \n",
    "\n",
    "- How to do model class selection? Structural risk results.\n",
    "- Dealing with kernels Fat margin theory\n",
    "- Incorporating priors over the models PAC Bayes theory\n",
    "- Is it possible to get distribution dependent bound? It is also known as Rademacher complexity.\n",
    "- How about regression ? Can derive similar results for nonparametric regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y \\rightarrow$ True label <br>\n",
    "$\\hat y \\rightarrow$ Predicted label <br>\n",
    "$X \\rightarrow$ Input data <br>\n",
    "$L(\\hat y,y):=\\lvert \\hat y-y \\rvert \\rightarrow$ Absolute error<br>\n",
    "$L(\\hat y,y):= (\\hat y-y)^2 \\rightarrow$ Squared error\n",
    "\n",
    "A Liner predictor can be defined by slop $w$ and intercept $w_0$\n",
    "$$\\hat f(\\vec x)=\\vec w \\cdot \\vec x+ w_0$$\n",
    "\n",
    "Which minimizes the loss\n",
    "$$\\min_{w,w_0} \\mathbb{E}_{(\\vec x,y)}[L(\\hat f(\\vec x),y)] $$\n",
    "The intercept can be absorbed via lifting and now it can be written as \n",
    "$$\\hat f(\\vec x)=\\vec w \\cdot \\vec x \\tag{1}$$\n",
    "Which minimizes the loss\n",
    "$$\\min_{w} \\mathbb{E}_{(\\vec x,y)}[L(\\hat f(\\vec x),y)] \\tag{2}$$\n",
    "- Parametric Regressor: Here we assume a particular form of the regressor and goal is to learn the parameter which minimizes the loss.\n",
    "- Non-Parametric Regressor: Here we do not assume any specific form of the regressor and the goal here is to learn the predictor directly from the input data so the error is minimized.\n",
    "\n",
    "we want to find a linear predictor $\\hat f$ given by equation $(1)$ which minimizes the loss given by equation $(2)$ <br>\n",
    "We estimate the parameter s by minimizing the corresponding loss on the training data:\n",
    "$$\\begin{align*}\n",
    "&\\argmin_w \\frac{1}{n}\\sum_{i=1}^n L(\\vec w\\cdot \\vec x_i, y_i)\\\\\n",
    "=&\\argmin_w \\frac{1}{n}\\sum_{i=1}^n(\\vec w\\cdot \\vec x_i-y_i)^2\\\\\n",
    "=&\\argmin_w  \\left\\lVert \\left\\lbrack \\begin{array}{c}\n",
    "\\dots X_1 \\dots\\\\\n",
    "\\dots X_i \\dots\\\\\n",
    "\\dots X_n \\dots\n",
    "\\end{array}\\right\\rbrack \\left\\lbrack \\begin{array}{c}\n",
    "\\;\\\\\n",
    "w\\\\\n",
    "\\;\n",
    "\\end{array}\\right\\rbrack -\\left\\lbrack \\begin{array}{c}\n",
    "y_1 \\\\\n",
    "y_i \\\\\n",
    "y_n \n",
    "\\end{array}\\right\\rbrack \\right\\rVert^2 \\\\\n",
    "=&\\argmin_w \\left\\lVert X \\vec w - \\vec y\\right\\rVert_2^2\n",
    "\\end{align*}$$\n",
    "Notice that every \n",
    "\n",
    "$$\\left\\lbrack \\begin{array}{c}\n",
    "\\dots X_i \\dots\\\\\n",
    "\\\\\n",
    "\\\\\n",
    "\\end{array}\\right\\rbrack \\left\\lbrack \\begin{array}{c}\n",
    "\\;\\\\\n",
    "w\\\\\n",
    "\\;\n",
    "\\end{array}\\right\\rbrack$$\n",
    "produces a single value as it is just a dot product.\n",
    "\n",
    "This is unconstrained problem, We can take the gradient and examine the stationary points.\n",
    "\n",
    "$$\\begin{align*}\n",
    "&&\\frac{\\partial}{\\partial \\vec w} \\left\\lVert X \\vec w - \\vec y\\right\\rVert^2 &=0\\\\\n",
    "&\\Rightarrow& 2X^T(X\\vec w-\\vec y) &=0 \\\\\n",
    "&\\Rightarrow& X^T(X\\vec w-\\vec y) &=0 \\\\\n",
    "&\\Rightarrow& X^TX\\vec w &=X^T\\vec y \\\\\n",
    "&\\Rightarrow& \\vec w &=(X^TX)^\\dagger X^T\\vec y \\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Here $(\\cdot)^\\dagger$ is called pseudo-inverse.<br>\n",
    "The above  equation is also called Ordinary Least Squares\n",
    "$$\\vec w_{ols} =(X^TX)^\\dagger X^T\\vec y $$\n",
    "The solution is unique and stable when $X^TX$ is invertible.\n",
    "\n",
    "\n",
    "<br><br>\n",
    "Now consider the column space view of the data \n",
    "$$\\mathbf X: \\left\\lbrack \\begin{array}{c}\n",
    "\\dots X_1 \\dots\\\\\n",
    "\\dots X_i \\dots\\\\\n",
    "\\dots X_n \\dots\n",
    "\\end{array}\\right\\rbrack \\rightarrow \\left\\lbrack \\begin{array}{c|c|c}\n",
    "\\ddot x_1  & \\cdots  & \\ddot x_d \\\\\n",
    "\\vdots  &  & \\vdots \\\\\n",
    " &  & \n",
    "\\end{array}\\right\\rbrack$$\n",
    "Find a $w$ such that the linear combination of $X$ is minimized.\n",
    "\n",
    "$$\\frac{1}{n} \\left\\lVert \\vec y-\\sum_{i=1}^d w_i \\ddot x_i \\right\\rVert:=\\text{residual}$$\n",
    "Say $\\hat y$ is the solution \n",
    "$$\\hat y:=X\\vec w_{ols}=\\sum_{i=1}^d w_{ols,i}\\ddot x_i$$\n",
    "\n",
    "- Thus $\\hat y$  is the orthogonal projection of $y$ onto the $\\text{span}(\\ddot x_1,\\dots,\\ddot x_d)$\n",
    "  $$\\hat y = X \\vec w_{ols}=\\underbrace{X(X^TX)^\\dagger X^T}_{\\text{Projection Matrix }\\prod} \\vec y$$\n",
    "    - Below pic shows the same:<br><br>\n",
    "    ![](CS5590_images/mspaint_IDUg2yv0VR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "$\\tiny  {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
