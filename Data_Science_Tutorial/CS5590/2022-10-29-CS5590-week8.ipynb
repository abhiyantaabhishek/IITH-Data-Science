{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "author: Abhishek Kumar Dubey\n",
    "badges: true\n",
    "categories:\n",
    "- Machine Learning\n",
    "date: '2022-10-29'\n",
    "description: Introduction to Learning Theory, Regression Formulation\n",
    "image: CS5590_images/Acrobat_DOQ241iPvG.png\n",
    "title: Machine Learning 8\n",
    "toc: true\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Towards formalizing ‘learning’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The basic process of learning\n",
    "- Observe a phenomenon\n",
    "- Construct a model from observations\n",
    "- Use that model to make decisions/predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A statistical machinery for learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phenomenon of interest:\n",
    "\n",
    "- Input space: $X$; Output space: $Y$\n",
    "- There is an unknown distribution $D$ over $(X,Y)$\n",
    "- The learner observes $m$ examples $(x_1 ,y_1,\\dots, x_m ,y_m )$ drawn from $D$\n",
    "\n",
    "Construct a model:\n",
    "\n",
    "- Let $F$ be a collection of models, where each $f: X \\rightarrow Y$ predicts $y$ given $x$\n",
    "- From $m$ observations, select a model $f_m$  in $F$ which predicts well.\n",
    "- Generalization error of $f$:\n",
    "  $$\\mathrm{err}(f):=\\mathbb{P}_{(x,y)\\sim D}\\left[f(x)\\ne y\\right]$$\n",
    "  Notice this error is calculated on the whole distribution $D$\n",
    "- We can say that we have learned a phenomenon if \n",
    "  $$\\mathrm{err}(f_m)-\\mathrm{err}(f^*)\\le \\epsilon \\quad f^*:=\\argmin_{f \\in F}\\mathrm{err}(f)$$\n",
    "  for any tolerance level $\\epsilon$ of our choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all tolerance levels $\\epsilon > 0$, and all confidence levels $\\delta > 0$, if there exists some model selection algorithm $\\mathcal{A}$ that selects $f_m^\\mathcal{A} \\in \\mathcal{F}$ from $m$ observations i.e. \n",
    " \n",
    " - $$\\mathcal{A}:(x_i,y_i)_i^m \\mapsto f_m^\\mathcal{A}$$\n",
    " \n",
    " - And \n",
    "   $$\\mathrm{err}(f_m^\\mathcal{A})-\\mathrm{err}(f^*)\\le \\epsilon$$\n",
    "   with probability at least $1-\\delta$ over the draw of the sample.\n",
    "\n",
    "We call \n",
    "\n",
    "- The model class $\\mathcal{F}$ is __PAC-Learnable__. (Probably Approximate Correct)\n",
    "- If $m$ is polynomial in $\\frac{1}{\\epsilon}$ and $\\frac{1}{\\delta}$ then $\\mathcal{F}$ is __Efficiently PAC-Learnable__.\n",
    "\n",
    "A popular algorithm:\n",
    "\n",
    "- Empirical risk minimization (ERM) algorithm.\n",
    "  $$f_m^{\\text{ERM}}:=\\argmin_{f \\in \\mathcal{F}}\\frac{1}{m}\\sum_{i=1}^m\\mathbf{1}\\{f(x_i)\\ne y_i\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAC Learning Simple Model Classes\n",
    "\n",
    "__Theorem (finite seize $\\mathcal{F}$ ):__\n",
    "\n",
    "- Pick any tolerance level $\\epsilon > 0,$ and any confidence level $\\delta > 0$ let $(x_1,y_1),\\dots,(x_m,y_m)$ be $m$ examples drawn from an unknown $\\mathcal{D}$ if $\\displaystyle m \\ge C \\cdot \\frac{1}{\\epsilon^2}\\ln\\frac{\\lvert \\mathcal{F}\\rvert}{\\delta}$, then with the probability at least $1- \\delta$\n",
    "$$\\mathrm{err}(f_m^\\mathrm{ERM})-\\mathrm{err}(f^*)\\le \\epsilon$$\n",
    "$$\\boxed{\\mathcal{F}\\text{ is efficiently PAC-learnable}}$$\n",
    "\n",
    "- Proof Sketch\n",
    "  - Define Generalization error of $f$\n",
    "    $$\\text{err}(f):=\\mathbb{E}_{(x,y)\\sim \\mathcal{D}}\\left[\\mathbf{1}\\{f(x_i)\\ne y_i\\}\\right]$$\n",
    "  - Define sample error of $f$\n",
    "    $$\\text{err}_m(f):=\\frac{1}{m}\\sum_{i=1}^m\\left[\\mathbf{1}\\{f(x_i)\\ne y_i\\}\\right]$$\n",
    "  Fix any $f \\in \\mathcal{F}$ and sample $(x_i,y_i)$, define  random variable \n",
    "  $$\\mathbf{Z}_i^f=\\mathbf{1}\\{f(x_i)\\ne y_i\\}$$\n",
    "  Now we can re-write generalization error and sample error as below\n",
    "  - Generalization error of $f$\n",
    "    $$\\text{err}(f):=\\mathbb{E}_{(x,y)\\sim \\mathcal{D}}\\left[\\mathbf{Z}_1^f\\right]$$\n",
    "  - sample error of $f$\n",
    "    $$\\text{err}_m(f):=\\frac{1}{m}\\sum_{i=1}^m\\left[\\mathbf{Z}_i^f\\right]$$\n",
    "\n",
    "### __Lemma (Chernoff-Hoeffding bound'63)__<br>\n",
    "Let $\\mathbf{Z}_1,\\dots,\\mathbf{Z}_m$ be $m$ Bernouli r.v. drawn independently from $\\mathbf{B(p)}$, for any tolerance level $\\epsilon > 0$\n",
    "$${\\mathcal{P}}_{{\\mathit{\\mathbf{Z}}}_i } \\left\\lbrack \\left \\lvert \\frac{1}{m}\\sum_{i=m}^m \\left\\lbrack {\\mathit{\\mathbf{Z}}}_i \\right\\rbrack -\\mathbb{E}\\left\\lbrack \\mathbf{Z}_1 \\right\\rbrack \\right \\rvert\\ge \\epsilon \\right\\rbrack \\le 2e^{-2\\epsilon^2 m}$$\n",
    "\n",
    "Analyze\n",
    "$$\\begin{align*}{}\n",
    "&{\\mathcal{P}}_{\\left(x_i ,y_i \\right)} \\left\\lbrack \\mathrm{exists}\\;f\\in \\mathcal{F},\\left \\lvert \\frac{1}{m}\\sum_{i=m}^m \\left\\lbrack {\\mathit{\\mathbf{Z}}}_i^f \\right\\rbrack -\\mathbb{E}\\left\\lbrack {\\mathbf{Z}}_1^f \\right\\rbrack \\right \\rvert\\ge \\epsilon \\right\\rbrack \\\\\n",
    "&\\qquad \\quad \\le \\sum_{f\\in \\mathcal{F}} {\\mathcal{P}}_{\\left(x_i ,y_i \\right)} \\left\\lbrack \\left \\lvert \\frac{1}{m}\\sum_{i=m}^m \\left\\lbrack {\\mathit{\\mathbf{Z}}}_i^f \\right\\rbrack -\\mathbb{E}\\left\\lbrack {\\mathbf{Z}}_1^f \\right\\rbrack  \\right \\rvert\\ge \\epsilon \\right\\rbrack \\\\\n",
    "&\\qquad  \\quad \\le 2{\\left \\lvert \\;\\mathcal{F}\\right \\rvert e}^{-2\\epsilon^2 m} \\\\\n",
    "&\\qquad  \\quad \\le \\delta \n",
    "\\end{align*}$$\n",
    "\n",
    "Equivalently by choosing $\\displaystyle m \\ge \\frac{1}{2 \\epsilon ^2}\\ln\\frac{2\\mathcal{F}}{\\delta}$ with probability at least  \n",
    "$1-\\delta,$ for __all__ $f \\in \\mathcal{F}$\n",
    "$$\\left \\lvert \\frac{1}{m}\\sum_{i=m}^m \\left\\lbrack {\\mathit{\\mathbf{Z}}}_i^f \\right\\rbrack -\\mathbb{E}\\left\\lbrack {\\mathbf{Z}}_1^f \\right\\rbrack  \\right \\rvert=\\left \\lvert \\mathrm{err}_m(f)-\\mathrm{err}(f) \\right \\rvert \\le \\epsilon$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning general concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VC dimension "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VC dimension is also known as Vapnik-Chervonenkis dimension.\n",
    "\n",
    "- Definition <br>\n",
    "  We say that a model class $\\mathcal{F}$ has VC dimension $d,$ if $d$ is the largest set of points $x_1,\\dots,x_d \\subset X$ Such that for all possible labelling of  $x_1,\\dots,x_d$ there exists some $f \\in \\mathcal{F}$ that achieves that labelling.\n",
    "  - Example: $\\mathcal{F}=$ Linear classifier in $\\mathbb{R}^2$<br><br>\n",
    "    ![](CS5590_images/Acrobat_FqAMtysA4S.png)<br>\n",
    "    $$\\text{VC}(\\mathcal{F})=3$$\n",
    "    Notice that we can change the structure of the data, for e.g. on the left side, data is in the from of triangle, we can not change it to form a line, i.e. we can not change the position of the data, but we can change the label as we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VC Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Theorem (Vapnik-Chervonenkis'71)__<br>\n",
    "  Chose any tolerance level $\\epsilon >0,$ and any confidence level $\\delta>0$ let $(x_1,y_1),\\dots,(x_m,y_m)$ be $m$ examples drawn from an unknown $\\mathcal{D},$ <br>\n",
    "  if $\\displaystyle m>C.\\frac{\\text{VC}(\\mathcal{F})\\ln(1/\\delta)}{\\epsilon^2},$ then with probability at least $1-\\delta$\n",
    "  $$\\mathrm{err}(f_m^{\\mathrm{ERM}})-\\mathrm{err}(f^*)\\le\\epsilon$$\n",
    "  $$\\boxed{\\mathcal{F} \\text{ is efficiently PAC-learnable}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tightness of VC Bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Theorem (VC lower bound)__<br>\n",
    "Let $\\mathcal{A}$ be any model selection algorithm that given $m$ samples, returns  a model from $\\mathcal{F}$, that is $\\mathcal{A}:(x_i,y_i)_{i=1}^m \\mapsto f_m^\\mathcal{A}$<br>\n",
    "For all tolerance level $0<\\epsilon <1,$ and all confidence levels $0<\\delta<1/4,$ there exists a distribution $\\mathcal {D}$ such that if $\\displaystyle m \\leq C \\cdot \\frac{\\mathrm{VC}(\\mathcal{F})}{\\epsilon^2}$\n",
    "$$\\mathbb{P}_{(x_i,y_i)}\\left[ \\left \\lvert \\mathrm{err}(f_m^{\\mathcal{A}})-\\mathrm{err}(f^*) \\right \\rvert > \\epsilon \\right]> \\delta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facts of VC dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VC dimension:\n",
    "    - A combinatorial concept to capture the true richness of $\\mathcal{F}$\n",
    "    - Often (but not always!) proportional to the degrees of freedom or the number of independent parameters in $\\mathcal{F}$\n",
    "- Other Observations\n",
    "    - VC dimension of a model class fully characterizes its learning ability!\n",
    "    - Results are agnostic to the underlying distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ERM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the  discussion it may seem that ERM algorithm is universally consistent. Not really though! <br>\n",
    "Below is a theorem which shows that error will always greater than some amount no matter what we do<br>\n",
    "\n",
    "- __Theorem (no free lunch, Devroye'82):__<br>\n",
    "  Pick any sample size $m$, any algorithm $\\mathcal{A}$ any tolerance $\\epsilon>0$ there exists a distribution $\\mathcal {D}$ such that:\n",
    "  $$\\mathrm{err}(f_m^{\\mathcal{A}})A>1/2-\\epsilon$$\n",
    "  while base optimal error, $\\displaystyle \\min_f \\mathrm{err}(f)=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further \n",
    "\n",
    "- How to do model class selection? Structural risk results.\n",
    "- Dealing with kernels Fat margin theory\n",
    "- Incorporating priors over the models PAC Bayes theory\n",
    "- Is it possible to get distribution dependent bound? It is also known as Rademacher complexity.\n",
    "- How about regression ? Can derive similar results for nonparametric regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y \\rightarrow$ True label <br>\n",
    "$\\hat y \\rightarrow$ Predicted label <br>\n",
    "$X \\rightarrow$ Input data <br>\n",
    "$L(\\hat y,y):=\\lvert \\hat y-y \\rvert \\rightarrow$ Absolute error<br>\n",
    "$L(\\hat y,y):= (\\hat y-y)^2 \\rightarrow$ Squared error\n",
    "\n",
    "A Liner predictor can be defined by slop $w$ and intercept $w_0$\n",
    "$$\\hat f(\\vec x)=\\vec w \\cdot \\vec x+ w_0$$\n",
    "\n",
    "Which minimizes the loss\n",
    "$$\\min_{w,w_0} \\mathbb{E}_{(\\vec x,y)}[L(\\hat f(\\vec x),y)] $$\n",
    "The intercept can be absorbed via lifting and now it can be written as \n",
    "$$\\hat f(\\vec x)=\\vec w \\cdot \\vec x \\tag{1}$$\n",
    "Which minimizes the loss\n",
    "$$\\min_{w} \\mathbb{E}_{(\\vec x,y)}[L(\\hat f(\\vec x),y)] \\tag{2}$$\n",
    "- Parametric Regressor: Here we assume a particular form of the regressor and goal is to learn the parameter which minimizes the loss.\n",
    "- Non-Parametric Regressor: Here we do not assume any specific form of the regressor and the goal here is to learn the predictor directly from the input data so the error is minimized.\n",
    "\n",
    "we want to find a linear predictor $\\hat f$ given by equation $(1)$ which minimizes the loss given by equation $(2)$ <br>\n",
    "We estimate the parameter s by minimizing the corresponding loss on the training data:\n",
    "$$\\begin{align*}\n",
    "&\\argmin_w \\frac{1}{n}\\sum_{i=1}^n L(\\vec w\\cdot \\vec x_i, y_i)\\\\\n",
    "=&\\argmin_w \\frac{1}{n}\\sum_{i=1}^n(\\vec w\\cdot \\vec x_i-y_i)^2\\\\\n",
    "=&\\argmin_w  \\left\\lVert \\left\\lbrack \\begin{array}{c}\n",
    "\\dots X_1 \\dots\\\\\n",
    "\\dots X_i \\dots\\\\\n",
    "\\dots X_n \\dots\n",
    "\\end{array}\\right\\rbrack \\left\\lbrack \\begin{array}{c}\n",
    "\\;\\\\\n",
    "w\\\\\n",
    "\\;\n",
    "\\end{array}\\right\\rbrack -\\left\\lbrack \\begin{array}{c}\n",
    "y_1 \\\\\n",
    "y_i \\\\\n",
    "y_n \n",
    "\\end{array}\\right\\rbrack \\right\\rVert^2 \\\\\n",
    "=&\\argmin_w \\left\\lVert X \\vec w - \\vec y\\right\\rVert_2^2\n",
    "\\end{align*}$$\n",
    "Notice that every \n",
    "\n",
    "$$\\left\\lbrack \\begin{array}{c}\n",
    "\\dots X_i \\dots\\\\\n",
    "\\\\\n",
    "\\\\\n",
    "\\end{array}\\right\\rbrack \\left\\lbrack \\begin{array}{c}\n",
    "\\;\\\\\n",
    "w\\\\\n",
    "\\;\n",
    "\\end{array}\\right\\rbrack$$\n",
    "produces a single value as it is just a dot product.\n",
    "\n",
    "This is unconstrained problem, We can take the gradient and examine the stationary points.\n",
    "\n",
    "$$\\begin{align*}\n",
    "&&\\frac{\\partial}{\\partial \\vec w} \\left\\lVert X \\vec w - \\vec y\\right\\rVert^2 &=0\\\\\n",
    "&\\Rightarrow& 2X^T(X\\vec w-\\vec y) &=0 \\\\\n",
    "&\\Rightarrow& X^T(X\\vec w-\\vec y) &=0 \\\\\n",
    "&\\Rightarrow& X^TX\\vec w &=X^T\\vec y \\\\\n",
    "&\\Rightarrow& \\vec w &=(X^TX)^\\dagger X^T\\vec y \\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Here $(\\cdot)^\\dagger$ is called pseudo-inverse.<br>\n",
    "The above  equation is also called Ordinary Least Squares\n",
    "$$\\vec w_{ols} =(X^TX)^\\dagger X^T\\vec y $$\n",
    "The solution is unique and stable when $X^TX$ is invertible.\n",
    "\n",
    "\n",
    "<br><br>\n",
    "Now consider the column space view of the data \n",
    "$$\\mathbf X: \\left\\lbrack \\begin{array}{c}\n",
    "\\dots X_1 \\dots\\\\\n",
    "\\dots X_i \\dots\\\\\n",
    "\\dots X_n \\dots\n",
    "\\end{array}\\right\\rbrack \\rightarrow \\left\\lbrack \\begin{array}{c|c|c}\n",
    "\\ddot x_1  & \\cdots  & \\ddot x_d \\\\\n",
    "\\vdots  &  & \\vdots \\\\\n",
    " &  & \n",
    "\\end{array}\\right\\rbrack$$\n",
    "Find a $w$ such that the linear combination of $X$ is minimized.\n",
    "\n",
    "$$\\frac{1}{n} \\left\\lVert \\vec y-\\sum_{i=1}^d w_i \\ddot x_i \\right\\rVert:=\\text{residual}$$\n",
    "Say $\\hat y$ is the solution \n",
    "$$\\hat y:=X\\vec w_{ols}=\\sum_{i=1}^d w_{ols,i}\\ddot x_i$$\n",
    "\n",
    "- Thus $\\hat y$  is the orthogonal projection of $y$ onto the $\\text{span}(\\ddot x_1,\\dots,\\ddot x_d)$\n",
    "  $$\\hat y = X \\vec w_{ols}=\\underbrace{X(X^TX)^\\dagger X^T}_{\\text{Projection Matrix }\\prod} \\vec y$$\n",
    "    - Below pic shows the same:<br><br>\n",
    "    ![](CS5590_images/mspaint_IDUg2yv0VR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Statistical Modeling View"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider $y_{\\text{clean}}$ is computed as $w\\cdot x_i$ and level $y_i$ is formed by corrupting $y_{\\text{clean}}$ by gaussian noise $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma ^2)$\n",
    "$$y_i:=y_{\\text{clean}}+\\epsilon_i=w\\cdot x_i + \\epsilon_i$$\n",
    "we can observe that\n",
    "$$y_i \\sim w\\cdot x_i +\\mathcal{N}(0,\\sigma ^2)=\\mathcal{N}(w\\cdot x_i,\\sigma ^2)$$\n",
    "Consider our data pair $(\\vec x_1,y_2),(\\vec x_2,y_2),\\cdots,(\\vec x_n,y_n)$ is drawn independently from a fixed underlying distribution ( also called the i.i.d assumption ).\n",
    "\n",
    "we need to select optimal model $\\vec f$ from all possible pool of the model $\\mathcal{F}$ such that we get\n",
    "\n",
    "- Maximum likelihood ( best fits the data)\n",
    "- Maximum a posteriori ( best fits the dta but incorporates prior assumptions)\n",
    "- Optimization of loss criterion ( best discriminates the labels) \n",
    "\n",
    "Given some i.i.d data say we have a model class $\\mathcal{P} = \\{\\mathcal{p}_\\theta \\mid \\theta \\in \\Theta \\}$ <br>\n",
    "We need to find the parameter settings $\\theta$ that best fits the data.\n",
    "\n",
    "WE can find the best fitting model via __Maximum likelihood extimation__.\n",
    "$$\\mathcal{L}(\\theta \\mid X) := P(X\\mid \n",
    "\\theta) = P(\\vec x_1, \\cdots,\\vec x_n\\mid \\theta) \\overset{\\text{i.i.d}}{=} \\prod_{i=1}^n \\mathcal{P}(\\vec x_i \\mid \\theta)= \\prod_{i=1}^n \\mathcal{P}_\\theta(\\vec x_i)$$\n",
    "Interpretation : How probable is the data given the model $p_\\theta$\n",
    "\n",
    "Parameter setting  that maximizes $\\mathcal{L}(\\theta \\mid X)$\n",
    "$$\\argmax_\\theta  \\prod_{i=1}^n \\mathcal{P}_\\theta(\\vec x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider ${\\mathcal{P}}_{\\left\\lbrace \\mu ,\\sigma^2 \\right\\rbrace } \\left(x\\right)$ is given as \n",
    "\n",
    "$${\\mathcal{P}}_{\\left\\lbrace \\mu ,\\sigma^2 \\right\\rbrace } \\left(x\\right):=\\frac{1}{\\sqrt{2\\pi \\sigma^2 }}\\exp \\left(-\\frac{{\\left(x-\\mu \\right)}^2 }{2\\sigma^2 }\\right)$$\n",
    "Then\n",
    "\n",
    "$$\\begin{align*}{}\n",
    "\\arg \\;\\max_{\\theta } \\mathcal{L}\\left(\\theta \\mid X\\right)&=\\arg \\;\\max_{\\mu ,\\sigma^2 } \\prod_{i=1}^n {\\mathcal{P}}_{\\left\\lbrace \\mu ,\\sigma^2 \\right\\rbrace } \\left(x_i \\right)\\\\\n",
    "&=\\arg \\;\\max_{\\mu ,\\sigma^2 } \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2 }}\\exp \\left(-\\frac{{\\left(x_i -\\mu \\right)}^2 }{2\\sigma^2 }\\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "Now, we know that the arg max of a function and it's log is same, so\n",
    "\n",
    "\n",
    "$$\\begin{align*}{}\n",
    "\\arg \\;\\max_{\\theta } \\mathcal{L}\\left(\\theta |X\\right)&=\\arg \\;\\max_{\\theta } \\;\\log \\mathcal{L}\\left(\\theta \\mid X\\right)\\\\\n",
    "&=\\arg \\;\\max_{\\mu ,\\sigma^2 } \\log \\left(\\prod_{i=1}^n {\\mathcal{P}}_{\\left\\lbrace \\mu ,\\sigma^2 \\right\\rbrace } \\left(x_i \\right)\\right)\\\\\n",
    "&=\\arg \\;\\max_{\\mu ,\\sigma^2 } \\log \\left(\\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2 }}\\exp \\left(-\\frac{{\\left(x_i -\\mu \\right)}^2 }{2\\sigma^2 }\\right)\\right)\\\\\n",
    "&=\\arg \\;\\max_{\\mu ,\\sigma^2 } \\sum_{i=1}^n \\log \\left(\\frac{1}{\\sqrt{2\\pi \\sigma^2 }}\\exp \\left(-\\frac{{\\left(x_i -\\mu \\right)}^2 }{2\\sigma^2 }\\right)\\right)\\\\\n",
    "&=\\arg \\;\\max_{\\mu ,\\sigma^2 } \\sum_{i=1}^n \\left\\lbrack -\\frac{1}{2}\\log \\left(2\\pi \\sigma^2 \\right)-\\frac{{\\left(x_i -\\mu \\right)}^2 }{2\\sigma^2 }\\right\\rbrack \\\\\n",
    "&=\\arg \\;\\max_{\\mu ,\\sigma^2 } \\left\\lbrack -\\frac{n}{2}\\log \\left(2\\pi \\sigma^2 \\right)-\\sum_{i=1}^n \\left\\lbrack \\frac{{\\left(x_i -\\mu \\right)}^2 }{2\\sigma^2 }\\right\\rbrack \\right\\rbrack \\\\\n",
    "&=\\arg \\;\\max_{\\mu ,\\sigma^2 } \\left\\lbrack -\\frac{n}{2}\\log \\left(2\\pi \\right)-\\frac{n}{2}\\log \\left(\\sigma^2 \\right)-\\frac{1}{2\\sigma^2 }\\sum_{i=1}^n {\\left(x_i -\\mu \\right)}^2 \\right\\rbrack \n",
    "\\end{align*}$$\n",
    "\n",
    "Now find derivative of above with $\\mu$\n",
    "\n",
    "$$\\begin{align*}{}\n",
    "\\nabla_{\\mu } \\left(\\mathcal{L}\\left(\\theta |X\\right)\\right)&=-\\frac{1}{2\\sigma^2 }\\sum_{i=1}^n \\left\\lbrack \\left(x_i -\\mu \\right)\\times \\left(-2\\mu \\right)\\right\\rbrack \\\\\n",
    "&=\\frac{\\mu }{\\sigma^2 }\\sum_{i=1}^n \\left(x_i -\\mu \\right)\\\\\n",
    "&=\\frac{\\mu }{\\sigma^2 }\\sum_{i=1}^n x_i -\\frac{\\mu }{\\sigma^2 }n\\mu \n",
    "\\end{align*}$$\n",
    "\n",
    "To find minimum equate it to zero\n",
    "\n",
    "$$\\begin{align*}{}\n",
    "&\\nabla_{\\mu } \\left(\\mathcal{L}\\left(\\theta |X\\right)\\right)=0\\\\\n",
    "\\Rightarrow \\quad& \\frac{\\mu }{\\sigma^2 }\\sum_{i=1}^n x_i -\\frac{\\mu }{\\sigma^2 }n\\mu =0\\\\\n",
    "\\Rightarrow \\quad& \\frac{\\mu }{\\sigma^2 }\\sum_{i=1}^n x_i =\\frac{\\mu }{\\sigma^2 }n\\mu \\\\\n",
    "\\Rightarrow \\quad& \\mu =\\frac{1}{n}\\sum_{i=1}^n x_i \n",
    "\\end{align*}$$\n",
    "\n",
    "Now find derivative of the same with $\\sigma$\n",
    "\n",
    "$$\\begin{align*}{}\n",
    "\\nabla_{\\mu } \\left(\\mathcal{L}\\left(\\theta |X\\right)\\right)&=-\\frac{n}{2}\\times \\frac{1}{\\sigma^2 }\\times 2\\sigma +\\frac{2}{2\\sigma^3 }\\sum_{i=1}^n {\\left(x_i -\\mu \\right)}^2 \\\\\n",
    "&=\\frac{-n}{\\sigma }+\\frac{1}{\\sigma^3 }\\sum_{i=1}^n {\\left(x_i -\\mu \\right)}^2 \n",
    "\\end{align*}$$\n",
    "\n",
    "To find minimum equate it to zero\n",
    "\n",
    "$$\\begin{align*}{}\n",
    "&\\nabla_{\\mu } \\left(\\mathcal{L}\\left(\\theta |X\\right)\\right)=0\\\\\n",
    "\\Rightarrow \\quad&\\frac{-n}{\\sigma }+\\frac{1}{\\sigma^3 }\\sum_{i=1}^n {\\left(x_i -\\mu \\right)}^2 =0\\\\\n",
    "\\Rightarrow \\quad&\\frac{n}{\\sigma }=\\frac{1}{\\sigma^3 }\\sum_{i=1}^n {\\left(x_i -\\mu \\right)}^2 \\\\\n",
    "\\Rightarrow \\quad&\\sigma^2 =\\frac{1}{n}\\sum_{i=1}^n {\\left(x_i -\\mu \\right)}^2 \n",
    "\\end{align*}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to linear Regression:\n",
    "How can we determine $w$ from Gaussian noise corrupted observations?<br>\n",
    "$S=(x_1,y_1),\\dots,(x_n,y_n)$ <br>\n",
    "Observation:\n",
    "$y_i\\sim w\\cdot x_i + \\mathcal{N}(o,\\sigma^2)=\\mathcal{N}(w\\cdot x_i,\\sigma^2)$<br>\n",
    "Let's try to use Maximum likelihood estimation to estimate Gaussian Noise<br>\n",
    "Parameter:<br>\n",
    "$$\\begin{align*}{}\n",
    "\\log \\;\\mathcal{L}\\left(w\\mid S\\right)&=\\sum_{i=1}^n \\log \\;p\\left(y_i \\mid w\\right)\\\\\n",
    "&\\propto\\sum \\frac{-{\\left(w\\cdot x_i -y_i \\right)}^2 }{2\\sigma^2 }\n",
    "\\end{align*}$$\n",
    "Ignoring the terms independent of $w$ and optimizing for $w$ yields the same ols results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can say that __Minimizing Ordinary sum of square is same as maximizing the gaussian log likelihood__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Least-Squared Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- complex models has a lot of parameters, so pron to overfitting.\n",
    "- Overfitting can be reduced by imposing a constraint on the overall magnitude of the parameters.\n",
    "- Two common types of regularization in linear regression:\n",
    "    - __L2 regularization__ (a.k.a. ridge regression): Find $w$ which minimizes:\n",
    "      $${\\sum_{j=1}^N \\left(y_j -\\sum_{i=0}^d w_i \\cdot x_i \\right)}^2 +\\lambda \\sum_{i=1}^d w_i^2$$\n",
    "      <br>Bigger $\\lambda$ imposes more constraints.\n",
    "    - __L1 regularization__ (a.k.a. lasso): Find $w$ which minimizes:\n",
    "      $${\\sum_{j=1}^N \\left(y_j -\\sum_{i=0}^d w_i \\cdot x_i \\right)}^2 +\\lambda \\sum_{i=1}^d \\left\\lvert w_i \\right\\rvert$$    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Ridge Regression\n",
    "\n",
    "Ridge regression can also be written as <br>\n",
    "$$\\left\\lVert \\mathbf{y}- \\mathbf{Xw} \\right\\rVert ^2 + \\lambda \\left\\lVert \\mathbf{w} \\right\\rVert^2$$\n",
    "Solve above for minimum $\\mathbf{ w}$ <br>\n",
    "$$\\begin{align*}\n",
    "&&&\\frac{\\partial}{\\partial \\vec w} \\left(\\left\\lVert  \\mathbf{y}- \\mathbf{Xw} \\right\\rVert ^2 + \\lambda \\left\\lVert \\mathbf{w} \\right\\rVert^2\\right) =0\\\\\n",
    "\\Rightarrow&&& 2\\mathbf{X}^T(\\mathbf{Xw}- \\mathbf y) +2\\lambda \\mathbf{w}=0 \\\\\n",
    "\\Rightarrow&&& 2{\\mathbf{X}}^T \\mathbf{X}\\mathbf{w}+2\\lambda \\mathbf{w}=2{\\mathbf{X}}^T \\mathbf{y}\\\\\n",
    "\\Rightarrow&&& \\left({\\mathbf{X}}^T \\mathbf{X}+\\lambda I\\right)\\mathbf{w}={\\mathbf{X}}^T \\mathbf{y}\\\\\n",
    "\\Rightarrow&&& \\mathbf{w}={{ \\left({\\mathbf{X}}^T \\mathbf{X}+\\lambda I \\right)}^{-1} \\mathbf{X}}^T \\mathbf{y}\n",
    "\\end{align*}$$\n",
    "\n",
    "__Inverse always exists for any__ $\\lambda >0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Least Squares and Partial Least Squares\n",
    "\n",
    "- Total least square models error in output and input.<br><br>\n",
    "  ![](CS5590_images/Acrobat_8phpzg8xnm.png)<br><br>\n",
    "- Partial Least Squares:\n",
    "  - Seeks to address the correlation between predictor variables in it's model\n",
    "  - Also called “Projection to Latent Structures” (PLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Methods\n",
    "\n",
    "- Linear Least-Squares Regression\n",
    "    - Partial Least-Squares\n",
    "    - Total Least-Squares\n",
    "    - Ridge Regression, LASSO\n",
    "- Kernel Regression\n",
    "- k-NN Regression\n",
    "- Regression Trees\n",
    "- Support Vector Regression\n",
    "- Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear Regression\n",
    "\n",
    "- it is also called kernel regression, recall kernel trick.\n",
    "- Key Idea: Map data to higher dimensional space (feature space) and perform linear regression in embedded space.\n",
    "\n",
    "### Kernel Regression\n",
    "\n",
    "$$\\begin{align*}{}\n",
    "&&& \\mathbf{w}={{\\left({\\mathbf{X}}^T \\mathbf{X}+\\lambda I\\right)}^{-1} \\mathbf{X}}^T \\mathbf{y}\\\\\n",
    "\\Rightarrow&&&\\left({\\mathbf{X}}^T \\mathbf{X}+\\lambda I\\right)\\mathbf{w}={\\mathit{\\mathbf{X}}}^T \\mathit{\\mathbf{y}}\\\\\n",
    "\\Rightarrow&&& {\\mathbf{X}}^T \\mathbf{X}\\mathbf{w}+\\lambda \\mathbf{w} ={\\mathit{\\mathbf{X}}}^T \\mathit{\\mathbf{y}}\\\\\n",
    "\\Rightarrow&&& \\lambda \\mathbf{w} ={\\mathit{\\mathbf{X}}}^T \\mathit{\\mathbf{y}}-{\\mathbf{X}}^T \\mathbf{X}\\mathbf{w}\\\\\n",
    "\\Rightarrow&&& \\mathbf{w}=\\lambda^{-1} \\left({\\mathit{\\mathbf{X}}}^T \\mathit{\\mathbf{y}}-{\\mathbf{X}}^T \\mathbf{X}\\mathbf{w}\\right)\\\\\n",
    "\\Rightarrow&&& \\mathbf{w}=\\lambda^{-1} {\\mathit{\\mathbf{X}}}^T \\left(\\mathit{\\mathbf{y}}-\\mathbf{Xw}\\right)\\\\\n",
    "\\Rightarrow&&& \\mathbf{w}= {\\mathit{\\mathbf{X}}}^T \\alpha\n",
    "\\end{align*}$$\n",
    "where $\\alpha =\\lambda^{-1} \\left(\\mathit{\\mathbf{y}}-\\mathbf{Xw}\\right)$ <br>\n",
    "now put value of $\\mathbf{w}$ in the above equation <br>\n",
    "$$\\begin{align*}{}\n",
    "&&&\\alpha =\\lambda^{-1} \\left(\\mathit{\\mathbf{y}}-{\\mathbf{XX}}^T \\alpha \\right)\\\\\n",
    "\\Rightarrow&&& \\lambda \\alpha =\\left(\\mathit{\\mathbf{y}}-{\\mathbf{XX}}^T \\alpha \\right)\\\\\n",
    "\\Rightarrow&&& \\lambda \\alpha +{\\mathbf{XX}}^T \\alpha =\\mathit{\\mathbf{y}}\\\\\n",
    "\\Rightarrow&&& \\alpha ={\\left({\\mathbf{XX}}^T +\\lambda I\\right)}^{-1} \\mathit{\\mathbf{y}}\\\\\n",
    "\\Rightarrow&&& \\alpha ={\\left(\\mathbf{G}+\\lambda I\\right)}^{-1} \\mathit{\\mathbf{y}}\n",
    "\\end{align*}$$\n",
    "where $\\mathit{\\mathbf{G}}={\\mathbf{XX}}^T$, or $G_{i,j} =\\langle {\\mathit{\\mathbf{X}}}_i ,{\\mathit{\\mathbf{X}}}_j \\rangle$<br>\n",
    "Need to only compute $G$, the Gram Matrix (or the inner products between data points)<br>\n",
    "<br>\n",
    "Solving $\\mathbf{w}={{\\left({\\mathbf{X}}^T \\mathbf{X}+\\lambda I\\right)}^{-1} \\mathbf{X}}^T \\mathbf{y}$ takes $O(d^3)$ but solving $\\mathit{\\mathbf{G}}={\\mathbf{XX}}^T$ takes $O(n^3)$ so if we have a problem where dimension is high and sample is less we go for gram matrix one, if we have less dimension then we can solve using original formulation.\n",
    "<br><br>\n",
    "Notice:<br>\n",
    "$\\mathbf{X}\\rightarrow n\\times d$<br>\n",
    "$\\mathbf{X}^T\\mathbf{X}\\rightarrow d\\times d$<br>\n",
    "$\\mathbf{X}\\mathbf{X}^T\\rightarrow n\\times n$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-NN Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of k-NN classification we used to find most common value (majority vote), but in case of regression we calculate mean value instead\n",
    "\n",
    "$$\\hat f(x_q)\\leftarrow\\frac{\\sum_{i=1}^kf(x_i)}{k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Trees\n",
    "\n",
    "- Tree for regression: exactly the same model but with a number in each leaf instead of a class<br><br>\n",
    "  ![](CS5590_images/Acrobat_bY1TNLer3d.png)<br><br>\n",
    "- A regression tree is a piecewise constant function of the input attributes<br><br>\n",
    "  ![](CS5590_images/Acrobat_7Tfvs7FF9M.png)<br><br>\n",
    "- To minimize the square error on the learning sample, the prediction at a leaf is the average output of the learning cases reaching that leaf\n",
    "- Impurity of a sample is defined by the variance of the output in that\n",
    "sample\n",
    "  $$I\\left(\\mathrm{LS}\\right)={\\mathrm{Var}}_{y\\mid \\mathrm{LS}} \\left\\lbrace y\\right\\rbrace =E_{y\\mid \\mathrm{LS}} \\left\\lbrack {\\left(y-E_{y\\mid \\mathrm{LS}} \\left\\lbrack y\\right\\rbrack \\right)}^2 \\right\\rbrack$$\n",
    "  where $\\mathrm{LS}$ is the dataset.\n",
    "- The best split is the one that reduces the most variance:\n",
    "  $$\\Delta I\\left(\\mathrm{LS},A\\right)={\\mathrm{Var}}_{y\\mid \\mathrm{LS}} \\left\\lbrace y\\right\\rbrace -\\sum_a \\frac{\\left\\lvert { \\mathrm{LS}}_a \\right\\rvert }{\\mathrm{\\left\\lvert LS \\right\\rvert}}{\\mathrm{Var}}_{y\\mid {\\mathrm{LS}}_a } \\left\\lbrace y\\right\\rbrace$$\n",
    "  where $\\mathrm{LS_a}$ is the subset of the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Tree Pruning\n",
    "\n",
    "- Exactly the same algorithms apply as regression tree classification: pre-pruning and post-pruning.\n",
    "- In practice, pruning is more important in regression because full trees are much more complex\n",
    "    - Each data instance can have a different output value and hence the full tree has as many leaves as there are training instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "$\\tiny  {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
