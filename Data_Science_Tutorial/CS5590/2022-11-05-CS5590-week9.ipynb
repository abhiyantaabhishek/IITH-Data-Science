{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "author: Abhishek Kumar Dubey\n",
    "badges: true\n",
    "categories:\n",
    "- Machine Learning\n",
    "date: '2022-11-05'\n",
    "description: Support Vector Regression,Linear Regression\n",
    "image: CS5590_images/chrome_xLZgcRJMUG.png\n",
    "title: Machine Learning 9\n",
    "toc: true\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regression\n",
    "\n",
    "- Given training data $\\{x_i,y_i\\}_{i=1}^n$\n",
    "- Find $w_1$ and $b$ <br>\n",
    "  such that $y=w_1+b$ optimally describes the data:<br><br>\n",
    "  ![](CS5590_images/Acrobat_xEaCsnna8F.png)<br><br>\n",
    "- The thinner the tube more complex the model.\n",
    "    - Lazy case (underfitting):<br><br>\n",
    "      ![](CS5590_images/Acrobat_e4ICDp5NzH.png)\n",
    "    - Suspiciously smart case (overfitting):<br><br>\n",
    "      ![](CS5590_images/Acrobat_e3nfNIlEnj.png)<br><br>\n",
    "    - Compromising case (good generalizability)<br><br>\n",
    "      ![](CS5590_images/Acrobat_Zumi4MZWQa.png)<br><br>\n",
    "\n",
    "- __Formulation__:<br><br>\n",
    "  \n",
    "  $$\\min_{w_1 ,b,\\xi_i ,{\\xi_i }^* } \\frac{1}{2}w_1^2 +C\\sum_i \\left(\\xi_i +\\xi_i^* \\right)$$\n",
    "  subject to :<br><br>\n",
    "  $$\\begin{align*}\n",
    "  y_i-(w_ix_{i1}) -b &\\leq \\epsilon +\\xi_i \\;\\;\\text{green line area below}\\\\\n",
    "  (w_ix_{i1}) +b -y_i&\\leq \\epsilon +\\xi_i^* \\;\\;\\text{red line area below}\\\\\n",
    "  \\xi_i,\\xi_i^*\\ge 0 \\quad & \\quad i=1,2,\\dots,n \n",
    "  \\end{align*}$$\n",
    "  ![](CS5590_images/Acrobat_HHfPmbZv6S.png)\n",
    "  ![](CS5590_images/Acrobat_L0ecctxzVs.png)\n",
    "  ![](CS5590_images/Acrobat_WgHPzjerND.png)<br><br>\n",
    "  $\\epsilon$ controls width of the tube.\n",
    "\n",
    "- Role of $C$\n",
    "    - Small $C$<br><br>\n",
    "      ![](CS5590_images/Acrobat_HvMiVCGSxG.png)<br><br>\n",
    "    - Big  $C$<br><br>\n",
    "      ![](CS5590_images/Acrobat_cHx9nWZEEl.png)<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear (Kernel) SVR\n",
    "\n",
    "  $$\\min_{w_1 ,b,\\xi_i ,{\\xi_i }^* } \\frac{1}{2}(w_1^2 +w_2^2 )+C\\sum_i \\left(\\xi_i +\\xi_i^* \\right)$$\n",
    "  subject to :<br><br>\n",
    "  $$\\begin{align*}\n",
    "  y_i-(\\mathbf{w}'\\phi( x_{i1})) -b &\\leq \\epsilon +\\xi_i \\\\\n",
    "  (\\mathbf{w}'\\phi( x_{i1})) +b -y_i&\\leq \\epsilon +\\xi_i^* \\\\\n",
    "  \\xi_i,\\xi_i^*\\ge 0 \\;&\\; i=1,2,\\dots,n \n",
    "  \\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  SVR: Derivation\n",
    "\n",
    "$$\\min_{w_1 ,b,\\xi_i ,{\\xi_i }^* } \\frac{1}{2}(\\left\\lVert \\mathbf{w} \\right\\rVert)+C\\sum_i \\left(\\xi_i +\\xi_i^* \\right)$$\n",
    "subject to :<br><br>\n",
    "$$\\begin{align*}\n",
    "y_i-(\\mathbf{w}'\\phi( x_{i})) -b &\\leq \\epsilon +\\xi_i \\\\\n",
    "(\\mathbf{w}'\\phi( x_{i})) +b -y_i&\\leq \\epsilon +\\xi_i^* \\\\\n",
    "\\xi_i,\\xi_i^*\\ge 0 \\;&\\; i=1,2,\\dots,n \n",
    "\\end{align*}$$\n",
    "Lagrangian:\n",
    "$$\\begin{align*}\n",
    " L:=\\;&\\frac{1}{2}(\\left\\lVert \\mathbf{w} \\right\\rVert)+C\\sum_i \\left(\\xi_i +\\xi_i^* \\right) \\\\\n",
    " &-\\sum_i(\\eta_i\\xi_i+\\eta_i^*\\xi_i^*)\\\\\n",
    " &-\\sum_i \\alpha_i(\\epsilon +\\xi_i -y_i+(\\mathbf{w}'\\phi( x_{i})) +b  )\\\\\n",
    " &-\\sum_i \\alpha_i^*(\\epsilon +\\xi_i^* +y_i -(\\mathbf{w}'\\phi( x_{i})) -b )\n",
    "\\end{align*}$$\n",
    "Minimize with respect to $\\mathbf{w},b,\\xi_i,\\xi_i^*$<br>\n",
    "Maximize with respect to $\\alpha_i,\\alpha_i^*,\\eta_i,\\eta_i^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR: Summary\n",
    "\n",
    "#### Strengths of SVR\n",
    "\n",
    "- No local minima\n",
    "- Scales relatively well to high-dimensional data\n",
    "- Trade-off between classifier complexity and error can be controlled explicitly via $C$ and $\\epsilon$\n",
    "- Overfitting is avoided (for any fixed $C$ and $\\epsilon$)\n",
    "- The “curse of dimensionality” is avoided through kernel functions\n",
    "\n",
    "#### Weaknesses of SVR\n",
    "\n",
    "- What is the best trade-off parameter $C$ and best $\\epsilon$ ?\n",
    "- What is a good transformation of the original space ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "- To predict an outcome variable that is categorical from one or more categorical or continuous predictor variables.\n",
    "- Used because having a categorical outcome variable violates the assumption of linearity in normal regression.\n",
    "- Let $X$ be the data instance, and $Y$ be the class label: Learn $P(Y\\mid X)$ directly\n",
    "    - Let $W = (W_1, W_2,\\dots, W_n), X=(X_1, X_2, \\dots , X_n), \\mathbf W\\cdot \\mathbf X$ is\n",
    "the dot product\n",
    "    - Sigmoid function:<br>\n",
    "      $$P(Y=1\\mid \\mathbf{X})=\\frac{1}{1+e^{-\\mathbf{WX}}}$$\n",
    "\n",
    "- Generative models, e.g., Naïve Bayes: <br>\n",
    "  If we estimate $P(X\\mid Y),P(Y)$ from the data and use bayesian rule to find $P(Y\\mid X=x)$ it can be considered as generative modeling.<br>\n",
    "  It can also generate the data $P(X)=\\sum_yP(y)P(X\\mid y)$\n",
    "- Discriminative models, e.g., Logistic Regression:<br>\n",
    "  If we estimate $P(Y\\mid X)$ directly from the data then it can be considered as discriminative modeling.\n",
    "- In logistic regression, we learn the conditional distribution $P(Y\\mid X)$\n",
    "- Let $P_y(X;W)$ be our estimate of $P(Y\\mid X)$, where $W$ is a vector of adjustable parameters.\n",
    "- Assume there are two classes, $y = 0$ and $y = 1$ and<br>\n",
    "  $$P_1( \\mathbf{X};\\mathbf{W})=\\frac{1}{1+e^{-\\mathbf{WX}}}$$\n",
    "  and \n",
    "  $$P_0( \\mathbf{X};\\mathbf{W})=1-\\frac{1}{1+e^{-\\mathbf{WX}}}$$\n",
    "  log odd \n",
    "  $$\\begin{align*}\n",
    "  \\log\\frac{P_1( \\mathbf{X};\\mathbf{W})}{P_0( \\mathbf{X};\\mathbf{W})}&=\\log\\frac{\\frac{1}{1+e^{-\\mathbf{WX}}}}{1-\\frac{1}{1+e^{-\\mathbf{WX}}}}\\\\\n",
    "  &=\\log\\frac{\\frac{1}{1+e^{-\\mathbf{WX}}}}{\\frac{1+e^{-\\mathbf{WX}}-1}{1+e^{-\\mathbf{WX}}}}\\\\\n",
    "  &=\\log e^\\mathbf{WX}\\\\\n",
    "  &=\\mathbf{WX}\n",
    "  \\end{align*}$$\n",
    "  That is, the log odds of class $1$ is a linear function of $\\mathbf{X}$\n",
    "  \n",
    "- We find  $\\mathbf{W}$ using Conditional data likelihood --- Probability of observed $Y$ values in the training data, conditioned on corresponding $X$ values.\n",
    "- We choose parameters $\\mathbf{W}$ that satisfy.\n",
    "  $$\\mathbf{W}=\\argmax_\\mathbf{W}\\prod_lP(y^l \\mid \\mathbf{X}^l,\\mathbf{W})$$\n",
    "  where \n",
    "  - w = $<w_0,w_1,\\dots,w_n>$ is the vector of parameters to be estimated,\n",
    "  - $y^l$ denotes the observed value of $Y$ in the $l^{\\text{th}}$ training example, and\n",
    "  - $\\mathbf{X}^l$ denotes the observed value of $X$ in the $l^{\\text{th}}$ training example\n",
    "- Equivalently, we can work with log of conditional likelihood:\n",
    "  $$\\mathbf{W}=\\argmax_\\mathbf{W}\\sum_l \\ln P(y^l \\mid \\mathbf{X}^l,\\mathbf{W})$$\n",
    "- Conditional data log likelihood, $l(W)$, can be written as\n",
    "  $$l(W)=\\sum_l y^l \\ln P(y^l=1 \\mid \\mathbf{X}^l,\\mathbf{W})+(1-y^l) \\ln P(y^l=0 \\mid \\mathbf{X}^l,\\mathbf{W})$$\n",
    "- Note here that $Y$ can take only values $0$ or $1$, so only one of the two terms in the expression will be non-zero for any given $y^l$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "- We need to estimate:\n",
    "  $$\\mathbf{W}=\\argmax_\\mathbf{W}\\sum_l \\ln P(y^l \\mid \\mathbf{X}^l,\\mathbf{W})$$\n",
    "- Equivalently, we can minimize negative log likelihood\n",
    "- This is convex – so, unique global minimum\n",
    "- __No closed-form solution though__. Iterative method required.\n",
    "- Use gradient ascent (descent) for the maximization (min) problem\n",
    "- The $i^{\\text{th}}$ component of the vector gradient has the form\n",
    "  $$\\frac{\\partial }{\\partial w_i }l\\left(\\mathbf{W}\\right)=\\sum_l x_i^l \\left(y^l -\\underbrace{\\hat{P} \\left(y^l  =1\\mid {\\mathit{\\mathbf{X}}}^l ,\\mathit{\\mathbf{W}}\\right)}_{\\text{Logistic regression prediction}}\\right)$$\n",
    "- Beginning with initial weights, we repeatedly update the weights in the direction of the gradient, changing the $i^{\\text{th}}$ weight according to\n",
    "  $$w_i \\leftarrow w_i+\\eta \\sum_l x_i^l \\left(y^l -\\hat{P} \\left(y^l =1\\mid {\\mathit{\\mathbf{X}}}^l ,\\mathit{\\mathbf{W}}\\right)\\right)$$\n",
    "- Overfitting can arise especially when data has very high dimensions and is sparse\n",
    "- One approach -> modified “penalized log likelihood function,”\n",
    "which penalizes large values of $\\mathbf{W}$, as before.\n",
    "  $$\\mathbf{W}=\\argmax_\\mathbf{W}\\sum_l \\ln P(y^l \\mid \\mathbf{X}^l,\\mathbf{W})-\\frac{\\lambda}{2}\\left\\lVert \\mathbf{W} \\right\\rVert^2$$\n",
    "- Derivative then becomes:\n",
    "  $$\\frac{\\partial }{\\partial w_i }l\\left(\\mathit{\\mathbf{W}}\\right)=\\sum_l x_i^l \\left(y^l -\\hat{P} \\left(y^l =1\\mid {\\mathit{\\mathbf{X}}}^l ,\\mathit{\\mathbf{W}}\\right)\\right)-\\lambda w_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- In general, NB and LR make different assumptions\n",
    "    - NB: Features independent given class -> assumption on $P(X\\mid Y)$\n",
    "    - LR: Functional form of $P(Y\\mid X)$, no assumption on $P(X\\mid Y)$\n",
    "- LR is a linear classifier\n",
    "    - decision rule is a hyperplane\n",
    "- LR optimized by conditional likelihood\n",
    "    - no closed-form solution\n",
    "    - Concave (convex) -> global optimum with gradient ascent (descent)\n",
    "- LR can be extended to multiple class using softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of Clustering Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In terms of objective:\n",
    "    - Monothetic: cluster members have some common property\n",
    "        - E.g. All are males aged 20-35, or all have X% response to test B\n",
    "    - Polythetic: cluster members are similar to each other\n",
    "        - Distance between elements defines membership\n",
    "- In terms of overlap of clusters\n",
    "    - Hard clustering: clusters do not overlap\n",
    "    - Soft clustering: clusters may overlap\n",
    "        - “Strength of association” between element and cluster\n",
    "- In terms of methodology\n",
    "    - Flat/partitioning (vs) hierarchical: Set of groups (vs) taxonomy\n",
    "    - Density-based (vs) Model/Distribution-based: DBSCAN vs GMMs\n",
    "    - Connectionist (vs) Centroid-based: k-means vs Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline\n",
    "\n",
    "- K-Means\n",
    "- Hierarchical Clustering\n",
    "- Graph-based/Spectral Clustering\n",
    "- DBSCAN\n",
    "- Model-based Clustering (GMM and Expectation Maximization)\n",
    "- Evaluation of Clustering Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Partitional clustering approach\n",
    "- Each cluster is associated with a centroid (center point)\n",
    "- Each point is assigned to the cluster with the closest centroid\n",
    "- Number of clusters, K, must be specified\n",
    "- The basic algorithm is very simple\n",
    "    1. Select $K$ points as initial centroids\n",
    "    2. __Repeat__:\n",
    "        -  Form $K$  clusters by assigning all point to the closest centroid\n",
    "        -  Recompute centroid for each cluster.\n",
    "    5. __until__ The centroid doesn't change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-  Initial centroids are often chosen randomly.\n",
    "    -  Clusters produced can vary from one run to another.\n",
    "    -  The centroid is (typically) the mean of the points in the cluster.\n",
    "-  ‘Closeness’ is measured by Euclidean distance, cosine similarity,correlation, etc.\n",
    "-  K-means will converge for common similarity measures mentioned above (local minimum though)\n",
    "-  Most of the convergence happens in the first few iterations.\n",
    "    -  Often the stopping condition is changed to ‘Until relatively few points change clusters’\n",
    "-  Nearby points may not end up in the same cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting Initial Centroids \n",
    "\n",
    "How difficult is this?\n",
    "\n",
    "- If there are $K$ ‘real’ clusters then the chance of selecting one centroid from each cluster is small\n",
    "    - Chance is relatively small when $K$ is large\n",
    "    - If clusters are the same size, $n$, then<br>\n",
    "      $$\\begin{align*}\n",
    "      p&=\\frac{\\text{number of ways to select one centroid from each cluster}}{\\text{Number of ways to select }K \\text{ centroid}}\\\\\n",
    "      &=\\frac{K!n^K}{(Kn)^K}\\\\\n",
    "      &=\\frac{K!}{K^K}\n",
    "      \\end{align*}$$\n",
    "    - For example, if $K = 10$, then probability $= 10!/10^{10} = 0.00036$\n",
    "      \n",
    "#### Possible Solutions\n",
    "\n",
    "-  Multiple runs\n",
    "    -  Helps, but probability is not on our side\n",
    "-  Sample and use hierarchical clustering to determine initial centroids\n",
    "-  Select more than $k$ initial centroids and then select among these initial centroids\n",
    "    -  Select most widely separated\n",
    "-  Bisecting $K-means\n",
    "    -  Not as susceptible to initialization issues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating k-Means Clusters\n",
    "\n",
    "-  Most common measure is Sum of Squared Error (SSE) \n",
    "    -  For each point, the error is the distance to the nearest cluster \n",
    "    -  To get SSE, we square these errors and sum them. \n",
    "      $$\\mathrm{SSE}=\\sum_{i=1}^K\\sum_{i\\in C_i}\\mathrm{dist}^2(m_i,x)$$\n",
    "-  $x$ is a data point in cluster $C_i$ and $m_i$ is the representative point for cluster $C_i$ \n",
    "-  Can show that $m_i$ corresponds to the center (mean) of the cluster \n",
    "-  Given two clusterings, we can choose the one with the smaller error \n",
    "-  One easy way to reduce SSE is to increase $K$, the number of clusters \n",
    "-  A good clustering with smaller $K$ can have a lower SSE than a poor clustering with higher $K$ \n",
    "-  Relatively faster than other clustering methods: $O($ # iterations * # clusters * # instances * # dimensions $)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitations\n",
    "\n",
    "-  k-Means has problems when clusters are of differing\n",
    "    -  Sizes, Densities, Non-globular shapes\n",
    "-  Sensitive to outliers\n",
    "-  The number of clusters $(K)$ is difficult to determine<br><br>\n",
    "   ![](CS5590_images/Acrobat_6cXliqPWl5.png)<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extensions\n",
    "\n",
    "- Use of various distance metrics\n",
    "    - Euclidean distance <br>\n",
    "        $$d(x,y)=\\sqrt{\\sum_{i=1}^n \\left\\lvert x_i-y_i \\right\\rvert^2}$$\n",
    "    - Manhattan (city-block) distance\n",
    "        $$d(x,y)=\\sum_{i=1}^n \\left\\lvert x_i-y_i \\right\\rvert$$\n",
    "    - Cosine distance\n",
    "        $$\\begin{align*}\n",
    "        &\\cos(x,y)=\\frac{\\sum_{i=1}^nx_iy_i}{\\sqrt{\\sum_{i=1}^nx_i^2}\\sqrt{\\sum_{i=1}^ny_i^2}}\\\\\n",
    "        &d(x,y)=1-\\cos(x,y)\n",
    "        \\end{align*}$$\n",
    "    - Chebyshev distance\n",
    "        $$\\mathrm{dist}(x_i,x_j)=\\max(\\left\\lvert x_{i1}-x_{j1} \\right\\rvert ,\\left\\lvert x_{i2}-x_{j2} \\right\\rvert ,\\dots ,\\left\\lvert x_{ir}-x_{jr} \\right\\rvert) $$\n",
    "- $k$-Medioids\n",
    "- Bisecting $k$-Means\n",
    "- $k$-Means $++$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we do not know how many clusters to use, so the idea of hierarchical clustering is to use all $k$ clusters hierarchically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Types of Clustering Methods\n",
    "\n",
    "  ![](CS5590_images/Acrobat_jvMmR6OD1l.png)<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical Clustering\n",
    "\n",
    "- Produces a set of nested clusters organized as a hierarchical tree\n",
    "- Can be visualized as a __dendrogram__\n",
    "    - A tree like diagram that records the sequences of merges or splits<br><br>\n",
    "      ![](CS5590_images/Acrobat_XlYsFF9aTO.png)<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strengths\n",
    "\n",
    "-  Do not have to assume any particular number of clusters \n",
    "    -  Any desired number of clusters can be obtained by ‘cutting’ the dendrogram at the proper level \n",
    "-  They may correspond to meaningful taxonomies \n",
    "    -  Example in biological sciences (e.g., animal kingdom, phylogeny reconstruction, …)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two main types of hierarchical clustering\n",
    "\n",
    "1.  Agglomerative: \n",
    "    -  Start with the points as individual clusters \n",
    "    -  At each step, merge the closest pair of clusters until only one cluster (or $k$ clusters) left \n",
    "2.  Divisive: \n",
    "    -  Start with one, all-inclusive cluster \n",
    "    -  At each step, split a cluster until each cluster contains a point (or there are $k$ clusters)\n",
    "- Traditional hierarchical algorithms use a similarity or distance matrix\n",
    "    - Merge or split one cluster at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agglomerative Clustering Algorithm\n",
    "\n",
    "-  More popular hierarchical clustering technique\n",
    "-  Basic algorithm is straightforward\n",
    "    1. Compute the proximity matrix\n",
    "    2. Let each data point be a cluster\n",
    "    3. __Repeat__\n",
    "        1. Merge the two closest clusters\n",
    "        2. Update the proximity matrix\n",
    "    4. __Until__ only a single cluster remains\n",
    "-  Key operation is the computation of the proximity of two clusters\n",
    "    -  Different approaches to defining the distance between clusters distinguish the different algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methodology\n",
    "\n",
    "- Start with clusters of individual points and a proximity matrix (pairwise distance between each data point)<br><br>\n",
    "  ![](CS5590_images/mspaint_4PJPGBUzQ2.png)<br><br>\n",
    "- After some merging steps, we have some clusters<br><br>\n",
    "  ![](CS5590_images/mspaint_11lcWyxY5z.png)<br><br>\n",
    "- We want to merge the two closest clusters (C2 and C5) and update the proximity matrix.<br><br>\n",
    "  ![](CS5590_images/mspaint_McTTx9mF84.png)<br><br>\n",
    "- The question is “How do we update the proximity matrix?”<br><br>\n",
    "  ![](CS5590_images/mspaint_EgIpeiigHW.png)<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can update the proximity matrix using Inter-cluster Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Inter-cluster Similarity\n",
    "\n",
    "- MIN (Single-link)<br><br>\n",
    "  ![](CS5590_images/Acrobat_DgthH0kVLa.png)<br><br>\n",
    "- MAX (Complete-link)<br><br>\n",
    "  ![](CS5590_images/Acrobat_XG7dtguqQd.png)<br><br>\n",
    "- Group Average (Average-link)<br><br>\n",
    "  ![](CS5590_images/Acrobat_V87JygBnBI.png)<br><br>\n",
    "- Distance Between Centroids<br><br>\n",
    "  ![](CS5590_images/Acrobat_XTf6gyPsxn.png)<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitations\n",
    "\n",
    "-  Once a decision is made to combine two clusters, it cannot be undone \n",
    "-  No objective function is directly minimized \n",
    "-  Different schemes have problems with one or more of the following: \n",
    "    -  Sensitivity to noise and outliers (MIN) \n",
    "    -  Difficulty handling different sized clusters and non-convex shapes (Group average, MAX) \n",
    "    -  Breaking large clusters (MAX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph-based/Spectral Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Associate each data item with a vertex in a weighted graph \n",
    "    -  weights on the edges between elements are large if the elements are similar and small if they are not. \n",
    "-  Cut the graph into connected components with relatively large interior weights by cutting edges with relatively low weights. \n",
    "-  Clustering becomes a graph cut problem.<br><br>\n",
    "  ![](CS5590_images/mspaint_MeMOc3QDXr.png)<br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
