<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Abhishek Kumar Dubey">
<meta name="dcterms.date" content="2022-10-08">
<meta name="description" content="Ensemble Classifier">

<title>IITH-Data-Science - Machine Learning 6</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../Data_Science_Tutorial/CS5590/2022-10-15-CS5590-week7.html" rel="next">
<link href="../../Data_Science_Tutorial/CS5590/2022-09-24-CS5590-week5.html" rel="prev">
<link href="../../logo.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-5ZQX02R26E"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-5ZQX02R26E', { 'anonymize_ip': true});
</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="IITH-Data-Science - Machine Learning 6">
<meta property="og:description" content="Ensemble Classifier">
<meta property="og:image" content="https://github.com/abhiyantaabhishek/IITH-Data-Science/Data_Science_Tutorial/CS5590/CS5590_images/Acrobat_EZY9SgFMeU.png">
<meta property="og:site-name" content="IITH-Data-Science">
<meta property="og:image:height" content="171">
<meta property="og:image:width" content="526">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">IITH-Data-Science</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">
 <span class="menu-text"><i class="fa-solid fa-house" aria-label="house"></i> Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="../../Data_Science_Tutorial/index.html" aria-current="page">
 <span class="menu-text"><i class="fa-solid fa-book-open-reader" aria-label="book-open-reader"></i> Data Science Tutorial</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../Data_Science_Hacks/index.html">
 <span class="menu-text"><i class="fa-solid fa-user-ninja" aria-label="user-ninja"></i> Data Science Hacks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text"><i class="fa-solid fa-address-card" aria-label="address-card"></i> About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/abhiyantaabhishek"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/abhishek-kumar-dubey-585a86179/"><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Machine Learning 6</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title d-none d-lg-block">Machine Learning 6</h1>
                  <div>
        <div class="description">
          Ensemble Classifier
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Machine Learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Abhishek Kumar Dubey </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 8, 2022</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../Data_Science_Tutorial/index.html" class="sidebar-item-text sidebar-link">Data Science Tutorial</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">CS5590</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth2 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS5590/2022-08-06-CS5590-week1.html" class="sidebar-item-text sidebar-link">Machine Learning 1</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS5590/2022-08-20-CS5590-week2.html" class="sidebar-item-text sidebar-link">Machine Learning 2</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS5590/2022-08-27-CS5590-week3.html" class="sidebar-item-text sidebar-link">Machine Learning 3</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS5590/2022-09-10-CS5590-week4.html" class="sidebar-item-text sidebar-link">Machine Learning 4</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS5590/2022-09-24-CS5590-week5.html" class="sidebar-item-text sidebar-link">Machine Learning 5</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS5590/2022-10-08-CS5590-week6.html" class="sidebar-item-text sidebar-link active">Machine Learning 6</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS5590/2022-10-15-CS5590-week7.html" class="sidebar-item-text sidebar-link">Machine Learning 7</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS5590/2022-10-29-CS5590-week8.html" class="sidebar-item-text sidebar-link">Machine Learning 8</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS5590/2022-11-05-CS5590-week9.html" class="sidebar-item-text sidebar-link">Machine Learning 9</a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">CS6660</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS6660/2022-09-24-CS6660-week5.html" class="sidebar-item-text sidebar-link">Distribution function</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS6660/2022-09-03-CS6660-week3.html" class="sidebar-item-text sidebar-link">Linear Algebra 1</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS6660/2022-09-17-CS6660-week4_2.html" class="sidebar-item-text sidebar-link">Linear Algebra 2</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS6660/2022-10-01-CS6660-week6.html" class="sidebar-item-text sidebar-link">Linear Algebra 3</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS6660/2022-08-06-CS6660-week1.html" class="sidebar-item-text sidebar-link">Probability Theory 1</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS6660/2022-08-13-CS6660-week2.html" class="sidebar-item-text sidebar-link">Probability Theory 2</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS6660/2022-09-17-CS6660-week4_1.html" class="sidebar-item-text sidebar-link">Probability Theory 3</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS6660/2022-10-15-CS6660-week7.html" class="sidebar-item-text sidebar-link">Probability Theory 5</a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#types-of-ensemble-classifiers" id="toc-types-of-ensemble-classifiers" class="nav-link active" data-scroll-target="#types-of-ensemble-classifiers"><span class="toc-section-number">1</span>  Types of Ensemble Classifiers</a></li>
  <li><a href="#bagging" id="toc-bagging" class="nav-link" data-scroll-target="#bagging"><span class="toc-section-number">2</span>  Bagging</a>
  <ul>
  <li><a href="#example-random-forests" id="toc-example-random-forests" class="nav-link" data-scroll-target="#example-random-forests"><span class="toc-section-number">2.1</span>  Example: Random Forests</a>
  <ul>
  <li><a href="#random-forests-algorithm" id="toc-random-forests-algorithm" class="nav-link" data-scroll-target="#random-forests-algorithm"><span class="toc-section-number">2.1.1</span>  Random Forests: Algorithm</a></li>
  <li><a href="#features-of-random-forests" id="toc-features-of-random-forests" class="nav-link" data-scroll-target="#features-of-random-forests"><span class="toc-section-number">2.1.2</span>  Features of Random Forests</a></li>
  </ul></li>
  <li><a href="#bagging-when" id="toc-bagging-when" class="nav-link" data-scroll-target="#bagging-when"><span class="toc-section-number">2.2</span>  Bagging: when</a></li>
  <li><a href="#bagging-why" id="toc-bagging-why" class="nav-link" data-scroll-target="#bagging-why"><span class="toc-section-number">2.3</span>  Bagging: Why</a></li>
  </ul></li>
  <li><a href="#boosting" id="toc-boosting" class="nav-link" data-scroll-target="#boosting"><span class="toc-section-number">3</span>  Boosting</a>
  <ul>
  <li><a href="#key-idea" id="toc-key-idea" class="nav-link" data-scroll-target="#key-idea"><span class="toc-section-number">3.1</span>  Key Idea</a></li>
  <li><a href="#adaboost-algorithm" id="toc-adaboost-algorithm" class="nav-link" data-scroll-target="#adaboost-algorithm"><span class="toc-section-number">3.2</span>  Adaboost Algorithm</a></li>
  <li><a href="#a-good-weak-learner" id="toc-a-good-weak-learner" class="nav-link" data-scroll-target="#a-good-weak-learner"><span class="toc-section-number">3.3</span>  A good weak learner</a></li>
  <li><a href="#gradient-boosting" id="toc-gradient-boosting" class="nav-link" data-scroll-target="#gradient-boosting"><span class="toc-section-number">3.4</span>  Gradient Boosting</a>
  <ul>
  <li><a href="#gradient-boosting-algorithm" id="toc-gradient-boosting-algorithm" class="nav-link" data-scroll-target="#gradient-boosting-algorithm"><span class="toc-section-number">3.4.1</span>  Gradient Boosting Algorithm</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#stacking-stacked-generalization" id="toc-stacking-stacked-generalization" class="nav-link" data-scroll-target="#stacking-stacked-generalization"><span class="toc-section-number">4</span>  Stacking (stacked generalization)</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<ul>
<li>Ensemble classification combines multiple classifiers to improve accuracy</li>
<li>Advantage
<ul>
<li>Large datasets: if dataset is too large we can train different models on subset of the data.</li>
<li>Small datasets: Can handle them with bootstrapping (random sampling)</li>
<li>Can solve complicated problems which can’t be solved using single classifier.</li>
</ul></li>
</ul>
<section id="types-of-ensemble-classifiers" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="types-of-ensemble-classifiers"><span class="header-section-number">1</span> Types of Ensemble Classifiers</h2>
<ul>
<li>Bagging (bootstrap aggregating)
<ul>
<li>Train several models using bootstrapped datasets</li>
<li>The majority classification is selected</li>
</ul></li>
<li>Boosting
<ul>
<li>Use several weak classifiers to create a strong classifier</li>
<li>Resample previously misclassified points</li>
</ul></li>
<li>Stacking (stacked generalization)
<ul>
<li>Train multiple tiers of classifiers</li>
<li>Higher tiers can correct lower tiers</li>
</ul></li>
</ul>
</section>
<section id="bagging" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="bagging"><span class="header-section-number">2</span> Bagging</h2>
<ul>
<li>Problem: we have only one dataset.</li>
<li>Solution: generate new ones of size n by <strong>bootstrapping</strong>, i.e.&nbsp;sampling it with replacement</li>
<li>Bagging works because it <strong>reduces variance</strong> by voting/averaging</li>
<li>Usually, the more classifiers the better</li>
<li>Some candidates:
<ul>
<li>Decision tree, decision stump, SVMs.</li>
<li>Can do this with regression too: Regression tree, linear regression</li>
</ul></li>
</ul>
<section id="example-random-forests" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="example-random-forests"><span class="header-section-number">2.1</span> Example: Random Forests</h3>
<ul>
<li>Random forests (RF) are a combination of tree predictors, it’s a variant of bagging.</li>
<li>Extremely successful, especially on Kaggle challenges.</li>
<li>Each tree depends on the values of a random vector sampled independently</li>
<li>The generalization error depends on the strength of the individual trees and the correlation between them</li>
<li>Using a random selection of features yields results robust w.r.t. noise</li>
</ul>
<section id="random-forests-algorithm" class="level4" data-number="2.1.1">
<h4 data-number="2.1.1" class="anchored" data-anchor-id="random-forests-algorithm"><span class="header-section-number">2.1.1</span> Random Forests: Algorithm</h4>
<ul>
<li>Given a training set <span class="math inline">S</span></li>
<li>For <span class="math inline">i = 1</span> to <span class="math inline">k</span> do:
<ul>
<li>Build subset <span class="math inline">S_i</span> by sampling with replacement from <span class="math inline">S</span></li>
<li>Learn tree <span class="math inline">T_i</span> from <span class="math inline">S_i</span>
<ul>
<li>At each node:
<ul>
<li>Choose best split from random subset of <span class="math inline">F</span> features</li>
</ul></li>
<li>Each tree grows to the largest extent, and no pruning</li>
</ul></li>
</ul></li>
<li>Make predictions according to majority vote of the set of <span class="math inline">k</span> trees.</li>
<li>If there are <span class="math inline">M</span> input variables, a number <span class="math inline">m</span> is specified such that at each node, <span class="math inline">m</span> variables are selected at random out of the <span class="math inline">M</span> and the best split on these <span class="math inline">m</span> is used to split the node. The value of <span class="math inline">m</span> is held constant during the forest growing.</li>
<li>Depending upon the value of <span class="math inline">m</span>, there are three slightly different systems:
<ul>
<li>Random splitter selection: <span class="math inline">m =1</span></li>
<li>Breiman’s bagger: <span class="math inline">m =</span> total number of predictor variables</li>
<li>Random forest: <span class="math inline">m &lt;&lt;</span> number of predictor variables. Breiman suggests three possible values for <span class="math inline">m:\frac{1}{2}\sqrt{M}, \sqrt{M}, \text{ and } 2\sqrt{M}</span></li>
</ul></li>
</ul>
</section>
<section id="features-of-random-forests" class="level4" data-number="2.1.2">
<h4 data-number="2.1.2" class="anchored" data-anchor-id="features-of-random-forests"><span class="header-section-number">2.1.2</span> Features of Random Forests</h4>
<ul>
<li>One of the best in the business</li>
<li>It runs efficiently on large data bases</li>
<li>It can handle thousands of input variables without variable deletion/reduction</li>
<li>It gives estimates of what variables are important in the classification</li>
<li><em>Does not overfit by design</em></li>
<li>The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. ( if correlation is high it means trees are similar and not so useful)</li>
</ul>
</section>
</section>
<section id="bagging-when" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="bagging-when"><span class="header-section-number">2.2</span> Bagging: when</h3>
<ul>
<li>Can help if data is noisy.</li>
<li>If learning algorithm is unstable, i.e.&nbsp;if small changes to the training set cause large changes in the learned classifier.</li>
</ul>
</section>
<section id="bagging-why" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="bagging-why"><span class="header-section-number">2.3</span> Bagging: Why</h3>
<ul>
<li>Let <span class="math inline">S= \{(x , y ), i=1\dots N\}</span> be the training dataset</li>
<li>Let <span class="math inline">\{S_k \}</span> be a sequence of training sets containing a sub-set of <span class="math inline">S</span></li>
<li>Let <span class="math inline">P</span> be the underlying distribution of <span class="math inline">S</span>.</li>
<li>Bagging replaces the prediction of the model with the majority of the predictions given by the classifiers <span class="math inline">S</span>. <span class="math display">\phi(x,P)=E_s(\phi (x,S_k))</span></li>
</ul>
</section>
</section>
<section id="boosting" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="boosting"><span class="header-section-number">3</span> Boosting</h2>
<ul>
<li>Use several weak classifiers to create a strong classifier</li>
<li>Resample previously misclassified points. (This is not done by bagging)</li>
<li>Strong Learner: This is the objective of machine learning where we take labelled data for training and produce classifier which can be <strong>arbitrarily accurate</strong>.</li>
<li>Weak Learner: Take labelled data for training and produce a classifier which is <strong>more accurate than random guessing</strong>.</li>
</ul>
<p>Can a set of weak learners create a single strong learner?</p>
<section id="key-idea" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="key-idea"><span class="header-section-number">3.1</span> Key Idea</h3>
<ul>
<li>An algorithm for constructing a “strong” classifier as linear combination of “simple” “weak” classifier <span class="math display">f(x)=\sum_{t=1}^T \alpha_th_t(x)</span></li>
<li>Final classification based on weighted vote of weak classifiers</li>
</ul>
</section>
<section id="adaboost-algorithm" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="adaboost-algorithm"><span class="header-section-number">3.2</span> Adaboost Algorithm</h3>
<p>Given <span class="math inline">(x_1,y_1),\dots (x_m,y_m)</span> where <span class="math inline">x_i \in X,y_i \in Y =\{-1,+1\}</span><br><br> Initialize <span class="math inline">D(i)=\frac{1}{m}</span><br><br> For <span class="math inline">t=1,\dots T</span>:</p>
<ul>
<li>Find the classifier <span class="math inline">h_t:X \rightarrow \{-1,+1\}</span> that minimizes the error with respect to the distribution <span class="math inline">D_t</span>:<br> <span class="math inline">\displaystyle h_t = \argmin _{h_j \in H} \epsilon_j</span>, where <span class="math inline">\displaystyle\epsilon_j=\sum_{i=1}^m D_t(i)[y_i \ne h_j(x_i)]</span></li>
<li>Prerequisite: <span class="math inline">\epsilon_t&lt;0.5</span>, otherwise stop.</li>
<li>Choose <span class="math inline">\alpha_t \in \mathbb{R}</span>, typically <span class="math inline">\displaystyle \alpha_t=\frac{1}{2}\ln \frac{1-\epsilon_t}{\epsilon_t}</span>, where <span class="math inline">\epsilon_t</span> is the weighted error rate of the classifier <span class="math inline">h_t</span><br> <span class="math inline">\boxed{\alpha_t \text{ stays same for all data points}}</span></li>
<li>Update <br> <span class="math display">D_{t+1} \left(i\right)=\frac{D_t \left(i\right)\exp \left(-\alpha_t y_t h_t \left(x_i \right)\right)}{Z_t }</span> where <span class="math inline">Z_t</span> is a normalization factor (chosen such that <span class="math inline">D_{t+1}</span> will be a distribution)</li>
</ul>
<p>output of the final classifier :<br> <span class="math display">H(x)=\text{Sign}\sum_{t=1}^T\alpha_t h_t(x)</span></p>
<p>Reweighting<br><br> <span class="math display">\begin{align*}{}
D_{t+1} \left(i\right)&amp;=\frac{D_t \left(i\right)\exp \left(-\alpha_t y_t h_t \left(x_i \right)\right)}{Z_t }\\
&amp;=\frac{D_t \left(i\right)\exp \left(-y_i \sum_{q=1}^t \alpha_q h_q \left(x_i \right)\right)}{m\prod_{q=1}^t Z_q }
\end{align*}</span></p>
<p>Notice</p>
<p><span class="math display">\exp \left(-\alpha_t y_t h_t \left(x_i \right)\right)  \begin{cases}
    \le1 &amp;\text{if } y_t=h_t(x_i) \xleftarrow{y\times h(x)=1} \\
    &gt;1 &amp;\text{if } y_t\ne h_t(x_i) \xleftarrow{y\times h(x)=-1}
\end{cases}</span></p>
<p>we can see that weight of wrongly classified example is increased and weight of correctly classified example is decreased.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Adaboost vs Random Forests
</div>
</div>
<div class="callout-body-container callout-body">
<p>Dietterich (1998) showed that when a fraction of the output labels in the training set are randomly altered, the accuracy of Adaboost degenerates, while bagging is more immune to the noise.</p>
</div>
</div>
</section>
<section id="a-good-weak-learner" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="a-good-weak-learner"><span class="header-section-number">3.3</span> A good weak learner</h3>
<ul>
<li>The set of weak rules (features) should be <strong>flexible enough to be (weakly) correlated</strong> with most conceivable relations between feature vector and label.</li>
<li><strong>Small enough to allow exhaustive search</strong> for the minimal weighted training error.</li>
<li><strong>Small enough to avoid over-fitting.</strong></li>
<li>Should be able to <strong>calculate predicted label very efficiently</strong></li>
<li>Rules can be <strong>“specialists”</strong> – predict only on a small subset of the input space and <strong>abstain from predicting</strong> on the rest (output 0).</li>
</ul>
</section>
<section id="gradient-boosting" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="gradient-boosting"><span class="header-section-number">3.4</span> Gradient Boosting</h3>
<p>Gradient Boosting = Gradient Descent + Boosting</p>
<ul>
<li>Fit an additive model (ensemble) in a forward stage-wise manner.</li>
<li>In each stage, introduce a weak learner to compensate the shortcomings of existing weak learners.</li>
<li>In Gradient Boosting, “shortcomings” are identified by gradients.</li>
<li>Recall that, in Adaboost, “shortcomings” are identified by high-weight data points.</li>
<li>Both high-weight data points and gradients tell us how to improve our model.</li>
</ul>
<section id="gradient-boosting-algorithm" class="level4" data-number="3.4.1">
<h4 data-number="3.4.1" class="anchored" data-anchor-id="gradient-boosting-algorithm"><span class="header-section-number">3.4.1</span> Gradient Boosting Algorithm</h4>
<p>Input: Training set <span class="math inline">{\left\lbrace \left(x_i ,y_i \right)\right\rbrace }_{i=1}^n</span>, a differentiable loss function <span class="math inline">L(y,F(x))</span>,Number of iteration <span class="math inline">M</span> <br><br> Algorithm: <br></p>
<ul>
<li><p>Initialize model with constant value:<br></p>
<p><span class="math display">F_0(x)=\argmin_\gamma \sum_{i=1}^nL(y_i,\gamma)</span></p></li>
<li><p>For <span class="math inline">m=1</span> to <span class="math inline">M</span></p>
<ul>
<li>Compute pseudo-residuals: <span class="math display">r_{im}=-\left[\frac{\partial L(y_i,F(x_i))}{\partial F(x_i)} \right]_{F(x)=F_{m-1}(x)}\;\text{for }i=1,\dots,n</span></li>
<li>Fit a base lerner (e.g.&nbsp;tree) <span class="math inline">h_m(x)</span> to pseudo-residual, i.e.&nbsp;train it using the set <span class="math inline">{\left\lbrace \left(x_i ,\boxed{r_{im}} \right)\right\rbrace }_{i=1}^n\;\;</span> <strong>Notice the training set now contains residual instead of original data</strong>.</li>
<li>Compute multiplier <span class="math inline">\gamma_m</span> by solving the following one-dimensional optimization problem: <span class="math display">\gamma_m=\argmin_\gamma \sum_{i=1}^nL(y_i,\underbrace{F_{m-1}(x_i)}_{\text{previous model}}+\gamma \times \underbrace{h_m(x_i)}_{\text{current model}})</span></li>
<li>Update the model: <span class="math display">F_m(x)=F_{m-1}(x)+\gamma h_m(x)</span></li>
</ul></li>
<li><p>Output <span class="math inline">F_M(x)</span></p></li>
</ul>
</section>
</section>
</section>
<section id="stacking-stacked-generalization" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="stacking-stacked-generalization"><span class="header-section-number">4</span> Stacking (stacked generalization)</h2>
<ul>
<li>Train multiple tiers of classifiers</li>
<li>Higher tiers can correct lower tiers</li>
<li>Combiner <span class="math inline">f ()</span> is another learner (Wolpert, 1992)</li>
<li>Idea:
<ul>
<li>Generate component (level 0) classifiers with part of the data (half, three quarters)</li>
<li>Train combiner (level 1) classifier to combine predictions of components using remaining data</li>
<li>Retrain component classifiers with all of training data</li>
</ul></li>
<li>In practice, often equivalent to voting <br><br> <img src="CS5590_images/Acrobat_2wGlX14eBZ.png" class="img-fluid"></li>
</ul>
<p><br><br><br> <span class="math inline">\tiny {\textcolor{#808080}{\boxed{\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}</span></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="abhiyantaabhishek/IITH-Data-Science" data-repo-id="R_kgDOILoB8A" data-category="Announcements" data-category-id="DIC_kwDOILoB8M4CSJcL" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../Data_Science_Tutorial/CS5590/2022-09-24-CS5590-week5.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Machine Learning 5</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../Data_Science_Tutorial/CS5590/2022-10-15-CS5590-week7.html" class="pagination-link">
        <span class="nav-page-text">Machine Learning 7</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Copyright 2022, Abhishek Kumar Dubey</div>   
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/abhiyantaabhishek">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/abhishek-kumar-dubey-585a86179/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>



</body></html>