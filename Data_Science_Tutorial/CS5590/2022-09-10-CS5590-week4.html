<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Abhishek Kumar Dubey">
<meta name="dcterms.date" content="2022-09-10">
<meta name="description" content="SVM, Lagrange Multipliers, KKT condition, Mercer’s condition, Kernel Trick">

<title>IITH-Data-Science - Machine Learning 4</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../Data_Science_Tutorial/CS5590/2022-09-24-CS5590-week5.html" rel="next">
<link href="../../Data_Science_Tutorial/CS5590/2022-08-27-CS5590-week3.html" rel="prev">
<link href="../../logo.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-5ZQX02R26E"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-5ZQX02R26E', { 'anonymize_ip': true});
</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="IITH-Data-Science - Machine Learning 4">
<meta property="og:description" content="SVM, Lagrange Multipliers, KKT condition, Mercer’s condition, Kernel Trick">
<meta property="og:image" content="https://github.com/abhiyantaabhishek/IITH-Data-Science/Data_Science_Tutorial/CS5590/CS5590_images/Acrobat_vct8kMqVEu.png">
<meta property="og:site-name" content="IITH-Data-Science">
<meta property="og:image:height" content="128">
<meta property="og:image:width" content="368">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">IITH-Data-Science</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">
 <span class="menu-text"><i class="fa-solid fa-house" aria-label="house"></i> Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="../../Data_Science_Tutorial/index.html" aria-current="page">
 <span class="menu-text"><i class="fa-solid fa-book-open-reader" aria-label="book-open-reader"></i> Data Science Tutorial</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../Data_Science_Hacks/index.html">
 <span class="menu-text"><i class="fa-solid fa-user-ninja" aria-label="user-ninja"></i> Data Science Hacks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text"><i class="fa-solid fa-address-card" aria-label="address-card"></i> About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/abhiyantaabhishek"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/abhishek-kumar-dubey-585a86179/"><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Machine Learning 4</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title d-none d-lg-block">Machine Learning 4</h1>
                  <div>
        <div class="description">
          SVM, Lagrange Multipliers, KKT condition, Mercer’s condition, Kernel Trick
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Machine Learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Abhishek Kumar Dubey </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">September 10, 2022</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../Data_Science_Tutorial/index.html" class="sidebar-item-text sidebar-link">Data Science Tutorial</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">CS5590</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth2 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS5590/2022-08-06-CS5590-week1.html" class="sidebar-item-text sidebar-link">Machine Learning 1</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS5590/2022-08-20-CS5590-week2.html" class="sidebar-item-text sidebar-link">Machine Learning 2</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS5590/2022-08-27-CS5590-week3.html" class="sidebar-item-text sidebar-link">Machine Learning 3</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS5590/2022-09-10-CS5590-week4.html" class="sidebar-item-text sidebar-link active">Machine Learning 4</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS5590/2022-09-24-CS5590-week5.html" class="sidebar-item-text sidebar-link">Machine Learning 5</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS5590/2022-10-08-CS5590-week6.html" class="sidebar-item-text sidebar-link">Machine Learning 6</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS5590/2022-10-15-CS5590-week7.html" class="sidebar-item-text sidebar-link">Machine Learning 7</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS5590/2022-10-29-CS5590-week8.html" class="sidebar-item-text sidebar-link">Machine Learning 8</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS5590/2022-11-05-CS5590-week9.html" class="sidebar-item-text sidebar-link">Machine Learning 9</a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">CS6660</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS6660/2022-09-03-CS6660-week3.html" class="sidebar-item-text sidebar-link">Linear Algebra 1</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS6660/2022-09-17-CS6660-week4_2.html" class="sidebar-item-text sidebar-link">Linear Algebra 2</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS6660/2022-10-01-CS6660-week6.html" class="sidebar-item-text sidebar-link">Linear Algebra 3</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS6660/2022-08-06-CS6660-week1.html" class="sidebar-item-text sidebar-link">Probability Theory 1</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS6660/2022-08-13-CS6660-week2.html" class="sidebar-item-text sidebar-link">Probability Theory 2</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS6660/2022-09-17-CS6660-week4_1.html" class="sidebar-item-text sidebar-link">Probability Theory 3</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS6660/2022-09-24-CS6660-week5.html" class="sidebar-item-text sidebar-link">Probability Theory 4</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Data_Science_Tutorial/CS6660/2022-10-15-CS6660-week7.html" class="sidebar-item-text sidebar-link">Probability Theory 5</a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#overview-and-history" id="toc-overview-and-history" class="nav-link active" data-scroll-target="#overview-and-history"><span class="toc-section-number">1</span>  Overview and History</a></li>
  <li><a href="#maximum-margin-classifier" id="toc-maximum-margin-classifier" class="nav-link" data-scroll-target="#maximum-margin-classifier"><span class="toc-section-number">2</span>  Maximum Margin Classifier</a></li>
  <li><a href="#estimate-the-margin" id="toc-estimate-the-margin" class="nav-link" data-scroll-target="#estimate-the-margin"><span class="toc-section-number">3</span>  Estimate the Margin</a></li>
  <li><a href="#maximize-the-margin" id="toc-maximize-the-margin" class="nav-link" data-scroll-target="#maximize-the-margin"><span class="toc-section-number">4</span>  Maximize the Margin</a></li>
  <li><a href="#using-lagrange-multipliers" id="toc-using-lagrange-multipliers" class="nav-link" data-scroll-target="#using-lagrange-multipliers"><span class="toc-section-number">5</span>  Using Lagrange Multipliers</a>
  <ul>
  <li><a href="#basics-of-lagrange-multipliers" id="toc-basics-of-lagrange-multipliers" class="nav-link" data-scroll-target="#basics-of-lagrange-multipliers"><span class="toc-section-number">5.1</span>  Basics of Lagrange Multipliers</a></li>
  <li><a href="#svm-standard-primal-form" id="toc-svm-standard-primal-form" class="nav-link" data-scroll-target="#svm-standard-primal-form"><span class="toc-section-number">5.2</span>  SVM standard (primal) form</a></li>
  <li><a href="#svm-dual" id="toc-svm-dual" class="nav-link" data-scroll-target="#svm-dual"><span class="toc-section-number">5.3</span>  SVM Dual</a></li>
  <li><a href="#solving-using-kkt-condition" id="toc-solving-using-kkt-condition" class="nav-link" data-scroll-target="#solving-using-kkt-condition"><span class="toc-section-number">5.4</span>  Solving using KKT condition</a></li>
  </ul></li>
  <li><a href="#soft-margin-svm" id="toc-soft-margin-svm" class="nav-link" data-scroll-target="#soft-margin-svm"><span class="toc-section-number">6</span>  Soft Margin SVM</a>
  <ul>
  <li><a href="#lagrangian-for-soft-margin-svm" id="toc-lagrangian-for-soft-margin-svm" class="nav-link" data-scroll-target="#lagrangian-for-soft-margin-svm"><span class="toc-section-number">6.1</span>  Lagrangian for Soft Margin SVM</a></li>
  </ul></li>
  <li><a href="#multi-class-classification-with-svms" id="toc-multi-class-classification-with-svms" class="nav-link" data-scroll-target="#multi-class-classification-with-svms"><span class="toc-section-number">7</span>  Multi-class Classification with SVMs</a></li>
  <li><a href="#kernel-trick" id="toc-kernel-trick" class="nav-link" data-scroll-target="#kernel-trick"><span class="toc-section-number">8</span>  Kernel Trick</a>
  <ul>
  <li><a href="#why-do-we-require-the-kernel-trick" id="toc-why-do-we-require-the-kernel-trick" class="nav-link" data-scroll-target="#why-do-we-require-the-kernel-trick"><span class="toc-section-number">8.1</span>  Why do we require the Kernel Trick</a></li>
  <li><a href="#how-do-we-do-the-kernel-trick" id="toc-how-do-we-do-the-kernel-trick" class="nav-link" data-scroll-target="#how-do-we-do-the-kernel-trick"><span class="toc-section-number">8.2</span>  How do we do the kernel Trick</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="overview-and-history" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="overview-and-history"><span class="header-section-number">1</span> Overview and History</h2>
<ul>
<li>It is a discriminative classifier.</li>
<li>Inspired by Statistical Learning.</li>
<li>Developed in 1992 by Vapnik, Guyon, Boser</li>
<li>Was one of the go-to methods in ML since mid 1990s (only recently displaced by deep learning.)</li>
</ul>
</section>
<section id="maximum-margin-classifier" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="maximum-margin-classifier"><span class="header-section-number">2</span> Maximum Margin Classifier</h2>
<ul>
<li>Formulation <br><br>
<ul>
<li><span class="math inline">f(\mathbf{X},\mathbf{W},b)=\mathrm{sign} (\mathbf{W} \cdot \mathbf{X}+b)</span><br><br> <img src="CS5590_images/Acrobat_vct8kMqVEu.png" class="img-fluid"></li>
<li>Basic formulation of SVM can only handle two classes.</li>
<li>There are improvised method to handle more than tow class.</li>
<li>The <strong>Maximum margin classifier</strong> is the linear classifier with the maximum margin. This is the simplest kind of SVM ( called an LSVM).</li>
</ul></li>
</ul>
</section>
<section id="estimate-the-margin" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="estimate-the-margin"><span class="header-section-number">3</span> Estimate the Margin</h2>
<ul>
<li><p>The points those lies on the two margin lines are called support vector.</p></li>
<li><p>The model is immune to removal of any non-support-vector data points.</p></li>
<li><p>The equation of the line is given by <span class="math inline">\mathbf{W}^T \cdot \mathbf{X}+b=0</span><br><br> <img src="CS5590_images/Acrobat_nYNJPaG71I.png" class="img-fluid"><br></p></li>
<li><p><span class="math inline">\mathbf{W}</span> is always normal to the line <span class="math inline">\mathbf{W}^T \cdot \mathbf{X}+b=0</span><br> This can be proved by taking two vector <span class="math inline">\mathbf{X_1}</span> and <span class="math inline">\mathbf{X_2}</span> on line <span class="math inline">\mathbf{W}^T \cdot \mathbf{X}+b=0</span>,<br> Now if we subtract the two vector we get <span class="math inline">\mathbf{W}^T(\mathbf{X_1}-\mathbf{X_2})=\mathbf{0} \Leftrightarrow (\mathbf{X_1}-\mathbf{X_2}) \perp \mathbf{W}</span>. The same is explained on <a href="https://datascience.stackexchange.com/a/49295">stack overflow</a>.</p></li>
<li><p>Dotted line <span class="math inline">\mathbf{X'-X}</span> is perpendicular to decision boundary so parallel to <span class="math inline">\mathbf{W}</span> let it’s length (magnitude) be <span class="math inline">r</span></p></li>
<li><p>The Unit vector along Dotted line <span class="math inline">\mathbf{X'-X}</span> is given by <span class="math inline">\frac{\mathit{\mathbf{W}}}{\left\lVert \mathit{\mathbf{W}} \right\rVert }</span></p></li>
<li><p>The equation of the dotted line <span class="math inline">\mathbf{X'-X}</span> can be also given by magnitude multiplied by unit vector : <br> <span class="math inline">\displaystyle r\cdot \frac{ \mathit{\mathbf{W}}}{\left\lVert \mathit{\mathbf{W}} \right\rVert }</span></p></li>
<li><p>But as the dotted line can be on any side of the main line so we need to multiply with <span class="math inline">y</span>, as <span class="math inline">y</span> takes value of <span class="math inline">1</span> or <span class="math inline">-1</span> depending on the side: <br> <span class="math inline">\displaystyle \mathbf{X'-X} = yr\cdot \frac{ \mathit{\mathbf{W}}}{\left\lVert \mathit{\mathbf{W}} \right\rVert }</span><br> <span class="math inline">\displaystyle \mathbf{X'} = \mathbf{X} - yr\cdot \frac{ \mathit{\mathbf{W}}}{\left\lVert \mathit{\mathbf{W}} \right\rVert }</span></p></li>
<li><p>Now since <span class="math inline">\mathbf{X'}</span> lies on the line so we can write <span class="math inline">\mathbf{W}^T \cdot \mathbf{X'}+b=0</span></p></li>
<li><p>Substituting value of <span class="math inline">\mathbf{X'}</span> in <span class="math inline">\mathbf{W}^T \cdot \mathbf{X'}+b=0</span> we get: <br> <span class="math inline">\displaystyle \mathbf{W}^T \cdot \left( \mathbf{X} - yr\cdot \frac{ \mathit{\mathbf{W}}}{\left\lVert \mathit{\mathbf{W}} \right\rVert } \right)+b=0</span><br><br></p></li>
<li><p>substituting <span class="math inline">\displaystyle \left\lVert \mathit{\mathbf{W}} \right\rVert =\sqrt{\mathit{\mathbf{W}}^T \mathit{\mathbf{W}}}</span> in above equation we get: <br> <span class="math inline">\displaystyle \mathbf{W}^T \cdot \left( \mathbf{X} - yr\cdot \frac{ \mathit{\mathbf{W}}}{\sqrt{\mathit{\mathbf{W}}^T \mathit{\mathbf{W}}}} \right)+b=0</span><br><br> <span class="math inline">\displaystyle \left( \mathbf{W}^T \mathbf{X} - yr\cdot \frac{ \mathit{\mathbf{W}}^T\mathit{\mathbf{W}}}{\sqrt{\mathit{\mathbf{W}}^T \mathit{\mathbf{W}}}} \right)+b=0</span><br><br> <span class="math inline">\displaystyle \left( \mathbf{W}^T \mathbf{X} - yr\cdot \sqrt{\mathit{\mathbf{W}}^T \mathit{\mathbf{W}}} \right)+b=0</span><br><br> <span class="math inline">\displaystyle \mathbf{W}^T \mathbf{X} - yr\cdot \left\lVert \mathit{\mathbf{W}} \right\rVert +b=0</span><br><br> <span class="math inline">\displaystyle \mathbf{W}^T \mathbf{X} +b = yr\cdot \left\lVert \mathit{\mathbf{W}} \right\rVert</span><br><br> <span class="math inline">\displaystyle r = \frac{\mathbf{W}^T \mathbf{X} +b}{y\cdot \left\lVert \mathit{\mathbf{W}} \right\rVert}</span><br><br></p></li>
<li><p>Since <span class="math inline">y</span> takes value of only <span class="math inline">1</span> or <span class="math inline">-1</span>, hence we can bring <span class="math inline">y</span> to numerator. <br></p>
<p><span class="math inline">\displaystyle r = y \frac{\mathbf{W}^T \mathbf{X} +b}{\left\lVert \mathit{\mathbf{W}} \right\rVert}</span></p></li>
<li><p>Since <span class="math inline">\mathbf{W}^T \cdot \mathbf{X}+b=0</span> and <span class="math inline">c\left(\mathbf{W}^T \cdot \mathbf{X}\right)+b=0</span> define the same plane, we have the freedom to choose the normalization of <span class="math inline">\mathbf{W}</span></p></li>
<li><p>Let us choose normalization such that <span class="math inline">\mathbf{W}^T \cdot \mathbf{X}_+ + b = +1</span> and <span class="math inline">\mathbf{W}^T \cdot \mathbf{X}_- +b = -1</span> for the positive and negative support vectors respectively.<br><br> <img src="CS5590_images/Acrobat_AwymHLT9aY.png" class="img-fluid"><br><br> Hence, Margin now is: <br><br> <span class="math inline">\displaystyle \left( +1 \right) \frac{\mathbf{W}^T \mathbf{X_+} +b}{ \left\lVert \mathit{\mathbf{W}} \right\rVert} + \left( -1 \right) \frac{\mathbf{W}^T \mathbf{X_-} +b}{ \left\lVert \mathit{\mathbf{W}} \right\rVert}</span><br><br> Since <span class="math inline">\mathbf{W}^T \mathbf{X_+} +b=+1</span> and <span class="math inline">\mathbf{W}^T \mathbf{X_-} +b=-1</span>, substituting these in above equation we get: <br><br> <span class="math inline">\displaystyle \left( +1 \right) \frac{\mathbf{W}^T \mathbf{X_+} +b}{ \left\lVert \mathit{\mathbf{W}} \right\rVert} + \left( -1 \right) \frac{\mathbf{W}^T \mathbf{X_-} +b}{ \left\lVert \mathit{\mathbf{W}} \right\rVert}=\displaystyle \left( +1 \right) \frac{+1}{ \left\lVert \mathit{\mathbf{W}} \right\rVert} + \left( -1 \right) \frac{-1}{ \left\lVert \mathit{\mathbf{W}} \right\rVert}=\frac{2}{\left\lVert \mathbf{W}\right\rVert}</span><br></p></li>
</ul>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p><br>Margin between the two support vector is given by: <span class="math display">\displaystyle    \frac{2}{\left\lVert \mathbf{W}\right\rVert}</span></p>
</div>
</div>
<p><br><br></p>
</section>
<section id="maximize-the-margin" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="maximize-the-margin"><span class="header-section-number">4</span> Maximize the Margin</h2>
<ul>
<li>Now we know the margin between the two support vector.</li>
<li>We need to maximize the margin in such a way that <span class="math inline">+1</span> class points lies on one side of the margin and <span class="math inline">-1</span> class points lies on the other side of the margin.</li>
<li>We can formulate this as the <em>quadratic optimization problem</em>:<br> Find <span class="math inline">\mathbf{W}</span> such that <br> <br> <span class="math inline">\displaystyle \rho = \frac{2}{\left\lVert \mathbf{W}\right\rVert }</span> is maximized; and for all <span class="math inline">\left\{ \left( \mathbf{X}_i,\mathbf{y}_i \right) \right\}</span><br> <br> and <span class="math inline">\mathbf{W}^T \cdot \mathbf{X}_i+b \ge 1</span> if <span class="math inline">y_i=+1</span><br> <br> and <span class="math inline">\mathbf{W}^T \cdot \mathbf{X}_i+b \le -1</span> if <span class="math inline">y_i=-1</span></li>
</ul>
<p>A better formulation is to minimize inverse of <span class="math inline">\rho</span> instead of maximizing it.</p>
<ul>
<li><p>We know that <br> <span class="math inline">\displaystyle \max \frac{2}{\left\lVert \mathbf{W}\right\rVert } =\min \frac{\left\lVert \mathbf{W}\right\rVert}{2} =\min\frac{\sqrt{ \mathbf{W}^T\mathbf{W}}}{2}</span><br></p></li>
<li><p>Instead of minimizing <span class="math inline">\displaystyle \frac {\left\lVert \mathbf{W}\right\rVert}{2}</span> we minimize <span class="math inline">\displaystyle \frac {\left\lVert \mathbf{W}\right\rVert^2}{2} = \displaystyle \frac{ \mathbf{W}^T\mathbf{W}}{2}</span> as both (with or without square) are equivalent. we select square one as math (derivative) becomes easy.<br></p></li>
</ul>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Maximization problem can be written in terms of minimization as follows:<br><br>Find <span class="math inline">\mathbf{W}</span> and <span class="math inline">b</span> such that<br><br><span class="math inline">\displaystyle \frac{\mathbf{W}^T\mathbf{W}}{2}</span> is minimized.<br><br>and for all <span class="math inline">\left\{ \left( \mathbf{X}_i,\mathbf{y}_i \right) \right\} : \displaystyle y_i\left( \mathbf{W}^T\mathbf{X}_i +b\right) \ge 1</span><br> <br></p>
</div>
</div>
</section>
<section id="using-lagrange-multipliers" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="using-lagrange-multipliers"><span class="header-section-number">5</span> Using Lagrange Multipliers</h2>
<section id="basics-of-lagrange-multipliers" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="basics-of-lagrange-multipliers"><span class="header-section-number">5.1</span> Basics of Lagrange Multipliers</h3>
<ul>
<li>Optimization problem:<br> <strong>Minimize</strong> : <span class="math inline">\displaystyle f\left( \overrightarrow{x} \right)</span> <br> <strong>Such that</strong> for all <span class="math inline">i,</span> <span class="math inline">\displaystyle g_i\left( \overrightarrow{x} \right)\le 0</span> <br></li>
<li>To solve the above problem we create augmented Lagrange function:<br><br> <span class="math inline">\displaystyle L\left( \overrightarrow{x},\overrightarrow{\lambda} \right):=f\left( \overrightarrow{x} \right)+\sum_{i=1}^{n}\lambda_ig_i\left( \overrightarrow{x} \right)</span> <br><br> <span class="math inline">\displaystyle \underbrace{L\left( \overrightarrow{x},\overrightarrow{\lambda} \right)}_{\text{lagrange function}} :=f\left( \overrightarrow{x} \right)+\sum_{i=1}^{n}\underbrace{\lambda_i}_{\text{lagrange variable  or dual varialbe }}g_i \left( \overrightarrow{x} \right)</span> <br><br></li>
<li>Observation:<br> For any feasible <span class="math inline">x</span> and all <span class="math inline">\lambda_i \ge 0</span>, <br><br> <span class="math inline">\displaystyle L\left( \overrightarrow{x},\overrightarrow{\lambda} \right):=f\left( \overrightarrow{x} \right)+\overbrace{\sum_{i=1}^{n}\overbrace{\lambda_i}^{\text{this is positve}} \underbrace{g_i\left( \overrightarrow{x} \right)}_{\text{this is negative}}}^{\text{This is negative}}</span> <br><br> Hence , <span class="math inline">\displaystyle L\left( \overrightarrow{x},\overrightarrow{\lambda} \right) \le f\left( \overrightarrow{x} \right)</span> <br> <span class="math inline">\displaystyle \Longrightarrow \max_{\lambda_i \ge 0} L\left( \overrightarrow{x},\overrightarrow{\lambda} \right) \le f\left( \overrightarrow{x} \right)</span> <br><br></li>
<li>So, the optimal value to the constrained optimization:<br><br> <span class="math inline">\displaystyle p^*:=\min_{\overrightarrow{x} } \max_{\lambda_i \ge 0} L\left( \overrightarrow{x},\overrightarrow{\lambda} \right)</span> <br><br> We can see that now problem becomes unconstrained in <span class="math inline">x</span> <br> Also <span class="math inline">p^*</span> is called <strong>The primal</strong> problem<br><br></li>
<li>Observation:<br> consider a function: <span class="math inline">\displaystyle \min_{\overrightarrow{x} } L\left( \overrightarrow{x},\overrightarrow{\lambda} \right)</span><br> Since <span class="math inline">p^*</span> is solution for maximum possible <span class="math inline">\lambda</span> so for any feasible <span class="math inline">x</span> and all <span class="math inline">\lambda_i \ge 0</span><br><br> <span class="math inline">\displaystyle p^* \ge \min_{\overrightarrow{x} } L\left( \overrightarrow{x},\overrightarrow{\lambda} \right)</span> <br> Thus:<br><br> <span class="math inline">\displaystyle d^*:= \max_{\lambda_i \ge 0} \min_{\overrightarrow{x} } L\left( \overrightarrow{x},\overrightarrow{\lambda} \right) \le p^*</span> <br><br> Also <span class="math inline">d^*</span> is called <strong>The dual</strong> problem<br><br></li>
</ul>
<p>In short:</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Optimization problem</strong>:<br><em>Minimize</em> : <span class="math inline">\displaystyle f\left( \overrightarrow{x} \right)</span> <br><em>Such that</em> for all <span class="math inline">i,</span> <span class="math inline">\displaystyle g_i\left( \overrightarrow{x} \right)\le 0</span> <br><em>Lagrange Function</em> : <span class="math inline">\displaystyle L\left( \overrightarrow{x},\overrightarrow{\lambda} \right):=f\left( \overrightarrow{x} \right)+\sum_{i=1}^{n}\lambda_ig_i\left( \overrightarrow{x} \right)</span> <br><br> <strong>Primal</strong>: <span class="math inline">\displaystyle p^*:=\min_{\overrightarrow{x} } \max_{\lambda_i \ge 0} L\left( \overrightarrow{x},\overrightarrow{\lambda} \right)</span> <br><br><strong>Dual</strong>: <span class="math inline">\displaystyle d^*:= \max_{\lambda_i \ge 0} \min_{\overrightarrow{x} } L\left( \overrightarrow{x},\overrightarrow{\lambda} \right)</span> <br><br></p>
</div>
</div>
<ul>
<li><p>Theorem (weak Lagrangian duality):<br><br> <span class="math inline">d^* \le p^*</span><br><br> This is also called as minimax inequality<br><br> <span class="math inline">p^*-d^*</span> is called duality gap<br><br></p></li>
<li><p>There are certain condition when duality gap becomes zero, for that we need to understand convexity</p>
<ul>
<li>A function <span class="math inline">f:\mathbb{R}^d \rightarrow \mathbb{R}</span> is called convex iff for any two point <span class="math inline">x</span> and <span class="math inline">x'</span> and <span class="math inline">\beta \in \left[ 0,1 \right]</span><br><br> <span class="math inline">f\left( \beta\overrightarrow{x}+\left( 1-\beta \right)\overrightarrow{x} \right) \le \beta f\left( \overrightarrow{x} \right)+\left( 1-\beta \right)f\left( \overrightarrow{x} \right)</span><br><br> <img src="CS5590_images/Acrobat_9xAcM49wbq.png" class="img-fluid"><br><br></li>
<li>A set <span class="math inline">S \subset\mathbb{R}^d</span> is called conved iff for any tow points <span class="math inline">x, x' \in S</span> and any <span class="math inline">\beta \in \left[ 0,1 \right]</span><br><br> <span class="math inline">\beta \overrightarrow{x}+\left( 1-\beta \right)\overrightarrow{x} \in S</span><br><br> <img src="CS5590_images/Acrobat_4WENH5ng1n.png" class="img-fluid"><br><br></li>
<li>Convex Optimization problem <br><br> <span class="math inline">\displaystyle \min_{\overrightarrow{x} \in \mathbb{R}^d } f\left( \overrightarrow{x} \right)</span> <br><br> subject to: <span class="math inline">\displaystyle g_i\left( \overrightarrow{x} \right)\le 0</span> for <span class="math inline">1 \le i \le n</span><br><br> is called convex optimization problem if:<br>
<ul>
<li>The objective function <span class="math inline">f\left( \overrightarrow{x} \right)</span> is convex function, and</li>
<li>the feasible set induced by the constraints <span class="math inline">g_i</span> is a convex set.<br><br></li>
</ul></li>
</ul></li>
<li><p>Theorem (strong Lagrangian duality):<br><br> if <span class="math inline">f</span> is convex and for a feasible point <span class="math inline">x^*</span><br><br> <span class="math inline">g_i\left( \overrightarrow{x^*} \right)&lt;0</span>, or <br><br> <span class="math inline">g_i\left( \overrightarrow{x^*} \right) \le 0</span> when <span class="math inline">g</span> is affine<br><br> Then <span class="math inline">d^*=p^*</span><br><br> This is called Slater’s condition.</p></li>
</ul>
</section>
<section id="svm-standard-primal-form" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="svm-standard-primal-form"><span class="header-section-number">5.2</span> SVM standard (primal) form</h3>
<p><span class="math inline">\displaystyle \min_{w,b}\frac{1}{2}\left\lVert \overrightarrow{w} \right\rVert^2</span> <br><br> such that: <span class="math inline">\forall</span> <span class="math inline">i,</span> <span class="math inline">y_i\left( \overrightarrow{w}\cdot \overrightarrow{x_i} +b \right) \ge 1</span></p>
<ul>
<li><p>Observations :</p>
<ul>
<li>Objective function is <em>convex</em></li>
<li>the constraints are <em>affine</em>, inducing a polytope constraint set.</li>
</ul></li>
<li><p>So SVM is a convex optimization problem (in fact a quadratic program)</p></li>
<li><p>Moreover, <strong>strong duality holds</strong>.</p></li>
<li><p>Lagrangian for SVM</p>
<ul>
<li>For Lagrangian the constraint should always written as less than <span class="math inline">0</span> format: <br><br> <span class="math inline">y_i\left( \overrightarrow{w}\cdot \overrightarrow{x_i} +b \right) -1 \ge 0</span> <br><br> <span class="math inline">- y_i\left( \overrightarrow{w}\cdot \overrightarrow{x_i} +b \right) +1 \le 0</span> <br><br> <span class="math inline">1 - y_i\left( \overrightarrow{w}\cdot \overrightarrow{x_i} +b \right) \le 0</span> <br><br></li>
<li>Now the Lagrangian for SVM can be written as:<br><br> <span class="math inline">\displaystyle L\left( \overrightarrow{w},b,\overrightarrow{\alpha} \right)=\frac{1}{2}\left\lVert \overrightarrow{w} \right\rVert^2 + \underbrace{\sum_{i = 1}^{n}\alpha_i\left( 1-y_i\left( \overrightarrow{w}\cdot\overrightarrow{x_i}+b \right) \right)}_{\text{appears like a hinge loss}}</span><br><br></li>
</ul></li>
</ul>
</section>
<section id="svm-dual" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="svm-dual"><span class="header-section-number">5.3</span> SVM Dual</h3>
<ul>
<li>Primal<br><br> <span class="math display">\displaystyle \min_{\overrightarrow{w},b }\max_{\overrightarrow{\alpha } \ge 0 }\frac{1}{2}\left\lVert \overrightarrow{w} \right\rVert^2 + \sum_{i = 1}^{n}\alpha_i\left( 1-y_i\left( \overrightarrow{w}\cdot\overrightarrow{x_i}+b   \right) \right)</span> <br><br></li>
<li>Dual<br><br> <span class="math display">\displaystyle\max_{\overrightarrow{\alpha } \ge 0 } \min_{\overrightarrow{w},b }\frac{1}{2}\left\lVert \overrightarrow{w} \right\rVert^2 + \sum_{i = 1}^{n}\alpha_i\left( 1-y_i\left( \overrightarrow{w}\cdot\overrightarrow{x_i}+b   \right) \right)</span> <br><br></li>
<li>Slater’s condition from convex optimization guarantees that these two optimization problems are equivalent!</li>
</ul>
</section>
<section id="solving-using-kkt-condition" class="level3" data-number="5.4">
<h3 data-number="5.4" class="anchored" data-anchor-id="solving-using-kkt-condition"><span class="header-section-number">5.4</span> Solving using KKT condition</h3>
<p>KKT stands for Karush-Kuhn-Tucker Condition - We solve Dual problem:<br><br> <span class="math inline">\displaystyle\max_{\overrightarrow{\alpha } \ge 0 } \min_{\overrightarrow{w},b }\frac{1}{2}\left\lVert \overrightarrow{w} \right\rVert^2 + \sum_{i = 1}^{n}\alpha_i\left( 1-y_i\left( \overrightarrow{w}\cdot\overrightarrow{x_i}+b \right) \right)</span><br><br></p>
<ul>
<li><p>We can solve for optimal <span class="math inline">w</span>, <span class="math inline">b</span> as function of <span class="math inline">\alpha</span><br><br> <span class="math inline">\displaystyle \frac{\partial L }{\partial \overrightarrow{w}}= w - \sum_{i}\alpha_iy_i\overrightarrow{x}_i=0 \Rightarrow w = \sum_{i}\alpha_iy_i\overrightarrow{x}_i</span><br><br> <span class="math inline">\displaystyle \frac{\partial L }{\partial b}= \sum_{i}\alpha_iy_i=0 \Rightarrow \sum_{i}\alpha_iy_i =0</span><br><br></p></li>
<li><p>substituting these values back in Dual we get: <br><br> <span class="math inline">\displaystyle \max_{\overrightarrow{\alpha } \ge 0 } \frac{1}{2} \left( \sum_{i}\alpha_iy_i\overrightarrow{x}_i \right) \cdot \left( \sum_{j}\alpha_jy_j\overrightarrow{x}_j \right) + \sum_{i = 1}^{n}\alpha_i\left( 1-y_i\left( \sum_{j}\alpha_jy_j\overrightarrow{x}_j \cdot\overrightarrow{x_i}+b \right) \right)</span><br><br> <span class="math inline">\displaystyle \max_{\overrightarrow{\alpha } \ge 0 } \frac{1}{2} \sum_{i,j}\alpha_i\alpha_jy_iy_j\overrightarrow{x}_i\cdot\overrightarrow{x} _j + \sum_{i = 1}^{n}\alpha_i-\sum_{i = 1}^{n} \left( \alpha_iy_i\left( \sum_{j}\alpha_jy_j\overrightarrow{x} _j\cdot\overrightarrow{x_i}+b\right) \right)</span><br><br> <span class="math inline">\displaystyle \max_{\overrightarrow{\alpha } \ge 0 } \frac{1}{2} \sum_{i,j}\alpha_i\alpha_jy_iy_j\overrightarrow{x}_i\cdot\overrightarrow{x} _j + \sum_{i = 1}^{n}\alpha_i- \sum_{i = 1}^{n}\alpha_iy_i\sum_{j}\alpha_jy_j\overrightarrow{x} _j\cdot\overrightarrow{x_i}+ \sum_{i = 1}^{n}\alpha_iy_ib</span><br><br> <span class="math inline">\displaystyle \max_{\overrightarrow{\alpha } \ge 0 } -\frac{1}{2} \sum_{i,j}\alpha_i\alpha_jy_iy_j\overrightarrow{x}_i\cdot\overrightarrow{x} _j + \sum_{i = 1}^{n}\alpha_i + \sum_{i = 1}^{n}\alpha_iy_ib</span><br><br> <span class="math inline">\displaystyle \max_{\overrightarrow{\alpha } \ge 0 } \sum_{i = 1}^{n}\alpha_i -\frac{1}{2} \sum_{i,j}\alpha_i\alpha_jy_iy_j\overrightarrow{x}_i\cdot\overrightarrow{x} _j</span><br><br></p></li>
<li><p>The above equation can also be written as: <br><br> <span class="math inline">\displaystyle \max \sum_{k = 1}^{R}\alpha_k - \frac{1}{2}\sum_{k=1}^{R}\sum_{l = 1}^{R}\alpha_k \alpha_l Q_{kl},</span> where <span class="math inline">\displaystyle Q_{kl} =y_ky_l\left( \mathbf{X_k}\cdot \mathbf{X_l} \right)</span><br><br> subject to constrains:<br><br><span class="math inline">\alpha_k \ge 0,</span> and <span class="math inline">\forall k ,</span> <span class="math inline">\displaystyle \sum_{k=1}^{R}\alpha_ky_k=0</span><br><br></p></li>
<li><p>Above problem can be solved using SMO (Sequential minimal optimization) or any other quadratic programming or gradient descent.</p></li>
<li><p>Once we solve we get optimum <span class="math inline">\alpha^*</span></p></li>
<li><p>Using <span class="math inline">\alpha^*</span> we can get <span class="math inline">w^*</span> as below:<br><br> <span class="math inline">\displaystyle w^* = \sum_{i}\alpha_i^*y_i\overrightarrow{x}_i</span><br><br></p></li>
<li><p><span class="math inline">b^*</span> can be calculated as follows:<br><br> <span class="math inline">y_i\left( \overrightarrow{w}^* \cdot \overrightarrow{x_i} +b \right) = 1</span> <br><br> <span class="math inline">y_i\left( \overrightarrow{w}^* \cdot \overrightarrow{x_i} \right) + y_ib = 1</span> <br><br> Multiplying <span class="math inline">y_i</span> both the sides: <br><br> <span class="math inline">y_i y_i\left( \overrightarrow{w}^* \cdot \overrightarrow{x_i} \right) + y_i y_ib = y_i</span> <br><br> <span class="math inline">y_i y_ib = y_i -y_i y_i\left( \overrightarrow{w}^* \cdot \overrightarrow{x_i} \right)</span> <br><br> <span class="math inline">y_i y_i b</span> can be written as <span class="math inline">b</span> because <span class="math inline">y_i</span> can be only <span class="math inline">\pm 1</span> so in either case <span class="math inline">y_i y_i =1</span><br><br> <span class="math inline">b = y_i \left( 1- y_i\left( \overrightarrow{w}^* \cdot \overrightarrow{x_i} \right) \right)</span> <br><br> <span class="math inline">b^* = - y_i \left( y_i\left( \overrightarrow{w}^* \cdot \overrightarrow{x_i} \right)- 1 \right)</span> <br><br></p></li>
<li><p>Now we can classify with:<br><br> <span class="math inline">f(\mathbf{X},\mathbf{W}^*,b^*)=\mathrm{sign} (\mathbf{W}^* \cdot \mathbf{X}+b^*)</span><br><br></p></li>
</ul>
</section>
</section>
<section id="soft-margin-svm" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="soft-margin-svm"><span class="header-section-number">6</span> Soft Margin SVM</h2>
<ul>
<li><p>Till now what we discussed is called Hard margin SVM.</p></li>
<li><p>In practice data is not always separable</p></li>
<li><p>We allow misclassification of the data point in soft margin SVM.</p></li>
<li><p>Soft margin SVM is also called as C-SVM</p></li>
<li><p>Now our Lagrangian is formulated as below:<br><br> <span class="math inline">\displaystyle \min_{w,b,\varepsilon}\frac{1}{2}\left\lVert \overrightarrow{w} \right\rVert^2 + c \sum_{j=1}^{N}\varepsilon_j</span> <br><br> such that: <span class="math inline">\forall</span> <span class="math inline">i,</span> <span class="math inline">y_i\left( \overrightarrow{w}\cdot \overrightarrow{x_i} +b \right) \ge 1-\varepsilon _i</span>, and <span class="math inline">\varepsilon _i\ge0</span> <br><br></p></li>
<li><p>Significance of <span class="math inline">\varepsilon</span><br><br></p>
<ul>
<li><span class="math inline">\displaystyle \min_{w,b,\varepsilon}\frac{1}{2}\left\lVert \overrightarrow{w} \right\rVert^2 + \overbrace{c}^{\text{Controls amount of misclassification}} \underbrace{\sum_{j=1}^{N}\varepsilon_j }_{\text{Minimize }\varepsilon \text{ also  } }</span><br><br> such that: <span class="math inline">\forall</span> <span class="math inline">i,</span> <span class="math inline">y_i\left( \overrightarrow{w}\cdot \overrightarrow{x_i} +b \right) \ge 1-\underbrace{\varepsilon _i}_{\text{to allow misclassification}} ,</span> and <span class="math inline">\overbrace{\varepsilon _i\ge0}^{\text{keep } \varepsilon \text{ positive} }</span> <br><br></li>
<li><span class="math inline">\varepsilon_i \ge 1 \Longleftrightarrow y_i \left( \overrightarrow{w}\cdot \overrightarrow{x_i} +b \right) &lt; 0,</span> i.e., misclassification.</li>
<li><span class="math inline">0&lt;\varepsilon_i &lt; 1 \Longleftrightarrow x_i</span> is correctly classified, but lies inside the margin</li>
<li><span class="math inline">\varepsilon_i =0 \Longleftrightarrow x_i</span> is correctly classified, and lies outside of margin</li>
<li><span class="math inline">\sum_{j=1}^{N}\varepsilon_j</span> is an upper bound on training errors.</li>
</ul></li>
</ul>
<section id="lagrangian-for-soft-margin-svm" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="lagrangian-for-soft-margin-svm"><span class="header-section-number">6.1</span> Lagrangian for Soft Margin SVM</h3>
<ul>
<li><p>We need to Solve:<br><br> <span class="math inline">\displaystyle \min_{w,b,\varepsilon}\frac{1}{2}\left\lVert \overrightarrow{w} \right\rVert^2 + c \sum_{i=1}\varepsilon_i</span> <br><br> such that: <span class="math inline">\forall</span> <span class="math inline">i,</span> <span class="math inline">y_i\left( \overrightarrow{w}\cdot \overrightarrow{x_i} +b \right) \ge 1-\varepsilon _i</span>, and <span class="math inline">\varepsilon _i\ge0</span> <br><br></p></li>
<li><p>For Lagrangian the constraint should always written as less than <span class="math inline">0</span> condition: <br><br> <span class="math inline">y_i\left( \overrightarrow{w}\cdot \overrightarrow{x_i} +b \right) \ge 1-\varepsilon _i</span>, and <span class="math inline">\varepsilon _i\ge0</span> <br><br> <span class="math inline">y_i\left( \overrightarrow{w}\cdot \overrightarrow{x_i} +b \right) - 1 + \varepsilon _i \ge 0</span>, and <span class="math inline">-\varepsilon _i \le 0</span> <br><br> <span class="math inline">1 - \varepsilon _i -y_i\left( \overrightarrow{w}\cdot \overrightarrow{x_i} +b \right) \le 0</span>, and <span class="math inline">-\varepsilon _i \le 0</span> <br><br></p></li>
<li><p>Lagrangian formulation of above problem:<br><br> <span class="math inline">\displaystyle L\left( \overrightarrow{w},b,\overrightarrow{\alpha},\overrightarrow{\beta},\overrightarrow{\varepsilon } \right)= \\ \frac{1}{2}\left\lVert \overrightarrow{w} \right\rVert^2 +c\sum_{i}\varepsilon_i + \sum_{i = 1}^{n}\alpha_i\left( 1- \varepsilon _i - y_i\left( \overrightarrow{w}\cdot\overrightarrow{x_i}+b \right) \right) -\sum_{i}\beta_i \varepsilon_i</span><br><br></p></li>
<li><p>Primal<br><br> <span class="math display">\displaystyle  \min_{ \left(  \overrightarrow{w},b,\overrightarrow{\varepsilon } \right)}\max_{\left(  \overrightarrow{\alpha } \ge 0 ,\overrightarrow{\beta} \ge 0  \right)}\frac{1}{2}\left\lVert \overrightarrow{w} \right\rVert^2 +c\sum_{i}\varepsilon_i + \sum_{i = 1}^{n}\alpha_i\left( 1- \varepsilon _i - y_i\left( \overrightarrow{w}\cdot\overrightarrow{x_i}+b   \right) \right) -\sum_{i}\beta_i \varepsilon_i  </span> <br><br></p></li>
<li><p>Dual<br><br> <span class="math display">\displaystyle  \max_{\left( \overrightarrow{\alpha } \ge 0 , \overrightarrow{\beta} \ge 0  \right)} \min_{\left( \overrightarrow{w},b,\overrightarrow{\varepsilon }  \right)}\frac{1}{2}\left\lVert \overrightarrow{w} \right\rVert^2 +c\sum_{i}\varepsilon_i + \sum_{i = 1}^{n}\alpha_i\left( 1- \varepsilon _i - y_i\left( \overrightarrow{w}\cdot\overrightarrow{x_i}+b   \right) \right) -\sum_{i}\beta_i \varepsilon_i  </span> <br><br></p></li>
<li><p>Same as Hard margin SVM we use KKT condition and solve minimization of dual:<br><br> <span class="math inline">\displaystyle \frac{\partial L }{\partial \overrightarrow{w}}= w - \sum_{i}\alpha_iy_i\overrightarrow{x}_i=0 \Rightarrow w = \sum_{i}\alpha_iy_i\overrightarrow{x}_i</span><br><br> <span class="math inline">\displaystyle \frac{\partial L }{\partial b}= \sum_{i}\alpha_iy_i=0 \Rightarrow \sum_{i}\alpha_iy_i =0</span><br><br> <span class="math inline">\displaystyle \frac{\partial L }{\partial \varepsilon _i}= c -\alpha_i - \beta_i = 0\Rightarrow c=\beta_i+\alpha_i</span><br><br></p>
<ul>
<li>Observation:<br><br> <span class="math inline">\overbrace{c}^{\text{Upper bound of }\alpha \text{ and }\beta} =\underbrace{\beta_i}_{\text{always +ve}} +\underbrace{\alpha_i}_{\text{always +ve}}</span><br><br> so we can say: <span class="math inline">0 \le \alpha_i \le c</span> <span class="math inline">\forall i</span><br><br></li>
</ul></li>
<li><p>substituting these values back in Dual we get: <br><br> <span class="math inline">\displaystyle \Rightarrow \max_{\overrightarrow{\alpha } \ge 0 , \overrightarrow{\beta} \ge 0} \frac{1}{2}\left( \sum_{i}\alpha_iy_i\overrightarrow{x}_i \right) \cdot \left( \sum_{j}\alpha_jy_j\overrightarrow{x}_j \right) +\sum_{i}\varepsilon_i\left( \beta_i+\alpha_i \right) + \sum_{i = 1}^{n}\alpha_i\left( 1- \varepsilon _i - y_i\left( \sum_{j}\alpha_jy_j\overrightarrow{x}_j \cdot\overrightarrow{x_i}+b\right) \right) -\sum_{i}\beta_i \varepsilon_i</span><br><br> <span class="math inline">\displaystyle \Rightarrow \max_{\overrightarrow{\alpha } \ge 0 , \overrightarrow{\beta} \ge 0} \frac{1}{2}\left( \sum_{i}\alpha_iy_i\overrightarrow{x}_i \right) \cdot \left( \sum_{j}\alpha_jy_j\overrightarrow{x}_j \right) +\sum_{i}\varepsilon_i \beta_i+\sum_{i}\varepsilon_i \alpha_i + \sum_{i = 1}^{n}\alpha_i - \sum_{i = 1}^{n}\alpha_i \varepsilon _i - \sum_{i = 1}^{n}\alpha_i y_i\left( \sum_{j}\alpha_jy_j\overrightarrow{x}_j \cdot\overrightarrow{x_i}+b \right) -\sum_{i}\beta_i \varepsilon_i</span><br><br> <span class="math inline">\displaystyle \Rightarrow \max_{\overrightarrow{\alpha } \ge 0 , \overrightarrow{\beta} \ge 0} \frac{1}{2} \sum_{i}\alpha_i \alpha_j y_i y_j \overrightarrow{x}_i \overrightarrow{x}_j +\sum_{i}\varepsilon_i \beta_i+\sum_{i}\varepsilon_i \alpha_i + \sum_{i = 1}^{n}\alpha_i - \sum_{i = 1}^{n}\alpha_i \varepsilon _i - \sum_{i = 1}^{n}\alpha_i y_i \sum_{j}\alpha_jy_j\overrightarrow{x}_j \cdot\overrightarrow{x_i}+ \sum_{i = 1}^{n}\alpha_i y_ib -\sum_{i}\beta_i \varepsilon_i</span><br><br> <span class="math inline">\displaystyle \Rightarrow \max_{\overrightarrow{\alpha } \ge 0 } \frac{1}{2} \sum_{i}\alpha_i \alpha_j y_i y_j \overrightarrow{x}_i \overrightarrow{x}_j + \sum_{i = 1}^{n}\alpha_i - \sum_{i = 1}^{n}\alpha_i y_i \sum_{j}\alpha_jy_j\overrightarrow{x}_j \cdot\overrightarrow{x_i}+ \sum_{i = 1}^{n}\alpha_i y_ib</span><br><br> <span class="math inline">\displaystyle \Rightarrow \max_{\overrightarrow{\alpha } \ge 0 } \frac{1}{2} \sum_{i}\alpha_i \alpha_j y_i y_j \overrightarrow{x}_i \overrightarrow{x}_j + \sum_{i = 1}^{n}\alpha_i - \sum_{i=1,j=1}^{n}\alpha_i \alpha_j y_i y_j \overrightarrow{x_i} \cdot \overrightarrow{x_j}+ \sum_{i = 1}^{n}\alpha_i y_ib</span><br><br> <span class="math inline">\displaystyle \Rightarrow \max_{\overrightarrow{\alpha } \ge 0 } \sum_{i = 1}^{n}\alpha_i -\frac{1}{2} \sum_{i}\alpha_i \alpha_j y_i y_j \overrightarrow{x}_i \overrightarrow{x}_j</span><br><br></p></li>
<li><p><strong>Notice neither <span class="math inline">\overrightarrow{\beta}</span> nor <span class="math inline">\overrightarrow{\varepsilon }</span> appears in the above equation</strong>.<br><br></p></li>
<li><p>The above equation can also be written as: <br><br> <span class="math inline">\displaystyle \max \sum_{k = 1}^{R}\alpha_k - \frac{1}{2}\sum_{k=1}^{R}\sum_{l = 1}^{R}\alpha_k \alpha_l Q_{kl},</span> where <span class="math inline">\displaystyle Q_{kl} =y_ky_l\left( \mathbf{X_k}\cdot \mathbf{X_l} \right)</span><br><br> subject to constrains:<br><br><span class="math inline">0 \le \alpha_k \le c,</span> and <span class="math inline">\forall k ,</span> <span class="math inline">\displaystyle \sum_{k=1}^{R}\alpha_ky_k=0</span><br><br></p></li>
</ul>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>One of the constraint of soft margin SVM is <span class="math inline">0 \le \alpha_k \le c</span> which is different for hard margin SVM constraint <span class="math inline">\alpha_k \ge 0</span></p>
</div>
</div>
</section>
</section>
<section id="multi-class-classification-with-svms" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="multi-class-classification-with-svms"><span class="header-section-number">7</span> Multi-class Classification with SVMs</h2>
<ul>
<li>SVM can handle only tow-class outputs.</li>
<li>what to do for multi-class case:
<ul>
<li>one vs all SVM
<ul>
<li>Learn N SVMs</li>
<li>SVM 1 learns class <span class="math inline">1</span> vs not class <span class="math inline">1</span></li>
<li>SVM 2 learns class <span class="math inline">2</span> vs not class <span class="math inline">2</span> and so on.</li>
<li>Then to predict the output for a new point, just predict with each SVM and fond out which one puts the prediction the furthest into the positive region.</li>
</ul></li>
<li>Other approaches:
<ul>
<li>pair-wise SVM</li>
<li>Tree-structured SVM</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="kernel-trick" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="kernel-trick"><span class="header-section-number">8</span> Kernel Trick</h2>
<section id="why-do-we-require-the-kernel-trick" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="why-do-we-require-the-kernel-trick"><span class="header-section-number">8.1</span> Why do we require the Kernel Trick</h3>
<ul>
<li><p>We found that after solving minimization problem of dual of SVM we get following: <br> <span class="math inline">\displaystyle \max \sum_{k = 1}^{R}\alpha_k - \frac{1}{2}\sum_{k=1}^{R}\sum_{l = 1}^{R}\alpha_k \alpha_l Q_{kl},</span> where <span class="math inline">\displaystyle Q_{kl} =y_ky_l\left( \mathbf{X_k}\cdot \mathbf{X_l} \right)</span><br><br> subject to constrains:<br><br><span class="math inline">0 \le \alpha_k \le c,</span> and <span class="math inline">\forall k ,</span> <span class="math inline">\displaystyle \sum_{k=1}^{R}\alpha_ky_k=0</span><br><br></p></li>
<li><p>But if the data can’t be separated linearly we transform the data to higher dimension space using the transformation <span class="math inline">\phi</span>. So that the data can be separated using a hyper-plane in higher dimension space. In that case the above equation changes as below : <br> <span class="math inline">\displaystyle \max \sum_{k = 1}^{R}\alpha_k - \frac{1}{2}\sum_{k=1}^{R}\sum_{l = 1}^{R}\alpha_k \alpha_l Q_{kl},</span> where <span class="math inline">\displaystyle Q_{kl} =y_ky_l \underbrace{\left(\mathbf{\Phi} \left( \mathbf{X}_k \right)\cdot \mathbf{\Phi} \left( \mathbf{X}_l \right)\right)}_{\text{Notice the term } \Phi }</span><br><br> subject to constrains:<br><br><span class="math inline">0 \le \alpha_k \le c,</span> and <span class="math inline">\forall k ,</span> <span class="math inline">\displaystyle \sum_{k=1}^{R}\alpha_ky_k=0</span><br><br></p></li>
<li><p>Then compute : <br> <span class="math inline">\displaystyle \mathbf{W} = \sum_{\text{k s.t } \alpha_k &gt;0 }\alpha_k^*y_k \mathbf{\Phi} \left( \mathbf{X}_k \right)</span></p></li>
<li><p>Then classify with <br> <span class="math inline">\displaystyle f(\mathbf{X},w,b)=\mathrm{sign}\left( \mathbf{W} \cdot \mathbf{\Phi}(\mathbf{X})+b \right)</span></p></li>
<li><p>Most important change : <br> <span class="math inline">\mathbf{X} \rightarrow \mathbf{\Phi}(\mathbf{X})</span></p></li>
<li><p>Notice that in the term <span class="math inline">\displaystyle Q_{kl} =y_ky_l \left(\mathbf{\Phi} \left( \mathbf{X}_k \right)\cdot \mathbf{\Phi} \left( \mathbf{X}_l \right)\right)</span> we must do <span class="math inline">\frac{R^2}{2}</span> dot products to get this matrix ready.<br><br> Assuming a quadratic polynomial kernel, each dot product requires <span class="math inline">\frac{m^2}{2}</span> addition and multiplication ( where <span class="math inline">m</span> is the dimension of <span class="math inline">X</span>) <br><br> The whole thing costs <span class="math inline">\frac{R^2m^2}{4}</span><br><br> This is the reason we require a trick so that we need not do this large computation.</p></li>
</ul>
</section>
<section id="how-do-we-do-the-kernel-trick" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="how-do-we-do-the-kernel-trick"><span class="header-section-number">8.2</span> How do we do the kernel Trick</h3>
<p>To understand we create a data in circular fashion as shown below:</p>
<div class="cell" data-execution_count="224">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">import</span> random</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="im">import</span> math</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="kw">def</span> get_points(rl,rh):</span>
<span id="cb1-6"><a href="#cb1-6"></a>    npoints <span class="op">=</span> <span class="dv">1000</span> <span class="co"># points to chose from</span></span>
<span id="cb1-7"><a href="#cb1-7"></a>    r <span class="op">=</span>  np.random.uniform(low<span class="op">=</span>rl, high<span class="op">=</span>rh, size<span class="op">=</span>npoints)</span>
<span id="cb1-8"><a href="#cb1-8"></a>    t <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">2</span><span class="op">*</span>np.pi, npoints, endpoint<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-9"><a href="#cb1-9"></a>    x <span class="op">=</span> r <span class="op">*</span> np.cos(t)</span>
<span id="cb1-10"><a href="#cb1-10"></a>    y <span class="op">=</span> r <span class="op">*</span> np.sin(t)</span>
<span id="cb1-11"><a href="#cb1-11"></a>    <span class="cf">return</span> x,y</span>
<span id="cb1-12"><a href="#cb1-12"></a></span>
<span id="cb1-13"><a href="#cb1-13"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">4</span>,<span class="dv">4</span>))</span>
<span id="cb1-14"><a href="#cb1-14"></a>x11,x21<span class="op">=</span>get_points(<span class="dv">2</span>,<span class="dv">4</span>)</span>
<span id="cb1-15"><a href="#cb1-15"></a>plt.scatter(x11,x21)<span class="op">;</span></span>
<span id="cb1-16"><a href="#cb1-16"></a>x12,x22<span class="op">=</span>get_points(<span class="dv">6</span>,<span class="dv">8</span>)</span>
<span id="cb1-17"><a href="#cb1-17"></a>plt.scatter(x12,x22)<span class="op">;</span></span>
<span id="cb1-18"><a href="#cb1-18"></a>plt.xlabel(<span class="st">'x1'</span>)</span>
<span id="cb1-19"><a href="#cb1-19"></a>plt.ylabel(<span class="st">'x2'</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="2022-09-10-CS5590-week4_files/figure-html/cell-2-output-1.png" class="img-fluid"></p>
</div>
</div>
<ul>
<li>The two circle can’t be separated by a line.</li>
<li>Now we transform the data to 3 dimension using below function.<br><br> <span class="math inline">\phi\left( \mathbf{X}\right) = \phi\left( \left( \begin{array}{c}  x_1\\  x_2 \end{array} \right) \right) = \left( \begin{array}{c} x_1^2 \\ \sqrt{2}x_1x_2 \\ x_2^2 \end{array} \right)</span></li>
<li>Python implementation of the same is shown below:</li>
</ul>
<div class="cell" data-execution_count="225">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="kw">def</span> transform(x1,x2):</span>
<span id="cb2-2"><a href="#cb2-2"></a>    <span class="cf">return</span> np.square(x1),np.sqrt(<span class="dv">2</span>)<span class="op">*</span>x1<span class="op">*</span>x2,np.square(x2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>We can see in below pic that how data becomes sparable in 3 dimensional space.</li>
</ul>
<div class="cell" data-execution_count="226">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="im">from</span> mpl_toolkits <span class="im">import</span> mplot3d</span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-3"><a href="#cb3-3"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">8</span>))</span>
<span id="cb3-4"><a href="#cb3-4"></a>ax <span class="op">=</span> plt.axes(projection<span class="op">=</span><span class="st">'3d'</span>)</span>
<span id="cb3-5"><a href="#cb3-5"></a>x11,x21,x31 <span class="op">=</span> transform(x11,x21)</span>
<span id="cb3-6"><a href="#cb3-6"></a>x12,x22,x32 <span class="op">=</span> transform(x12,x22)</span>
<span id="cb3-7"><a href="#cb3-7"></a>ax.scatter3D(x11,x21,x31)<span class="op">;</span></span>
<span id="cb3-8"><a href="#cb3-8"></a>ax.scatter3D(x12,x22,x32)<span class="op">;</span></span>
<span id="cb3-9"><a href="#cb3-9"></a>ax.set_xlabel(<span class="st">'x1'</span>)</span>
<span id="cb3-10"><a href="#cb3-10"></a>ax.set_ylabel(<span class="st">'x2'</span>)</span>
<span id="cb3-11"><a href="#cb3-11"></a>ax.set_zlabel(<span class="st">'x3'</span>)<span class="op">;</span></span>
<span id="cb3-12"><a href="#cb3-12"></a>ax.view_init(<span class="dv">10</span>, <span class="dv">80</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="2022-09-10-CS5590-week4_files/figure-html/cell-4-output-1.png" class="img-fluid"></p>
</div>
</div>
<ul>
<li><p>In case of SVM we need the dot product of the transformed data, not the transformed data itself.</p></li>
<li><p>Consider two vectors <span class="math inline">\mathbf{a}</span> and <span class="math inline">\mathbf{b}</span>, we will apply following transformation on it :<br><br> <span class="math inline">\phi\left( \mathbf{X}\right) = \phi\left( \left( \begin{array}{c}  x_1\\  x_2 \end{array} \right) \right) = \left( \begin{array}{c} x_1^2 \\ \sqrt{2}x_1x_2 \\ x_2^2 \end{array} \right)</span> <br><br> We get below result: <br><br> <span class="math inline">\phi\left( \mathbf{a}\right)^T\phi\left( \mathbf{b}\right)= \left( \begin{array}{c} a_1^2 \\ \sqrt{2}a_1a_2 \\ a_2^2 \end{array} \right)^T \cdot \left( \begin{array}{c} b_1^2 \\ \sqrt{2}b_1b_2 \\ b_2^2 \end{array} \right)= a_1^2 b_1^2 + 2 a_1 b_1 a_2 b_2 +a_2^2 b_2^2= \left( a_1b_1+a_2b_2\right)^2= \left( \left( \begin{array}{c}  a_1\\  a_2 \end{array} \right)^T \cdot \left( \begin{array}{c}  b_1\\  b_2 \end{array} \right) \right)^2 = \left( \mathbf{a}^T \cdot \mathbf{b} \right)^2</span></p></li>
<li><p>The kernel function here is polynomial function.</p></li>
<li><p>we can see that we don’t even need to use <span class="math inline">\phi</span> we can get the result just from <span class="math inline">\left( \mathbf{a}^T \cdot \mathbf{b} \right)^2</span></p></li>
<li><p>So we never need to transformed the data to higher domain still we get the same benefit at the less computation cost.</p></li>
<li><p>The same is explained below:</p></li>
</ul>
<div class="cell" data-execution_count="251">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">16</span>,<span class="dv">16</span>))</span>
<span id="cb4-2"><a href="#cb4-2"></a>x11,x21<span class="op">=</span>get_points(<span class="dv">2</span>,<span class="dv">4</span>)</span>
<span id="cb4-3"><a href="#cb4-3"></a>x12,x22<span class="op">=</span>get_points(<span class="dv">6</span>,<span class="dv">8</span>)</span>
<span id="cb4-4"><a href="#cb4-4"></a>same_domain_result <span class="op">=</span> np.square(x11<span class="op">*</span>x12<span class="op">+</span>x21<span class="op">*</span>x22)</span>
<span id="cb4-5"><a href="#cb4-5"></a>ax <span class="op">=</span> fig.add_subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb4-6"><a href="#cb4-6"></a>ax.scatter(x11,x21)<span class="op">;</span></span>
<span id="cb4-7"><a href="#cb4-7"></a>ax.scatter(x12,x22)<span class="op">;</span></span>
<span id="cb4-8"><a href="#cb4-8"></a>ax.set_xlabel(<span class="st">'x1'</span>)</span>
<span id="cb4-9"><a href="#cb4-9"></a>ax.set_ylabel(<span class="st">'x2'</span>)<span class="op">;</span></span>
<span id="cb4-10"><a href="#cb4-10"></a>ax.set_title(<span class="st">' Original domain'</span>)</span>
<span id="cb4-11"><a href="#cb4-11"></a></span>
<span id="cb4-12"><a href="#cb4-12"></a>x11,x21,x31 <span class="op">=</span> transform(x11,x21)</span>
<span id="cb4-13"><a href="#cb4-13"></a>x12,x22,x32 <span class="op">=</span> transform(x12,x22)</span>
<span id="cb4-14"><a href="#cb4-14"></a>transformed_domain_result <span class="op">=</span> x11<span class="op">*</span>x12<span class="op">+</span>x21<span class="op">*</span>x22<span class="op">+</span>x31<span class="op">*</span>x32</span>
<span id="cb4-15"><a href="#cb4-15"></a>ax <span class="op">=</span> fig.add_subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, projection<span class="op">=</span><span class="st">'3d'</span>)</span>
<span id="cb4-16"><a href="#cb4-16"></a>ax.scatter3D(x11,x21,x31)<span class="op">;</span></span>
<span id="cb4-17"><a href="#cb4-17"></a>ax.scatter3D(x12,x22,x32)<span class="op">;</span></span>
<span id="cb4-18"><a href="#cb4-18"></a>ax.set_xlabel(<span class="st">'x1'</span>)</span>
<span id="cb4-19"><a href="#cb4-19"></a>ax.set_ylabel(<span class="st">'x2'</span>)</span>
<span id="cb4-20"><a href="#cb4-20"></a>ax.set_zlabel(<span class="st">'x3'</span>)<span class="op">;</span></span>
<span id="cb4-21"><a href="#cb4-21"></a>ax.view_init(<span class="dv">10</span>, <span class="dv">80</span>)<span class="op">;</span></span>
<span id="cb4-22"><a href="#cb4-22"></a>ax.set_title(<span class="st">'Transformed domain '</span>)</span>
<span id="cb4-23"><a href="#cb4-23"></a></span>
<span id="cb4-24"><a href="#cb4-24"></a>ax <span class="op">=</span> fig.add_subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb4-25"><a href="#cb4-25"></a>ax.scatter(<span class="bu">range</span>(<span class="bu">len</span>(same_domain_result)),same_domain_result)</span>
<span id="cb4-26"><a href="#cb4-26"></a>ax.set_title(<span class="st">'Result in original domain $( \mathbf</span><span class="sc">{a}</span><span class="st">^T \cdot \mathbf</span><span class="sc">{b}</span><span class="st"> )^2$'</span>)</span>
<span id="cb4-27"><a href="#cb4-27"></a></span>
<span id="cb4-28"><a href="#cb4-28"></a>ax <span class="op">=</span> fig.add_subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">4</span>)</span>
<span id="cb4-29"><a href="#cb4-29"></a>ax.scatter(<span class="bu">range</span>(<span class="bu">len</span>(transformed_domain_result)),transformed_domain_result)</span>
<span id="cb4-30"><a href="#cb4-30"></a>ax.set_title(<span class="st">'Result in transformed domain $\phi( \mathbf</span><span class="sc">{a}</span><span class="st">)^T\phi( \mathbf</span><span class="sc">{b}</span><span class="st">)$ '</span>)<span class="op">;</span></span>
<span id="cb4-31"><a href="#cb4-31"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="2022-09-10-CS5590-week4_files/figure-html/cell-5-output-1.png" class="img-fluid"></p>
</div>
</div>
<ul>
<li><p>From above figure we can see that we get exact same result in original domain without transforming data to the higher dimensional space</p></li>
<li><p>Not all functions are kernel functions.</p>
<ul>
<li>Need to be decomposable: <span class="math inline">K(a,b)=\phi(a)\cdot \phi(b)</span></li>
</ul></li>
<li><p>Mercer’s condition : To expand kernel function <span class="math inline">K(X,Y)</span> into a dot product, i.e.&nbsp;<span class="math inline">K(x,y)=\Phi(x).\Phi(y)</span>, <span class="math inline">K(x,y)</span> has to be positive <a href="https://mathworld.wolfram.com/PositiveSemidefiniteMatrix.html#:~:text=A%20positive%20semidefinite%20matrix%20is,Language%20using%20PositiveSemidefiniteMatrixQ%5Bm%5D.">semi-definite</a> function, i.e., for any function <span class="math inline">f(X)</span> whose <span class="math inline">\displaystyle \int f^2(x)dx</span> is finite, the following inequality holds:<br> <span class="math display">\displaystyle \int dx dy f(x) K(x,y) f(y) \ge 0</span></p></li>
<li><p>It is not easy to select the kernel function which will work best for the given data.</p></li>
<li><p>RBF kernels are considered good in general, especially for images (and other smooth functions/data).</p></li>
<li><p>For discrete data, chi-square kernel is preferred.</p></li>
<li><p>we can also do Multiple Kernel learning.</p></li>
<li><p>If still it doesn’t work we can use cross-validation to select a kernel function from some basic options.</p></li>
<li><p>Same kernel trick can also be applied to other methods including:</p>
<ul>
<li>Kernel k-NN</li>
<li>Kernel Perceptron</li>
<li>Kernelized Linear Regression</li>
<li>etc.</li>
</ul></li>
</ul>
<p><br><br><br> <span class="math inline">\tiny {\textcolor{#808080}{\boxed{\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}</span></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="abhiyantaabhishek/IITH-Data-Science" data-repo-id="R_kgDOILoB8A" data-category="Announcements" data-category-id="DIC_kwDOILoB8M4CSJcL" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../Data_Science_Tutorial/CS5590/2022-08-27-CS5590-week3.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Machine Learning 3</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../Data_Science_Tutorial/CS5590/2022-09-24-CS5590-week5.html" class="pagination-link">
        <span class="nav-page-text">Machine Learning 5</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Copyright 2022, Abhishek Kumar Dubey</div>   
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/abhiyantaabhishek">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/abhishek-kumar-dubey-585a86179/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>



</body></html>