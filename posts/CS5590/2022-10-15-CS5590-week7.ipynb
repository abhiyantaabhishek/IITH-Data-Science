{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "author: Abhishek Kumar Dubey\n",
    "badges: true\n",
    "categories:\n",
    "- Machine Learning\n",
    "date: '2022-10-15'\n",
    "description: Classifier System Design\n",
    "image: CS5590_images/Acrobat_DOQ241iPvG.png\n",
    "title: Machine Learning 7\n",
    "toc: true\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Numeric\n",
    "    - Zero mean, unit variance: $x’= (x-\\mu)/ \\sigma$\n",
    "    - In interval $[0,1]: x'=(x -\\min)/(\\max - \\min)$\n",
    "- Categorical\n",
    "    - Encoded as number in such a way that there is no sense of ordering, for e.g. if there are 3 classes apple, orange and banana, and encoded as $1,2,3$ respectively, it appears as apple comes first than orange, which is not correct. So the correct way to encode is one hot encoding.\n",
    "    - Also here only equality testing is meaningful. \n",
    "- Ordinal\n",
    "    - Encoded as numbers to preserve ordering\n",
    "    - $\\le, \\ge$ operations meaningful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction from Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Images\n",
    "\t- Pixel values, Segment and extract features, Handcrafted features: HOG, SIFT,etc\n",
    "\t- Deep learned features!\n",
    "- Text\n",
    "\t- Bag of words, Ngrams\n",
    "\t- Deep learned features!\n",
    "- Speech\n",
    "\t- Mel Frequency Cepstral Coefficients (MFCCs), Other frequency based features\n",
    "\t- Deep learned features!\n",
    "- Time varying sensor Data\n",
    "\t- Statistical and moment based features (mean, variance, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Structured input/Structured output\n",
    "\t- One fix: Attribute = root-to-leaf paths\n",
    "- Missing data\n",
    "\t- Fix: Fill in the value, Introduce special label, remove instance, remove attribute, Use classifiers that can handle missing values\n",
    "- Outliers\n",
    "\t- Fix: Remove, Threshold, Visualize!\n",
    "- Data assumptions\n",
    "\t- Generated how? Sources?\n",
    "\t- Smooth? Linear? Noise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all classifiers attempt to reduce global quantities such as the error rate, not taking the data distribution into consideration.\n",
    "\n",
    "As a result, examples from the overwhelming class are well classified whereas examples from the minority class tend to be misclassified.\n",
    "\n",
    "- Are all classifiers sensitive to class imbalance?\n",
    "    - Decision Tree:Very sensitive to class imbalances. This is because the algorithm works globally, not paying attention to specific data points.\n",
    "    - Multi Layer perceptrons (MLPs): are less prone to the class imbalance problem. This is because of their flexibility: their solution gets adjusted by each data point in a bottom up manner as well as by the overall data set in a top down manner.\n",
    "    - Support Vector Machines (SVMs) SVMs are even less prone to the class imbalance problem than MLPs because they are only concerned with a few support vectors, the data points located close to the boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Collect more data!\n",
    "- Change your performance metric:\n",
    "\t- Confusion Matrix, Precision Recall, F1 score, etc.\n",
    "- Resample dataset\n",
    "- Generate synthetic samples\n",
    "- Try penalized models\n",
    "- Try a different perspective anomaly/change detection\n",
    "<br><br>\n",
    "- At the data Level: Re Sampling\n",
    "\t- Oversampling (Random or Directed)\n",
    "\t- Under-sampling (Random or Directed), (not good for model performance)\n",
    "\t- Active Sampling\n",
    "- At the Algorithmic Level:\n",
    "\t- Adjusting the Costs\n",
    "\t- Adjusting the decision threshold / probabilistic estimate at the tree leaf\n",
    "<br><br>\n",
    "- Under-sampling (random and directed) is not effective and can even hurt performance.\n",
    "- Random oversampling helps quite dramatically. Directed oversampling makes a bit of a difference by helping slightly more.\n",
    "- Cost adjusting is about as effective as Directed oversampling. Generally, however, it is found to be slightly more useful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE = Synthetic Minority Oversampling Technique\n",
    "\n",
    "- For each minority example $k$, compute nearest minority class examples $(i,j,l,n,m)$\n",
    "- Synthetically generate event $k_1$ such that $k_1$ lies between $k$ and $i$\n",
    "- Randomly chose an example out of $5$ closest points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Large Datasets\n",
    "\n",
    "- At large data scales, the performance of different algorithms converge such that performance differences virtually disappear.\n",
    "- Given a large enough data set, the algorithm you'd want to use is the one that is computationally less expensive.\n",
    "- It's only at smaller data scales that the performance differences between algorithms matter.\n",
    "- CPUs vs GPUs\n",
    "\t- Deep learning has greatly benefited from GPUs\n",
    "- Map Reduce/ Hadoop , Apache Spark, Vowpal Wabbit frameworks\n",
    "\t- Many learning algorithms amenable to partitioning of computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalization Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Components of generalization error\n",
    "- Bias: how much the average model over all training sets differ from the true model?\n",
    "\t- Error due to inaccurate assumptions/simplifications made by the model\n",
    "- Variance: how much models estimated from different training sets differ from each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MSE in terms of bias and variance<br><br>\n",
    "  $$\\color{blue} \\text{MSE}=\\color{red} \\underbrace{\\text{Bias}^2}_{\\text{error due to incorrect assumption}} + \\color{green} \\underbrace{\\text{Variance}}_{\\text{error due to variance in training}} + \\color{purple} \\underbrace{\\text{Noise}}_{\\text{Unavoidable error}} \\tag{1}$$\n",
    "  <br><br>\n",
    "  Consider True function $q$ and Estimator $f_i = f(X_i)$ on sample $X_i$<br>\n",
    "  Bias = ${\\left(\\mathit{\\mathbf{E}}\\left\\lbrack f\\right\\rbrack -q\\right)}^2$<br>\n",
    "  Variance = $\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(f-\\mathit{\\mathbf{E}}\\left\\lbrack f\\right\\rbrack \\right)}^2 \\right\\rbrack$<br>\n",
    "  $\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(f-q\\right)}^2 \\right\\rbrack ={\\left(\\mathit{\\mathbf{E}}\\left\\lbrack f\\right\\rbrack -q\\right)}^2 +\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(f-\\mathit{\\mathbf{E}}\\left\\lbrack f\\right\\rbrack \\right)}^2 \\right\\rbrack$<br>\n",
    "  Proof:<br>\n",
    "  __LHS__: <br>\n",
    "  $$\\begin{align*}{}\n",
    "  \\text{LHS} &=\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(f-q\\right)}^2 \\right\\rbrack \\\\\n",
    "  &=\\mathit{\\mathbf{E}}\\left\\lbrack f^2 +q^2 -2f\\cdot q\\right\\rbrack \\\\\n",
    "  &=\\mathit{\\mathbf{E}}\\left\\lbrack f^2 \\right\\rbrack +q^2 -2\\cdot \\mathit{\\mathbf{E}}\\left\\lbrack f\\right\\rbrack \\cdot q\n",
    "  \\end{align*}$$\n",
    "  __RHS__:\n",
    "  $$\\begin{align*}{}\n",
    "  \\text{RHS} &={\\left(\\mathit{\\mathbf{E}}\\left\\lbrack f\\right\\rbrack -q\\right)}^2 +\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(f-\\mathit{\\mathbf{E}}\\left\\lbrack f\\right\\rbrack \\right)}^2 \\right\\rbrack \n",
    "  \\\\\n",
    "  &={\\left(\\mathit{\\mathbf{E}}\\left\\lbrack f\\right\\rbrack \\right)}^2 +q^2 -2\\cdot \\mathit{\\mathbf{E}}\\left\\lbrack f\\right\\rbrack \\cdot q+\\mathit{\\mathbf{E}}\\left\\lbrack f^2 +{\\left(\\mathit{\\mathbf{E}}\\left\\lbrack f\\right\\rbrack \\right)}^2 -2\\cdot \\mathit{\\mathbf{E}}\\left\\lbrack f\\right\\rbrack \\cdot f\\right\\rbrack \\\\\n",
    "  &=\\cancel{{\\left(\\mathit{\\mathbf{E}}\\left\\lbrack f\\right\\rbrack \\right)}^2} +q^2 -2\\cdot \\mathit{\\mathbf{E}}\\left\\lbrack f\\right\\rbrack \\cdot q+\\mathit{\\mathbf{E}}\\left\\lbrack f^2 \\right\\rbrack +\\cancel{{\\left(\\mathit{\\mathbf{E}}\\left\\lbrack f\\right\\rbrack \\right)}^2} -\\cancel{2\\cdot \\mathit{\\mathbf{E}}\\left\\lbrack f\\right\\rbrack \\cdot \\mathit{\\mathbf{E}}\\left\\lbrack f\\right\\rbrack} \\\\\n",
    "  &=\\mathit{\\mathbf{E}}\\left\\lbrack f^2 \\right\\rbrack +q^2 -2\\cdot \\mathit{\\mathbf{E}}\\left\\lbrack f\\right\\rbrack \\cdot q\n",
    "  \\end{align*}$$\n",
    "  LHS=RHS hence proved\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From equation $(1)$ we can see that when MSE is constant and if we try to reduce the variance Bias has to increase and vice versa.\n",
    "\n",
    "- Models with too few parameters are inaccurate because of a __large bias__ bias (not enough flexibility).\n",
    "- Models with too many parameters are inaccurate because of a __large variance__ (too much sensitivity to the sample).\n",
    "- __Underfitting__: Model is too “simple” to represent all the relevant class characteristics\n",
    "\t- High bias and low variance\n",
    "\t- High training error and high test error\n",
    "- __Overfitting__: Model is too “complex” and fits irrelevant characteristics (noise) in the data\n",
    "\t- Low bias and high variance\n",
    "\t- Low training error and high test error\n",
    "\n",
    ":::{.callout-tip}\n",
    "In case of classification, variance dominates bias. Very roughly, this is because we only need to make a discrete decision\n",
    "rather than get an exact value.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Bias and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create multiple training set using bootstrap replicates.\n",
    "- Apply learning algorithm on each replicates to obtain hypothesis.\n",
    "- compute predicted value for each hypothesis on the data which did not appear on the bootstrap replicate the hypothesis was trained on.\n",
    "- compute the average prediction \n",
    "- Estimate bias \n",
    "- Estimate variance.\n",
    "- Assume noise is 0\n",
    "    - If we have multiple data points with the same $x$ value, then we can estimate the noise not generally available in machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Inferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How to reduce variance of classifier\n",
    "\t- Choose a simpler classifier\n",
    "\t- Regularize the parameters\n",
    "\t- Get more training data\n",
    "- Training Error and Cross Validation\n",
    "\t- Suppose we use the training error to estimate the difference between the true model prediction and the learned model prediction.\n",
    "\t- The training error is _downward biased_: on average it _underestimates_ the generalization error.\n",
    "\t- Cross validation is nearly unbiased; it _slightly_ _overestimates_ the generalization error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KNN\n",
    "\t- Choose higher $k$\n",
    "- Decision Trees\n",
    "\t- Pruning\n",
    "- Naïve Bayes\n",
    "\t- Parametric models automatically act as regularizers\n",
    "- SVMs\n",
    "\t- Control $c$ value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "If the wight value is high the model is likely to over fit, so we want weight to be small. Consider a dataset in $2$-D, Hiving large variance across $y$-axis, now consider a model which overfits to these data, as do so $W$ has to be larger because for very small change in $x,\\;y$ will have to change by very high amount (due to such data distribution) which can be achieved only if $W$ is large. It is shown in the below pic.\n",
    ":::\n",
    "<br>\n",
    "![](CS5590_images/mspaint_AEgund0GVg.png)<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-based Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pick a model\n",
    "- pick a criteria to optimize (aka objective function)\n",
    "- develop a learning algorithm (aka Find $W$ and $b$ that minimizes the loss)\n",
    "- Generally, we don’t want huge weights\n",
    "\t- If weights are large, a small change in a feature can result in a large change in the prediction\n",
    "\t- Also, can give too much weight to any one feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization in Model based ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A regularizer is an additional criteria to the loss function to make sure that we don’t overfit\n",
    "- It’s called a regularizer since it tries to keep the parameters more normal/regular\n",
    "- It is a bias (inductive bias) on the model forces the learning to prefer certain types (smaller) of weights over others (larger).\n",
    "  $$\\argmin_{w,b} \\sum_{i=1}^n \\mathrm{loss}\\left(y,y^{\\prime } \\right)+\\lambda \\cdot \\boxed{\\mathrm{regulizer}\\left(w,b\\right)}$$\n",
    "- Type of norm regularizer\n",
    "  - 1-norm ( sum of weights )\n",
    "    $$r(w,b)=\\sum_{w_j} \\left\\lvert w_j \\right\\rvert $$\n",
    "  - 2-norm ( sum of squared weights )\n",
    "    $$r(w,b)=\\sum_{w_j} \\sqrt{\\left\\lvert w_j \\right\\rvert^2} $$    \n",
    "  - $p$-norm ( sum of squared weights )\n",
    "    $$r(w,b)=\\sum_{w_j} \\sqrt[p]{\\left\\lvert w_j \\right\\rvert^p}=\\left\\lVert w\\right\\rVert ^p$$   \n",
    "\n",
    ":::{.callout-tip}\n",
    "- Smaller values of $p, (p < 2)$ encourage sparser vectors<br>\n",
    "- Larger values of $p$ discourage large weights more.\n",
    "- All $p$ norms penalize larger weights.\n",
    "- $p < 2$ tends to create sparse i.e. lots of $0$ weights)\n",
    "\n",
    "Notice below pic\n",
    ":::    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](CS5590_images/Acrobat_DOQ241iPvG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1 is popular because it tends to result in sparse solutions (i.e. lots of zero weights)\n",
    "- However, it is not differentiable, so it only works for gradient descent solvers\n",
    "- L2 is also popular because for some loss functions, it can be solved directly (no gradient descent required, though often iterative solvers still)\n",
    "- Lp is less popular since they don’t tend to shrink the weights enough"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
