{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "author: Abhishek Kumar Dubey\n",
    "badges: true\n",
    "categories:\n",
    "- Machine Learning\n",
    "date: '2022-10-08'\n",
    "description: Ensemble Classifier\n",
    "image: CS5590_images/Acrobat_EZY9SgFMeU.png\n",
    "title: Machine Learning 6\n",
    "toc: true\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ensemble classification combines multiple classifiers to\n",
    "improve accuracy\n",
    "- Advantage\n",
    "  - Large datasets: if dataset is too large we can train different models on subset of the data.\n",
    "  - Small datasets: Can handle them with bootstrapping (random sampling)\n",
    "  - Can solve complicated problems which can't be solved using single classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Ensemble Classifiers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bagging (bootstrap aggregating)\n",
    "  - Train several models using bootstrapped datasets\n",
    "  - The majority classification is selected\n",
    "- Boosting\n",
    "  - Use several weak classifiers to create a strong classifier\n",
    "  - Resample previously misclassified points\n",
    "- Stacking (stacked generalization)\n",
    "  - Train multiple tiers of classifiers\n",
    "  - Higher tiers can correct lower tiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Problem: we have only one dataset.\n",
    "- Solution: generate new ones of size n by __bootstrapping__, i.e.     sampling it with replacement\n",
    "- Bagging works because it __reduces variance__ by voting/averaging\n",
    "- Usually, the more classifiers the better\n",
    "- Some candidates:\n",
    "  - Decision tree, decision stump, SVMs.\n",
    "  - Can do this with regression too: Regression tree, linear regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random forests (RF) are a combination of tree predictors, it's a variant of bagging.\n",
    "- Extremely successful, especially on Kaggle challenges.\n",
    "- Each tree depends on the values of a random vector sampled independently\n",
    "- The generalization error depends on the strength of the individual trees and the correlation between them\n",
    "- Using a random selection of features yields results robust w.r.t. noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests: Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given a training set $S$\n",
    "- For $i = 1$ to $k$ do:\n",
    "  - Build subset $S_i$ by sampling with replacement from $S$\n",
    "  - Learn tree $T_i$ from $S_i$\n",
    "    - At each node:\n",
    "      - Choose best split from random subset of $F$ features\n",
    "    - Each tree grows to the largest extent, and no pruning\n",
    "- Make predictions according to majority vote of the set of $k$ trees.\n",
    "- If there are $M$ input variables, a number $m$ is specified such that at each node, $m$ variables are selected at random out of the $M$ and the best split on these $m$ is used to split the node. The value of $m$ is held constant during the forest growing.\n",
    "- Depending upon the value of $m$, there are three slightly different systems:\n",
    "  - Random splitter selection: $m =1$\n",
    "  - Breiman’s bagger: $m =$ total number of predictor variables\n",
    "  - Random forest: $m <<$ number of predictor variables. Breiman suggests three possible\n",
    "values for $m:\\frac{1}{2}\\sqrt{M}, \\sqrt{M}, \\text{ and } 2\\sqrt{M}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features of Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One of the best in the business\n",
    "- It runs efficiently on large data bases\n",
    "- It can handle thousands of input variables without variable deletion/reduction\n",
    "- It gives estimates of what variables are important in the classification\n",
    "- _Does not overfit by design_\n",
    "- The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. ( if correlation is high it means trees are similar and not so useful)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging: when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can help if data is noisy.\n",
    "- If learning algorithm is unstable, i.e. if small changes to the\n",
    "training set cause large changes in the learned classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging: Why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let $S= \\{(x , y ), i=1\\dots N\\}$ be the training dataset\n",
    "- Let $\\{S_k \\}$ be a sequence of training sets containing a sub-set of $S$\n",
    "- Let $P$ be the underlying distribution of $S$.\n",
    "- Bagging replaces the prediction of the model with the majority of the predictions given by the classifiers $S$.\n",
    "$$\\phi(x,P)=E_s(\\phi (x,S_k))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use several weak classifiers to create a strong classifier\n",
    "- Resample previously misclassified points. (This is not done by bagging)\n",
    "- Strong Learner: This is the objective of machine learning where we take labelled data for training and produce classifier which can be __arbitrarily accurate__.\n",
    "- Weak Learner: Take labelled data for training and produce a classifier which is __more accurate than random guessing__.\n",
    "\n",
    "Can a set of weak learners create a single strong learner?\n",
    "\n",
    "## Key Idea\n",
    "- An algorithm for constructing a “strong” classifier as linear combination of “simple” “weak” classifier\n",
    "  $$f(x)=\\sum_{t=1}^T \\alpha_th_t(x)$$\n",
    "- Final classification based on weighted vote of weak classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $(x_1,y_1),\\dots (x_m,y_m)$ where $x_i \\in X,y_i \\in Y =\\{-1,+1\\}$<br><br>\n",
    "Initialize $D(i)=\\frac{1}{m}$<br><br>\n",
    "For $t=1,\\dots T$:\n",
    "\n",
    "- Find the classifier $h_t:X \\rightarrow \\{-1,+1\\}$ that minimizes the error with respect to the distribution $D_t$:<br>\n",
    "  $\\displaystyle h_t = \\argmin _{h_j \\in H} \\epsilon_j$, where $\\displaystyle\\epsilon_j=\\sum_{i=1}^m D_t(i)[y_i \\ne h_j(x_i)]$\n",
    "- Prerequisite: $\\epsilon_t<0.5$, otherwise stop.\n",
    "- Choose $\\alpha_t \\in \\mathbb{R}$, typically $\\displaystyle \\alpha_t=\\frac{1}{2}\\ln \\frac{1-\\epsilon_t}{\\epsilon_t}$, where $\\epsilon_t$ is the weighted error rate of the classifier $h_t$<br>\n",
    "$\\boxed{\\alpha_t \\text{ stays same for all data points}}$\n",
    "- Update <br>\n",
    "  $$D_{t+1} \\left(i\\right)=\\frac{D_t \\left(i\\right)\\exp \\left(-\\alpha_t y_t h_t \\left(x_i \\right)\\right)}{Z_t }$$\n",
    "  where $Z_t$ is a normalization factor (chosen such that $D_{t+1}$ will be a distribution)\n",
    "\n",
    "\n",
    "output of the final classifier :<br>\n",
    "$$H(x)=\\text{Sign}\\sum_{t=1}^T\\alpha_t h_t(x)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reweighting<br><br>\n",
    "$$\\begin{align*}{}\n",
    "D_{t+1} \\left(i\\right)&=\\frac{D_t \\left(i\\right)\\exp \\left(-\\alpha_t y_t h_t \\left(x_i \\right)\\right)}{Z_t }\\\\\n",
    "&=\\frac{D_t \\left(i\\right)\\exp \\left(-y_i \\sum_{q=1}^t \\alpha_q h_q \\left(x_i \\right)\\right)}{m\\prod_{q=1}^t Z_q }\n",
    "\\end{align*}$$\n",
    "\n",
    "Notice\n",
    "\n",
    "$$\\exp \\left(-\\alpha_t y_t h_t \\left(x_i \\right)\\right)  \\begin{cases}\n",
    "    \\le1 &\\text{if } y_t=h_t(x_i) \\xleftarrow{y\\times h(x)=1} \\\\\n",
    "    >1 &\\text{if } y_t\\ne h_t(x_i) \\xleftarrow{y\\times h(x)=-1} \n",
    " \\end{cases}$$\n",
    "\n",
    " we can see that weight of wrongly classified example is increased and weight of correctly classified example is decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "## Adaboost vs Random Forests\n",
    "\n",
    "Dietterich (1998) showed that when a fraction of the output labels in the training set are randomly altered, the accuracy of Adaboost degenerates, while bagging is more immune to the noise.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A good weak learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The set of weak rules (features) should be __flexible enough to be (weakly) correlated__ with most conceivable relations between feature vector and label.\n",
    "- __Small enough to allow exhaustive search__ for the minimal weighted training error.\n",
    "- __Small enough to avoid over-fitting.__\n",
    "- Should be able to __calculate predicted label very efficiently__\n",
    "- Rules can be __“specialists”__ – predict only on a small subset of the input space and __abstain from predicting__ on the rest (output 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting = Gradient Descent + Boosting\n",
    "\n",
    "- Fit an additive model (ensemble) in a forward stage-wise manner.\n",
    "- In each stage, introduce a weak learner to compensate the shortcomings of existing weak learners.\n",
    "- In Gradient Boosting, “shortcomings” are identified by gradients.\n",
    "- Recall that, in Adaboost, “shortcomings” are identified by high-weight data points.\n",
    "- Both high-weight data points and gradients tell us how to improve our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: Training set ${\\left\\lbrace \\left(x_i ,y_i \\right)\\right\\rbrace }_{i=1}^n$, a differentiable loss function $L(y,F(x))$,Number of iteration $M$\n",
    "<br><br>\n",
    "Algorithm: <br>\n",
    "\n",
    "- Initialize model with constant value:<br>\n",
    "\n",
    "   $$F_0(x)=\\argmin_\\gamma \\sum_{i=1}^nL(y_i,\\gamma)$$\n",
    "\n",
    "- For $m=1$ to $M$\n",
    "    - Compute pseudo-residuals:\n",
    "      $$r_{im}=-\\left[\\frac{\\partial L(y_i,F(x_i))}{\\partial F(x_i)} \\right]_{F(x)=F_{m-1}(x)}\\;\\text{for }i=1,\\dots,n$$\n",
    "    - Fit a base lerner (e.g. tree) $h_m(x)$ to pseudo-residual, i.e. train it using the set ${\\left\\lbrace \\left(x_i ,\\boxed{r_{im}} \\right)\\right\\rbrace }_{i=1}^n\\;\\;$ __Notice the training set now contains residual instead of original data__.\n",
    "    - Compute multiplier $\\gamma_m$ by solving the following one-dimensional optimization problem:\n",
    "      $$\\gamma_m=\\argmin_\\gamma \\sum_{i=1}^nL(y_i,\\underbrace{F_{m-1}(x_i)}_{\\text{previous model}}+\\gamma \\times \\underbrace{h_m(x_i)}_{\\text{current model}})$$\n",
    "    - Update the model:\n",
    "      $$F_m(x)=F_{m-1}(x)+\\gamma h_m(x)$$\n",
    "- Output $F_M(x)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking (stacked generalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train multiple tiers of classifiers\n",
    "- Higher tiers can correct lower tiers\n",
    "- Combiner $f ()$ is another learner (Wolpert, 1992)\n",
    "- Idea:\n",
    "    - Generate component (level 0) classifiers with part of the data (half, three quarters)\n",
    "    - Train combiner (level 1) classifier to combine predictions of components using remaining data\n",
    "    - Retrain component classifiers with all of training data\n",
    "- In practice, often equivalent to voting <br><br>\n",
    "![](CS5590_images/Acrobat_2wGlX14eBZ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "$\\tiny  {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
