{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "author: Abhishek Kumar Dubey\n",
    "badges: false\n",
    "categories:\n",
    "- Probability Theory\n",
    "date: '2022-10-15'\n",
    "description: Joint, Conditional, Marginal, Discrete Convolution  \n",
    "image: CS6660_images/chrome_pPYR2UCfXR.png\n",
    "title: Probability Theory 5\n",
    "toc: true\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint mass function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "Most examples will involve two random variables, but everything can be generalized for more of them.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition\n",
    "Suppose two discrete random variables $X$ and $Y$ are defined on a common probability space, and can take on values\n",
    "$x_1, x_2, \\dots$ and $y_1, y_2, \\dots,$ respectively. The joint probability mass function of them is defined as <br>\n",
    "$$p(x_i , y_j ) = P{X = x_i , Y = y_j}, i = 1, 2, \\dots , j = 1, 2, \\dots .$$\n",
    "This function contains all information about the joint distribution of $X$ and $Y$.<br>\n",
    "Any joint mass function satisfies :\n",
    "$$p(x,y)\\ge 0, \\; \\forall x,y \\in\\mathbb{R}$$\n",
    "$$\\sum_{i,j}p(x_i,j_j)=1$$\n",
    "Any function with above properties is a joint probability mass function.\n",
    "\n",
    "## Marginal mass functions \n",
    "Marginal mass functions are <br>\n",
    "$$p_X(x_i):=P\\{X=x_i\\}$$ \n",
    "and \n",
    "$$p_Y(y_i):=P\\{Y=y_i\\}$$\n",
    "Also\n",
    "$$p_X(x_i)=\\sum_jp(x_i,j_j)$$\n",
    "and\n",
    "$$p_Y(y_j)=\\sum_ip(x_i,j_j)$$ \n",
    "\n",
    "## Example\n",
    "An urn has $3$ red, $4$ white, $5$ black balls. Drawing $3$ at once, let $X$ be the number of red, $Y$ the number of white balls drawn.\n",
    "The joint mass function is:\n",
    "\n",
    "| $Y \\downarrow$  $X \\rightarrow$ | 0 | 1 | 2 | 3 |$p_Y(\\cdot)$\n",
    "|---------|:-----|------:|:------:|:------:|:------:|\n",
    "| __0__    | $\\displaystyle \\frac{\\binom{5}{3}}{\\binom{12}{3}}$   |   $\\displaystyle \\frac{\\binom{3}{1}\\binom{5}{2}}{\\binom{12}{3}}$  |    $\\displaystyle \\frac{\\binom{3}{2}\\binom{5}{1}}{\\binom{12}{3}}$   |  $\\displaystyle \\frac{\\binom{3}{3}\\binom{5}{0}}{\\binom{12}{3}}$ |  $\\displaystyle \\frac{\\binom{8}{3}}{\\binom{12}{3}}$\n",
    "| __1__     |  $\\displaystyle \\frac{\\binom{4}{1}\\binom{5}{2}}{\\binom{12}{3}}$   |    $\\displaystyle \\frac{\\binom{4}{1}\\binom{3}{1}\\binom{5}{1}}{\\binom{12}{3}}$  |   $\\displaystyle \\frac{\\binom{4}{1}\\binom{3}{2}}{\\binom{12}{3}}$    | 0| $\\displaystyle \\frac{\\binom{4}{1}\\binom{8}{2}}{\\binom{12}{3}}$ \n",
    "| __2__       |  $\\displaystyle \\frac{\\binom{4}{2}\\binom{5}{1}}{\\binom{12}{3}}$    |      $\\displaystyle \\frac{\\binom{4}{2}\\binom{3}{1}}{\\binom{12}{3}}$  |   0    |0| $\\displaystyle \\frac{\\binom{4}{2}\\binom{8}{1}}{\\binom{12}{3}}$ \n",
    "| __3__       |  $\\displaystyle \\frac{\\binom{4}{3}}{\\binom{12}{3}}$     |     0 |   0    |  0|   $\\displaystyle \\frac{\\binom{4}{3}}{\\binom{12}{3}}$ \n",
    "| $p_X(\\cdot)$       |   $\\displaystyle \\frac{\\binom{9}{3}}{\\binom{12}{3}}$    |      $\\displaystyle \\frac{\\binom{3}{1}\\binom{9}{2}}{\\binom{12}{3}}$  |    $\\displaystyle \\frac{\\binom{3}{2}\\binom{9}{1}}{\\binom{12}{3}}$   |  $\\displaystyle \\frac{\\binom{3}{3}}{\\binom{12}{3}}$ | 1\n",
    "\n",
    ": Table shows Joint probability distribution. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional mass function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition \n",
    "\n",
    "suppose $p_Y(y_j)>0$. The conditional mass function of $X$ given $Y=y_j$ is defined by \n",
    "$$p_{X \\mid Y}(x \\mid y_i):= P\\{X=x \\mid Y=y_j\\}=\\frac{\\overbrace{p(x,y_i)}^{\\text{joint}} }{\\underbrace{p_Y(y_i)}_{\\text{marginal}} }$$\n",
    "As the conditional probability was a proper probability, this is a proper mass function: $\\forall x,y_i$\n",
    "$$p_{X \\mid Y}(x \\mid y_j) \\ge 0, \\qquad \\sum_i p_{X|Y}(x_i \\mid y_j)=1$$\n",
    "\n",
    "### Example\n",
    "\n",
    "Let $X$ and $Y$ have joint mass function \n",
    "\n",
    "| $X \\downarrow$  $Y \\rightarrow$| 0 | 1 | \n",
    "|---------|:-----|------:|\n",
    "| __0__      | 0.4   |  0.2 |\n",
    "| __1__     | 0.1  | 0.3 |\n",
    "\n",
    "\n",
    ": joint distribution\n",
    "\n",
    "The conditional distribution of $X$ given $Y=0$ is\n",
    "$$p_{X\\mid Y}(0 \\mid 0)= \\frac{p(0,0)}{p_Y(0)}= \\frac{p(0,0)}{p(0,0)+p(1,0)}=\\frac{0.4}{0.4+0.1}=\\frac{4}{5}$$ \n",
    "$$p_{X\\mid Y}(1 \\mid 0)= \\frac{p(1,0)}{p_Y(0)}= \\frac{p(1,0)}{p(0,0)+p(1,0)}=\\frac{0.1}{0.4+0.1}=\\frac{1}{5}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent Random Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition\n",
    "\n",
    "Random variables $X$ and $Y$ are independent, if events formulated with them are so, That is, if for every $A, B \\sube \\mathbb{R}$\n",
    "$$P\\{X\\in A,Y\\in B\\}=P\\{X\\in A\\} \\cdot P \\{ Y \\in B\\} $$\n",
    "\n",
    ":::{.callout-tip}\n",
    "The abbreviation i.i.d. is used for independent and identically distributed random variables.\n",
    ":::\n",
    "\n",
    "Two random variables $X$ and $Y$ are independent if and only if their joint mass function factorizes into the product of the marginals :\n",
    "$$p(x_i,y_i)=p_X(x_i)\\cdot p_Y(y_i), \\qquad \\forall x_i,y_i$$  \n",
    "\n",
    "## Discrete Convolution\n",
    "\n",
    "Let $X$ and $Y$ be independent, integer valued random variables with respective mass functions $p_X$ and $p_Y$ . Then\n",
    "$$p_{X+Y}(k) = \\sum_{i=-\\infty}^\\infty p_X(k âˆ’ i) \\cdot p_Y (i), \\qquad (\\forall k \\in \\mathbb{Z})$$\n",
    "This formula is called discrete convolution of the mass function $p_X$ and $p_Y$\n",
    "\n",
    "- Proof \n",
    "  $$\\begin{align*}\n",
    "  p_{X+Y}(K)&=P\\{X+Y=K\\}\\\\\n",
    "  &=\\sum_{i=-\\infty}^\\infty P\\{X=k-i,Y=i\\}\\\\\n",
    "  &=\\sum_{i=-\\infty}^\\infty P_X(k-i)\\cdot p_Y(i)\n",
    "  \\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X \\sim \\textrm{Poi}(\\lambda)$ and $Y \\sim \\textrm{Poi}(\\mu)$ be independent than: <br>\n",
    "$X+Y \\sim \\textrm{Poi}(\\lambda+\\mu)$\n",
    "\n",
    "- proof\n",
    "  $$\\begin{align*}\n",
    "  p_{X+Y}(K)&=\\sum_{i=-\\infty}^\\infty P_X(k-i)\\cdot p_Y(i) \\\\\n",
    "  &= \\sum_{i=-\\infty}^\\infty \\frac{\\lambda^{(k-i)}}{(k-i)!}e^{-\\lambda}\\cdot \\frac{\\mu^i}{i!}e^{-\\mu} \\\\\n",
    "  &= e^{-\\lambda - \\mu}\\frac{1}{k!}\\sum_{i=-\\infty}^\\infty \\frac{k!}{(k-i)!\\cdot i!}\\lambda^{(k-i)} \\cdot \\mu^i \\\\\n",
    "  &= e^{-\\lambda - \\mu}\\frac{1}{k!}\\sum_{i=-\\infty}^\\infty \\binom{k}{i} \\lambda^{(k-i)} \\cdot \\mu^i \\\\\n",
    "  &= e^{-(\\lambda + \\mu)}\\frac{1}{k!}(\\lambda + \\mu)^k \\\\\n",
    "  &=\\textrm{Poi}(\\lambda+\\mu)\n",
    "  \\end{align*}$$\n",
    "\n",
    "Let $X,Y$ be i.i.d. $\\textrm{Geom}(p)$ variables then: <br>\n",
    "$X+Y$ is not geometric.\n",
    "\n",
    "- proof\n",
    "  $$\\begin{align*}\n",
    "  p_{X+Y}(K)&=\\sum_{i=-\\infty}^\\infty P_X(k-i)\\cdot p_Y(i) \\\\\n",
    "  &=\\sum_{i=1}^{k-1}(1-p)^{k-i-1}p\\cdot (1-p)^{i-1}p \\\\\n",
    "  &=(k-1)(1-p)^{k-2}p^2 \n",
    "  \\end{align*}$$  \n",
    "  Hence $X+Y$ is not Geometric, _it's actually called Negative Binomial._\n",
    "\n",
    "Let $X \\sim \\textrm{Binom}(n,p)$ and $Y \\sim \\textrm{Binom}(m,p)$ be independent (__notice the same__ $\\mathbf p$) then :<br>\n",
    "$X+Y \\sim \\textrm{Binom}(n+m,p)$\n",
    "\n",
    "- proof\n",
    "  $$\\begin{align*}\n",
    "  p_{X+Y}(K)&=\\sum_{i=-\\infty}^\\infty P_X(k-i)\\cdot p_Y(i) \\\\\n",
    "  &=\\sum_{i=0}^k \\binom{n}{k-i}p^{k-i}(1-p)^{n-k+i}\\cdot \\binom{m}{i}p^i(1-p)^{m-i} \\\\\n",
    "  &=p^k(1-p)^{m+n-k}\\sum_{i=0}^k \\binom{n}{k-i} \\binom{m}{i}  \\\\\n",
    "  &=\\binom{m+n}{k}  p^k(1-p)^{m+n-k} \\\\\n",
    "  &=\\textrm{Binom}(n+m,p)\n",
    "  \\end{align*}$$  \n",
    "  To prove above equation we used the fact that $\\sum_{i=0}^k \\binom{n}{k-i} \\binom{m}{i}=\\binom{m+n}{k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $X$ and $Y$ are independent continuous random variables with respective densities $f_X$ and $f_Y$. Then their sum is a continuous random variable with density\n",
    "$$f_{X+Y}(a)=\\int_{- \\infty} ^ \\infty f_X(a-y)\\cdot f_Y(y)dy, \\qquad (\\forall a \\in \\mathbb{R})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gamma distribution\n",
    "\n",
    "Let $X$ and $Y$ be i.i.d. $\\mathrm{Exp}(\\lambda),$ and the density of their sum $(a\\ge 0)$\n",
    "\n",
    "$$\\begin{align*}\n",
    "f_{X+Y}(a)&=\\int_{-\\infty}^\\infty f_X(a-y) \\cdot f_Y(y)dy \\\\\n",
    "&=\\int_{0}^a \\lambda e^{-\\lambda (a-y)}\\cdot \\lambda e^{-\\lambda y}dy \\\\\n",
    "&=\\lambda^2 e^{-\\lambda a}\\cdot y \\Big |_0^a \\\\\n",
    "&= \\lambda^2 a \\cdot e^{-\\lambda a}\n",
    "\\end{align*}$$\n",
    "This density is called $\\mathrm{Gamma}(2,\\lambda)$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Let $X \\sim \\mathrm{Exp}(\\lambda)$ and $Y \\sim \\mathrm{Gamma}(2,\\lambda)$ be i.i.d. again\n",
    "\n",
    "$$\\begin{align*}\n",
    "f_{X+Y}(a)&=\\int_{-\\infty}^\\infty f_X(a-y) \\cdot f_Y(y)dy \\\\\n",
    "&=\\int_{0}^a \\lambda e^{-\\lambda (a-y)}\\cdot \\lambda^2 y \\cdot e^{-\\lambda y}dy \\\\\n",
    "&=\\lambda^3 e^{-\\lambda a}\\cdot \\frac{y^2}{2} \\Big |_0^a \\\\\n",
    "&= \\frac{\\lambda^3 a^2 \\cdot e^{-\\lambda a}}{2}\n",
    "\\end{align*}$$\n",
    "This density is called $\\mathrm{Gamma}(3,\\lambda)$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Let $X \\sim \\mathrm{Exp}(\\lambda)$ and $Y \\sim \\mathrm{Gamma}(3,\\lambda)$ be i.i.d. again\n",
    "\n",
    "$$\\begin{align*}\n",
    "f_{X+Y}(a)&=\\int_{-\\infty}^\\infty f_X(a-y) \\cdot f_Y(y)dy \\\\\n",
    "&=\\int_{0}^a \\lambda e^{-\\lambda (a-y)}\\cdot\\frac{\\lambda^3 y^2 \\cdot e^{-\\lambda y}}{2}dy \\\\\n",
    "&=\\lambda^3 e^{-\\lambda a}\\cdot \\frac{y^3}{2\\cdot 3} \\Big |_0^a \\\\\n",
    "&= \\frac{\\lambda^4 a^3 \\cdot e^{-\\lambda a}}{2\\cdot 3} \\\\\n",
    "&= \\frac{\\lambda^4 a^3 \\cdot e^{-\\lambda a}}{3!}\n",
    "\\end{align*}$$\n",
    "This density is called $\\mathrm{Gamma}(4,\\lambda)$\n",
    "\n",
    "\n",
    "<br><br><br>\n",
    "The convolution of $n$ i.i.d. $\\mathrm{Exp}(\\lambda)$ distributions results in the $\\mathrm{Gamma}(n,\\lambda)$ density:\n",
    "\n",
    "$$f(X)=\\frac{\\lambda^n X^{n-1} \\cdot e^{-\\lambda X}}{(n-1)!},\\qquad \\forall X\\ge 0 \\tag{1}$$\n",
    "and zero otherwise.\n",
    "<br><br><br><br>\n",
    "This is the density of the sum of n i.i.d. $\\mathrm{Exp}(\\lambda)$ random variables. In particular, $\\mathrm{Gamma}(1,\\lambda) \\equiv  \\mathrm{Exp}(\\lambda)$\n",
    "<br><br><br><br>\n",
    "\n",
    "Now if we integrate $f(X)$ it should equal to $1$\n",
    "$$\\begin{align*}\n",
    "\\int_{-\\infty}^\\infty f(x) &= \\int_{-\\infty}^\\infty \\frac{\\lambda^n X^{n-1} \\cdot e^{-\\lambda X}}{(n-1)!} dx \\\\\n",
    "&=  \\int_{-\\infty}^\\infty \\frac{ (\\lambda X)^{n-1} \\cdot e^{-\\lambda X}}{(n-1)!} \\lambda dx \\\\\n",
    "&=1\n",
    "\\end{align*}$$\n",
    "Now we write $Z= \\lambda X, \\;dZ=\\lambda dX$,from above equation we get:\n",
    "$$(n-1)! = \\int_{-\\infty}^\\infty  (Z)^{n-1} \\cdot e^{-Z}  dZ $$\n",
    "\n",
    "The Gamma function is defined for every $\\alpha > 0$ real numbers, by\n",
    "$$\\Gamma(\\alpha) :=\\int_{-\\infty}^\\infty  Z^{\\alpha-1} \\cdot e^{-Z}  dZ$$\n",
    "In particular, $\\Gamma(n)=(n-1)!$ for positive integer $n$\n",
    "<br><br>\n",
    "Using equation $(1)$ we can write Gamma distribution \n",
    "\n",
    "$$f(X)=\\frac{\\lambda^n X^{n-1} \\cdot e^{-\\lambda X}}{\\Gamma(n)}, \\qquad \\forall X\\ge 0$$\n",
    "and zero otherwise.\n",
    "\n",
    "<br><br>\n",
    "If $X \\sim \\mathrm{Gamma}(\\alpha , \\lambda),$ then\n",
    "\n",
    "$$EX= \\frac{\\alpha}{\\lambda}, \\qquad \\mathrm{Var}X=\\frac{\\alpha}{\\lambda ^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation, covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectation is defined as \n",
    "$$EX = \\sum_i X_i\\cdot p(X_i), \\qquad EX=\\int_{-\\infty}^\\infty Xf(X)dX$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of Expectation\n",
    "\n",
    "- Simple monotonicity property :\n",
    "  If $a \\le X \\le b$ then, $a \\le EX \\le b$ <br>\n",
    "  Proof:<br>\n",
    "  $$\\begin{align*}\n",
    "  & a =a\\cdot 1 =a \\sum_i p(X_i) \\\\\n",
    "  & \\le\\sum_i X_i p(X_i) \\le \\\\\n",
    "  & b \\sum_i p(X_i) =b \\cdot 1=b\n",
    "  \\end{align*}$$\n",
    "- Expectation of _functions of variables_<br>\n",
    "  Let $X$ and $Y$ be the random variables and $g: \\mathbb{R}\\times \\mathbb{R} \\rightarrow \\mathbb{R}$ function then \n",
    "  $$\\mathbf{E}g(X,Y)=\\sum_{i,j}g(X_i,Y_j)\\cdot p(X_i,Y_j)$$\n",
    "- Expectation of sums and differences:<br>\n",
    "  - Let $X$ and $Y$ be the random variables then $E(X+Y)=EX+EY$ and $E(X-Y)=EX-EY$<br>\n",
    "    Proof:\n",
    "    $$\\begin{align*}\n",
    "    E(X\\pm Y) &= \\sum _{i,j}(X_i \\pm Y_j)\\cdot P(X_i, Y_j) \\\\\n",
    "    &=\\sum_i \\sum_j X_i \\cdot P(X_i, Y_j) + \\sum_i \\sum_j Y_j \\cdot P(X_i, Y_j) \\\\\n",
    "    &=\\sum_i X_i\\cdot p_X(X_i) \\pm \\sum_j Y_j\\cdot p_Y(Y_j)\\\\\n",
    "    &=EX \\pm EY\n",
    "    \\end{align*}$$\n",
    "  - Let $X$ and $Y$ be the random variable such that $X \\le Y$, then $EX \\le EY$<br>\n",
    "    Proof: <br>\n",
    "    The difference $Y-X$ is non negative and difference of it's expectation is also non negative\n",
    "    $$\\begin{align*}\n",
    "    & &E(Y-X) &\\ge0 \\\\\n",
    "    & \\Rightarrow & EY-EX &\\ge 0 \\\\\n",
    "    & \\Rightarrow & -EX &\\ge -EY \\\\\n",
    "    &\\Rightarrow & EX &\\le EY \\\\\n",
    "    \\end{align*}$$\n",
    "\n",
    "\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b92a85dd111e7f234cbdfd9e3dc75ce12717f6161daa18a82d26f0e8e840c205"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
