{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "author: Abhishek Kumar Dubey\n",
    "badges: false\n",
    "categories:\n",
    "- Probability Theory\n",
    "date: '2022-10-15'\n",
    "description: Joint, Conditional, Marginal, Discrete Convolution  \n",
    "image: CS6660_images/chrome_pPYR2UCfXR.png\n",
    "title: Probability Theory 5\n",
    "toc: true\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint mass function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "Most examples will involve two random variables, but everything can be generalized for more of them.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n",
    "Suppose two discrete random variables $X$ and $Y$ are defined on a common probability space, and can take on values\n",
    "$x_1, x_2, \\dots$ and $y_1, y_2, \\dots,$ respectively. The joint probability mass function of them is defined as <br>\n",
    "$$p(x_i , y_j ) = P{X = x_i , Y = y_j}, i = 1, 2, \\dots , j = 1, 2, \\dots .$$\n",
    "This function contains all information about the joint distribution of $X$ and $Y$.<br>\n",
    "Any joint mass function satisfies :\n",
    "$$p(x,y)\\ge 0, \\; \\forall x,y \\in\\mathbb{R}$$\n",
    "$$\\sum_{i,j}p(x_i,j_j)=1$$\n",
    "Any function with above properties is a joint probability mass function.\n",
    "\n",
    "### Marginal mass functions \n",
    "Marginal mass functions are <br>\n",
    "$$p_X(x_i):=P\\{X=x_i\\}$$ \n",
    "and \n",
    "$$p_Y(y_i):=P\\{Y=y_i\\}$$\n",
    "Also\n",
    "$$p_X(x_i)=\\sum_jp(x_i,y_j)$$\n",
    "and\n",
    "$$p_Y(y_j)=\\sum_ip(x_i,y_j)$$ \n",
    "\n",
    "### Example\n",
    "An urn has $3$ red, $4$ white, $5$ black balls. Drawing $3$ at once, let $X$ be the number of red, $Y$ the number of white balls drawn.\n",
    "The joint mass function is:\n",
    "\n",
    "| $Y \\downarrow$  $X \\rightarrow$ | 0 | 1 | 2 | 3 |$p_Y(\\cdot)$\n",
    "|---------|:-----|------:|:------:|:------:|:------:|\n",
    "| __0__    | $\\displaystyle \\frac{\\binom{5}{3}}{\\binom{12}{3}}$   |   $\\displaystyle \\frac{\\binom{3}{1}\\binom{5}{2}}{\\binom{12}{3}}$  |    $\\displaystyle \\frac{\\binom{3}{2}\\binom{5}{1}}{\\binom{12}{3}}$   |  $\\displaystyle \\frac{\\binom{3}{3}\\binom{5}{0}}{\\binom{12}{3}}$ |  $\\displaystyle \\frac{\\binom{8}{3}}{\\binom{12}{3}}$\n",
    "| __1__     |  $\\displaystyle \\frac{\\binom{4}{1}\\binom{5}{2}}{\\binom{12}{3}}$   |    $\\displaystyle \\frac{\\binom{4}{1}\\binom{3}{1}\\binom{5}{1}}{\\binom{12}{3}}$  |   $\\displaystyle \\frac{\\binom{4}{1}\\binom{3}{2}}{\\binom{12}{3}}$    | 0| $\\displaystyle \\frac{\\binom{4}{1}\\binom{8}{2}}{\\binom{12}{3}}$ \n",
    "| __2__       |  $\\displaystyle \\frac{\\binom{4}{2}\\binom{5}{1}}{\\binom{12}{3}}$    |      $\\displaystyle \\frac{\\binom{4}{2}\\binom{3}{1}}{\\binom{12}{3}}$  |   0    |0| $\\displaystyle \\frac{\\binom{4}{2}\\binom{8}{1}}{\\binom{12}{3}}$ \n",
    "| __3__       |  $\\displaystyle \\frac{\\binom{4}{3}}{\\binom{12}{3}}$     |     0 |   0    |  0|   $\\displaystyle \\frac{\\binom{4}{3}}{\\binom{12}{3}}$ \n",
    "| $p_X(\\cdot)$       |   $\\displaystyle \\frac{\\binom{9}{3}}{\\binom{12}{3}}$    |      $\\displaystyle \\frac{\\binom{3}{1}\\binom{9}{2}}{\\binom{12}{3}}$  |    $\\displaystyle \\frac{\\binom{3}{2}\\binom{9}{1}}{\\binom{12}{3}}$   |  $\\displaystyle \\frac{\\binom{3}{3}}{\\binom{12}{3}}$ | 1\n",
    "\n",
    ": Table shows Joint probability distribution. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional mass function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition \n",
    "\n",
    "suppose $p_Y(y_j)>0$. The conditional mass function of $X$ given $Y=y_j$ is defined by \n",
    "$$p_{X \\mid Y}(x \\mid y_i):= P\\{X=x \\mid Y=y_j\\}=\\frac{\\overbrace{p(x,y_i)}^{\\text{joint}} }{\\underbrace{p_Y(y_i)}_{\\text{marginal}} }$$\n",
    "As the conditional probability was a proper probability, this is a proper mass function: $\\forall x,y_i$\n",
    "$$p_{X \\mid Y}(x \\mid y_j) \\ge 0, \\qquad \\sum_i p_{X|Y}(x_i \\mid y_j)=1$$\n",
    "\n",
    "#### Example\n",
    "\n",
    "Let $X$ and $Y$ have joint mass function \n",
    "\n",
    "| $X \\downarrow$  $Y \\rightarrow$| 0 | 1 | \n",
    "|---------|:-----|------:|\n",
    "| __0__      | 0.4   |  0.2 |\n",
    "| __1__     | 0.1  | 0.3 |\n",
    "\n",
    "\n",
    ": joint distribution\n",
    "\n",
    "The conditional distribution of $X$ given $Y=0$ is\n",
    "$$p_{X\\mid Y}(0 \\mid 0)= \\frac{p(0,0)}{p_Y(0)}= \\frac{p(0,0)}{p(0,0)+p(1,0)}=\\frac{0.4}{0.4+0.1}=\\frac{4}{5}$$ \n",
    "$$p_{X\\mid Y}(1 \\mid 0)= \\frac{p(1,0)}{p_Y(0)}= \\frac{p(1,0)}{p(0,0)+p(1,0)}=\\frac{0.1}{0.4+0.1}=\\frac{1}{5}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent Random Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n",
    "\n",
    "Random variables $X$ and $Y$ are independent, if events formulated with them are so, That is, if for every $A,B\\subseteq \\mathbb{R}$\n",
    "$$P\\{X\\in A,Y\\in B\\}=P\\{X\\in A\\} \\cdot P \\{ Y \\in B\\} $$\n",
    "\n",
    ":::{.callout-tip}\n",
    "The abbreviation i.i.d. is used for independent and identically distributed random variables.\n",
    ":::\n",
    "\n",
    "Two random variables $X$ and $Y$ are independent if and only if their joint mass function factorizes into the product of the marginals :\n",
    "$$p(x_i,y_i)=p_X(x_i)\\cdot p_Y(y_i), \\qquad \\forall x_i,y_i$$  \n",
    "\n",
    "### Discrete Convolution\n",
    "\n",
    "Let $X$ and $Y$ be independent, integer valued random variables with respective mass functions $p_X$ and $p_Y$ . Then\n",
    "$$p_{X+Y}(k) = \\sum_{i=-\\infty}^\\infty p_X(k − i) \\cdot p_Y (i), \\qquad (\\forall k \\in \\mathbb{Z})$$\n",
    "This formula is called discrete convolution of the mass function $p_X$ and $p_Y$\n",
    "\n",
    "- Proof \n",
    "  $$\\begin{align*}\n",
    "  p_{X+Y}(K)&=P\\{X+Y=K\\}\\\\\n",
    "  &=\\sum_{i=-\\infty}^\\infty P\\{X=k-i,Y=i\\}\\\\\n",
    "  &=\\sum_{i=-\\infty}^\\infty P_X(k-i)\\cdot p_Y(i)\n",
    "  \\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X \\sim \\textrm{Poi}(\\lambda)$ and $Y \\sim \\textrm{Poi}(\\mu)$ be independent than: <br>\n",
    "$X+Y \\sim \\textrm{Poi}(\\lambda+\\mu)$\n",
    "\n",
    "- proof\n",
    "  $$\\begin{align*}\n",
    "  p_{X+Y}(K)&=\\sum_{i=-\\infty}^\\infty P_X(k-i)\\cdot p_Y(i) \\\\\n",
    "  &= \\sum_{i=-\\infty}^\\infty \\frac{\\lambda^{(k-i)}}{(k-i)!}e^{-\\lambda}\\cdot \\frac{\\mu^i}{i!}e^{-\\mu} \\\\\n",
    "  &= e^{-\\lambda - \\mu}\\frac{1}{k!}\\sum_{i=-\\infty}^\\infty \\frac{k!}{(k-i)!\\cdot i!}\\lambda^{(k-i)} \\cdot \\mu^i \\\\\n",
    "  &= e^{-\\lambda - \\mu}\\frac{1}{k!}\\sum_{i=-\\infty}^\\infty \\binom{k}{i} \\lambda^{(k-i)} \\cdot \\mu^i \\\\\n",
    "  &= e^{-(\\lambda + \\mu)}\\frac{1}{k!}(\\lambda + \\mu)^k \\\\\n",
    "  &=\\textrm{Poi}(\\lambda+\\mu)\n",
    "  \\end{align*}$$\n",
    "\n",
    "Let $X,Y$ be i.i.d. $\\textrm{Geom}(p)$ variables then: <br>\n",
    "$X+Y$ is not geometric.\n",
    "\n",
    "- proof\n",
    "  $$\\begin{align*}\n",
    "  p_{X+Y}(K)&=\\sum_{i=-\\infty}^\\infty P_X(k-i)\\cdot p_Y(i) \\\\\n",
    "  &=\\sum_{i=1}^{k-1}(1-p)^{k-i-1}p\\cdot (1-p)^{i-1}p \\\\\n",
    "  &=(k-1)(1-p)^{k-2}p^2 \n",
    "  \\end{align*}$$  \n",
    "  Hence $X+Y$ is not Geometric, _it's actually called Negative Binomial._\n",
    "\n",
    "Let $X \\sim \\textrm{Binom}(n,p)$ and $Y \\sim \\textrm{Binom}(m,p)$ be independent (__notice the same__ $\\mathbf p$) then :<br>\n",
    "$X+Y \\sim \\textrm{Binom}(n+m,p)$\n",
    "\n",
    "- proof\n",
    "  $$\\begin{align*}\n",
    "  p_{X+Y}(K)&=\\sum_{i=-\\infty}^\\infty P_X(k-i)\\cdot p_Y(i) \\\\\n",
    "  &=\\sum_{i=0}^k \\binom{n}{k-i}p^{k-i}(1-p)^{n-k+i}\\cdot \\binom{m}{i}p^i(1-p)^{m-i} \\\\\n",
    "  &=p^k(1-p)^{m+n-k}\\sum_{i=0}^k \\binom{n}{k-i} \\binom{m}{i}  \\\\\n",
    "  &=\\binom{m+n}{k}  p^k(1-p)^{m+n-k} \\\\\n",
    "  &=\\textrm{Binom}(n+m,p)\n",
    "  \\end{align*}$$  \n",
    "  To prove above equation we used the fact that $\\sum_{i=0}^k \\binom{n}{k-i} \\binom{m}{i}=\\binom{m+n}{k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $X$ and $Y$ are independent continuous random variables with respective densities $f_X$ and $f_Y$. Then their sum is a continuous random variable with density\n",
    "$$f_{X+Y}(a)=\\int_{- \\infty} ^ \\infty f_X(a-y)\\cdot f_Y(y)dy, \\qquad (\\forall a \\in \\mathbb{R})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gamma distribution\n",
    "\n",
    "Let $X$ and $Y$ be i.i.d. $\\mathrm{Exp}(\\lambda),$ and the density of their sum $(a\\ge 0)$\n",
    "\n",
    "$$\\begin{align*}\n",
    "f_{X+Y}(a)&=\\int_{-\\infty}^\\infty f_X(a-y) \\cdot f_Y(y)dy \\\\\n",
    "&=\\int_{0}^a \\lambda e^{-\\lambda (a-y)}\\cdot \\lambda e^{-\\lambda y}dy \\\\\n",
    "&=\\lambda^2 e^{-\\lambda a}\\cdot y \\Big |_0^a \\\\\n",
    "&= \\lambda^2 a \\cdot e^{-\\lambda a}\n",
    "\\end{align*}$$\n",
    "This density is called $\\mathrm{Gamma}(2,\\lambda)$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Let $X \\sim \\mathrm{Exp}(\\lambda)$ and $Y \\sim \\mathrm{Gamma}(2,\\lambda)$ be i.i.d. again\n",
    "\n",
    "$$\\begin{align*}\n",
    "f_{X+Y}(a)&=\\int_{-\\infty}^\\infty f_X(a-y) \\cdot f_Y(y)dy \\\\\n",
    "&=\\int_{0}^a \\lambda e^{-\\lambda (a-y)}\\cdot \\lambda^2 y \\cdot e^{-\\lambda y}dy \\\\\n",
    "&=\\lambda^3 e^{-\\lambda a}\\cdot \\frac{y^2}{2} \\Big |_0^a \\\\\n",
    "&= \\frac{\\lambda^3 a^2 \\cdot e^{-\\lambda a}}{2}\n",
    "\\end{align*}$$\n",
    "This density is called $\\mathrm{Gamma}(3,\\lambda)$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Let $X \\sim \\mathrm{Exp}(\\lambda)$ and $Y \\sim \\mathrm{Gamma}(3,\\lambda)$ be i.i.d. again\n",
    "\n",
    "$$\\begin{align*}\n",
    "f_{X+Y}(a)&=\\int_{-\\infty}^\\infty f_X(a-y) \\cdot f_Y(y)dy \\\\\n",
    "&=\\int_{0}^a \\lambda e^{-\\lambda (a-y)}\\cdot\\frac{\\lambda^3 y^2 \\cdot e^{-\\lambda y}}{2}dy \\\\\n",
    "&=\\lambda^3 e^{-\\lambda a}\\cdot \\frac{y^3}{2\\cdot 3} \\Big |_0^a \\\\\n",
    "&= \\frac{\\lambda^4 a^3 \\cdot e^{-\\lambda a}}{2\\cdot 3} \\\\\n",
    "&= \\frac{\\lambda^4 a^3 \\cdot e^{-\\lambda a}}{3!}\n",
    "\\end{align*}$$\n",
    "This density is called $\\mathrm{Gamma}(4,\\lambda)$\n",
    "\n",
    "\n",
    "<br><br><br>\n",
    "The convolution of $n$ i.i.d. $\\mathrm{Exp}(\\lambda)$ distributions results in the $\\mathrm{Gamma}(n,\\lambda)$ density:\n",
    "\n",
    "$$f(X)=\\frac{\\lambda^n X^{n-1} \\cdot e^{-\\lambda X}}{(n-1)!},\\qquad \\forall X\\ge 0 \\tag{1}$$\n",
    "and zero otherwise.\n",
    "<br><br><br><br>\n",
    "This is the density of the sum of n i.i.d. $\\mathrm{Exp}(\\lambda)$ random variables. In particular, $\\mathrm{Gamma}(1,\\lambda) \\equiv  \\mathrm{Exp}(\\lambda)$\n",
    "<br><br><br><br>\n",
    "\n",
    "Now if we integrate $f(X)$ it should equal to $1$\n",
    "$$\\begin{align*}\n",
    "\\int_{-\\infty}^\\infty f(x) &= \\int_{-\\infty}^\\infty \\frac{\\lambda^n X^{n-1} \\cdot e^{-\\lambda X}}{(n-1)!} dx \\\\\n",
    "&=  \\int_{-\\infty}^\\infty \\frac{ (\\lambda X)^{n-1} \\cdot e^{-\\lambda X}}{(n-1)!} \\lambda dx \\\\\n",
    "&=1\n",
    "\\end{align*}$$\n",
    "Now we write $Z= \\lambda X, \\;dZ=\\lambda dX$,from above equation we get:\n",
    "$$(n-1)! = \\int_{-\\infty}^\\infty  (Z)^{n-1} \\cdot e^{-Z}  dZ $$\n",
    "\n",
    "The Gamma function is defined for every $\\alpha > 0$ real numbers, by\n",
    "$$\\Gamma(\\alpha) :=\\int_{-\\infty}^\\infty  Z^{\\alpha-1} \\cdot e^{-Z}  dZ$$\n",
    "In particular, $\\Gamma(n)=(n-1)!$ for positive integer $n$\n",
    "<br><br>\n",
    "Using equation $(1)$ we can write Gamma distribution \n",
    "\n",
    "$$f(X)=\\frac{\\lambda^n X^{n-1} \\cdot e^{-\\lambda X}}{\\Gamma(n)}, \\qquad \\forall X\\ge 0$$\n",
    "and zero otherwise.\n",
    "\n",
    "<br><br>\n",
    "If $X \\sim \\mathrm{Gamma}(\\alpha , \\lambda),$ then\n",
    "\n",
    "$$EX= \\frac{\\alpha}{\\lambda}, \\qquad \\mathrm{Var}X=\\frac{\\alpha}{\\lambda ^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation, covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectation is defined as \n",
    "$$EX = \\sum_i X_i\\cdot p(X_i), \\qquad EX=\\int_{-\\infty}^\\infty Xf(X)dX$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Properties of Expectation\n",
    "\n",
    "- Simple monotonicity property :\n",
    "  If $a \\le X \\le b$ then, $a \\le EX \\le b$ <br>\n",
    "  Proof:<br>\n",
    "  $$\\begin{align*}\n",
    "  & a =a\\cdot 1 =a \\sum_i p(X_i) \\\\\n",
    "  & \\le\\sum_i X_i p(X_i) \\le \\\\\n",
    "  & b \\sum_i p(X_i) =b \\cdot 1=b\n",
    "  \\end{align*}$$\n",
    "- Expectation of _functions of variables_<br>\n",
    "  Let $X$ and $Y$ be the random variables and $g: \\mathbb{R}\\times \\mathbb{R} \\rightarrow \\mathbb{R}$ function then \n",
    "  $$\\mathbf{E}g(X,Y)=\\sum_{i,j}g(X_i,Y_j)\\cdot p(X_i,Y_j)$$\n",
    "- Expectation of sums and differences:<br>\n",
    "  - Let $X$ and $Y$ be the random variables then $E(X+Y)=EX+EY$ and $E(X-Y)=EX-EY$<br>\n",
    "    Proof:\n",
    "    $$\\begin{align*}\n",
    "    E(X\\pm Y) &= \\sum _{i,j}(X_i \\pm Y_j)\\cdot P(X_i, Y_j) \\\\\n",
    "    &=\\sum_i \\sum_j X_i \\cdot P(X_i, Y_j) + \\sum_i \\sum_j Y_j \\cdot P(X_i, Y_j) \\\\\n",
    "    &=\\sum_i X_i\\cdot p_X(X_i) \\pm \\sum_j Y_j\\cdot p_Y(Y_j)\\\\\n",
    "    &=EX \\pm EY\n",
    "    \\end{align*}$$\n",
    "  - Let $X$ and $Y$ be the random variable such that $X \\le Y$, then $EX \\le EY$<br>\n",
    "    Proof: <br>\n",
    "    The difference $Y-X$ is non negative and difference of it's expectation is also non negative\n",
    "    $$\\begin{align*}\n",
    "    & &E(Y-X) &\\ge0 \\\\\n",
    "    & \\Rightarrow & EY-EX &\\ge 0 \\\\\n",
    "    & \\Rightarrow & -EX &\\ge -EY \\\\\n",
    "    &\\Rightarrow & EX &\\le EY \\\\\n",
    "    \\end{align*}$$\n",
    "\n",
    "  - __Example__ (sample mean) <br>\n",
    "    Let $X_1,X_2, \\dots , X_n$ be identically distributed random variables with mean $\\mu$. Their sample mean is \n",
    "    $$\\bar{X}:=\\frac{1}{n}\\sum_{i=1}^nX_i$$\n",
    "    It's expectation is \n",
    "    $$\\begin{align*}\n",
    "    E\\bar{X}&=E\\left(\\frac{1}{n}\\sum_{i=1}^nX_i\\right)\\\\\n",
    "    &=\\frac{1}{n}E\\left(\\sum_{i=1}^nX_i\\right)\\\\\n",
    "    &=\\frac{1}{n}\\sum_{i=1}^nE\\left(X_i\\right)\\\\\n",
    "    &=\\frac{1}{n}\\sum_{i=1}^n\\mu\\\\\n",
    "    &=\\mu\n",
    "    \\end{align*}$$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Independence__<br>\n",
    "Let $X$ and $Y$ be independent random variables, and $g,h$ be functions. Then\n",
    "$$\\mathbf{E}(g(X)\\cdot h(Y))= \\mathbf{E}g(X)\\cdot \\mathbf{E}h(Y) \\tag{2}$$\n",
    "proof: <br>\n",
    "$$\\begin{align*}\n",
    "\\mathbf{E}(g(X)\\cdot h(Y))&=\\int \\int g(X)h(Y)p_{XY}(X,Y)dXdY \\\\\n",
    "&=\\int \\int g(X)h(Y)p_{X}(X)p_{Y}(Y)dXdY \\\\\n",
    "&=\\int  g(X)p_{X}(X)dX\\int h(Y)p_{Y}(Y)dXdY \\\\\n",
    "&=\\mathbf{E}g(X)\\cdot \\mathbf{E}h(Y)\n",
    "\\end{align*}$$\n",
    "Here we used the fact that the joint probability distribution factorizes $(p_{XY}(x,y)=p_{X}(x)p_{Y}(y))$ as $X$ and $Y$ are independent \n",
    "\n",
    "<br><br>\n",
    "__Covariance__ <br>\n",
    "The covariance of the random variable $X$ and $Y$ is \n",
    "$$\\mathbf{Cov}(X,Y)=\\mathbf{E}[(X-\\mathbf{E}X)\\cdot (Y-\\mathbf{E}Y)]$$ \n",
    "Another form of the above formula\n",
    "$$\\mathbf{Cov}(X,Y)=\\mathbf{E}(XY)-\\mathbf{E}X\\mathbf{E}Y \\tag{3}$$\n",
    "Proof:\n",
    "$$\\begin{align*}\n",
    "\\mathbf{E}[(X-\\mathbf{E}X)\\cdot (Y-\\mathbf{E}Y)]&=\\mathbf{E}[XY-Y\\mathbf{E}X-X\\mathbf{E}Y+\\mathbf{E}X\\mathbf{E}Y]\\\\\n",
    "&=\\mathbf{E}(XY)-\\mathbf{E}Y\\mathbf{E}X-\\mathbf{E}X\\mathbf{E}Y+\\mathbf{E}X\\mathbf{E}Y \\\\\n",
    "&= \\mathbf{E}(XY)-\\mathbf{E}X\\mathbf{E}Y\n",
    "\\end{align*}$$\n",
    "\n",
    "Now we know that\n",
    "$$\\mathbf{Cov}(X,Y)=\\mathbf{E}(XY)-\\mathbf{E}X\\mathbf{E}Y$$\n",
    "Using equation $(2)$ and $(3)$ we can say for independent random variables $X$ and $Y,\\;$ $\\mathbf{Cov}(X,Y)=0,$ as  $\\mathbf{E}(XY)=\\mathbf{E}X\\mathbf{E}Y$\n",
    "$$\\mathbf{Cov}(X,Y)=\\mathbf{E}(XY)-\\mathbf{E}X\\mathbf{E}Y=0$$\n",
    "But this is not true other way around. _covariance zero doesn't necessarily mean random variables are independent_. \n",
    "\n",
    ":::{.callout-tip}\n",
    "- If random variables $X$ and $Y$ are independent, then $\\mathbf{Cov}(X,Y)=0$ <br>\n",
    "- But if $\\mathbf{Cov}(X,Y)=0,$ doesn't necessarily mean $X$ and $Y$ are independent. \n",
    ":::\n",
    "\n",
    "<br>\n",
    "\n",
    "- Properties  <br>\n",
    "  - Covariance is positive semidefinite: $\\mathbf{Cov}(X,X)=\\mathbf{Var}(X)\\ge0,$\n",
    "  - Covariance is symmetric: $\\mathbf{Cov}(X,Y)=\\mathbf{Cov}(Y,X)$\n",
    "  - Almost bilinear:<br>\n",
    "    Fix $a_i , b, c_j , d$ real numbers. Covariance is \n",
    "    $$\\mathbf{Cov}\\left(\\sum_ia_iX_i+b,\\sum_jc_jY_j+d \\right)=\\sum_{i,j}a_ic_j\\mathbf{Cov}(X_i,Y_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X_1, X_2, \\dots , X_n$ be random variables. Then\n",
    "$$\\mathbf{Var}\\sum_{i=1}^n X_i =\\sum_{i=1}^n \\mathbf{Var}\\left(X_i \\right)+2\\sum_{1\\le i\\le j\\le n} \\mathbf{Cov}\\left(X_i ,X_j \\right)$$\n",
    "In particular, variances of __independent__ random variables are additive.\n",
    "<br><br>\n",
    "Proof:\n",
    "$$\\begin{align*}{}\n",
    "\\mathbf{Var}\\sum_{i=1}^n X_i &=\\mathbf{Cov}\\left(\\sum_{i=1}^n X_i ,\\sum_{j=1}^n X_j \\right)\\\\\n",
    "&=\\sum_{i=1,j=1}^n \\mathbf{Cov}\\left(X_i ,X_j \\right)\\\\\n",
    "&=\\sum_{i=1}^n \\mathbf{Cov}\\left(X_i ,X_i \\right)+\\sum_{i\\not= j} \\mathbf{Cov}\\left(X_i ,X_j \\right)\\\\\n",
    "&=\\sum_{i=1}^n \\mathbf{Var}\\left(X_i \\right)+\\sum_{i<j} \\mathbf{Cov}\\left(X_i ,X_j \\right)+\\sum_{i>j} \\mathbf{Cov}\\left(X_i ,X_j \\right)\\\\\n",
    "&=\\sum_{i=1}^n \\mathbf{Var}\\left(X_i \\right)+2\\sum_{1\\le i\\le j\\le n} \\mathbf{Cov}\\left(X_i ,X_j \\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "Notice that for __independent__ variables.\n",
    "$$\\begin{align*}{}\n",
    "\\mathbf{Var}\\left(X-Y\\right)&=\\mathbf{Var}\\left(X+-\\left(Y\\right)\\right)\\\\\n",
    "&=\\mathbf{Var}\\left(X\\right)+\\mathbf{Var}\\left(-Y\\right)+2\\mathbf{Cov}\\left(X,-Y\\right)\\\\\n",
    "&=\\mathbf{Var}\\left(X\\right)+\\mathbf{Var}\\left(Y\\right)-2\\mathbf{Cov}\\left(X,Y\\right)\\\\\n",
    "&=\\mathbf{Var}\\left(X\\right)+\\mathbf{Var}\\left(Y\\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "Above used the  fact that <br>\n",
    "$\\mathbf{Var}\\left(X+Y\\right)=\\mathbf{Var}\\left(X\\right)+\\mathbf{Var}\\left(Y\\right)+2\\mathbf{Cov}\\left(X,Y\\right)$, Here is the proof:\n",
    "\n",
    "$$\\begin{align*}{}\n",
    "\\mathbf{Var}\\left(X+Y\\right)&=\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(X+Y\\right)}^2 \\right\\rbrack -{\\left(\\mathit{\\mathbf{E}}\\left\\lbrack X+Y\\right\\rbrack \\right)}^2 \\\\\n",
    "&=\\mathit{\\mathbf{E}}\\left\\lbrack X^2 +Y^2 +2\\mathrm{XY}\\right\\rbrack -{\\left(\\mathit{\\mathbf{E}}\\left\\lbrack X\\right\\rbrack +\\mathit{\\mathbf{E}}\\left\\lbrack Y\\right\\rbrack \\right)}^2 \\\\\n",
    "&=\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack +\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack +2\\mathit{\\mathbf{E}}\\left\\lbrack X Y\\right\\rbrack -{\\left(\\mathit{\\mathbf{E}}\\left\\lbrack X\\right\\rbrack \\right)}^2 -{\\left(\\mathit{\\mathbf{E}}\\left\\lbrack Y\\right\\rbrack \\right)}^2 -2\\mathit{\\mathbf{E}}\\left\\lbrack X\\right\\rbrack \\mathit{\\mathbf{E}}\\left\\lbrack Y\\right\\rbrack \\\\\n",
    "&=\\left(\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack -{\\left(\\mathit{\\mathbf{E}}\\left\\lbrack X\\right\\rbrack \\right)}^2 \\right)+\\left(\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack -{\\left(\\mathit{\\mathbf{E}}\\left\\lbrack Y\\right\\rbrack \\right)}^2 \\right)+2\\left(\\mathit{\\mathbf{E}}\\left\\lbrack XY\\right\\rbrack -\\mathit{\\mathbf{E}}\\left\\lbrack X\\right\\rbrack \\mathit{\\mathbf{E}}\\left\\lbrack Y\\right\\rbrack \\right)\\\\\n",
    "&=\\mathbf{Var}\\left(X\\right)+\\mathbf{Var}\\left(Y\\right)+2\\mathbf{Cov}\\left(X,Y\\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "- __Example__ (variance of the sample mean)<br>\n",
    "  Suppose that $X_j$'s are i.i.d. each with variance $\\sigma^2$, The sample mean is <br>\n",
    "  $$\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i$$\n",
    "  It's Variance is \n",
    "  $$\\begin{align*}{}\n",
    "  \\mathbf{Var}\\left(\\bar{X} \\right)&=\\mathbf{Var}\\left(\\frac{1}{n}\\sum_{i=1}^n X_i \\right)\\\\\n",
    "  &=\\frac{1}{n^2 }\\mathbf{Var}\\left(\\sum_{i=1}^n X_i \\right)\\\\\n",
    "  &=\\frac{1}{n^2 }\\left(\\sum_{i=1}^n \\mathbf{Var}\\left(X_i \\right)\\right)\\\\\n",
    "  &=\\frac{1}{n^2 }n\\sigma^2 \\\\\n",
    "  &=\\frac{\\sigma^2 }{n}\n",
    "  \\end{align*}$$\n",
    "  _Variance of the sample mean decreases with $n$, that’s why we like sample averages._<br>\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Example__ (unbiased sample variance) <br>\n",
    "  Suppose we are given the values of $X_1,X_2,\\dots X_n$ of an i.i.d. sequence of random variables with mean $\\mu$ and variance $\\sigma^2$.<br>\n",
    "  We know that the sample mean $\\bar{X}$ <br>\n",
    "  has mean $\\mu$ and <br>\n",
    "  small variance $\\frac{\\sigma^2}{n}$ <br>\n",
    "  Therefore it serves as a good estimator for the value of $\\mu$. But what should we use to estimate the variance $\\sigma^2$? This quantity is the unbiased sample variance:\n",
    "  $$S^2 :=\\frac{1}{n-1}\\sum_{i=1}^n {\\left(X-\\bar{X} \\right)}^2$$\n",
    "  Now we compute the expected value of sample variance\n",
    "  $$\\begin{align*}{}\n",
    "  {\\mathrm{ES}}^2 &=\\mathit{\\mathbf{E}}\\left\\lbrack \\frac{1}{n-1}\\sum_{i=1}^n {\\left(X-\\bar{X} \\right)}^2 \\right\\rbrack \\\\\n",
    "  &=\\frac{1}{n-1}\\sum_{i=1}^n \\mathit{\\mathbf{E}}\\left\\lbrack {\\left(X-\\bar{X} \\right)}^2 \\right\\rbrack \\\\\n",
    "  &=\\frac{n}{n-1}\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(X-\\bar{X} \\right)}^2 \\right\\rbrack \n",
    "  \\end{align*}$$\n",
    "  we get\n",
    "  $${\\mathrm{ES}}^2=\\frac{n}{n-1}\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(X-\\bar{X} \\right)}^2 \\right\\rbrack \\tag{4}$$\n",
    "  Next notice that\n",
    "  $$\\mathit{\\mathbf{E}}\\left\\lbrack X-\\bar{X} \\right\\rbrack =\\mathit{\\mathbf{E}}\\left\\lbrack X\\right\\rbrack -\\mathit{\\mathbf{E}}\\left\\lbrack \\bar{X} \\right\\rbrack =\\mu -\\mu =0$$\n",
    "  also\n",
    "  $$\\begin{align*}{}\n",
    "  \\mathbf{Var}\\left(X-\\bar{X} \\right)&=\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(X-\\bar{X} \\right)}^2 \\right\\rbrack -{\\left(\\mathit{\\mathbf{E}}\\left\\lbrack X-\\bar{X} \\right\\rbrack \\right)}^2 \\\\\n",
    "  \\mathbf{Var}\\left(X-\\bar{X} \\right)&=\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(X-\\bar{X} \\right)}^2 \\right\\rbrack \\\\\n",
    "  \\Rightarrow \\mathit{\\mathbf{E}}\\left\\lbrack {\\left(X-\\bar{X} \\right)}^2 \\right\\rbrack &=\\mathbf{Var}\\left(X-\\bar{X} \\right)\\\\\n",
    "  &=\\mathbf{Var}\\left(X\\right)+\\mathbf{Var}\\left(\\bar{X} \\right)-2\\mathbf{Cov}\\left(X,\\bar{X} \\right)\n",
    "  \\end{align*}$$\n",
    "  Now , \n",
    "  $$\\begin{align*}{}\n",
    "  \\mathbf{Cov}\\left(X,\\bar{X} \\right)&=\\mathbf{Cov}\\left(X,\\frac{1}{n}\\sum_j X_j \\right)\\\\\n",
    "  &=\\frac{1}{n}\\sum_j \\mathbf{Cov}\\left(X,X_j \\right)\\\\\n",
    "  &=\\frac{1}{n}\\mathbf{Cov}\\left(X,X\\right)\\\\\n",
    "  &=\\frac{\\sigma^2 }{n}\n",
    "  \\end{align*}$$\n",
    "  we have now\n",
    "  $$\\mathbf{Var}\\left(X\\right)=\\sigma^2 ,\\mathbf{Var}\\left(\\bar{X} \\right)=\\frac{\\sigma^2 }{n},\\mathbf{Cov}\\left(X,\\bar{X} \\right)=\\frac{\\sigma^2 }{n}$$    \n",
    "  Putting it all back in equation $(4)$ we get \n",
    "  $$\\begin{align*}{}\n",
    "  \\mathit{\\mathbf{E}}\\left\\lbrack S^2 \\right\\rbrack &=\\frac{n}{n-1}\\left(\\sigma^2 +\\frac{\\sigma^2 }{n}-2\\frac{\\sigma^2 }{n}\\right)\\\\\n",
    "  &=\\frac{n}{n-1}\\left(\\sigma^2 -\\frac{\\sigma^2 }{n}\\right)\\\\\n",
    "  &=\\frac{n}{n-1}\\frac{\\left(n-1\\right)\\sigma^2 }{n}\\\\\n",
    "  &=\\sigma^2 \n",
    "  \\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Example__ (Binomal distribution)<br>\n",
    "  Suppose that $n$ independent trails are made, each succeeding with probability $p$. Define $X_i$ as the indicator of success in the $i^{\\text{th}}$ trail, $i=1,2,\\dots ,n$. Then\n",
    "  $$X=\\sum_{i=1}^n X_i$$\n",
    "  Counts the total number of success, therefore $X \\sim \\mathrm{Binom}(n,p)$. It's  variance is\n",
    "\n",
    "  $$\\begin{align*}{}\n",
    "  \\mathbf{Var}\\left(X\\right)&=\\mathbf{Var}\\left(\\sum_{i=1}^n X_i \\right)\\\\\n",
    "  &=\\sum_{i=1}^n \\mathbf{Var}\\left(X_i \\right)\\\\\n",
    "  &=\\sum_{i=1}^n p\\left(1-p\\right)\\\\\n",
    "  &=n\\cdot p\\left(1-p\\right)\n",
    "  \\end{align*}$$\n",
    "\n",
    "- __Example__ (Gamma distribution) <br>\n",
    "  Let $n$ be a positive integer, $\\lambda > 0$ real, and $X \\sim \\mathrm{Gamma}(n,\\lambda)$, Then we know <br>\n",
    "  $$X\\overset{d}{=} \\sum_{i=1}^n X_i$$\n",
    "  where $X_1,X_2,\\dots, X_n$ are i.i.d. $\\mathrm{Exp}(\\lambda)$. Therefore\n",
    "  $$\\begin{align*}{}\n",
    "  \\mathbf{Var}\\left(X\\right)&=\\mathbf{Var}\\left(\\sum_{i=1}^n X_i \\right)\\\\\n",
    "  &=\\sum_{i=1}^n \\mathbf{Var}\\left(X_i \\right)\\\\\n",
    "  &=\\sum_{i=1}^n \\frac{1}{\\lambda^2 }\\\\\n",
    "  &=\\frac{n}{\\lambda^2 }\n",
    "  \\end{align*}$$\n",
    "  Here $\\overset{d}{=}$ means “equal in distribution”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cauchy-Schwarz inequality "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every $X$ and $Y$\n",
    "$$|\\mathit{\\mathbf{E}}\\left\\lbrack \\mathrm{XY}\\right\\rbrack |\\le \\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack }\\cdot \\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack }$$\n",
    "with equality iff $Y=\\text{Const}. \\cdot X \\text{a.s.}$ \n",
    "\n",
    "Proof:\n",
    "\n",
    "$$\\begin{align*}{}\n",
    "0 &\\le {\\mathit{\\mathbf{E}}\\left\\lbrack \\frac{X}{\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack }}\\pm \\frac{Y}{\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack }}\\right\\rbrack }^2 \\\\\n",
    "&=\\mathit{\\mathbf{E}}\\left\\lbrack \\frac{X^2 }{\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack }\\right\\rbrack +\\mathit{\\mathbf{E}}\\left\\lbrack \\frac{Y^2 }{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack }\\right\\rbrack \\pm 2\\mathit{\\mathbf{E}}\\left\\lbrack \\frac{\\mathrm{XY}}{\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack \\;}\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack }}\\right\\rbrack \\\\\n",
    "&=2\\pm 2\\mathit{\\mathbf{E}}\\left\\lbrack \\frac{\\mathrm{XY}}{\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack \\;}\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack }}\\right\\rbrack \n",
    "\\end{align*}$$\n",
    "\n",
    "For $-$ case:\n",
    "\n",
    "$$\\begin{align*}{}\n",
    "&&0 &\\le 2-2\\mathit{\\mathbf{E}}\\left\\lbrack \\frac{\\mathrm{XY}}{\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack \\;}\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack }}\\right\\rbrack \\\\\n",
    "&\\Rightarrow & -2 &\\le -2\\mathit{\\mathbf{E}}\\left\\lbrack \\frac{\\mathrm{XY}}{\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack \\;}\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack }}\\right\\rbrack \\\\\n",
    "&\\Rightarrow & -\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack \\;}\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack } & \\le -\\mathit{\\mathbf{E}}\\left\\lbrack \\mathrm{XY}\\right\\rbrack \\\\\n",
    "&\\Rightarrow & \\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack \\;}\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack }& \\ge \\mathit{\\mathbf{E}}\\left\\lbrack \\mathrm{XY}\\right\\rbrack \\\\\n",
    "&\\Rightarrow & \\mathit{\\mathbf{E}}\\left\\lbrack \\mathrm{XY}\\right\\rbrack &\\le \\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack \\;}\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack }\n",
    "\\end{align*}$$\n",
    "\n",
    "For $+$ case:\n",
    "$$\\begin{align*}{}\n",
    "&&0 &\\le 2+2\\mathit{\\mathbf{E}}\\left\\lbrack \\frac{\\mathrm{XY}}{\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack \\;}\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack }}\\right\\rbrack \\\\\n",
    "&\\Rightarrow & -2 &\\le +2\\mathit{\\mathbf{E}}\\left\\lbrack \\frac{\\mathrm{XY}}{\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack \\;}\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack }}\\right\\rbrack \\\\\n",
    "&\\Rightarrow & -\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack \\;}\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack } &\\le \\mathit{\\mathbf{E}}\\left\\lbrack \\mathrm{XY}\\right\\rbrack \\\\\n",
    "&\\Rightarrow & \\mathit{\\mathbf{E}}\\left\\lbrack \\mathrm{XY}\\right\\rbrack &\\ge -\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack \\;}\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack }\n",
    "\\end{align*}$$\n",
    "\n",
    "Using Cauchy-Schwarz inequality we can get below important relations:\n",
    "\n",
    "- $\\mathit{\\mathbf{E}}\\left\\lbrack |\\mathrm{XY}|\\right\\rbrack \\le \\sqrt  {\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack }\\cdot \\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack }$<br>\n",
    "  Proof:\n",
    "  \n",
    "  $$\\begin{align*}{}\n",
    "  \\mathit{\\mathbf{E}}\\left\\lbrack |\\mathrm{XY}|\\right\\rbrack &=\\mathit{\\mathbf{E}}\\left\\lbrack |X|\\cdot |Y|\\right\\rbrack \\\\\n",
    "  &\\le \\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack {|X|}^2 \\right\\rbrack }\\cdot \\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack {|Y|}^2 \\right\\rbrack }\\\\\n",
    "  &=\\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack X^2 \\right\\rbrack }\\cdot \\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack Y^2 \\right\\rbrack }\n",
    "  \\end{align*}$$\n",
    "\n",
    "- $\\big|\\mathbf{Cov}\\left(X,Y\\right)\\big|\\le \\mathrm{SD}\\;X\\cdot \\mathrm{SD}\\;Y$ <br>\n",
    "  Proof:\n",
    "  $$\\begin{align*}{}\n",
    "  \\big|\\mathbf{Cov}\\left(X,Y\\right)\\big|&=\\big|\\mathit{\\mathbf{E}}\\left\\lbrack \\left(X-\\mathit{\\mathbf{E}}\\left\\lbrack X\\right\\rbrack \\right)\\left(Y-\\mathit{\\mathbf{E}}\\left\\lbrack Y\\right\\rbrack \\right)\\right\\rbrack \\big|\\\\\n",
    "  &\\le \\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(X-\\mathit{\\mathbf{E}}\\left\\lbrack X\\right\\rbrack \\right)}^2 \\right\\rbrack }\\cdot \\sqrt{\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(Y-\\mathit{\\mathbf{E}}\\left\\lbrack Y\\right\\rbrack \\right)}^2 \\right\\rbrack }\\\\\n",
    "  &=\\mathrm{SD}\\;X\\cdot \\mathrm{SD}\\;Y\n",
    "  \\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation coefficient of random variables $X$ and $Y$ is\n",
    "$$\\rho \\left(X,Y\\right):=\\frac{\\mathbf{Cov}\\left(X,Y\\right)}{\\mathrm{SD}\\;X\\cdot \\mathrm{SD}\\;Y}$$\n",
    "\n",
    "- $-1 \\le \\rho(X,Y) \\le 1$<br>\n",
    "  Proof :\n",
    "  $$\\begin{align*}{}\n",
    "  &&\\big|\\mathbf{Cov}\\left(X,Y\\right)\\big| &\\le \\mathrm{SD}\\;X\\cdot \\mathrm{SD}\\;Y\\\\\n",
    "  \\Rightarrow &&\\frac{\\big|\\mathbf{Cov}\\left(X,Y\\right)\\big|}{\\mathrm{SD}\\;X\\cdot \\mathrm{SD}\\;Y}&\\le 1\\\\\n",
    "  \\Rightarrow &&-1\\le \\frac{\\mathbf{Cov}\\left(X,Y\\right)}{\\mathrm{SD}\\;X\\cdot \\mathrm{SD}\\;Y}&\\le \\\\\n",
    "  \\Rightarrow &&-1 \\le \\rho(X,Y) &\\le 1\n",
    "  \\end{align*}$$\n",
    "  and the “equality iff” part of Cauchy-Schwarz implies that we have equality iff $Y = aX$, that is, $Y = aX +b$ for some fixed $a, b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditional expectation of $X$, given $Y = y_j$ is \n",
    "$$\\mathbf{E}(X \\mid Y=y_j):=\\sum_i X_i \\times \\underbrace{ p_{X\\mid Y}(x_i\\mid y_j)}_{\\text{conditional mass function}}$$\n",
    "\n",
    "- Example :<br>\n",
    "  Let $X$ and $Y$ be independent $\\text{Poi}(\\lambda)$ and $\\text{Poi}(\\mu)$ variables, and $Z = X + Y$. Find the conditional expectation $\\mathbf {E}(X \\mid Z = k)$.<br>\n",
    "  Conditional mass function for $0\\le i\\le k$ is : <br>\n",
    "  $$p_{X\\mid Z} \\left(i\\mid k\\right)=\\frac{p\\left(i,k\\right)}{p_Z \\left(k\\right)}$$\n",
    "  where $p(i, k)$ is the joint mass function of $X$ and $Z$ at $(i, k)$.<br>\n",
    "  Now,\n",
    "  $$\\begin{align*}{}\n",
    "  p\\left(i,k\\right)&=P\\left\\lbrace X=i,Z=k\\right\\rbrace \\\\\n",
    "  &=P\\left\\lbrace X=i,X+Y=k\\right\\rbrace \\\\\n",
    "  &=P\\left\\lbrace X=i,Y=k-i\\right\\rbrace \\\\\n",
    "  &=e^{-\\lambda } \\frac{\\lambda^i }{i!}\\cdot e^{-\\mu } \\frac{\\mu^{\\left(k-i\\right)} }{\\left(k-i\\right)!}\n",
    "  \\end{align*}$$\n",
    "  We know that \n",
    "  $$Z=X+Y\\sim \\mathrm{Poi}\\left(\\lambda +\\mu \\right)$$\n",
    "  so,\n",
    "  $$p_Z \\left(k\\right)=e^{-\\left(\\lambda +\\mu \\right)} \\frac{{\\left(\\lambda +\\mu \\right)}^k }{k!}$$\n",
    "  Now using these values we can compute conditional mass function\n",
    "  $$\\begin{align*}{}\n",
    "  p_{X|Z} \\left(i|k\\right)&=\\frac{p\\left(i,k\\right)}{p_Z \\left(k\\right)}\\\\\n",
    "  &=e^{-\\lambda } \\frac{\\lambda^i }{i!}\\cdot e^{-\\mu } \\frac{\\mu^{\\left(k-i\\right)} }{\\left(k-i\\right)!}\\times \\frac{1}{e^{-\\left(\\lambda +\\mu \\right)} \\frac{{\\left(\\lambda +\\mu \\right)}^k }{k!}}\\\\\n",
    "  &=e^{-\\lambda } \\frac{\\lambda^i }{i!}\\cdot e^{-\\mu } \\frac{\\mu^{\\left(k-i\\right)} }{\\left(k-i\\right)!}\\times \\frac{k!}{e^{-\\left(\\lambda +\\mu \\right)} {\\left(\\lambda +\\mu \\right)}^k }\\\\\n",
    "  &=\\frac{k!}{\\left(k-i\\right)!\\cdot i!}e^{-\\left(\\lambda +\\mu \\right)} \\times \\frac{{\\lambda^i \\mu }^{\\left(k-i\\right)} }{e^{-\\left(\\lambda +\\mu \\right)} {\\left(\\lambda +\\mu \\right)}^k }\\\\\n",
    "  &={\\left({{k}\\atop{i}}\\right)}\\times \\frac{{\\lambda^i \\mu }^{\\left(k-i\\right)} }{{\\left(\\lambda +\\mu \\right)}^k }\\\\\n",
    "  &={\\left({{k}\\atop{i}}\\right)}\\times \\frac{{\\lambda^i \\mu }^{\\left(k-i\\right)} }{{\\left(\\lambda +\\mu \\right)}^k }\\\\\n",
    "  &={\\left({{k}\\atop{i}}\\right)}\\times \\frac{{\\lambda^i \\mu }^{\\left(k-i\\right)} }{{\\left(\\lambda +\\mu \\right)}^i {\\left(\\lambda +\\mu \\right)}^{k-i} }\\\\\n",
    "  &={\\left({{k}\\atop{i}}\\right)}\\times {\\left(\\frac{\\lambda }{\\lambda +\\mu }\\right)}^i \\times {\\left(\\frac{\\mu }{\\lambda +\\mu }\\right)}^{k-1} \\\\\n",
    "  &={\\left({{k}\\atop{i}}\\right)}\\times p^i \\times \\left(1-p\\right)^{k-i}\n",
    "  \\end{align*}$$\n",
    "  where $p=\\left(\\frac{\\lambda }{\\lambda +\\mu }\\right)$<br>\n",
    "  We conclude $(X \\mid Z = k) \\sim \\text{Binom}(k, p),$<br>\n",
    "  Therefore \n",
    "  $$\\mathbf {E}(X \\mid Z = k)=kp=k\\cdot \\frac{\\lambda }{\\lambda +\\mu }$$\n",
    "  We abbreviate above expression as \n",
    "  $$\\mathbf {E}(X \\mid Z )=Z\\cdot \\frac{\\lambda }{\\lambda +\\mu }$$\n",
    "  Notice $\\mathbf {E}(X \\mid Z )$ dependts only on $Z$ not on $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tower rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathit{\\mathbf{E}}\\left\\lbrack \\mathit{\\mathbf{E}}\\left\\lbrack X\\mid Y\\right\\rbrack \\right\\rbrack =\\mathit{\\mathbf{E}}\\left\\lbrack X\\right\\rbrack$<br>\n",
    "Intution on [stack overflow.](https://math.stackexchange.com/a/41543/738892)<br>\n",
    "Proof:\n",
    "\n",
    "$$\\begin{align*}{}\n",
    "\\mathit{\\mathbf{E}}\\left\\lbrack \\mathit{\\mathbf{E}}\\left\\lbrack X\\mid Y\\right\\rbrack \\right\\rbrack &=\\sum_j E\\left\\lbrack X\\mid Y=y_j \\right\\rbrack {\\cdot p}_Y \\left(y_j \\right)\\\\\n",
    "&=\\sum_j \\left(\\sum_i x_i \\cdot p_{x\\mid y} \\left(x_i ,y_j \\right)\\right){\\cdot p}_Y \\left(y_j \\right)\\\\\n",
    "&=\\sum_j \\sum_i x\\cdot p_{x\\mid y} \\left(x_i ,y_j \\right){\\cdot p}_Y \\left(y_j \\right)\\\\\n",
    "&=\\sum_j \\sum_i x_i p\\left(x_i ,y_j \\right)\\\\\n",
    "&=\\sum_i x_i \\sum_j p\\left(x_i ,y_j \\right)\\\\\n",
    "&=\\sum_i x_i p_X \\left(x_i \\right)\\\\\n",
    "&=\\mathit{\\mathbf{E}}\\left\\lbrack X\\right\\rbrack \n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "Below topics are skipped, complete it later:\n",
    "\n",
    "- Conditional variance\n",
    "- Random sums\n",
    "- Moment generating functions\n",
    "- Independent sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concentration Bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov’s inequality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X$ be a __non-negative__ random variable. Then for all $a > 0$\n",
    "reals,\n",
    "$$\\mathit{\\mathbf{P}}\\left\\lbrace X\\ge a\\right\\rbrace \\le \\frac{\\mathit{\\mathbf{E}}\\left\\lbrack X\\right\\rbrack }{a}$$\n",
    "Of course this inequality is useless for $a ≤ \\mathit{\\mathbf{E}}\\left\\lbrack X\\right\\rbrack $.<br>\n",
    "Proof: <br>\n",
    "Let indicator random variable $I$ be defined as \n",
    "$$I = \\begin{cases}\n",
    "   1 &\\text{if } X\\ge a \\\\\n",
    "   0 &\\text{if } X<a\n",
    "\\end{cases}$$\n",
    "Then \n",
    "$$\\begin{align*}{}\n",
    "\\mathit{\\mathbf{E}}\\left\\lbrack I\\right\\rbrack &=1\\cdot \\mathit{\\mathbf{P}}\\left(X\\ge a\\right)+0\\cdot P\\left(X<a\\right)\\\\\n",
    "&=\\mathit{\\mathbf{P}}\\left(X\\ge a\\right)\n",
    "\\end{align*}$$\n",
    "Also if we look at Indicator random variable we can say that $I\\le \\frac{X}{a}$ Because if $X\\ge a$ then $\\frac{X}{a}\\ge 1$, when $0\\le X<a$ then $0\\le\\frac{X}{a}<1$\n",
    "\n",
    "\n",
    "Hence \n",
    "\n",
    "$$\\begin{align*}{}\n",
    "\\mathit{\\mathbf{E}}\\left\\lbrack \\mathrm{I}\\right\\rbrack &\\le \\frac{\\mathit{\\mathbf{E}}\\left\\lbrack X\\right\\rbrack }{a}\\\\\n",
    "\\Rightarrow P\\left\\lbrace X\\ge a\\right\\rbrace &\\le \\frac{\\mathit{\\mathbf{E}}\\left\\lbrack X\\right\\rbrack }{a}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Example <br>\n",
    "  A coin is flipped $n$ times what is the probability of getting $90\\%$ heads.\n",
    "  Using Markov’s inequality\n",
    "  $$P\\left\\lbrace X\\ge 0\\ldotp 9\\right\\rbrace \\le \\frac{\\mathit{\\mathbf{E}}\\left\\lbrack X\\right\\rbrack }{a}=\\frac{\\frac{n}{2}}{0\\ldotp 9n}=\\frac{5}{9}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chebyshev’s inequality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X$ be a random variable with mean $\\mu$ and variance $\\sigma^2$ both\n",
    "finite. Then for all $b > 0$ reals,\n",
    "$$\\mathit{\\mathbf{P}}\\left\\lbrace |X-\\mu |\\ge b\\right\\rbrace \\le \\frac{\\mathbf{Var}\\left(X\\right)}{b^2 }$$\n",
    "Of course this inequality is useless for $b ≤ \\mathbf{SD}X$.<br>\n",
    "Proof:<br>\n",
    "Apply Markov’s inequality on the random variable $(X − \\mu)^2 \\ge 0$\n",
    "\n",
    "$$\\begin{align*}{}\n",
    "\\mathit{\\mathbf{P}}\\left\\lbrace |X-\\mu |\\ge b\\right\\rbrace &=\\mathit{\\mathbf{P}}\\left\\lbrace {\\left(X-\\mu \\right)}^2 \\ge b^2 \\right\\rbrace \\\\\n",
    "&\\le \\frac{\\mathit{\\mathbf{E}}\\left\\lbrack {\\left(X-\\mu \\right)}^2 \\right\\rbrack }{b^2 }=\\frac{\\mathbf{Var}\\left(X\\right)}{b^2 }\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Example <br>\n",
    "  A coin is flipped $n$ times what is the probability of getting $90\\%$ heads.<br>\n",
    "  Chebyshev’s inequality<br>\n",
    "  Consider $X_1,X_2,\\dots,X_n,$ Each $X_i=1$ if the $i^{\\text{th}}$ toss is head and $X_i=0$ if $i^{\\text{th}}$  toss is tail.<br>\n",
    "  $$\\mathit{\\mathbf{E}}\\left\\lbrack X_i \\right\\rbrack =\\frac{1}{2}$$\n",
    "  $$\\mathit{\\mathbf{E}}\\left\\lbrack X_i^2 \\right\\rbrack =\\frac{1}{2}$$\n",
    "  Now we find variance \n",
    "  $$\\mathbf{Var}\\left(X_i \\right)=\\mathit{\\mathbf{E}}\\left\\lbrack X_i^2 \\right\\rbrack -{\\left(\\mathit{\\mathbf{E}}\\left\\lbrack X_i \\right\\rbrack \\right)}^2 =\\frac{1}{2}-\\frac{1}{4}=\\frac{1}{4}$$\n",
    "  Now,\n",
    "  $$\\begin{align*}{}\n",
    "  |X| &\\ge 0\\ldotp 9\\\\\n",
    "  \\Rightarrow |X-\\mu |&\\ge 0\\ldotp 9-\\mu \\\\\n",
    "  \\Rightarrow |X-\\mu |&\\ge 0\\ldotp 9-0\\ldotp 5\\\\\n",
    "  \\Rightarrow |X-\\mu |&\\ge 0\\ldotp 4\n",
    "  \\end{align*}$$\n",
    "  Now can use Chebyshev’s inequality\n",
    "\n",
    "  $$\\begin{align*}{}\n",
    "  \\mathit{\\mathbf{P}}\\left\\lbrace |X-\\mu |  \\ge b\\right\\rbrace &\\le \\frac{\\mathbf{Var}\\left(X\\right)}{b^2 }\\\\\n",
    "  \\mathit{\\mathbf{P}}\\left\\lbrace |X-\\mu | \\ge 0\\ldotp 4\\right\\rbrace &\\le \\frac{\\frac{1}{4}n}{{\\left(0\\ldotp 4n\\right)}^2 }\\\\\n",
    "  &=\\frac{\\frac{1}{4}n}{0\\ldotp 16n^2 }=\\frac{25}{16n}\n",
    "  \\end{align*}$$\n",
    "  So \n",
    "  $$|X|\\ge0.9=\\frac{25}{16n}$$\n",
    "  We can see that Chebyshev’s inequality provides better bound than Markov’s inequality and the bound becomes better as $n$ increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "$\\tiny  {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Subruk, IIT Hyderabad }}}}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b92a85dd111e7f234cbdfd9e3dc75ce12717f6161daa18a82d26f0e8e840c205"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
