{"title":"Probability Theory 3","markdown":{"yaml":{"author":"Abhishek Kumar Dubey","badges":false,"categories":["Probability Theory"],"date":"2022-09-17","description":"Binomial, Poisson","image":"CS6660_images/chrome_pPYR2UCfXR.png","title":"Probability Theory 3","toc":true},"headingText":"Bernoulli, Binomial","containsRefs":false,"markdown":"\n\n\n\n\n- Definition: <br>\nSuppose that $n$ independent trails are performed, each succeeding with probability $p$. Let $X$ count the number of success within the $n$ trails. Then $X$ has the Binomial distribution with parameters  $n$ and $p$ of, in short $X~\\mathrm{Binom}\\left(n,p\\right)$<br>\n$\\displaystyle p\\left(i\\right)=P\\left\\lbrace X=i\\right\\rbrace = {n \\choose i} \\times p^i \\times {\\left(1-p\\right)}^{n-i} ,\\;\\;\\;\\;i=0,1,\\ldotp \\ldotp \\ldotp \\ldotp ,n$<br>\n\n- Binomial : <br><br>\n$\\boxed{\\displaystyle p\\left(i\\right)=P\\left\\lbrace X=i\\right\\rbrace = {n \\choose i} \\times p^i \\times {\\left(1-p\\right)}^{n-i}}$ $\\;\\;\\;\\;i=0,1,\\ldotp \\ldotp \\ldotp \\ldotp ,n$ <br><br>\n$\\displaystyle \\mathrm{EX}=\\mathrm{np},$ and $\\mathrm{VarX}=\\mathrm{np}\\left(1-p\\right)$<br><br>\n- Binomial mass function :<br><br>\n$\\displaystyle \\sum_{i=0}^n p\\left(i\\right)=\\sum_{i=0}^n P\\left\\lbrace X=i\\right\\rbrace = \\sum_{i=0}^n {n \\choose i} \\times p^i \\times {\\left(1-p\\right)}^{n-i}$<br><br>\n\n- If we consider just one trail (event), it is called bernoulli, if we consider more trails (events) it is called binomial. \n- In particular, the Bernoulli $(\\rho)$ variable can take on values 0 or 1, with respective probabilities. <br>\n  $\\rho(1) = p$ and $\\rho(0) = 1-p$\n\n- Remarks : <br>\n $\\displaystyle \\sum_{i=0}^n p\\left(i\\right)=\\sum_{i=0}^n P\\left\\lbrace X=i\\right\\rbrace = \\sum_{i=0}^n {n \\choose i} \\times p^i \\times {\\left(1-p\\right)}^{n-i} =[p+(1-p)]^n=1$<br>\n\n## Expectation  $\\displaystyle EX =np$\n\n- $\\boxed {EX = np}$<br><br>\n    - proof (using trick): <br><br>\n      Trick : $\\displaystyle i=\\frac{d}{dt}{{t}}^{i}{\\Big|}_{t=1}\\,$ <br><br>\n      $\\displaystyle EX =\\sum_{i=0}^n {n \\choose i} \\times i  \\times p^i \\times (1-p)^{n-i}$ <br><br>\n      using the trick we get <br><br>\n      $\\displaystyle EX =\\frac{d}{dt}  \\left(  \\sum_{i=0}^n {n \\choose i} \\times  t^{i}  \\times p^i \\times (1-p)^{n-i}   \\right)  {\\Big|}_{t=1}$ <br><br>\n      $\\displaystyle EX =\\frac{d}{dt}  \\left(  \\sum_{i=0}^n {n \\choose i} \\times  {(tp)}^{i}   \\times (1-p)^{n-i}   \\right)  {\\Big|}_{t=1}$ <br><br>\n      $\\displaystyle EX =\\frac{d}{dt}  \\left( tp+1-p  \\right)^n  {\\Big|}_{t=1}$ <br><br>\n      $\\displaystyle EX =n \\left( tp+1-p  \\right)^{n-1}\\cdot p  {\\Big|}_{t=1}$ <br><br>\n      $\\displaystyle EX =n p$ <br><br> <br><br>     \n    - Proof (normal way) : <br><br>\n      $\\displaystyle EX =\\sum_{i=0}^n {n \\choose i} \\times i  \\times p^i \\times (1-p)^{n-i}$ <br><br>\n      we can start the sum from $1$ as $0^{\\text{th}}$ term will become zero <br><br>\n      $\\displaystyle EX =\\sum_{i=1}^n {n \\choose i} \\times i  \\times p^i \\times (1-p)^{n-i}$ <br><br>\n      we know that <br><br>\n      $\\displaystyle i \\cdot {n \\choose i} = n \\cdot {n-1 \\choose i-1}$<br><br>\n      so we get <br><br>\n      $\\displaystyle EX =\\sum_{i=1}^n  n \\times {n-1 \\choose i-1}  \\times p^i \\times (1-p)^{n-i}$ <br><br>\n      $\\displaystyle EX =np \\sum_{i=1}^n  {n-1 \\choose i-1}  \\times p^{i-1} \\times (1-p)^{n-i}$ <br><br>\n      $\\displaystyle EX =np \\sum_{i=1}^n  {n-1 \\choose i-1}  \\times p^{i-1} \\times (1-p)^{(n-1)-(i-1)}$ <br><br>\n      Consider $n-1=m$ and $i-1=j$, we get <br><br>\n      $\\displaystyle EX =np \\sum_{j=0}^n  {m \\choose j}  \\times p^{j} \\times (1-p)^{m-j}$ <br><br>\n      $\\displaystyle EX =np \\underbrace{\\sum_{j=0}^n  {m \\choose j}  \\times p^{j} \\times (1-p)^{m-j}}_{=1}$ <br><br>\n      $\\displaystyle EX =np$\n      \n     \n\n## Variance  $\\displaystyle \\mathrm{Var} X  =  np(1-p)$\n\n- $\\boxed {\\mathrm{Var} X = np(1-p)}$ <br><br>\n    - proof (using trick): <br>\n      Trick : $\\displaystyle i(i-1)=\\frac{d}{dt^2}{{t}}^{i}{\\Big|}_{t=1}\\,$ <br><br>\n      First find $E[X(X-1)]$<br><br>\n      $\\displaystyle E[X(X-1)] =\\sum_{i=0}^n {n \\choose i} \\times i\\times(i-1)  \\times p^i \\times (1-p)^{n-i}$ <br><br>\n      $\\displaystyle E[X(X-1)] =\\sum_{i=0}^n {n \\choose i} \\times  \\frac{d}{dt^2}{{t}}^{i}{\\Big|}_{t=1}   \\times p^i \\times (1-p)^{n-i} \\;\\;\\;$ We got this using the trick <br><br>\n      $\\displaystyle E[X(X-1)] =\\frac{d}{dt^2} \\left( \\sum_{i=0}^n {n \\choose i} \\times  {{t}}^{i}   \\times p^i \\times (1-p)^{n-i} \\right)  {\\Big|}_{t=1}$  <br><br>\n      $\\displaystyle E[X(X-1)] =\\frac{d}{dt^2} \\left(   tp  + (1-p) \\right)^n  {\\Big|}_{t=1}$  <br><br>\n      $\\displaystyle E[X(X-1)] = n(n-1) \\left(   tp  + 1-p \\right)^{n-2}p\\cdot p  {\\Big|}_{t=1}$  <br><br>\n      $\\displaystyle E[X(X-1)] = n(n-1)p^2$  <br><br>  \n      Now, <br>\n      $\\displaystyle \\mathrm{Var} X  = E(X^2)-(EX)^2$    <br><br>\n      $\\displaystyle \\mathrm{Var} X  = \\left( E(X^2)-E(X) \\right)+ \\left(E(X)- (EX)^2 \\right)$    <br><br>\n      $\\displaystyle \\mathrm{Var} X  =  E\\left[X^2-X\\right] + \\left(E(X)- (EX)^2 \\right)$    <br><br>\n      $\\displaystyle \\mathrm{Var} X  =  E\\left[ X(X-1) \\right] + \\left(E(X)- (EX)^2 \\right)$    <br><br>\n      $\\displaystyle \\mathrm{Var} X  =  n(n-1)p^2 + np- (np)^2$    <br><br>\n      $\\displaystyle \\mathrm{Var} X  =  (np)^2-np^2 + np- (np)^2$    <br><br>\n      $\\displaystyle \\mathrm{Var} X  =  -np^2 + np$    <br><br>\n      $\\displaystyle \\mathrm{Var} X  =  np(1-p)$    <br><br><br><br>\n    - Proof (normal way) : <br><br>\n      $\\displaystyle \\mathrm{Var} X  = E(X^2)-(EX)^2$    <br><br>\n      First we find $E(X^2)$<br><br>\n      $\\displaystyle E(X^2) =\\sum_{i=0}^n {n \\choose i} \\times i^2  \\times p^i \\times (1-p)^{n-i}$ <br><br>\n      we can start the sum from $1$ as $0^{\\text{th}}$ term will become zero <br><br>\n      $\\displaystyle E(X^2) =\\sum_{i=1}^n {n \\choose i} \\times i^2  \\times p^i \\times (1-p)^{n-i}$ <br><br>\n      we know that <br><br>\n      $\\displaystyle i \\cdot i \\cdot {n \\choose i} = n \\cdot i \\cdot{n-1 \\choose i-1}$<br><br>     \n      so we get <br><br> \n      $\\displaystyle E(X^2) =\\sum_{i=1}^n n \\cdot i \\cdot{n-1 \\choose i-1}   \\times p^i \\times (1-p)^{n-i}$ <br><br>\n      $\\displaystyle E(X^2) =n \\cdot p \\sum_{i=1}^n  i \\cdot{n-1 \\choose i-1}   \\times p^{i-1} \\times (1-p)^{n-i}$ <br><br>\n      $\\displaystyle E(X^2) =n \\cdot p \\sum_{i=1}^n  i \\cdot{n-1 \\choose i-1}   \\times p^{i-1} \\times (1-p)^{(n-1)-(i-1)}$ <br><br>\n      Consider $n-1=m$ and $i-1=j$, we get <br><br>\n      $\\displaystyle E(X^2) =n \\cdot p \\sum_{j=0}^n  \\left( j+1 \\right) \\cdot{m \\choose j}   \\times p^{j} \\times (1-p)^{m-j}$ <br><br>\n      $\\displaystyle E(X^2) =n \\cdot p \\underbrace{ \\sum_{j=0}^n  j \\cdot{m \\choose j}   \\times p^{j} \\times (1-p)^{m-j} }_{\\text{same as we did in expectation,so it's }mp}   \\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; +n \\cdot p \\underbrace{\\sum_{j=0}^n {m \\choose j}   \\times p^{j} \\times (1-p)^{m-j}}_{\\text{sum of all probability  }=1}$ <br><br>\n      $\\displaystyle E(X^2) =n \\cdot p \\times m\\cdot p+n \\cdot p$<br><br>\n      $\\displaystyle E(X^2) =n \\cdot p \\times (n-1)\\cdot p+ n \\cdot p$<br><br>\n      $\\displaystyle E(X^2) =(np)^2-np^2+np$<br><br>\n      $\\displaystyle E(X^2) =(np)^2+np(1-p)$<br><br>   \n      Now, <br><br>\n      $\\displaystyle \\mathrm{Var} X  = E(X^2) - (EX)^2$<br><br> \n      $\\displaystyle \\mathrm{Var} X  = (np)^2+np(1-p) -(np)^2$    <br><br> \n      $\\displaystyle \\mathrm{Var} X  = np(1-p)$    <br><br> \n    - Proof (yet another way, considering events are independent):<br><br>\n      if $Y$ is just a Bernoulli trail, <br>\n      $Y=\\left\\lbrace \\begin{array}{ll}\n      1, & \\mathrm{with}\\;\\mathrm{probability}\\;P\\\\\n      0, & \\mathrm{with}\\;\\mathrm{probability}\\;1-P \n      \\end{array}\\right.$<br>   \n      $\\mathrm{VarY}={\\mathrm{EY}}^2 -{\\left(\\mathrm{EY}\\right)}^2 =\\left(1^2 \\cdot P +0^2 \\cdot (1-P) \\right)-P^2 \n      \\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=P \\cdot \\left(1-P \\right)$<br>    \n      Let $X$ be multiple copies of independent $Y$\n      If all $Y$ are independent the we can say <br>\n      $\\displaystyle \\mathrm{Var}\\left( \\sum_{i=1}^n Y_i \\right) =  \\sum_{i=1}^n  \\mathrm{Var}\\left( Y_i \\right)$<br>\n      Proof of above statement:<br>\n      Consider only $2$ Events $Y_1$ and $Y_2$ for simplicity<br>\n      $\\displaystyle \\mathrm{Var}(Y_1+Y_2) = E\\left[(Y_1+Y_2)^2 \\right] - \\left(E[Y_1+Y_2]\\right)^2$<br>\n      $\\displaystyle  = E\\left[Y_1^2 + 2Y_1Y_2 + Y_2^2 \\right] - \\left((EY_1)^2+2(EY_1)(EY_2)+(EY_2)^2\\right)$<br>\n      $\\displaystyle  = E(Y_1^2) + 2E(Y_1Y_2) + E(Y_2^2)  -(EY_1)^2-2(EY_1)(EY_2)-(EY_2)^2$<br>\n      $\\displaystyle  = E(Y_1^2) -(EY_1)^2  + E(Y_2^2)-(EY_2)^2 + 2(EY_1)(EY_2)  -2(EY_1)(EY_2)$<br>\n      $\\displaystyle  =\\mathrm{Var}Y_1+\\mathrm{Var}Y_2$<br>\n      In Above proof  we used $2E(Y_1Y_2)=2(EY_1)(EY_2)$<br>\n      We can do that if the 2 events are independent<br>\n      Proof of the same:<br>\n      $\\displaystyle  E(Y_1Y_2)= \\sum_{Y_1}\\sum_{Y_2} Y_1 Y_2 P(Y_1=y_1,Y_2=y_2)$<br>\n      $\\displaystyle  E(Y_1Y_2)= \\sum_{Y_1}\\sum_{Y_2} Y_1 Y_2 P(Y_1=y_1)P(Y_2=y_2)\\;\\;$ as $Y_1$ and $Y2$ are independent <br>\n      $\\displaystyle  E(Y_1Y_2)= \\sum_{Y_1}Y_1 P(Y_1=y_1) \\sum_{Y_2}  Y_2 P(Y_2=y_2)$ <br>\n      $\\displaystyle  E(Y_1Y_2)= E(Y_1) E(Y_2)$ <br><br>\n      Hence we can say,<br><br>\n      $\\displaystyle \\mathrm{Var}X = \\mathrm{Var}\\left( \\sum_{i=1}^n Y_i \\right) =  \\sum_{i=1}^n  \\mathrm{Var}\\left( Y_i \\right) = np(1-p) \\;\\;\\;$ Considering all $Y_i$ has variance of $p(1-p)$ <br>\n\n# Poisson  <br><br>\n\n- Definition <br>\nFix a positive real number $\\lambda$. The random variable $X$ is Poisson distributed with parameter $\\lambda$, in short $X ∼ \\mathrm{Poi}(\\lambda)$, if it is non-negative integer valued, and its mass function is <br><br>\n$\\displaystyle \\rho (i) = P\\left\\{ X=i \\right\\} = \\frac{\\lambda^i}{i!} e^{-\\lambda}\\;\\;\\;\\;\\;i=0,1,2,\\dots$  <br><br>\n\n- Take $Y \\sim \\mathrm{Binom}(n, p)$ with large $n$, small $p$, such that $np \\simeq \\lambda$. Then $Y$ is approximately $\\mathrm{Poisson}(\\lambda)$ distributed.\n\n- Proof of Poisson formula <br>\nBinomial formula:<br>\n$\\displaystyle p\\left(i\\right)=P\\left\\lbrace X=i\\right\\rbrace = {n \\choose i} \\times p^i \\times {\\left(1-p\\right)}^{n-i}$<br><br>\nwe say that poisson is approximation of binomial when $n \\rightarrow \\infty$, and $\\lambda =np \\Rightarrow p=\\frac{\\lambda}{n}$ so we get<br><br>\n$\\displaystyle \\rho \\left(i\\right) = \\lim_{n \\rightarrow \\infty}{n \\choose i} \\times \\left( \\frac{\\lambda}{n} \\right)^i \\times {\\left( 1-\\frac{\\lambda}{n} \\right)}^{n-i}$<br><br>\n$\\displaystyle \\rho \\left(i\\right) = \\lim_{n \\rightarrow \\infty} \\frac{n!}{i!(n-i)!}\\times \\left( \\frac{\\lambda}{n} \\right)^i \\times {\\left( 1-\\frac{\\lambda}{n} \\right)}^{n-i}$<br><br>\n$\\displaystyle \\rho \\left(i\\right) = \\lim_{n \\rightarrow \\infty} \\frac{n \\cdot (n-1)\\cdot (n-2) \\dots \\cdot (n-i+1)!}{i!} \\times \\left( \\frac{\\lambda}{n} \\right)^i \\times {\\left( 1-\\frac{\\lambda}{n} \\right)}^{n-i}$<br><br>\n$\\displaystyle \\rho \\left(i\\right) = \\lim_{n \\rightarrow \\infty} \\frac{n \\cdot (n-1)\\cdot (n-2) \\dots \\cdot (n-i+1)!}{i!} \\times \\left( \\frac{\\lambda}{n} \\right)^i \\times {\\left( 1-\\frac{\\lambda}{n} \\right)}^{n} {\\left( 1-\\frac{\\lambda}{n} \\right)}^{i}$<br><br>\n$\\displaystyle \\rho \\left(i\\right) = \\frac{\\lambda^i}{i!}  \\lim_{n \\rightarrow \\infty} \\frac{\\overbrace{n \\cdot (n-1)\\cdot (n-2) \\dots \\cdot (n-i+1)!}^{i\\;\\;\\text{ terms}} }{n^i}  \\times {\\left( 1-\\frac{\\lambda}{n} \\right)}^{n} {\\left( 1-\\frac{\\lambda}{n} \\right)}^{i}$<br><br>\n$\\displaystyle \\rho \\left(i\\right) = \\frac{\\lambda^i}{i!}  \\lim_{n \\rightarrow \\infty} \\frac{n^i +\\dots}{n^i}  \\times  \\lim_{n \\rightarrow \\infty}{\\left( 1-\\frac{\\lambda}{n} \\right)}^{n} \\times  \\lim_{n \\rightarrow \\infty}{\\left( 1-\\frac{\\lambda}{n} \\right)}^{i}$<br><br>\nWe know that $\\displaystyle \\;\\; \\lim_{n \\rightarrow \\infty}{\\left( 1+\\frac{a}{x} \\right)}^{x} = e^a$<br><br>\n$\\displaystyle \\rho \\left(i\\right) = \\frac{\\lambda^i}{i!}  \\underbrace{\\lim_{n \\rightarrow \\infty} \\frac{n^i +\\dots}{n^i}}_{=1}   \\times  \\underbrace{\\lim_{n \\rightarrow \\infty}{\\left( 1-\\frac{\\lambda}{n} \\right)}^{n}}_{e^{-\\lambda}}  \\times  \\underbrace{\\lim_{n \\rightarrow \\infty}{\\left( 1-\\frac{\\lambda}{n} \\right)}^{i}}_{=1,\\;i \\text{ is very small } }$<br><br>\n$\\boxed{\\displaystyle \\rho \\left(i\\right) = \\frac{\\lambda^i}{i!} e^{-\\lambda}}$\n\n## Expectation  $\\displaystyle EX =\\lambda$\n\n- $\\boxed {EX = \\lambda}$<br><br>\n  $EX = \\displaystyle \\sum_{i=0}^\\infty i \\times \\rho(i) = \\sum_{i=0}^\\infty i \\times \\frac{\\lambda^i}{i!} e^{-\\lambda}$<br><br>\n  $EX = \\displaystyle  \\sum_{i=1}^\\infty i \\times \\frac{\\lambda^i}{i!} e^{-\\lambda}$<br><br>\n  $EX = \\displaystyle  \\lambda \\sum_{i=1}^\\infty i \\times \\frac{\\lambda^{i-1}}{i!} e^{-\\lambda}$<br><br>\n  $EX = \\displaystyle  \\lambda \\sum_{i=1}^\\infty \\frac{\\lambda^{i-1}}{(i-1)!} e^{-\\lambda}$<br><br>\n  consider $i-1=j$ <br><br>\n  $EX = \\displaystyle  \\lambda \\underbrace{\\sum_{j=0}^\\infty \\frac{\\lambda^{j}}{j!} e^{-\\lambda}}_{=1}$<br><br>  \n  $EX = \\displaystyle  \\lambda$<br><br>  \n\n\n## Variance  $\\displaystyle \\mathrm{Var} X  = \\lambda$\n\n- $\\boxed {\\mathrm{Var} X = \\lambda}$ <br><br>\n  $\\displaystyle E\\left[X(X-1)\\right]=\\sum_{i=0}^n i(i-1)\\rho(i)$ <br><br>\n  $\\displaystyle E\\left[X(X-1)\\right]=\\sum_{i=0}^n i(i-1) \\frac{\\lambda ^i}{i!}e^{-\\lambda}$ <br><br>\n  $\\displaystyle E\\left[X(X-1)\\right]=\\sum_{i=2}^n i(i-1) \\frac{\\lambda ^i}{i!}e^{-\\lambda}$ <br><br>\n  $\\displaystyle E\\left[X(X-1)\\right]=\\sum_{i=2}^n  \\frac{\\lambda ^i}{(i-2)!}e^{-\\lambda}$ <br><br>\n  $\\displaystyle E\\left[X(X-1)\\right]= \\lambda ^2\\sum_{i=2}^n  \\frac{\\lambda ^{i-2}}{(i-2)!}e^{-\\lambda}$ <br><br>\n  Consder $i-2=j$ <br><br>\n  $\\displaystyle E\\left[X(X-1)\\right]= \\lambda ^2\\underbrace{\\sum_{j=0}^n  \\frac{\\lambda ^{j}}{j!}e^{-\\lambda}}_{=1}$ <br><br>\n  $\\displaystyle E\\left[X(X-1)\\right]= \\lambda ^2$  <br><br>\n  Now, <br><br>\n  $\\displaystyle \\mathrm{Var}X = E(X^2)-(EX)^2$<br><br>\n  $\\displaystyle \\mathrm{Var}X = E\\left[X(X-1)\\right] +EX-(EX)^2$<br><br>\n  $\\displaystyle \\mathrm{Var}X = \\lambda^2 +\\lambda-\\lambda^2$<br><br>\n  $\\displaystyle \\mathrm{Var}X = \\lambda$<br><br>  \n\n# Geometric\n\n- Definition <br>\nSuppose that independent trials, each succeeding with probability $p$, are repeated until the first success. The total number $X$ of trials made has the $\\mathrm{Geometric}(p)$ distribution (in short, $X \\sim \\mathrm{Geom}(p)$). <br>\n$X$ can take on positive integers, with probabilities <br><br>\n$\\boxed{ \\displaystyle  \\rho(i) = (1-p)^{i-1} \\cdot p}$ $\\;\\;\\;\\;\\; i=1,2,3,\\dots$<br><br>\nThat is a function, we verify by $\\rho(i) \\ge 0$ and <br><br>\n$\\displaystyle \\sum_{i=1}^\\infty \\rho(i) = \\sum_{i=1}^\\infty (1-p)^{i-1} \\cdot p = \\frac{p}{1-(1-p)}=1$<br><br>\n- Remarks <br>\nFor a Geometric$(p)$ random variable and any $k \\ge 1$ we have <br>\n$P \\left\\{ X \\ge k \\right\\}=(1-p)^{k-1}$ (We have at least $k-1$ failures).\n- Corollary <br>\nThe Geometric random variable is (discrete) memory-less :for every $k \\ge 1 , n \\ge 0$<br><br>\n$\\displaystyle  P \\left\\{ X \\ge n + k \\mid X>n  \\right\\}=P \\left\\{ X \\ge k \\right\\}$<br><br>\nProof : <br>\n$\\displaystyle  P \\left\\{ X \\ge n + k \\mid X>n  \\right\\}= \\frac {P\\left\\{ X \\ge n + k \\right\\} \\cap P\\left\\{X>n  \\right\\}}{P\\left\\{X>n  \\right\\}}$<br>\nif $X \\ge n+k$ then $X$ is also greater than $n$ as $k$ is at least $1$<br>\n$\\displaystyle  P \\left\\{ X \\ge n + k \\mid X>n  \\right\\}= \\frac {P\\left\\{ X \\ge n + k \\right\\} }{P\\left\\{X>n  \\right\\}}$<br>\n$\\displaystyle  P \\left\\{ X \\ge n + k \\mid X>n  \\right\\}= \\frac {(1-p)^{n+k-1} }{(1-p)^n}= (1-p)^{k-1}=P \\left\\{ X>k  \\right\\}$<br>\n\n## Expectation  $\\displaystyle EX =\\frac{1}{p}$\n\n- $\\boxed {EX = \\frac{1}{p}}$<br><br>\n    - proof (using trick): <br><br>\n      Trick : $\\displaystyle i=\\frac{d}{dt}{{t}}^{i}{\\Big|}_{t=1}\\,$ <br><br>\n      $\\displaystyle EX =  \\sum_{i=1}^\\infty i \\cdot  (1-p)^{i-1} \\cdot p$ <br><br>\n      $\\displaystyle EX =  \\sum_{i=0}^\\infty \\frac{d}{dt}{t}^{i}{\\Big|}_{t=1}   (1-p)^{i-1} \\cdot p$ <br><br>\n      $\\displaystyle EX =  \\frac{d}{dt} \\left( \\sum_{i=0}^\\infty {t}^{i}   (1-p)^{i-1} \\cdot p \\right)  {\\Bigg|}_{t=1}$<br><br>\n      $\\displaystyle EX =  \\frac{d}{dt} \\left( \\frac{p}{1-p}\\sum_{i=0}^\\infty {t}^{i}   (1-p)^{i}  \\right)  {\\Bigg|}_{t=1}$<br><br>\n      $\\displaystyle EX =  \\frac{p}{1-p} \\cdot \\frac{d}{dt} \\left(   \\frac{1}{1-t(1-p)}   \\right)  {\\Bigg|}_{t=1}$<br><br>\n      $\\displaystyle EX =  \\frac{p}{1-p} \\cdot  \\left(   \\frac{1-p}{\\left(1-t(1-p)\\right)^2}   \\right)  {\\Bigg|}_{t=1}$<br><br>\n      $\\displaystyle EX =  \\frac{1}{p}$\n    - Proof (normal way):<br><br>\n      $\\displaystyle EX =  \\sum_{i=1}^\\infty i \\cdot  (1-p)^{i-1} \\cdot p$ <br><br>\n\n      ::: column-screen-inset\n      $$\\begin{align*}{}\n       EX &=   p + 2 \\cdot (1-p)\\cdot p + 3 \\cdot (1-p)^2 \\cdot p+\\cdots  &   \\qquad (1) \\\\\n       EX(1-p) &=  \\qquad\\;\\;\\; (1-p)\\cdot p + 2 \\cdot (1-p)^2\\cdot p + 3 \\cdot (1-p)^3 \\cdot p +\\cdots &  \\qquad (2) \n      \\end{align*}$$\n      :::\n      \n      Subtracting equation $(1)$ from $(2)$ we get <br><br>\n      $\\displaystyle p \\cdot EX = p +  (1-p)\\cdot p + (1-p)^2 \\cdot p +\\cdots$<br><br>\n      $\\displaystyle  EX = 1 +  (1-p) + (1-p)^2  +\\cdots$<br><br>\n      $\\displaystyle EX = \\frac{1}{1-(1-p)}$<br><br>\n      $\\displaystyle EX = \\frac{1}{p}$\n      \n\n## Variance  $\\displaystyle \\mathrm{Var} X  = \\frac{1-p}{p^2}$\n\n- $\\boxed {\\mathrm{Var} X = \\frac{1-p}{p^2}}$ <br><br>\n    - Proof (using trick);<br><br>\n      Trick : $\\displaystyle i(i-1)=\\frac{d}{dt^2}{{t}}^{i}{\\Big|}_{t=1}\\,$ <br><br>\n      First find $E[X(X-1)]$<br><br>\n      $\\displaystyle E[X(X-1)] =\\sum_{i=1}^\\infty  i\\times(i-1)   \\times (1-p)^{i-1} \\times p$ <br><br>    \n      $\\displaystyle E[X(X-1)] =\\sum_{i=1}^\\infty  \\frac{d}{dt^2}{t}^{i}{\\Big|}_{t=1}   \\times (1-p)^{i-1} \\times p$ <br><br>    \n      $\\displaystyle E[X(X-1)] =p \\frac{d}{dt^2} \\left( \\sum_{i=1}^\\infty{t}^{i}(1-p)^{i-1} \\right) {\\Bigg|}_{t=1}$ <br><br>  \n      $\\displaystyle E[X(X-1)] = \\frac{p}{1-p} \\cdot \\frac{d}{dt^2} \\left( \\sum_{i=1}^\\infty{t}^{i}(1-p)^{i} \\right) {\\Bigg|}_{t=1}$ <br><br>  \n      $\\displaystyle E[X(X-1)] = \\frac{p}{1-p} \\cdot \\frac{d}{dt^2} \\left( \\frac{1}{1-t(1-p)} \\right) {\\Bigg|}_{t=1}$ <br><br> \n      $\\displaystyle E[X(X-1)] = \\frac{p}{1-p} \\cdot  \\left( \\frac{2(1-p)(1-p)}{(\\left( 1-t(1-p) \\right)^3 } \\right) {\\Bigg|}_{t=1}$ <br><br> \n      $\\displaystyle E[X(X-1)] = \\frac{p}{1-p} \\cdot  \\left( \\frac{2(1-p)(1-p)}{p^3 } \\right)$ <br><br> \n      $\\displaystyle E[X(X-1)] =  \\left( \\frac{2(1-p)}{p^2 } \\right)$ <br><br>\n      Now, <br><br>\n      $\\displaystyle \\mathrm{Var}X = E(X^2)-(EX)^2$<br><br>\n      $\\displaystyle \\mathrm{Var}X = E\\left[X(X-1)\\right] +EX-(EX)^2$<br><br>   \n      $\\displaystyle \\mathrm{Var}X = \\left( \\frac{2(1-p)}{p^2 } \\right) + \\frac{1}{p} -\\left( \\frac{1}{p} \\right)^2$<br><br>    \n      $\\displaystyle \\mathrm{Var}X = \\left( \\frac{2-2p}{p^2 } \\right) + \\frac{1}{p} -\\left( \\frac{1}{p} \\right)^2$<br><br> \n      $\\displaystyle \\mathrm{Var}X = \\frac{2-2p+p-1}{p^2}$<br><br> \n      $\\displaystyle \\mathrm{Var}X = \\frac{1-p}{p^2}$<br><br> \n    - proof (normal way):<br><br> \n      we first solve for $E(X^2)$<br><br> \n      $\\displaystyle  E(X^2) = \\left( \\sum_{i=1}^\\infty  i^2   \\times (1-p)^{i-1} \\times p \\right)$ <br><br> \n\n      ::: column-screen-inset\n      $$\\begin{align*}{}\n        E(X^2) &=   p + 4 \\cdot (1-p) \\cdot p + 9 \\cdot (1-p)^2 \\cdot p + \\cdots  &     (3) \\\\\n        (1-p) E(X^2) &=  \\qquad\\;\\;\\; \\;(1-p) \\cdot p + 4 \\cdot (1-p)^2 \\cdot p + 9 \\cdot (1-p)^3 \\cdot p + \\cdots  &    (4) \n      \\end{align*}$$\n      :::\n\n       <br><br>\n      subtracting eqution $(4)$ from equation $(3)$     <br><br>  \n      $\\displaystyle E(X^2) = p+3 \\cdot (1-p) + 5 \\cdot (1-p)^2 + 7 \\cdot (1-p)^3 + \\cdots$<br><br> \n      This is AGP, the sum is given by $s_{\\infty}=\\frac{a}{1-r}+\\frac{dr}{(1-r)^2}, \\;\\;\\;\\;$ for $r<1$<br><br> \n      In our case $r=(1-p),\\;a=1,\\;d=2\\;$ so we get, <br><br> \n      $\\displaystyle E(X^2) =\\frac{1}{1-(1-p)}+\\frac{2\\cdot (1-p)}{(1-(1-p))^2}$<br><br> \n      $\\displaystyle E(X^2) =\\frac{1}{p}+\\frac{2-2p}{p^2}$<br><br> \n      $\\displaystyle E(X^2) =\\frac{p+2-2p}{p^2}$<br><br> \n      $\\displaystyle E(X^2) =\\frac{2-p}{p^2}$<br><br> \n      $\\displaystyle  \\mathrm{Var}X = E(X^2) - (EX)^2$ <br><br> \n      $\\displaystyle  \\mathrm{Var}X = \\frac{2-p}{p^2} - \\left(\\frac{1}{p}\\right)^2$ <br><br> \n      $\\displaystyle  \\mathrm{Var}X = \\frac{1-p}{p^2}$ <br><br> \n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"center","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"css":["../../styles.css"],"html-math-method":"katex","output-file":"2022-09-17-CS6660-week4_1.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","comments":{"giscus":{"repo":"abhiyantaabhishek/IITH-Data-Science","category":"Announcements"}},"toc-location":"right","theme":"cosmo","title-block-banner":true,"author":"Abhishek Kumar Dubey","badges":false,"categories":["Probability Theory"],"date":"2022-09-17","description":"Binomial, Poisson","image":"CS6660_images/chrome_pPYR2UCfXR.png","title":"Probability Theory 3"},"extensions":{"book":{"multiFile":true}}}}}