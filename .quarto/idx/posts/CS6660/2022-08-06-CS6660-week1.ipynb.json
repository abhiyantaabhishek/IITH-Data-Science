{"title":"Probability Theory  1","markdown":{"yaml":{"author":"Abhishek Kumar Dubey","badges":false,"categories":["Probability Theory"],"date":"2022-08-06","description":"Basics, Bayes Theorem","image":"CS6660_images/2022-08-06-CS6660-week1-preview.jpg","title":"Probability Theory  1","toc":true},"headingText":"Basics","containsRefs":false,"markdown":"\n\n\n\n\n\n### The Union and intersection\nThe union $E\\cup F\\;$ of events $E$ and $F$ always means $E$ OR $F$ , The intersection $E\\cap F$ of events $E$ and $F$ always means $E$ AND $F$<br>\n\n:::{.callout-tip}\n\nThe union $\\bigcup_i E_{i\\;}$ of events $E_{i\\;\\;}$ always means at least one of the $E_i$'s, The intersection $\\bigcap_i E_i$ of events $E_{i\\;}$ always means each of the $E_i$'s\n\n:::\n\n### Complementary Events\nThe complement of an event is  $E^c =\\bar{E} =E^* :=\\Omega -E$\n\n### Simple Properties of events\n- commutativity: <br>\n$\\begin{array}{l}\nE\\cup F=F\\cup E\\\\\nE\\cap F=F\\cap E\n\\end{array}$\n- Associativity: <br>\n$\\begin{array}{l}\nE\\cup \\left(F\\cup G\\right)=\\left(E\\cup F\\right)\\cup G=E\\cup F\\cup G\\\\\nE\\cap \\left(F\\cap G\\right)=\\left(E\\cap F\\right)\\cap G=E\\cap F\\cap G\n\\end{array}$\n- Distributivity: <br>\n$\\begin{array}{l}\n\\left(E\\cup F\\right)\\cap G=\\left(E\\cap G\\right)\\cup \\left(F\\cap G\\right)\\\\\n\\left(E\\cap F\\right)\\cup G=\\left(E\\cup G\\right)\\cap \\left(F\\cup G\\right)\\;\n\\end{array}$\n- De Morgan's Law: <br>\n$\\begin{array}{l}\n{\\left(E\\cup F\\right)}^c =E^{c\\;} \\cap F^{c\\;} \\\\\n{\\left(E\\cap F\\right)}^{c\\;} =E^c \\cup F^c \n\\end{array}$ Similarly\n$\\begin{array}{l}\n{\\left({\\bigcup_{\\;\\;} }_i E_{i\\;} \\right)}^c ={\\bigcap_{\\;} }_i E_{i\\;}^{c\\;} \\\\\n{\\left({\\bigcap_{\\;} }_i E_{i\\;} \\right)}^{c\\;} ={\\bigcup_{\\;} }_i E_i^{c\\;} \n\\end{array}$\n\n### Definition (axioms of probability)\nThe probability $P$ on a sample space $\\Omega$ assigns  numbers to events $\\Omega$ of in such a way that\n1. The probability of any event is non-negative : $P\\left\\lbrace E\\right\\rbrace \\ge 0$\n2. The probability if the sample space is one : $P\\left\\lbrace \\Omega \\;\\right\\rbrace =1$\n3. For any finitely or countably infinitely many manually exclusive events $E_{1,} E_2 ,\\ldotp \\ldotp \\ldotp$ , \n$P\\left\\lbrace {\\bigcup_{i\\;} E_{i\\;} }_{\\;} \\right\\rbrace =\\sum_i P\\left\\lbrace E_i \\right\\rbrace \\;$\n\n### A few simple facts   \nInclusion-exclusion principle: <br>\n- For any events $E$ and $F$, $P\\left\\lbrace E\\cup F\\right\\rbrace =P\\left\\lbrace E\\right\\rbrace +P\\left\\lbrace F\\right\\rbrace -P\\left\\lbrace E\\cap F\\right\\rbrace$ <br>\n- For any events $E$,  $F$ and $G$:<br> \n$P\\left\\lbrace E\\cup F\\cup G\\right\\rbrace =P\\left\\lbrace E\\right\\rbrace +P\\left\\lbrace F\\right\\rbrace +P\\left\\lbrace G\\right\\rbrace -P\\left\\lbrace E\\cap F\\right\\rbrace -P\\left\\lbrace E\\cap G\\right\\rbrace -P\\left\\lbrace F\\cap G\\right\\rbrace +P\\left\\lbrace E\\cap F\\cap G\\right\\rbrace$<br>\n- Generally:<br>\n$p\\left\\lbrace E_1 \\cup E_2 \\cup E_3 \\cup \\ldotp \\ldotp \\ldotp \\cup E_n \\right\\rbrace =\\sum_{1\\le i\\le n} P\\left\\lbrace E_i \\right\\rbrace -\\sum_{1\\le i_1 \\le i_2 \\le n} P\\left\\lbrace E_{i_1 } \\cap E_{i_2 } \\right\\rbrace +\\sum_{1\\le i_1 \\le i_2 \\le i_{3\\;} \\le n} \\left\\lbrace P\\left\\lbrace E_{i_1 } \\cap E_{i_2 } \\cap E_{i_3 } \\right\\rbrace \\right\\rbrace -\\ldotp \\ldotp \\ldotp \\;+{\\left(-1\\right)}^{n+1} P\\left\\lbrace E_1 \\cap E_2 \\cap E_3 \\right\\rbrace$\n\n### Boole's inequality\n - For any events $E_1 ,E_2 ,\\ldotp \\ldotp \\ldotp E_n$<br>\n $P\\left\\lbrace \\bigcup_{i=1}^n E_i \\right\\rbrace \\le \\sum_{i=1}^n P\\left\\lbrace E_i \\right\\rbrace$<br>\n <br>\n\n\n### Example\nOut of $n$  people, what is the probability that there are no coinciding birthdays?\n$|\\Omega |={365}^n$<br>\n$|E|=365\\ldotp 364\\ldotp \\ldotp \\ldotp \\left(365-n+1\\right)=\\frac{365!}{\\left(365-n\\right)!}$ <br>\n$P\\left\\lbrace E\\right\\rbrace =\\frac{|E|}{|\\Omega |}=\\frac{365!}{\\left(365-n\\right)!{365}^n }$\n\n### Conditional Probability\nLet $F$ be an Event with $P\\left\\lbrace F\\right\\rbrace >0$ . then the conditional probability $E$ of given $F$  is defined  as:\n$P\\left\\lbrace E|F\\right\\rbrace :=\\frac{P\\left\\lbrace E\\cap F\\right\\rbrace }{P\\left\\lbrace F\\right\\rbrace }$ <br>\n\n:::{.callout-note}\n\nConditional Probability can be interpreted as:<br>\"In what proportion of case in $F$ will also $E$ occur?\" or \"How does the probability of both $E$ and $F$ compare to the probability  of $F$ only?\"<br>\n\n:::\n\n<br>\nconditional probability is a proper probability and it satisfies  the axioms:<br>\n\n1. The conditional probability of any event is non-negative  :$P\\left\\lbrace E|F\\right\\rbrace \\ge 0$\n2. The conditional probability if the sample space is one :$P\\left\\lbrace \\Omega |F\\;\\right\\rbrace =1$\n3. For any finitely or countably infinitely many manually exclusive events $E_{1,} E_2 ,\\ldotp \\ldotp \\ldotp$ , $P\\left\\lbrace {\\bigcup_{i\\;} E_{i\\;} |F}_{\\;} \\right\\rbrace =\\sum_i P\\left\\lbrace E_i |F\\right\\rbrace \\;$\n\n### Corollary\n- $P\\left\\lbrace E^c |F\\right\\rbrace =1-P\\left\\lbrace E|F\\right\\rbrace$\n- $P\\left\\lbrace \\phi |F\\right\\rbrace =0$\n- $P\\left\\lbrace E|F\\right\\rbrace =1-P\\left\\lbrace E^c |F\\right\\rbrace \\le 1$\n- $P\\left\\lbrace \\left(E\\cup G\\right)|F\\right\\rbrace =P\\left\\lbrace E|F\\right\\rbrace +P\\left\\lbrace G|F\\right\\rbrace -P\\left\\lbrace E\\cap F|F\\right\\rbrace$<br>\nproof:<br>\n$\\begin{array}{l}\nP\\left\\lbrace \\left(E\\cup G\\right)|F\\right\\rbrace =\\frac{P\\left\\lbrace \\left(E\\cup G\\right)\\cap F\\right\\rbrace }{P\\left\\lbrace F\\right\\rbrace }=\\frac{P\\left\\lbrace \\left(E\\cap F\\right)\\cup \\left(G\\cap F\\right)\\right\\rbrace }{P\\left\\lbrace F\\right\\rbrace }\\\\\n=\\frac{P\\left(E\\cap F\\right)+P\\left(G\\cap F\\right)-P\\left\\lbrace \\left(E\\cap F\\right)\\cap \\;\\left(G\\cap F\\right)\\right\\rbrace }{P\\left\\lbrace F\\right\\rbrace }=\\frac{P\\left(E\\cap F\\right)+P\\left(G\\cap F\\right)-P\\left\\lbrace E\\cap G\\cap F\\right\\rbrace }{P\\left\\lbrace F\\right\\rbrace }\\\\\n=P\\left\\lbrace E|F\\right\\rbrace +P\\left\\lbrace G|F\\right\\rbrace -P\\left\\lbrace E\\cap F|F\\right\\rbrace \n\\end{array}$\n- if $E\\subseteq G$ then $P\\left\\lbrace \\left(G-E\\right)|F\\right\\rbrace =P\\left\\lbrace G|F\\right\\rbrace -P\\left\\lbrace E|F\\right\\rbrace$<br>\nproof:<br>\n$\\begin{array}{l}\nP\\left\\lbrace G|F\\right\\rbrace -P\\left\\lbrace E|F\\right\\rbrace =\\frac{\\;P\\left\\lbrace G\\cap F\\right\\rbrace }{P\\left\\lbrace \\mathrm{F}\\right\\rbrace }-\\frac{\\;P\\left\\lbrace E\\cap F\\right\\rbrace }{P\\left\\lbrace \\mathrm{F}\\right\\rbrace }\\\\\n=\\frac{P\\left\\lbrace \\left(G\\cap F\\right)-\\left(E\\cap F\\right)\\right\\rbrace \\;}{P\\left\\lbrace \\mathrm{F}\\right\\rbrace }=\\frac{P\\left\\lbrace \\left(G\\cap F\\right)\\cap {\\left(E\\cap F\\right)}^c \\right\\rbrace \\;}{P\\left\\lbrace \\mathrm{F}\\right\\rbrace }\\\\\n=\\frac{P\\left\\lbrace \\left(G\\cap F\\right)\\cap {\\left(E^{c\\;} \\cup F^{c\\;} \\right)}^{\\;} \\right\\rbrace \\;}{P\\left\\lbrace \\mathrm{F}\\right\\rbrace }=\\frac{P\\left\\lbrace G\\cap \\left(F\\cap {\\left(E^{c\\;} \\cup F^{c\\;} \\right)}^{\\;} \\right)\\right\\rbrace \\;}{P\\left\\lbrace \\mathrm{F}\\right\\rbrace }\\\\\n=\\frac{P\\left\\lbrace G\\cap \\left({\\left({F\\cap \\;E}^{c\\;} \\right)\\cup \\left({F\\cap \\;F}^{c\\;} \\right)}^{\\;} \\right)\\right\\rbrace \\;}{P\\left\\lbrace \\mathrm{F}\\right\\rbrace }=\\frac{P\\left\\lbrace G\\cap {F\\cap \\;E}^{c\\;} \\right\\rbrace \\;}{P\\left\\lbrace \\mathrm{F}\\right\\rbrace }\\\\\n=\\frac{P\\left\\lbrace G\\cap {\\;E}^{c\\;} \\cap F\\right\\rbrace \\;}{P\\left\\lbrace \\mathrm{F}\\right\\rbrace }=\\frac{P\\left\\lbrace \\left(G-{\\;E}^{\\;} \\right)\\cap F\\right\\rbrace \\;}{P\\left\\lbrace \\mathrm{F}\\right\\rbrace }=P\\left\\lbrace \\left(G-E\\right)|F\\right\\rbrace \n\\end{array}$<br>\nif $A\\subseteq B$ then $P\\left(B\\right)-P\\left(A\\right)=P\\left(B_{\\;} -A\\right)$ Here as $E\\subseteq G$ so $\\left(E\\cap F\\right)\\subseteq \\left(G\\cap F\\right)$ so we can write $P\\left\\lbrace G\\cap F\\right\\rbrace -\\;P\\left\\lbrace E\\cap F\\right\\rbrace=P\\left\\lbrace \\left(G\\cap F\\right)-\\left(E\\cap F\\right)\\right\\rbrace$<br>\n\n\n![](CS6660_images/3.jpg)\n\n- if $E\\subseteq G$ then $P\\left\\lbrace E|F\\right\\rbrace \\le P\\left\\lbrace G|F\\right\\rbrace$\n\n### Multiplication Rule\nFor $E_1 ,E_2 ,\\ldotp \\ldotp \\ldotp E_n$ events:<br>\n$P\\left\\lbrace E_1 \\cap E_2 \\cap \\ldotp \\ldotp \\ldotp \\cap E_n \\right\\rbrace =P\\left\\lbrace E_1 \\right\\rbrace \\ldotp P\\left\\lbrace E_2 |E_1 \\right\\rbrace \\ldotp P\\left\\lbrace E_3 |E_1 \\cap E_2 \\right\\rbrace \\ldotp \\ldotp \\ldotp \\ldotp P\\left\\lbrace E_n |E_1 \\cap E_2 \\cap \\ldotp \\ldotp \\ldotp \\cap E_{n-1} \\right\\rbrace$\n\n### The law of total probability\nThis is also known as partition theorem<br>\nFor any events $E$ and $F$ <br>\n$P\\left\\lbrace E\\right\\rbrace =P\\left\\lbrace E|F\\right\\rbrace \\ldotp P\\left\\lbrace E\\right\\rbrace +P\\left\\lbrace E|F^c \\right\\rbrace \\ldotp P\\left\\lbrace F^c \\right\\rbrace$<br>\n$P\\left\\lbrace E\\right\\rbrace =\\sum_i P\\left\\lbrace E|F_i \\right\\rbrace \\ldotp P\\left\\lbrace F_i \\right\\rbrace$\n\n### Bayes' Theorem\nFor any events $E$ and $F$ <br>\n$P\\left\\lbrace F|E\\right\\rbrace =\\frac{P\\left\\lbrace E|F\\right\\rbrace \\ldotp P\\left\\lbrace F\\right\\rbrace }{P\\left\\lbrace E|F\\right\\rbrace \\ldotp P\\left\\lbrace F\\right\\rbrace +P\\left\\lbrace E|F^c \\right\\rbrace \\ldotp P\\left\\lbrace F^c \\right\\rbrace }$<br>\n\n:::{.callout-important}\n\nif ${\\left\\lbrace F_i \\right\\rbrace }_i$  is a complete system of events, then<br> $$P\\left\\lbrace F_i |E\\right\\rbrace =\\frac{\\;P\\left\\lbrace E|F_i \\right\\rbrace \\ldotp P\\left\\lbrace F_i \\right\\rbrace }{\\sum_j \\;P\\left\\lbrace E|F_j \\right\\rbrace \\ldotp P\\left\\lbrace F_j \\right\\rbrace }$$\n\n:::\n\n### Independence\nEvent $E$ and $F$ are independent if $P\\left\\lbrace E|F\\right\\rbrace =P\\left\\lbrace E\\right\\rbrace$  or $P\\left\\lbrace E\\cap F\\right\\rbrace =P\\left\\lbrace E\\right\\rbrace \\cdot P\\left\\lbrace F\\right\\rbrace$<br>\n\n:::{.callout-important}\n\nMutually exclusive events are necessarily also dependent events because one's existence depends on the other's non-existence.Dependent events are not necessarily mutually exclusive\n\n:::\n<br>\n\n- If $A$ and $B$ are  independent then  $A^c$ and $B$ are also also independent<br>\nProof:<br>\n$P\\left(A^c |B\\right)=\\frac{P\\left(A^c \\cap B\\right)}{P\\left(B\\right)}=\\frac{P\\left(B\\right)-P\\left(A^{\\;} \\cap \\;\\;B\\right)}{P\\left(B\\right)}=1-P\\left(A|B\\right)=1-P\\left(A\\right)=P\\left(A^c \\right)$<br><br>\n![](CS6660_images/2.jpg)\n\nThree events $E$, $F$ and $G$  are (mutually) independent if\n- $P\\left\\lbrace E\\cap F\\right\\rbrace =P\\left\\lbrace E\\right\\rbrace \\ldotp P\\left\\lbrace F\\right\\rbrace$\n- $P\\left\\lbrace E\\cap G\\right\\rbrace =P\\left\\lbrace E\\right\\rbrace \\ldotp P\\left\\lbrace G\\right\\rbrace$\n- $P\\left\\lbrace F\\cap G\\right\\rbrace =P\\left\\lbrace F\\right\\rbrace \\ldotp P\\left\\lbrace G\\right\\rbrace$\n- $P\\left\\lbrace E\\cap F\\cap G\\right\\rbrace =P\\left\\lbrace E\\right\\rbrace \\ldotp P\\left\\lbrace F\\right\\rbrace \\ldotp P\\left\\lbrace G\\right\\rbrace$\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"center","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"css":["../../styles.css"],"html-math-method":"katex","highlight-style":"tango","output-file":"2022-08-06-CS6660-week1.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","comments":{"giscus":{"repo":"abhiyantaabhishek/IITH-Data-Science","category":"Announcements"}},"toc-location":"right","theme":"cosmo","title-block-banner":true,"author":"Abhishek Kumar Dubey","badges":false,"categories":["Probability Theory"],"date":"2022-08-06","description":"Basics, Bayes Theorem","image":"CS6660_images/2022-08-06-CS6660-week1-preview.jpg","title":"Probability Theory  1"},"extensions":{"book":{"multiFile":true}}}}}