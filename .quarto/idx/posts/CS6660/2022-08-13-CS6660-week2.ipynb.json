{"title":"Probability Theory 2","markdown":{"yaml":{"author":"Abhishek Kumar Dubey","badges":false,"categories":["Probability Theory"],"date":"2022-08-13","description":"Mass Function","image":"CS6660_images/2022-08-13-CS6660-week2-preview.jpg","title":"Probability Theory 2","toc":true},"headingText":"Indicator random variable","containsRefs":false,"markdown":"\n\n\n\nA random variable is a function from a sample space $\\Omega$ to the real numbers $\\mathbb{R}$<br>\nA random variable $X$ that can take on finitely or countably  infinitely many possible values is called discrete.\n\n$X=\\left\\lbrace \\begin{array}{ll}\n1, & \\mathrm{if}\\;\\mathrm{event}\\;E\\;\\mathrm{occurs}\\\\\n0, & \\mathrm{if}\\;\\mathrm{event}\\;E^c \\;\\mathrm{occurs}\n\\end{array}\\right.$<br>\n$\\mathrm{EX}=0\\cdot p\\left(0\\right)+1\\cdot p\\left(1\\right)=P\\left\\lbrace E\\right\\rbrace$<br>\n$\\mathrm{VarX}={\\mathrm{EX}}^2 -{\\left(\\mathrm{EX}\\right)}^2 =\\left(1^2 \\cdot P\\left\\lbrace E\\right\\rbrace +0^2 \\cdot P\\left\\lbrace E^c \\right\\rbrace \\right)-{\\left(P\\left\\lbrace E\\right\\rbrace \\right)}^2 =P\\left\\lbrace E\\right\\rbrace \\cdot \\left(1-P\\left\\lbrace E\\right\\rbrace \\right)$<br>\n$\\mathrm{SD}\\;X=\\sqrt{P\\left\\lbrace E\\right\\rbrace \\cdot \\left(1-P\\left\\lbrace E\\right\\rbrace \\right)}$<br>\n\n\n## Mass function\nLet $X$ be a discrete random variable with possible values $X_1 ,X_2 ,\\ldotp \\ldotp \\ldotp$ The probability mass function (pmf), or distribution of a random variable tells us the probabilities of these possible values: <br>\n$p_X \\left(x_i \\right)=P\\left\\lbrace X=x_i \\right\\rbrace$\nFor any discrete random variable $X$ <br>\n$p\\left(X_i \\right)\\ge 0,$ and $\\sum_i p\\left(X_i \\right)=1$\n\n\n\n\n## Expectation\n- The expection, or mean, or expected value of a discrete random variable $X$ is defined as<br>\n$EX=\\sum_i X_i \\cdot p\\left(X_i \\right)$, provided that this sum exists\n- Expected value is not necessarily a possible value\n- Expected value can be infinity \n- Expected value many not exist \n- $E\\left(\\mathrm{aX}+b\\right)=a\\cdot \\mathrm{EX}+b$<br>\n    - proof:<br>\n        $E\\left(\\mathrm{aX}+b\\right)=\\sum_i \\left(\\mathrm{aX}+b\\right)\\cdot p\\left(i\\right)=a\\sum_i X\\cdot p\\left(i\\right)+b\\sum_i p\\left(i\\right)=a\\cdot \\mathrm{EX}+b$\n- ${\\mathrm{EX}}^n =E\\left(X^n \\right)\\not= {\\left(\\mathrm{EX}\\right)}^n$\n\n## Variance\nThe variance and the standard deviation of a random variable are defined as $\\mathrm{VarX}:={E\\left(X-\\mathrm{EX}\\right)}^2$ and $\\mathrm{SDX}:=\\sqrt{\\;\\mathrm{VarX}}$\n\n### Properties of the Variance\n- $\\mathrm{VarX}={\\mathrm{EX}}^2 -{\\left(\\mathrm{EX}\\right)}^2$\n    - proof :<br>\n\n    $$\\begin{align*}{}\n    \\mathrm{VarX}&:={E\\left(X-\\mathrm{EX}\\right)}^2 \\\\\n    &=E\\left(\\left(X^2 -2\\cdot X\\cdot \\mathrm{EX}+{\\left(\\mathrm{EX}\\right)}^2 \\right)\\right)\\\\\n    &=E\\left(X^2 \\right)-E\\left(2\\cdot X\\cdot \\mathrm{EX}\\right)+{E\\left({\\left(\\mathrm{EX}\\right)}^2 \\right)}^{\\;} \\\\\n    &=E\\left(X^2 \\right)-2\\cdot \\mathrm{EX}\\cdot \\mathrm{EX}+{\\left(\\mathrm{EX}\\right)}^2 ={E\\left({\\mathrm{X}}^2 \\right)}^{\\;} -{\\left(\\mathrm{EX}\\right)}^2 \n    \\end{align*}$$\n\n    <br>\n\n    Here $EX$ is a constant so can come out of $E\\left(2\\cdot X\\cdot \\mathrm{EX}\\right)$ and it becomes $2\\cdot \\mathrm{EX}\\cdot \\mathrm{EX}$ also expectation has no effect on  ${E\\left({\\left(\\mathrm{EX}\\right)}^2 \\right)}^{\\;}$ for the same reason so it becomes ${\\left(\\mathrm{EX}\\right)}^2$\n    -   corollary:  for any $X,{\\mathrm{EX}}^2 \\ge {\\left(\\mathrm{EX}\\right)}^2$  here  equality  hold only if $X=$constant a.s.  (almost always means probability one) \n- $\\mathrm{Var}\\left(\\mathrm{aX}+b\\right)=a^2 \\cdot \\mathrm{VarX}$\n    - proof<br>\n    $$\\begin{align*}{}\n    \\mathrm{Var}\\left(\\mathrm{aX}+b\\right)&={E\\left(\\mathrm{aX}+b\\right)}^2 -{\\left(E\\left(\\mathrm{aX}+b\\right)\\right)}^2 \\\\\n    &=E\\left(a^2 X^2 +2\\mathrm{abX}+b^2 \\right)-{\\left(\\mathrm{aEX}+b\\right)}^2 \\\\\n    &=a^2 {\\cdot \\mathrm{EX}}^2 +2\\mathrm{ab}\\cdot \\mathrm{EX}+b^2 -a^2 \\cdot {\\left(\\mathrm{EX}\\right)}^2 -2\\mathrm{ab}\\cdot \\mathrm{EX}-b^2 \\\\\n    &=a^2 \\left({\\mathrm{EX}}^2 -{\\left(\\mathrm{EX}\\right)}^2 \\right)=a^2 \\mathrm{VarX}\n    \\end{align*}$$\n- $\\mathrm{Var}\\left(X+b\\right)=\\mathrm{VarX}=\\mathrm{Var}\\left(-X\\right)$\n\n:::{.callout-tip}\n\n$\\mathrm{VarX}={\\mathrm{EX}}^2 -{\\left(\\mathrm{EX}\\right)}^2$<br>$\\mathrm{Var}\\left(\\mathrm{aX}+b\\right)=a^2 \\cdot \\mathrm{VarX}$<br>$\\mathrm{Var}\\left(X+b\\right)=\\mathrm{VarX}=\\mathrm{Var}\\left(-X\\right)$\n\n:::\n\n## Bernoulli, Binomial\nSuppose that $n$ independent trails are performed, each succeeding with probability $p$. Let $X$ count the number of success within the $n$ trails. Then $X$ has the Binomial distribution with parameters  $n$ and $p$ of, in short $X~\\mathrm{Binom}\\left(n,p\\right)$\n\n### Binomial Function\n$p\\left(i\\right)=P\\left\\lbrace X=i\\right\\rbrace = {n \\choose i} p^i {\\left(1-p\\right)}^{n-i} ,\\;\\;\\;\\;i=0,1,\\ldotp \\ldotp \\ldotp \\ldotp ,n$<br>\n$\\mathrm{EX}=\\mathrm{np},$ and $\\mathrm{VarX}=\\mathrm{np}\\left(1-p\\right)$\n\n\n<br><br><br>\n$\\tiny  {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Subruk, IIT Hyderabad }}}}$\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"center","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"css":["../../styles.css"],"html-math-method":"katex","highlight-style":"tango","number-sections":true,"output-file":"2022-08-13-CS6660-week2.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","comments":{"giscus":{"repo":"abhiyantaabhishek/IITH-Data-Science","category":"Announcements"}},"toc-location":"right","theme":"cosmo","title-block-banner":true,"author":"Abhishek Kumar Dubey","badges":false,"categories":["Probability Theory"],"date":"2022-08-13","description":"Mass Function","image":"CS6660_images/2022-08-13-CS6660-week2-preview.jpg","title":"Probability Theory 2"},"extensions":{"book":{"multiFile":true}}}}}