{"title":"Probability Theory 4","markdown":{"yaml":{"author":"Abhishek Kumar Dubey","badges":false,"categories":["Probability Theory"],"date":"2022-09-24","description":"Uniform, Exponential, Normal","image":"CS6660_images/chrome_pPYR2UCfXR.png","title":"Probability Theory 4","toc":true},"headingText":"Distribution function","containsRefs":false,"markdown":"\n\n\n\n\n## Cumulative distribution function \n\n- The cumulative distribution function (CDF) of a random variable $X$ is given by <br>\n  $F : \\mathbb{R} \\rightarrow [0,1], \\;\\;\\;\\; x \\rightarrow F(x) = p\\{ X \\le x\\}$ <br>\n  Notice that the function is well defined for any random variable.<br>\n  CDF is defined as capital $F$ or sometimes as capital $F_X$\n- Remark: Ther distribution function contains all relevant information about the distribution of our random variable. E.g., for any fixed     $a<b$  <br>\n $\\mathsf{P}\\{a<X\\leq b\\}=\\mathsf{P}\\{X\\leq b\\}-\\mathsf{P}\\{X\\leq a\\}=F(b)-\\mathsf{P}(a)$<br>\n $\\mathsf{P}\\{a\\leq X<b\\}=\\mathsf{P}\\{a<X\\leq b\\} -\\mathsf{P}\\{X\\leq b\\} +\\mathsf{P}\\{X\\leq a\\}$<br>\n- Probability at jump is given by difference between jump points, e.g. <br><br>\n![](CS6660_images/Acrobat_Zy9lqOtKqv.png)<br><br>\n$p(X = 2) = \\frac{7}{8} - \\frac{4}{8} =\\frac{3}{8}$\n\n- A random variable with piecewise constant distribution function is called discrete. It's mass function values equal to the jump size in the distribution function.\n\n- Proposition <br>\nA cumulative distribution function $F$ <br>\n    - Is non-decreasing\n    - has limit $\\lim_{x \\rightarrow - \\infty} F(X) =0$ on the left;\n    - has limit $\\lim_{x \\rightarrow + \\infty} F(X) =1$ on the right;\n    - Is continuous form the right.\n- Any function $F$ with the above properties is a cumulative distribution function. There is a sample space and a random variable on it that realizes this distribution function.\n\n- One Use case  of CDF <br>\n  Suppose we have a tool that can generate only uniform random variable.<br>\n  But we want to generate the data which follows a specific probability distribution $f$ whose CDF is $F_X$. <br>\n  To achieve thais we generate random number using uniform distribution in range $[0,1]$, say $a$.\n  Now look at CDF $F_X$ and find $A$ such that $F_X(S)=a$ <br>\n  Output $A$.<br>\n  The output will be distributed as per $f$\n\n\n# Density function\n\n- Suppose that  a random variable has it's distribution function in the form of <br>\n$\\displaystyle F(a) = \\int _{- \\infty}^a f(X) dX, \\;\\;\\;\\; \\forall a \\in \\mathbb{R}$ <br>\nwith a function $f \\ge 0$. Then the distribution is called (absolutely) continuous, and $f$ is the probability density function (pdf).\n- Proposition <br>\nA probability density function $f$ \n    - Is non-negative\n    - Has total integral $\\displaystyle \\int _{- \\infty} ^ \\infty f(x) dx =1$\n- Any function $f$ with the above properties is a probability density function. There is a sample space and a continuous random variable on it that realizes this density.\n\n## Properties of the density function \n\n- Proposition <br>\n  For any subset $B \\subseteq \\mathbb{R},$ <br>\n  $\\displaystyle P\\{X \\in B \\} = \\int _B f(x) dx$\n- Corollary <br>\n  Indeed, for a continuous random variable $X,$<br>\n  $\\displaystyle P\\{X \\in a \\} = \\int _{\\{a\\}} f(x) dx = 0 \\;\\;\\;\\;\\; \\forall a \\in \\mathbb{R}$\n- Corollary <br>\n  For a small $\\varepsilon ,$  <br>\n  $\\displaystyle P\\{X \\in (a,a+\\varepsilon] \\} = \\int _a ^{a + \\varepsilon} f(x) dx \\simeq  f(a) \\cdot \\varepsilon$\n- There is no particular value that $X$ can take on with positive chance. We can only talk about intervals, and density tells us the likelihood that $X$ is around a point $a$.\n- To get to the density from an absolutely continuous distribution function, <br> \n  $\\displaystyle f(a) = \\frac{dF(a)}{da}\\;\\;\\;\\; a \\in \\mathbb{R}$ \n\n# Expectation, Variance\n\n- The way of defining the expectation will be no surprise for anyone (c.f the discrete case): <br>\n- Definition: <br>\n  The expected value of a continuous random variable $X$ is defined by <br>\n  $\\displaystyle EX = \\int_{- \\infty}^ \\infty X \\cdot f(X)dX,$ <br>\n  if the integral exists.\n- Proposition <br>\n  Let $X$  be a continuous random variable, and $g$ an $\\mathbb{R } \\rightarrow \\mathbb{R }$ function then <br>\n   $\\displaystyle Eg(X)= \\int _{- \\infty} ^{\\infty} g(X) \\cdot f(X) dX, \\;\\;\\;\\;\\;$ if exists.\n- We can define <br>\n  Moments: <br>\n  $\\displaystyle EX^n= \\int _{- \\infty} ^{\\infty} X^n \\cdot f(X) dX,$ \n  Absolute moments: <br>\n  $\\displaystyle E|X|^n= \\int _{- \\infty} ^{\\infty} |X| ^n \\cdot f(X) dX,$   <br>\n  variance $\\text{Var}X = E(X-EX)^2=EX^2-(EX)^2$ and standard deviation $\\text{SD}X= \\sqrt{\\text{Var}X }$ as in the discrete case. These enjoy the same properties as before.\n\n# Uniform \n\n- Definition <br>\n  Fix $\\alpha < \\beta$ reals. We say that $X$ has the uniform distribution over the interval $(\\alpha , \\beta ),$ in short, $X \\sim U(\\alpha , \\beta),$ if it's density is given by <br><br>\n  $f(X) = \\left\\lbrace \\begin{array}{c c}\n  \\displaystyle \\frac{1}{\\beta -\\alpha} & \\text{if } X \\in (\\alpha,\\beta),\\\\\n  0, & \\text{otherwise}\n  \\end{array}\\right .$ <br><br>\n- Notice that this is exactly the value of the constant that makes this a density.<br><br><br>\n- Integrating  the density <br><br>\n  $F(X) = \\left\\lbrace \\begin{array}{l l}\n  0,&\\text{if } X \\le \\alpha, \\\\\n  \\displaystyle \\frac{X-\\alpha}{\\beta -\\alpha}, & \\text{if } X \\in (\\alpha,\\beta),\\\\\n  1, & \\text{otherwise}\n  \\end{array}\\right .$ <br><br>\n![](CS6660_images/Acrobat_BY1t0aZJd5.png)<br><br>\n- Remarks: <br>\n  If $X \\sim U (\\alpha , \\beta),$ and $\\alpha < a <b < \\beta$ then <br>\n  $\\displaystyle P\\{a < X \\le b\\} = \\int_a^b f(X)dX=\\frac{b-a}{\\beta - \\alpha }$<br>\n  Probabilities are computed by proportions of lengths. \n\n\n## Expectation, Variance of uniform \n\n- For $X \\sim U(\\alpha , \\beta ),$ <br><br>\n  $\\boxed{ \\displaystyle  EX =\\frac{\\alpha + \\beta }{2}, \\;\\;\\;\\; \\text{Var}X = \\frac{(\\beta - \\alpha)^2}{12}}$<br><br>\n- proof of $EX$ <br>\n  $\\displaystyle  EX= \\int_{- \\infty}^{\\infty} Xf(X)dX= \\int_{\\alpha}^{\\beta} \\frac{X}{\\beta - \\alpha}dX=\\frac{\\frac{\\beta ^2}{2} - \\frac{\\alpha ^2}{2}}{\\beta - \\alpha}=\\frac{\\alpha + \\beta}{2}$\n- proof of $\\text{Var}X$ <br>\n  First find $EX^2$<br>\n  $\\displaystyle  EX^2= \\int_{- \\infty}^{\\infty} X^2f(X)dX= \\int_{\\alpha}^{\\beta} \\frac{X^2}{\\beta - \\alpha}dX=\\frac{\\frac{\\beta ^3}{3} - \\frac{\\alpha ^3}{3}}{3(\\beta - \\alpha)}=\\frac{\\alpha^2+\\alpha \\beta  + \\beta^2}{3}$<br><br>\n  $\\displaystyle \\text{Var}X = EX^2-(EX)^2 = \\frac{\\alpha^2+\\alpha \\beta  + \\beta^2}{3}- \\frac{(\\alpha + \\beta)^2}{4}=\\frac{ \\beta^2 -2\\alpha \\beta + \\alpha^2}{12}$<br><br>\n  $\\displaystyle \\text{Var}X = EX^2-(EX)^2 = \\frac{(\\beta-\\alpha )^2}{12}$\n\n\n# Exponential \n\n- The Exponential is a very special distribution because of its memoryless property. It is often considered as a waiting time, and is widely used in the theory of stochastic processes.\n\n- Definition <br>\n  Fix a positive parameter $\\lambda$. $X$ is said to have the Exponential distribution with parameter $\\lambda$ or, in short, $X \\sim \\text{Exp}(\\lambda )$, if its density is given by<br><br>\n  $\\displaystyle f(x) =  \\left\\lbrace \\begin{array}{l l}\n  0, & \\text{if }x\\le 0 \\\\\n  \\lambda e^{-\\lambda x}, & \\text{if }x\\ge 0 \\\\\n  \\end{array}\\right .$\n- Integration of density exponential density function <br><br>\n  $\\displaystyle F(X) = \\int _{-\\infty}^0 f(z)dz = \\int _{0}^x f(z)dz+ \\int _{0}^x f(z)dz$<br><br>\n  $\\displaystyle F(X) = \\int _{-\\infty}^0 0dz +  \\int _{0}^x  \\lambda e^{-\\lambda z} dz$<br><br>\n  $\\displaystyle F(X) =\\int _{0}^x  \\lambda e^{-\\lambda z} =  \\lambda \\frac{e^{- \\lambda z}}{- \\lambda} \\bigg|_{0}^x = 1-e^{-\\lambda x}$<br><br>\n  $\\displaystyle F(x) = \n  \\begin{cases} \n  0, & \\text{if }x\\le 0 \\\\\n  1-e^{-\\lambda x}, & \\text{if }x\\ge 0 \\\\  \n  \\end{cases}$<br><br>\n![](CS6660_images/Acrobat_dTAC8JheSA.png)\n\n## Expectation, Variance of  Exponential \n \n\n- For $X \\sim \\text{Exp}(\\alpha , \\beta ),$ <br><br>\n  $\\boxed{ \\displaystyle  EX =\\frac{1 }{\\lambda}, \\;\\;\\;\\; \\text{Var}X = \\frac{1}{\\lambda^2}}$<br><br>\n- Proof of $EX$ <br><br>\n  $\\displaystyle EX = \\int _0 ^\\infty X \\lambda e^{-\\lambda X}dX$<br><br>\n  Tips integration by parts: <br>\n  $d(uv) = udv+vdu$ <br>\n  $udv = d(uv) - vdu$ <br>\n  $\\int udv = uv - \\int vdu$ <br><br>\n  In our case $u=X   \\;\\;\\; v=-e^{-\\lambda X} \\Rightarrow dv= \\lambda  e^{-\\lambda X}$<br><br>\n  $\\displaystyle \\int _0 ^\\infty udv = -\\left(X\\cdot e^{-\\lambda X} \\right)\\big| _0 ^\\infty - \\int _0 ^\\infty \\left( -e^{-\\lambda X}\\right) dX$<br><br>\n$\\displaystyle \\int _0 ^\\infty udv = 0-   \\frac{e^{-\\lambda X}}{\\lambda} \\Bigg|_0 ^\\infty = \\frac{1}{\\lambda}$<br><br>\n$\\displaystyle EX = \\frac{1}{\\lambda}$<br><br>\n- Proof of $\\text{Var}X$<br><br>\n  First we find $E(X^2)$<br><br>\n  $\\displaystyle E(X^2) = \\int _0 ^\\infty X^2 \\lambda e^{-\\lambda X}dX$<br><br>\n  In this case $u=X^2   \\;\\;\\; v=-e^{-\\lambda X} \\Rightarrow dv= \\lambda  e^{-\\lambda X}$<br><br>\n  $\\displaystyle \\int _0 ^\\infty udv = -\\left(X^2\\cdot e^{-\\lambda X} \\right)\\big| _0 ^\\infty - 2\\int _0 ^\\infty \\left( -e^{-\\lambda X}\\right) XdX$<br><br>\n  $\\displaystyle \\int _0 ^\\infty udv = 0+ 2\\int _0 ^\\infty \\left( e^{-\\lambda X}\\right) XdX$<br><br>\n  $\\displaystyle \\int _0 ^\\infty udv = 0+ 2\\frac{1}{\\lambda} \\int _0 ^\\infty \\lambda\\left( e^{-\\lambda X}\\right) XdX$<br><br>\n  $\\displaystyle \\int _0 ^\\infty udv = 0+ \\frac{2}{\\lambda}\\underbrace{\\int _0 ^\\infty \\lambda \\left( e^{-\\lambda X}\\right) XdX}_{\\text{This is same as }EX=\\frac{1}{\\lambda}}$<br><br>\n  $\\displaystyle \\int _0 ^\\infty udv =\\frac{2}{\\lambda^2}=E(X^2)$<br><br>\n  Now,<br><br>\n  $\\displaystyle \\text{Var}X = E(X^2)-(EX)^2=\\frac{2}{\\lambda^2} - \\left(\\frac{1}{\\lambda}\\right)^2=\\frac{1}{\\lambda^2}$\n\n- Thinking about $X$  as a waiting time, we now see that $\\lambda$  describes how fast the event we wait for, happens. Therefore $\\lambda$ is also called the rate of the exponential waiting time.\n\n## The memoryless property\n\n- The exponential is the only continuous non-negative memory less distribution. That is, the only distribution with $X \\ge 0$ and\n$P\\{X > t + s \\mid X > t\\} = P\\{X > s\\} \\;\\;\\;\\;\\;(\\forall\\; t,\\;\\; s \\ge 0)$\n    - Suppose we have waited for time $t$. The chance of waiting an additional time $s$ is the same as if we would start waiting anew. The distribution does not remember its past.\n- Proof <br>\n$\\displaystyle  P \\left\\{ X > t + s \\mid X>t  \\right\\}= \\frac {P\\left\\{ X > t + s \\right\\} \\cap P\\left\\{X>t  \\right\\}}{P\\left\\{X>t  \\right\\}}$<br>\nif $X > t+s$  then $X$ is also greater than $t$ <br>\n$\\displaystyle  P \\left\\{ X \\ge t + s \\mid X>t  \\right\\}= \\frac {P\\left\\{ X \\ge t + s \\right\\} }{P\\left\\{X>t  \\right\\}}$<br>\n$\\displaystyle  P \\left\\{ X \\ge t + s \\mid X>t  \\right\\}= \\frac {\\lambda e^{-\\lambda (t+s)} }{\\lambda e^{-\\lambda t}}=  e^{-\\lambda s}=P \\left\\{ X>s  \\right\\}$<br>\n\n# Normal \n\n- Definition <br>\n Let $\\mu \\in \\mathbb{R}, \\sigma > 0$ be ral parameters, $X$ has the normal distribution with parameters $\\mu$ and $\\sigma^2$ or in short $X \\sim \\mathcal{N}(\\mu,\\sigma ^2),$ if it's density is given by <br><br>\n $\\displaystyle f(x)=\\frac{1}{\\sqrt{2\\pi}\\cdot\\sigma}\\cdot e^{-\\frac{1}{2} \\left(\\frac{X-\\mu}{\\sigma}\\right)^2}\\;\\;\\;\\;X\\in \\mathbb{R}$<br><br>\n![](CS6660_images/Acrobat_Q2rnYtnNwB.png)<br><br>\n- The case $\\mu =0, \\sigma^2=1$ is called standard normal distribution $X \\sim \\mathcal{N}(0,1),$ Its density is denoted by $\\phi$ and it's distribution is denoted by $\\Phi$<br><br>\n$\\displaystyle \\phi(x)=\\frac{1}{\\sqrt{2\\pi}\\cdot\\sigma}\\cdot e^{-\\frac{ X^2}{2}}$<br><br>\n$\\displaystyle \\Phi(x)=\\int_{-\\infty }^X \\frac{1}{\\sqrt{2\\pi}\\cdot\\sigma}\\cdot e^{-\\frac{ y^2}{2}}dy\\;\\;\\;\\;X\\in \\mathbb{R}$<br><br>\n- Remarks : The standard normal distribution $\\phi(x)=\\int_{-\\infty }^X \\frac{1}{\\sqrt{2\\pi}\\cdot\\sigma}\\cdot e^{-\\frac{ y^2}{2}}dy$ has no closed from, its values wil be looked up in tables.\n\n## Symmetry <br>\n- For any $z \\in \\mathbb{R},$ $\\;\\;\\Phi (-Z ) = 1- \\Phi(Z)$<br>\n  Proof: <br>\n  The standard normal distribution is symmetric: if $X \\sim \\mathcal{N}(0,1)$ and also $-X \\sim \\mathcal{N}(0,1)$, Therefore <br>\n  $\\Phi(-Z)=P\\{ X<-Z\\} = P\\{-X > Z \\} = P\\{ X >Z\\} = 1 - \\Phi(Z)$\n\n## Linear transformations \n\n\n - Let $X \\sim \\mathcal{N}(\\mu,\\sigma ^2)$ and $\\alpha , \\beta \\in \\mathbb{R}$, Then $\\alpha X + \\beta \\sim \\mathcal{N}(\\alpha \\mu + \\beta , \\alpha^2 \\sigma^2)$<br>\n proof:<br>\n we prove for positive $\\alpha$, for negative it's similar. Start with the distribution function $Y = \\alpha X +\\beta$<br><br>\n $\\displaystyle F_Y(y)=P\\{ Y <y\\} = P\\{ \\alpha X + \\beta <y\\} = P\\left\\{ X < \\frac{y-\\beta}{\\alpha}\\right\\}$<br><br>\n $\\displaystyle F_Y(y)= P\\left\\{ X < \\frac{y-\\beta}{\\alpha}\\right\\}=F_X\\left(\\frac{y-\\beta}{\\alpha}\\right)$<br><br>\n $\\displaystyle  f_y \\left(y\\right)=\\frac{d}{\\textrm{d}y}F_y \\left(y\\right)=\\frac{d}{\\textrm{d}y}F_X \\left(\\frac{y-\\beta }{\\alpha }\\right)=f_X \\left(\\frac{y-\\beta }{\\alpha }\\right)\\frac{1}{\\alpha }$<br><br>\n $\\displaystyle  f_y \\left(y\\right)=\\frac{1}{\\sqrt{2\\pi }\\sigma }\\mathrm{Exp}\\left(-\\frac{1}{2}{\\left(\\frac{\\left(\\frac{y-\\beta }{\\alpha }\\right)-\\mu }{\\sigma }\\right)}^2 \\right)\\frac{1}{\\alpha }$<br><br>\n $\\displaystyle  f_y \\left(y\\right)=\\frac{1}{\\sqrt{2\\pi }\\sigma }\\mathrm{Exp}\\left(-\\frac{1}{2}{\\left(\\frac{y-\\beta -\\mu \\alpha }{\\sigma \\alpha \\;}\\right)}^2 \\right)\\frac{1}{\\alpha }$<br><br>\n $\\displaystyle  f_y \\left(y\\right)=\\frac{1}{\\sqrt{2\\pi }\\sigma }\\mathrm{Exp}\\left(-\\frac{\\left(y-\\beta -\\mu \\alpha \\right)^2 }{2{\\left(\\sigma \\alpha \\right)}^2 \\;}\\right)\\frac{1}{\\alpha }$<br><br>\n $\\displaystyle  f_y \\left(y\\right)=\\frac{1}{\\sqrt{2\\pi }\\sigma \\alpha }\\mathrm{Exp}\\left(-\\frac{\\left(y-\\left(\\mu \\alpha +\\beta \\right)\\right)^2 }{2{\\left(\\sigma \\alpha \\right)}^2 \\;}\\right)$<br><br>\n which implies the statement $Y \\sim \\mathcal{N}(\\alpha \\mu + \\beta , \\alpha^2 \\sigma^2)$\n - If $X \\sim \\mathcal{N}(\\mu, \\sigma ^2),$ then its standardized version $\\frac{X-\\mu}{\\sigma} \\sim \\mathcal{N}(0, 1)$ <br>\n   Just use $\\alpha = \\frac{1}{\\sigma}$ and $\\beta = -\\frac{\\mu}{\\sigma}$\n\n- If $X \\sim \\mathcal{N}(0, 1)$ is standard normal, then its mean is $0$ and its variance is $1$.<br>\n Proof: <br>\n That the mean is zero follows from symmetry. For the variance we need to calculate <br><br>\n $\\displaystyle EX^{2}=\\int_{-\\infty}^{\\infty}\\frac{x^{2}}{\\sqrt{2\\pi}}\\cdot\\mathrm{e}^{-x^{2}/2}\\,\\mathrm{d}x$<br><br>\n using intergration by parts.\n TODO : complete the integration\n\n- If $X \\sim \\mathcal{N}(\\mu,\\sigma ^2)$ then its mean is $\\mu$ and its variance is $\\sigma ^2$<br><br>\n  Proof: <br>\n  $\\displaystyle \\mathrm{EX}=\\sigma \\cdot E\\left(\\frac{X-\\mu }{\\sigma }\\right)+\\mu =0+\\mu =\\mu$ <br>\n  $\\displaystyle \\mathrm{VarX}=\\sigma^2 \\mathrm{Var}\\left(\\frac{X-\\mu }{\\sigma }\\right)=\\sigma^2 \\cdot 1=\\sigma^2$ <br><br>\n  $X \\sim \\mathcal{N}(\\mu,\\sigma ^2)$ is also said to be normal distribution with mean $\\mu$ and variance $\\sigma^2$\n\n## Why normal \n\n- Theorem (DeMoivre-Laplace)<br>\n  Fix $p$ and let $X_n \\sim \\text{Binom}(n,p)$. Then for every fixed $a<b$ reals,<br>\n  $\\displaystyle \\lim_{n\\to \\infty } P\\left\\lbrace a<\\frac{X_n -\\mathrm{np}}{\\sqrt{\\mathrm{np}\\left(1-\\mathrm{np}\\right)}}\\le b\\right\\rbrace =\\Phi \\left(b\\right)-\\Phi \\left(a\\right)$ <br>\n  That is, take  $X_n \\sim \\text{Binom}(n,p)$  with large $n$, fixed (not small) $p$. then $\\frac{X_n -\\mathrm{np}}{\\sqrt{\\mathrm{np}\\left(1-\\mathrm{np}\\right)}}$ is approximately $\\mathcal{N}(0,1)$ distributed.<br>\n  This will be a special case in the Central Limit Theorem, In fact, Normal will appear in many similar scenarios. Measured quantities, heights or people, length of these lectures, etc.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"center","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"css":["../../styles.css"],"html-math-method":"katex","output-file":"2022-09-24-CS6660-week5.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","comments":{"giscus":{"repo":"abhiyantaabhishek/IITH-Data-Science","category":"Announcements"}},"toc-location":"right","theme":"cosmo","title-block-banner":true,"author":"Abhishek Kumar Dubey","badges":false,"categories":["Probability Theory"],"date":"2022-09-24","description":"Uniform, Exponential, Normal","image":"CS6660_images/chrome_pPYR2UCfXR.png","title":"Probability Theory 4"},"extensions":{"book":{"multiFile":true}}}}}