{"title":"Linear Algebra 2","markdown":{"yaml":{"author":"Abhishek Kumar Dubey","badges":false,"categories":["Linear Algebra"],"date":"2022-09-17","description":"Gaussian Elimination, Row-Echelon, Minus 1 Trick","title":"Linear Algebra 2","toc":true},"headingText":"Gaussian Elimination","containsRefs":false,"markdown":"\n\n\n\n\n- Gaussian Elimination are elementary transformation of subsystems fo linear equations, which transforms the equation systems into a simple form.\n- Elementary transformation that keep the solution set the same, but that transform the equation system.\n- Process\n    - Exchange fo two equations (rows in the matrix representing the system of equations)\n    - Multiplication of an equation (row) with a constant $\\lambda \\in \\mathbb{R}-\\{0\\}$\n    - Addition of two equations (rows)\n\n- Example : <br><br>\n$\\begin{array}{r r r r r r r r r r}\n-2x_1 & + & 4x_2 & - & 2x_3 & - & x_4 & + & 4x_5 & = & -3 \\\\\n4x_1 & - & 8x_2 & + & 3x_3 & - & 3x_4 & + & x_5 & = & 2 \\\\\nx_1 & - & 2x_2 & + & x_3 & - & x_4 & + & x_5 & = & 0 \\\\\nx_1 & - & 2x_2 &   &    & - & 3x_4 & + & 4x_5 & = & a \\\\\n\\end{array}$ <br><br>\nBuild the _augmented matrix (in the form of $\\left[A \\mid b\\right]$ ) <br><br>\n$\\left[ \\begin{array}{r r r r r | r}\n-2 & 4 & -2 & -1 & 4 & -3 \\\\\n 4 &-8 &  3 & -3 & 1 &  2 \\\\\n 1 &-2 &  1 & -1 & 1 &  0 \\\\\n 1 &-2 &  0 & -3 & 4 &  a \\\\\n\\end{array}\\right]$ <br><br>\nExchange row $3$ and row $1$  for simplicity <br><br>\n$\\left[ \\begin{array}{r r r r r | r}\n 1 &-2 &  1 & -1 & 1 &  0 \\\\\n 4 &-8 &  3 & -3 & 1 &  2 \\\\\n-2 & 4 & -2 & -1 & 4 & -3 \\\\\n 1 &-2 &  0 & -3 & 4 &  a \\\\\n\\end{array}\\right]$ <br><br>\nPerform operations $\\{ R_2 \\leftarrow R_2 - 4R_1, R_3 \\leftarrow  R_3+2R_1, R_4 \\leftarrow R_4-R_1 \\}$<br><br>\n$\\left[ \\begin{array}{r r r r r | r}\n 1 &-2 &  1 & -1 & 1 &  0 \\\\\n 0 & 0 & -1 &  1 & -3 &  2 \\\\\n 0 & 0 &  0 & -3 & 6 & -3 \\\\\n 0 & 0 & -1 & -2 & 3 &  a \\\\\n\\end{array}\\right]$ <br><br>\nPerform operations $\\{ R_4 \\leftarrow R_4 - R_2 -R_3 \\}$<br><br>\n$\\left[ \\begin{array}{r r r r r | r}\n 1 &-2 &  1 & -1 &  1 &  0 \\\\\n 0 & 0 & -1 &  1 & -3 &  2 \\\\\n 0 & 0 &  0 & -3 &  6 & -3 \\\\\n 0 & 0 &  0 &  0 &  0 &  a \\\\\n\\end{array}\\right]$ <br><br>\nPerform operations $\\{ R_2 \\leftarrow -R_2 , R_3 \\leftarrow -\\frac{1}{3}R_3  \\}$<br><br>\n$\\left[ \\begin{array}{r r r r r | r}\n 1 &-2 &  1 & -1 &  1 &  0 \\\\\n 0 & 0 &  1 & -1 &  3 & -2 \\\\\n 0 & 0 &  0 &  1 & -2 &  1 \\\\\n 0 & 0 &  0 &  0 &  0 &  a+1 \\\\\n\\end{array}\\right]$ <br><br>\nAbove matrix is in __row-echelon from__<br><br>\nConvert above matrix to normal equation form <br><br>\n$\\begin{array}{l l l l l  l r}\n x_1 &-2x_2 &  +x_3 & -x_4 &  +x_5 & = & 0 \\\\\n   &   &  +x_3 & -x_4 &  +3x_5 &  = & -2 \\\\\n   &   &    & +x_4 & -2x_5 &   = & 1 \\\\\n   &   &    &    &  +0 &   = & a+1 \\\\\n\\end{array}$ <br><br>\nOnly for $a=1$ this system can be solved, and can be solved using   back substitution <br><br>\n$\\left[  \\begin{array}{r} \nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4 \\\\\nx_5 \\\\\n\\end{array}\\right] =\n\\left[  \\begin{array}{r} \n 2 \\\\\n 0\\\\\n-1 \\\\\n1 \\\\\n0 \\\\\n\\end{array}\\right]$ <br><br>\nGeneral solution of the above equation can be found as explained earlier, and is given as below <br> <br>\n$$\\left\\{x\\in\\mathbb{R}^{5}:x= {\\left[\\begin{array}{r}{2}\\\\ {0}\\\\ {-1}\\\\ {1} \\\\0 \\end{array}\\right]}    + \\lambda_{1}{\\left[\\begin{array}{r}{2}\\\\ {1}\\\\ {0}\\\\ {0} \\\\0 \\end{array}\\right]}   + \\lambda_{2}{\\left[\\begin{array}{r}{2}\\\\ {0}\\\\ {-1}\\\\ {2} \\\\1\\end{array}\\right]},\\;\\;\\ \\lambda_{1},\\lambda_{2}\\in\\mathbb{R}\\right\\}$$\n\n# Row-Echelon Form\n\n- Example matrix <br><br>\n$\\left[ \\begin{array}{r r r r r | r}\n \\boxed 1 &-2 &  1 & -1 &  1 &  0 \\\\\n 0 & 0 &  \\boxed  1 & -1 &  3 & -2 \\\\\n 0 & 0 &  0 &  \\boxed  1 & -2 &  1 \\\\\n 0 & 0 &  0 &  0 &  0 &  a+1 \\\\\n\\end{array}\\right]$ <br><br>\n\n- Definition <br>\n  A matrix is in row-echelon form if \n    - All rows that contains only zeros are at the bottom ot the matrix; correspondingly, all rows that contains at least one nonzero element are on top of rows that contain only zeros.\n    - Looking at nonzero rows only, the first nonzero number from the left ( also called the pivot or the leading coefficient ) is always strictly to the right of the pivot of the row above it.\n    \n\n- Basic and free variables : The variables corresponding to the pivots in the row-echelon form are called basic variables and the other variables are free variables. For example, $x_1; x_3; x_4$ are basic variables, whereas $x_2;x_5$ are free variables in above example matrix. \n\n# Reduced row Echelon Forms \n\n\n- Definition <br>\n  An equation system is in reduced row echelon form ( also: rwo reduced echelon form or row canonical from ) if \n    - It is in row echelon from.\n    - Every pivot is 1.\n    - The pivot is the only nonzero entry in its column.\n\n# Solving $AX=0$ (Minus $1$ Trick)\n\n- Example Matrix <br><br>\n$\\left[ \\begin{array}{r r r r r}\n1 & 3 & 0 & 0 & 3 \\\\ \n0 & 0 & 1 & 0 & 9 \\\\ \n0 & 0 & 0 & 1 & -4 \\\\ \n\\end{array}\\right]$<br><br>\nFor finding the solutions of $AX=0$ is to look at the non-pivot columns, which we will need to express as a (linear) combination of the pivot columns. <br><br>\n$\\left\\{x\\in\\mathbb{R}^{5}:x=  \\lambda_{1}{\\left[\\begin{array}{r}3\\\\ -1\\\\ 0\\\\ 0 \\\\0 \\end{array}\\right]}   + \\lambda_{2}{\\left[\\begin{array}{r}3\\\\ 0\\\\ 9\\\\ -4 \\\\ -1\\end{array}\\right]},\\;\\;\\ \\lambda_{1},\\lambda_{2}\\in\\mathbb{R}\\right\\}$ <br><br>\nInsert row in place of free variable ( $2^{\\text{nd}}$ and $5^{\\text{th}}$) with $-1$ value in place of dependent variable as shown below  <br><br>\n$\\tilde{A}  =  \\left[ \\begin{array}{r r r r r}\n1 & 3 & 0 & 0 & 3 \\\\ \n\\boxed 0 & \\boxed { \\mathbf {-1}} & \\boxed 0 & \\boxed 0 & \\boxed 0 \\\\ \n0 & 0 & 1 & 0 & 9 \\\\ \n0 & 0 & 0 & 1 & -4 \\\\ \n\\boxed 0 & \\boxed 0 & \\boxed 0 & \\boxed 0 & \\boxed {\\mathbf {-1}} \\\\ \n\\end{array}\\right]$<br><br>\nNow the solution for $AX=0$ can be found by the columns of the the free variable which is $2^{\\text{nd}}$ and $5^{\\text{th}}$ column. \n\n\n# Solving a System of Liner Equation \n\n- Inversion of matrices which are not square and non-invertible.<br>\n  $Ax=b$ is given as $x=A^{-1}b$ <br>\n  $Ax=b \\Longleftrightarrow A^TAx=A^Tb \\Longleftrightarrow x= (A^TA)^{-1}A^Tb$<br>\n  This is called _Moore-Penrose pseudo-inverse_ <br>\n  The disadvantage of this method is that it requires a lot of computation.\n\n- Gaussian Elimination plays a key role in\n    - Computing determinants \n    - Checking whether a set of vectors is linearly independent \n    - Computing the rank of the matrix\n    - Determining a basis of the vector space.\n\n# Vector Space\n\n- A vector space $V$ is a set that is closed under finite vector addition and scalar multiplication.\n    - we two vectors are added or multiplied it should stay in the same vector space.\n\n- Vector space properties <br>\n  In order for $V$ to be a vector space, the following condition must hold for all elements $X,Y,Z \\in V$ and any scalars $r,s \\in F$:\n    - Commutativity: $X+Y=Y+X$\n    - Associativity of vector addition: $(X+Y)+Z = X+(Y+Z)$\n    - Additive Identity: For all $X, \\;\\; 0+X = X+0=X$\n    - Existence of additive inverse: For any $X$, there exists a  $-X$  such that $X+(-X) =0$\n    - Associativity of vector multiplication: $r(sX) = (rs)X$\n    - Distributivity of scalar sums: $(r+s)X = rX+sX$\n    - Scalar multiplication identity: $1X=X$\n\n# Subspace\n\n- Let $V$ be a vector space, and let $W$ be the subspace of $V$, if $W$ is a vector space with respect to the operation in $V$, then $W$ is called a subspace of $V$\n- Let $V$ be the vector space, with operations $+$ and $\\;\\cdot \\;$ and let $W$ be subset of $V$. Then $W$ is subspace of $V$ if and only if the following conditions hold.\n    - $W$ is non-empty: The zero vector belongs  to $W$.\n    - Closure under $+$: If $u$ and $v$ are any vectors in $W$, the $u+v$ is in $W$.\n    - Closure under $\\cdot \\;$:if $v$ is any vector in $W$, and $c$ is nay real number, then $c \\cdot v$ si in $W$.\n\n# Liner Combination \n\n- Consider a vector space $V$ and a finite number of vectors <br>\n  $x_1, \\dots , x_k \\in V$, then every $v \\in V$ of the form <br>\n  $\\displaystyle v=\\lambda_1 x_1+ \\dots + \\lambda_kx_k = \\sum_{i=1}^k \\lambda_i x_i \\in V$ <br>\n  $\\lambda_1,\\dots , \\lambda_k \\in \\mathbb{R}$ is a linear combination fo the vectors $x_1, \\dots ,x_k$<br>\n  $\\displaystyle  0=\\sum_{i=1}^k 0 x_i$\n\n# Linear Independence \n\n- Let us consider a vector space $V$ with $k \\in \\mathbb{N}$  and $x_1, \\dots ,x_k \\in V$. If there is a non trivial linear combination, such that $0=\\sum_{i=1}^k \\lambda_i x_i$ with at least one $\\lambda _i \\ne 0,$ the vectors $x_1, \\dots , x_k$ are _linearly independent_. If only the trivial solution exists, i.e., $\\lambda_1 =\\dots=\\lambda_k=0$ the vectors  $x_1, \\dots , x_k$ are linearly independent.\n- If at-least one of the  vectors $x_1, \\dots , x_k$ is $0$ then they are linearly dependent. The same holds if two vectors are identical.\n\n- To find if a system is linear independent, perform gaussian elimination, if there is no non-pivot column then the vectors are independent.\n\n<br><br><br>\n$\\tiny  {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Srijith, IIT Hyderabad }}}}$\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"center","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"css":["../../styles.css"],"html-math-method":"katex","highlight-style":"tango","number-sections":true,"output-file":"2022-09-17-CS6660-week4_2.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","comments":{"giscus":{"repo":"abhiyantaabhishek/IITH-Data-Science","category":"Announcements"}},"toc-location":"right","theme":"cosmo","title-block-banner":true,"author":"Abhishek Kumar Dubey","badges":false,"categories":["Linear Algebra"],"date":"2022-09-17","description":"Gaussian Elimination, Row-Echelon, Minus 1 Trick","title":"Linear Algebra 2"},"extensions":{"book":{"multiFile":true}}}}}