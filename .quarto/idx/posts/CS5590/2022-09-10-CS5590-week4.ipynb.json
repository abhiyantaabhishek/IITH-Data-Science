{"title":"Machine Learning 4","markdown":{"yaml":{"author":"Abhishek Kumar Dubey","badges":true,"categories":["Machine Learning"],"date":"2022-09-10","description":"SVM, Lagrange Multipliers, KKT condition, Mercerâ€™s condition, Kernel Trick","image":"CS5590_images/Acrobat_vct8kMqVEu.png","title":"Machine Learning 4","toc":true},"headingText":"Overview and History","containsRefs":false,"markdown":"\n\n\n\n\n- It is a discriminative classifier.\n- Inspired by Statistical Learning.\n- Developed in 1992 by Vapnik, Guyon, Boser \n- Was one of the go-to methods in ML since mid 1990s (only recently displaced by deep learning.)    \n\n# Maximum Margin Classifier\n\n- Formulation <br><br>\n    - $f(\\mathbf{X},\\mathbf{W},b)=\\mathrm{sign} (\\mathbf{W} \\cdot \\mathbf{X}+b)$<br><br> \n![](CS5590_images/Acrobat_vct8kMqVEu.png)\n    - Basic formulation of SVM can only handle two classes. \n    - There are improvised method to handle more than tow class.\n    - The __Maximum margin classifier__ is the linear classifier with the maximum margin. This is the simplest kind of SVM ( called an LSVM).\n\n\n# Estimate the Margin\n\n- The points those lies on the two margin lines are called support vector.\n- The model is immune to removal of any non-support-vector data points.\n- The equation of the line  is given by $\\mathbf{W}^T \\cdot \\mathbf{X}+b=0$<br><br>\n![](CS5590_images/Acrobat_nYNJPaG71I.png)<br>\n- $\\mathbf{W}$ is always normal to the line $\\mathbf{W}^T \\cdot \\mathbf{X}+b=0$<br> \n  This can be proved by taking two vector $\\mathbf{X_1}$ and  $\\mathbf{X_2}$ on line  $\\mathbf{W}^T \\cdot \\mathbf{X}+b=0$,<br> \n  Now if we subtract the two vector we get $\\mathbf{W}^T(\\mathbf{X_1}-\\mathbf{X_2})=\\mathbf{0} \\Leftrightarrow  (\\mathbf{X_1}-\\mathbf{X_2}) \\perp \\mathbf{W}$. The same is explained on [stack overflow](https://datascience.stackexchange.com/a/49295).\n- Dotted line $\\mathbf{X'-X}$ is perpendicular to decision boundary so parallel to $\\mathbf{W}$ let it's length (magnitude) be $r$\n- The Unit vector along Dotted line $\\mathbf{X'-X}$ is given by $\\frac{\\mathit{\\mathbf{W}}}{\\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert }$\n- The equation of the dotted line $\\mathbf{X'-X}$ can be also given by magnitude multiplied by unit vector : <br>\n  $\\displaystyle r\\cdot \\frac{ \\mathit{\\mathbf{W}}}{\\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert }$\n- But as the dotted line can be on any side of the main line so we need to multiply with $y$, as $y$ takes value of $1$ or $-1$ depending on the side: <br>\n  $\\displaystyle \\mathbf{X'-X} = yr\\cdot \\frac{ \\mathit{\\mathbf{W}}}{\\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert }$<br>\n  $\\displaystyle \\mathbf{X'} = \\mathbf{X} - yr\\cdot \\frac{ \\mathit{\\mathbf{W}}}{\\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert }$\n- Now since $\\mathbf{X'}$ lies on the line so we can write  $\\mathbf{W}^T \\cdot \\mathbf{X'}+b=0$\n- Substituting value of $\\mathbf{X'}$ in $\\mathbf{W}^T \\cdot \\mathbf{X'}+b=0$ we get: <br>\n  $\\displaystyle \\mathbf{W}^T \\cdot \\left( \\mathbf{X} - yr\\cdot \\frac{ \\mathit{\\mathbf{W}}}{\\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert } \\right)+b=0$<br><br>\n- substituting $\\displaystyle \\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert =\\sqrt{\\mathit{\\mathbf{W}}^T \\mathit{\\mathbf{W}}}$ in above equation we get: <br>\n  $\\displaystyle \\mathbf{W}^T \\cdot \\left( \\mathbf{X} - yr\\cdot \\frac{ \\mathit{\\mathbf{W}}}{\\sqrt{\\mathit{\\mathbf{W}}^T \\mathit{\\mathbf{W}}}} \\right)+b=0$<br><br>\n  $\\displaystyle  \\left(  \\mathbf{W}^T \\mathbf{X} -   yr\\cdot \\frac{ \\mathit{\\mathbf{W}}^T\\mathit{\\mathbf{W}}}{\\sqrt{\\mathit{\\mathbf{W}}^T \\mathit{\\mathbf{W}}}} \\right)+b=0$<br><br>\n  $\\displaystyle  \\left(  \\mathbf{W}^T \\mathbf{X} -   yr\\cdot \\sqrt{\\mathit{\\mathbf{W}}^T \\mathit{\\mathbf{W}}} \\right)+b=0$<br><br>\n  $\\displaystyle    \\mathbf{W}^T \\mathbf{X} -   yr\\cdot \\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert  +b=0$<br><br>\n  $\\displaystyle    \\mathbf{W}^T \\mathbf{X}  +b =  yr\\cdot \\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert$<br><br>\n  $\\displaystyle    r = \\frac{\\mathbf{W}^T \\mathbf{X}  +b}{y\\cdot \\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert}$<br><br>\n- Since $y$ takes value of only $1$ or $-1$, hence we can bring $y$ to numerator. <br>\n\n  $\\displaystyle    r = y \\frac{\\mathbf{W}^T \\mathbf{X}  +b}{\\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert}$\n\n\n- Since $\\mathbf{W}^T \\cdot \\mathbf{X}+b=0$ and $c\\left(\\mathbf{W}^T \\cdot \\mathbf{X}\\right)+b=0$ define the same plane, we have the freedom to choose the normalization of $\\mathbf{W}$ \n- Let us choose normalization such that $\\mathbf{W}^T \\cdot \\mathbf{X}_+ + b = +1$  and $\\mathbf{W}^T \\cdot \\mathbf{X}_- +b = -1$ for the positive and negative support vectors respectively.<br><br>\n![](CS5590_images/Acrobat_AwymHLT9aY.png)<br><br>\nHence, Margin now is: <br><br>\n$\\displaystyle  \\left( +1 \\right) \\frac{\\mathbf{W}^T \\mathbf{X_+} +b}{ \\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert} +  \\left( -1 \\right) \\frac{\\mathbf{W}^T \\mathbf{X_-} +b}{ \\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert}$<br><br>\nSince $\\mathbf{W}^T \\mathbf{X_+} +b=+1$ and $\\mathbf{W}^T \\mathbf{X_-} +b=-1$, substituting these  in above equation we get: <br><br>\n$\\displaystyle  \\left( +1 \\right) \\frac{\\mathbf{W}^T \\mathbf{X_+} +b}{ \\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert} +  \\left( -1 \\right) \\frac{\\mathbf{W}^T \\mathbf{X_-} +b}{ \\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert}=\\displaystyle  \\left( +1 \\right) \\frac{+1}{ \\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert} +  \\left( -1 \\right) \\frac{-1}{ \\left\\lVert \\mathit{\\mathbf{W}} \\right\\rVert}=\\frac{2}{\\left\\lVert \\mathbf{W}\\right\\rVert}$<br>\n\n\n:::{.callout-tip}\n\n<br>Margin between the two support vector is   given  by: $$\\displaystyle    \\frac{2}{\\left\\lVert \\mathbf{W}\\right\\rVert}$$\n\n:::\n\n<br><br>\n\n# Maximize the Margin\n\n- Now we know the margin between the two support vector.\n- We need to maximize the margin in such a way that $+1$ class points lies on one side of the margin and $-1$ class points lies on the other side of the margin.\n- We can formulate this as the _quadratic optimization problem_:<br>\n  Find $\\mathbf{W}$ such that <br> <br>\n  $\\displaystyle \\rho = \\frac{2}{\\left\\lVert \\mathbf{W}\\right\\rVert }$ is maximized; and for all $\\left\\{ \\left( \\mathbf{X}_i,\\mathbf{y}_i \\right) \\right\\}$<br> <br>\n  and $\\mathbf{W}^T \\cdot \\mathbf{X}_i+b \\ge 1$ if $y_i=+1$<br> <br>\n  and $\\mathbf{W}^T \\cdot \\mathbf{X}_i+b \\le -1$ if $y_i=-1$\n\nA better formulation is to minimize inverse of $\\rho$ instead of maximizing it.\n\n- We know that <br>\n  $\\displaystyle  \\max \\frac{2}{\\left\\lVert \\mathbf{W}\\right\\rVert } =\\min  \\frac{\\left\\lVert \\mathbf{W}\\right\\rVert}{2} =\\min\\frac{\\sqrt{ \\mathbf{W}^T\\mathbf{W}}}{2}$<br>\n\n-  Instead of minimizing  $\\displaystyle  \\frac {\\left\\lVert \\mathbf{W}\\right\\rVert}{2}$ we minimize $\\displaystyle  \\frac {\\left\\lVert \\mathbf{W}\\right\\rVert^2}{2} = \\displaystyle  \\frac{ \\mathbf{W}^T\\mathbf{W}}{2}$   as both (with or without square)   are equivalent. we select square one  as math (derivative) becomes easy.<br>\n\n:::{.callout-tip}\n\nMaximization problem can be written in terms of minimization as follows:<br><br>Find $\\mathbf{W}$ and $b$ such that<br><br>$\\displaystyle \\frac{\\mathbf{W}^T\\mathbf{W}}{2}$ is minimized.<br><br>and for all  $\\left\\{ \\left( \\mathbf{X}_i,\\mathbf{y}_i \\right) \\right\\} : \\displaystyle y_i\\left( \\mathbf{W}^T\\mathbf{X}_i +b\\right) \\ge 1$<br>\n<br>\n\n:::\n\n# Using Lagrange Multipliers\n\n## Basics of Lagrange Multipliers\n\n- Optimization problem:<br>\n  __Minimize__ : $\\displaystyle f\\left( \\overrightarrow{x}  \\right)$ <br>\n  __Such that__ for all $i,$ $\\displaystyle g_i\\left( \\overrightarrow{x}  \\right)\\le 0$ <br>\n- To solve the above problem we create augmented Lagrange function:<br><br>\n  $\\displaystyle L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda}   \\right):=f\\left( \\overrightarrow{x}  \\right)+\\sum_{i=1}^{n}\\lambda_ig_i\\left( \\overrightarrow{x}  \\right)$ <br><br>\n  $\\displaystyle \\underbrace{L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda}   \\right)}_{\\text{lagrange function}} :=f\\left( \\overrightarrow{x}  \\right)+\\sum_{i=1}^{n}\\underbrace{\\lambda_i}_{\\text{lagrange variable  or dual varialbe }}g_i \\left( \\overrightarrow{x}  \\right)$ <br><br>\n- Observation:<br>\n  For any feasible $x$ and all $\\lambda_i \\ge 0$,  <br><br>\n  $\\displaystyle L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda}   \\right):=f\\left( \\overrightarrow{x}  \\right)+\\overbrace{\\sum_{i=1}^{n}\\overbrace{\\lambda_i}^{\\text{this is positve}} \\underbrace{g_i\\left( \\overrightarrow{x}  \\right)}_{\\text{this is negative}}}^{\\text{This is negative}}$ <br><br>\n  Hence , $\\displaystyle L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda}   \\right) \\le f\\left( \\overrightarrow{x}  \\right)$ <br>\n  $\\displaystyle \\Longrightarrow \\max_{\\lambda_i \\ge 0} L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda}   \\right) \\le f\\left( \\overrightarrow{x}  \\right)$ <br><br>\n- So, the optimal value to the constrained optimization:<br><br>\n  $\\displaystyle p^*:=\\min_{\\overrightarrow{x} } \\max_{\\lambda_i \\ge 0} L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda}   \\right)$ <br><br>\n  We can see that now problem becomes unconstrained in $x$ <br>\n  Also $p^*$ is called __The primal__ problem<br><br>\n- Observation:<br>\n  consider a function: $\\displaystyle \\min_{\\overrightarrow{x} }  L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda}   \\right)$<br>\n  Since   $p^*$ is solution for maximum possible $\\lambda$ so for any feasible $x$ and all $\\lambda_i \\ge 0$<br><br>\n  $\\displaystyle p^* \\ge \\min_{\\overrightarrow{x} }  L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda}   \\right)$ <br>\n  Thus:<br><br>\n  $\\displaystyle d^*:= \\max_{\\lambda_i \\ge 0} \\min_{\\overrightarrow{x} } L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda}   \\right) \\le p^*$ <br><br>\n  Also $d^*$ is called __The dual__ problem<br><br>\n  \n\n\nIn short:\n\n:::{.callout-note}\n\n__Optimization problem__:<br>_Minimize_ : \n$\\displaystyle f\\left( \\overrightarrow{x}  \\right)$ <br>_Such that_ for all $i,$ $\\displaystyle g_i\\left( \\overrightarrow{x}  \\right)\\le 0$ <br>_Lagrange Function_ : $\\displaystyle L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda}   \\right):=f\\left( \\overrightarrow{x}  \\right)+\\sum_{i=1}^{n}\\lambda_ig_i\\left( \\overrightarrow{x}  \\right)$ <br><br> __Primal__: \n$\\displaystyle p^*:=\\min_{\\overrightarrow{x} } \\max_{\\lambda_i \\ge 0} L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda}   \\right)$ <br><br>__Dual__: $\\displaystyle d^*:= \\max_{\\lambda_i \\ge 0} \\min_{\\overrightarrow{x} } L\\left( \\overrightarrow{x},\\overrightarrow{\\lambda}   \\right)$ <br><br>\n\n:::\n\n\n- Theorem (weak Lagrangian duality):<br><br>\n  $d^* \\le p^*$<br><br>\n  This is also called as minimax inequality<br><br>\n  $p^*-d^*$ is called duality gap<br><br>\n\n- There are certain condition when duality gap becomes zero, for that we need to understand convexity\n  - A function $f:\\mathbb{R}^d \\rightarrow \\mathbb{R}$ is called convex iff for any two point $x$ and $x'$ and $\\beta \\in \\left[ 0,1 \\right]$<br><br>\n    $f\\left( \\beta\\overrightarrow{x}+\\left( 1-\\beta \\right)\\overrightarrow{x}   \\right) \\le \\beta f\\left( \\overrightarrow{x}  \\right)+\\left( 1-\\beta \\right)f\\left( \\overrightarrow{x}  \\right)$<br><br>\n![](CS5590_images/Acrobat_9xAcM49wbq.png)<br><br>\n  - A set $S \\subset\\mathbb{R}^d$ is called conved iff for any tow points  $x, x' \\in S$ and any  $\\beta \\in \\left[ 0,1 \\right]$<br><br>\n    $\\beta \\overrightarrow{x}+\\left( 1-\\beta \\right)\\overrightarrow{x} \\in S$<br><br>\n![](CS5590_images/Acrobat_4WENH5ng1n.png)<br><br>\n  - Convex Optimization problem <br><br>\n    $\\displaystyle \\min_{\\overrightarrow{x} \\in \\mathbb{R}^d } f\\left( \\overrightarrow{x}  \\right)$ <br><br>\n    subject to: $\\displaystyle g_i\\left( \\overrightarrow{x}  \\right)\\le 0$ for $1 \\le i \\le n$<br><br>\n    is called convex optimization problem if:<br>\n    - The objective function $f\\left( \\overrightarrow{x}  \\right)$ is convex function, and\n    - the feasible set induced by the constraints $g_i$ is a convex set.<br><br>\n- Theorem (strong Lagrangian duality):<br><br>\n  if $f$ is convex and for a feasible point $x^*$<br><br>\n  $g_i\\left( \\overrightarrow{x^*}  \\right)<0$, or <br><br>\n  $g_i\\left( \\overrightarrow{x^*}  \\right) \\le 0$ when $g$ is affine<br><br>\n  Then $d^*=p^*$<br><br>\n  This is called Slater's condition. \n\n\n\n## SVM standard (primal) form\n\n$\\displaystyle \\min_{w,b}\\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2$ <br><br>\nsuch that: $\\forall$ $i,$ $y_i\\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i}   +b \\right) \\ge 1$ \n\n- Observations :\n    - Objective function is _convex_\n    - the constraints are _affine_, inducing a polytope constraint set.\n- So SVM is a convex optimization problem (in fact a quadratic program)\n- Moreover, __strong duality holds__.\n\n\n- Lagrangian for SVM\n    - For Lagrangian the constraint should always  written as less than $0$ format: <br><br>\n      $y_i\\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i}   +b \\right) -1 \\ge 0$ <br><br>\n      $- y_i\\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i}   +b \\right) +1 \\le 0$ <br><br>\n      $1 - y_i\\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i}   +b \\right) \\le 0$ <br><br>\n    - Now the Lagrangian for SVM can be written as:<br><br>\n      $\\displaystyle L\\left( \\overrightarrow{w},b,\\overrightarrow{\\alpha}   \\right)=\\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 + \\underbrace{\\sum_{i = 1}^{n}\\alpha_i\\left( 1-y_i\\left( \\overrightarrow{w}\\cdot\\overrightarrow{x_i}+b   \\right) \\right)}_{\\text{appears like a hinge loss}}$<br><br>\n\n## SVM Dual\n\n- Primal<br><br>\n  $$\\displaystyle \\min_{\\overrightarrow{w},b }\\max_{\\overrightarrow{\\alpha } \\ge 0 }\\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 + \\sum_{i = 1}^{n}\\alpha_i\\left( 1-y_i\\left( \\overrightarrow{w}\\cdot\\overrightarrow{x_i}+b   \\right) \\right)$$\n  <br><br>\n- Dual<br><br>\n  $$\\displaystyle\\max_{\\overrightarrow{\\alpha } \\ge 0 } \\min_{\\overrightarrow{w},b }\\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 + \\sum_{i = 1}^{n}\\alpha_i\\left( 1-y_i\\left( \\overrightarrow{w}\\cdot\\overrightarrow{x_i}+b   \\right) \\right)$$\n  <br><br>\n- Slater's condition from convex optimization guarantees that these two optimization problems are equivalent!\n\n## Solving using KKT condition\n\nKKT stands for Karush-Kuhn-Tucker Condition\n- We solve Dual problem:<br><br>\n  $\\displaystyle\\max_{\\overrightarrow{\\alpha } \\ge 0 } \\min_{\\overrightarrow{w},b }\\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 + \\sum_{i = 1}^{n}\\alpha_i\\left( 1-y_i\\left( \\overrightarrow{w}\\cdot\\overrightarrow{x_i}+b   \\right) \\right)$<br><br>\n\n- We can solve for optimal $w$, $b$ as function of $\\alpha$<br><br>\n  $\\displaystyle \\frac{\\partial L }{\\partial \\overrightarrow{w}}=  w - \\sum_{i}\\alpha_iy_i\\overrightarrow{x}_i=0 \\Rightarrow w  =  \\sum_{i}\\alpha_iy_i\\overrightarrow{x}_i$<br><br>\n  $\\displaystyle \\frac{\\partial L }{\\partial b}=  \\sum_{i}\\alpha_iy_i=0 \\Rightarrow  \\sum_{i}\\alpha_iy_i =0$<br><br>\n\n\n\n\n- substituting these values back in Dual we get: <br><br>\n  $\\displaystyle \\max_{\\overrightarrow{\\alpha } \\ge 0 }  \\frac{1}{2} \\left( \\sum_{i}\\alpha_iy_i\\overrightarrow{x}_i  \\right) \\cdot \\left( \\sum_{j}\\alpha_jy_j\\overrightarrow{x}_j \\right) + \\sum_{i = 1}^{n}\\alpha_i\\left( 1-y_i\\left( \\sum_{j}\\alpha_jy_j\\overrightarrow{x}_j \\cdot\\overrightarrow{x_i}+b   \\right) \\right)$<br><br>\n  $\\displaystyle \\max_{\\overrightarrow{\\alpha } \\ge 0 } \\frac{1}{2} \\sum_{i,j}\\alpha_i\\alpha_jy_iy_j\\overrightarrow{x}_i\\cdot\\overrightarrow{x} _j  + \\sum_{i = 1}^{n}\\alpha_i-\\sum_{i = 1}^{n}  \\left(     \\alpha_iy_i\\left( \\sum_{j}\\alpha_jy_j\\overrightarrow{x} _j\\cdot\\overrightarrow{x_i}+b\\right) \\right)$<br><br>\n  $\\displaystyle \\max_{\\overrightarrow{\\alpha } \\ge 0 } \\frac{1}{2} \\sum_{i,j}\\alpha_i\\alpha_jy_iy_j\\overrightarrow{x}_i\\cdot\\overrightarrow{x} _j  + \\sum_{i = 1}^{n}\\alpha_i- \\sum_{i = 1}^{n}\\alpha_iy_i\\sum_{j}\\alpha_jy_j\\overrightarrow{x} _j\\cdot\\overrightarrow{x_i}+  \\sum_{i = 1}^{n}\\alpha_iy_ib$<br><br>\n  $\\displaystyle \\max_{\\overrightarrow{\\alpha } \\ge 0 } -\\frac{1}{2} \\sum_{i,j}\\alpha_i\\alpha_jy_iy_j\\overrightarrow{x}_i\\cdot\\overrightarrow{x} _j  + \\sum_{i = 1}^{n}\\alpha_i +  \\sum_{i = 1}^{n}\\alpha_iy_ib$<br><br>\n  $\\displaystyle \\max_{\\overrightarrow{\\alpha } \\ge 0 } \\sum_{i = 1}^{n}\\alpha_i  -\\frac{1}{2} \\sum_{i,j}\\alpha_i\\alpha_jy_iy_j\\overrightarrow{x}_i\\cdot\\overrightarrow{x} _j$<br><br>\n\n- The above equation can also be written as: <br><br>\n  $\\displaystyle \\max \\sum_{k = 1}^{R}\\alpha_k  - \\frac{1}{2}\\sum_{k=1}^{R}\\sum_{l = 1}^{R}\\alpha_k \\alpha_l Q_{kl},$ where $\\displaystyle Q_{kl} =y_ky_l\\left( \\mathbf{X_k}\\cdot \\mathbf{X_l} \\right)$<br><br>\n  subject to constrains:<br><br>$\\alpha_k \\ge 0,$ and $\\forall k ,$ $\\displaystyle \\sum_{k=1}^{R}\\alpha_ky_k=0$<br><br>\n\n- Above problem can be solved using SMO (Sequential minimal optimization) or any other quadratic programming or gradient descent. \n- Once we solve we get optimum $\\alpha^*$\n- Using  $\\alpha^*$   we can get $w^*$ as below:<br><br>\n $\\displaystyle  w^*  =  \\sum_{i}\\alpha_i^*y_i\\overrightarrow{x}_i$<br><br>\n- $b^*$ can be calculated as follows:<br><br>\n  $y_i\\left( \\overrightarrow{w}^* \\cdot \\overrightarrow{x_i}   +b \\right) = 1$ <br><br>\n  $y_i\\left( \\overrightarrow{w}^* \\cdot \\overrightarrow{x_i} \\right) +  y_ib = 1$ <br><br>\n  Multiplying $y_i$ both the sides: <br><br>\n  $y_i y_i\\left( \\overrightarrow{w}^* \\cdot \\overrightarrow{x_i} \\right) +  y_i y_ib = y_i$ <br><br>\n  $y_i y_ib = y_i -y_i y_i\\left( \\overrightarrow{w}^* \\cdot \\overrightarrow{x_i} \\right)$ <br><br>\n  $y_i y_i b$ can be written as $b$ because $y_i$ can be only $\\pm 1$ so in either case $y_i y_i =1$<br><br>\n  $b = y_i \\left( 1-  y_i\\left( \\overrightarrow{w}^* \\cdot \\overrightarrow{x_i} \\right)  \\right)$ <br><br>\n  $b^* = - y_i \\left( y_i\\left( \\overrightarrow{w}^* \\cdot \\overrightarrow{x_i} \\right)- 1  \\right)$ <br><br> \n\n- Now we can classify with:<br><br>\n$f(\\mathbf{X},\\mathbf{W}^*,b^*)=\\mathrm{sign} (\\mathbf{W}^* \\cdot \\mathbf{X}+b^*)$<br><br> \n\n# Soft Margin SVM\n\n- Till now what we discussed is called Hard margin SVM.\n- In practice data is not always separable\n- We allow misclassification of the data point in soft margin SVM.\n- Soft margin SVM is also called as C-SVM\n\n- Now our Lagrangian is formulated as below:<br><br>\n$\\displaystyle \\min_{w,b,\\varepsilon}\\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 + c \\sum_{j=1}^{N}\\varepsilon_j$  <br><br>\nsuch that: $\\forall$ $i,$ $y_i\\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i}   +b \\right) \\ge 1-\\varepsilon _i$, and $\\varepsilon _i\\ge0$ <br><br>\n\n- Significance of $\\varepsilon$<br><br>\n    - $\\displaystyle \\min_{w,b,\\varepsilon}\\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 + \\overbrace{c}^{\\text{Controls amount of misclassification}}  \\underbrace{\\sum_{j=1}^{N}\\varepsilon_j }_{\\text{Minimize }\\varepsilon \\text{ also  } }$<br><br> \n    such that: $\\forall$ $i,$ $y_i\\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i}   +b \\right) \\ge 1-\\underbrace{\\varepsilon _i}_{\\text{to allow misclassification}} ,$ and $\\overbrace{\\varepsilon _i\\ge0}^{\\text{keep } \\varepsilon \\text{ positive} }$ <br><br> \n    - $\\varepsilon_i \\ge 1 \\Longleftrightarrow y_i \\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i}   +b \\right) < 0,$ i.e., misclassification.\n    - $0<\\varepsilon_i < 1 \\Longleftrightarrow  x_i$ is correctly classified, but lies inside the margin\n    - $\\varepsilon_i =0 \\Longleftrightarrow  x_i$ is correctly classified, and lies outside of margin\n    - $\\sum_{j=1}^{N}\\varepsilon_j$ is an upper bound on training errors.\n\n## Lagrangian for Soft Margin SVM\n\n\n- We need to Solve:<br><br>\n$\\displaystyle \\min_{w,b,\\varepsilon}\\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 + c \\sum_{i=1}\\varepsilon_i$  <br><br>\nsuch that: $\\forall$ $i,$ $y_i\\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i}   +b \\right) \\ge 1-\\varepsilon _i$, and $\\varepsilon _i\\ge0$ <br><br>\n- For Lagrangian the constraint should always  written as less than $0$ condition: <br><br>\n  $y_i\\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i}   +b \\right) \\ge 1-\\varepsilon _i$, and $\\varepsilon _i\\ge0$ <br><br>\n  $y_i\\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i}   +b \\right) - 1 + \\varepsilon _i \\ge 0$, and $-\\varepsilon _i \\le 0$ <br><br>\n  $1 - \\varepsilon _i  -y_i\\left( \\overrightarrow{w}\\cdot \\overrightarrow{x_i}   +b \\right) \\le 0$, and $-\\varepsilon _i \\le 0$ <br><br>\n- Lagrangian formulation of above problem:<br><br>\n  $\\displaystyle L\\left( \\overrightarrow{w},b,\\overrightarrow{\\alpha},\\overrightarrow{\\beta},\\overrightarrow{\\varepsilon }     \\right)= \\\\ \\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 +c\\sum_{i}\\varepsilon_i + \\sum_{i = 1}^{n}\\alpha_i\\left( 1- \\varepsilon _i - y_i\\left( \\overrightarrow{w}\\cdot\\overrightarrow{x_i}+b   \\right) \\right) -\\sum_{i}\\beta_i \\varepsilon_i$<br><br>\n\n\n- Primal<br><br>\n  $$\\displaystyle  \\min_{ \\left(  \\overrightarrow{w},b,\\overrightarrow{\\varepsilon } \\right)}\\max_{\\left(  \\overrightarrow{\\alpha } \\ge 0 ,\\overrightarrow{\\beta} \\ge 0  \\right)}\\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 +c\\sum_{i}\\varepsilon_i + \\sum_{i = 1}^{n}\\alpha_i\\left( 1- \\varepsilon _i - y_i\\left( \\overrightarrow{w}\\cdot\\overrightarrow{x_i}+b   \\right) \\right) -\\sum_{i}\\beta_i \\varepsilon_i  $$\n  <br><br>\n- Dual<br><br>\n  $$\\displaystyle  \\max_{\\left( \\overrightarrow{\\alpha } \\ge 0 , \\overrightarrow{\\beta} \\ge 0  \\right)} \\min_{\\left( \\overrightarrow{w},b,\\overrightarrow{\\varepsilon }  \\right)}\\frac{1}{2}\\left\\lVert \\overrightarrow{w} \\right\\rVert^2 +c\\sum_{i}\\varepsilon_i + \\sum_{i = 1}^{n}\\alpha_i\\left( 1- \\varepsilon _i - y_i\\left( \\overrightarrow{w}\\cdot\\overrightarrow{x_i}+b   \\right) \\right) -\\sum_{i}\\beta_i \\varepsilon_i  $$\n  <br><br>\n\n- Same as Hard margin SVM we use KKT condition and solve minimization of dual:<br><br>\n   $\\displaystyle \\frac{\\partial L }{\\partial \\overrightarrow{w}}=  w - \\sum_{i}\\alpha_iy_i\\overrightarrow{x}_i=0 \\Rightarrow w  =  \\sum_{i}\\alpha_iy_i\\overrightarrow{x}_i$<br><br>\n  $\\displaystyle \\frac{\\partial L }{\\partial b}=  \\sum_{i}\\alpha_iy_i=0 \\Rightarrow  \\sum_{i}\\alpha_iy_i =0$<br><br>\n  $\\displaystyle \\frac{\\partial L }{\\partial \\varepsilon _i}= c -\\alpha_i  - \\beta_i = 0\\Rightarrow  c=\\beta_i+\\alpha_i$<br><br>\n  - Observation:<br><br>\n   $\\overbrace{c}^{\\text{Upper bound of }\\alpha \\text{ and }\\beta} =\\underbrace{\\beta_i}_{\\text{always +ve}} +\\underbrace{\\alpha_i}_{\\text{always +ve}}$<br><br>\n   so we can say: $0 \\le \\alpha_i \\le c$ $\\forall i$<br><br>\n\n\n- substituting these values back in Dual we get: <br><br>\n$\\displaystyle \\Rightarrow   \\max_{\\overrightarrow{\\alpha } \\ge 0 , \\overrightarrow{\\beta} \\ge 0} \\frac{1}{2}\\left( \\sum_{i}\\alpha_iy_i\\overrightarrow{x}_i  \\right) \\cdot \\left( \\sum_{j}\\alpha_jy_j\\overrightarrow{x}_j \\right) +\\sum_{i}\\varepsilon_i\\left( \\beta_i+\\alpha_i \\right) + \\sum_{i = 1}^{n}\\alpha_i\\left( 1- \\varepsilon _i - y_i\\left( \\sum_{j}\\alpha_jy_j\\overrightarrow{x}_j \\cdot\\overrightarrow{x_i}+b\\right) \\right) -\\sum_{i}\\beta_i \\varepsilon_i$<br><br>\n$\\displaystyle \\Rightarrow \\max_{\\overrightarrow{\\alpha } \\ge 0 , \\overrightarrow{\\beta} \\ge 0} \\frac{1}{2}\\left( \\sum_{i}\\alpha_iy_i\\overrightarrow{x}_i  \\right) \\cdot \\left( \\sum_{j}\\alpha_jy_j\\overrightarrow{x}_j \\right) +\\sum_{i}\\varepsilon_i \\beta_i+\\sum_{i}\\varepsilon_i  \\alpha_i  +  \\sum_{i = 1}^{n}\\alpha_i - \\sum_{i = 1}^{n}\\alpha_i \\varepsilon _i - \\sum_{i = 1}^{n}\\alpha_i y_i\\left( \\sum_{j}\\alpha_jy_j\\overrightarrow{x}_j \\cdot\\overrightarrow{x_i}+b   \\right)  -\\sum_{i}\\beta_i \\varepsilon_i$<br><br>\n$\\displaystyle \\Rightarrow \\max_{\\overrightarrow{\\alpha } \\ge 0 , \\overrightarrow{\\beta} \\ge 0} \\frac{1}{2} \\sum_{i}\\alpha_i \\alpha_j y_i y_j \\overrightarrow{x}_i \\overrightarrow{x}_j   +\\sum_{i}\\varepsilon_i \\beta_i+\\sum_{i}\\varepsilon_i  \\alpha_i  +  \\sum_{i = 1}^{n}\\alpha_i - \\sum_{i = 1}^{n}\\alpha_i \\varepsilon _i - \\sum_{i = 1}^{n}\\alpha_i y_i \\sum_{j}\\alpha_jy_j\\overrightarrow{x}_j \\cdot\\overrightarrow{x_i}+  \\sum_{i = 1}^{n}\\alpha_i y_ib     -\\sum_{i}\\beta_i \\varepsilon_i$<br><br>\n$\\displaystyle \\Rightarrow \\max_{\\overrightarrow{\\alpha } \\ge 0 } \\frac{1}{2} \\sum_{i}\\alpha_i \\alpha_j y_i y_j \\overrightarrow{x}_i \\overrightarrow{x}_j     +  \\sum_{i = 1}^{n}\\alpha_i  - \\sum_{i = 1}^{n}\\alpha_i y_i \\sum_{j}\\alpha_jy_j\\overrightarrow{x}_j \\cdot\\overrightarrow{x_i}+  \\sum_{i = 1}^{n}\\alpha_i y_ib$<br><br>\n$\\displaystyle \\Rightarrow \\max_{\\overrightarrow{\\alpha } \\ge 0 } \\frac{1}{2} \\sum_{i}\\alpha_i \\alpha_j y_i y_j \\overrightarrow{x}_i \\overrightarrow{x}_j     +  \\sum_{i = 1}^{n}\\alpha_i  - \\sum_{i=1,j=1}^{n}\\alpha_i \\alpha_j y_i  y_j \\overrightarrow{x_i} \\cdot \\overrightarrow{x_j}+  \\sum_{i = 1}^{n}\\alpha_i y_ib$<br><br>\n$\\displaystyle \\Rightarrow \\max_{\\overrightarrow{\\alpha } \\ge 0 } \\sum_{i = 1}^{n}\\alpha_i  -\\frac{1}{2} \\sum_{i}\\alpha_i \\alpha_j y_i y_j \\overrightarrow{x}_i \\overrightarrow{x}_j$<br><br>\n\n- __Notice neither $\\overrightarrow{\\beta}$ nor $\\overrightarrow{\\varepsilon }$ appears in the above equation__.<br><br>\n\n- The above equation  can also be written as: <br><br>\n  $\\displaystyle \\max \\sum_{k = 1}^{R}\\alpha_k  - \\frac{1}{2}\\sum_{k=1}^{R}\\sum_{l = 1}^{R}\\alpha_k \\alpha_l Q_{kl},$ where $\\displaystyle Q_{kl} =y_ky_l\\left( \\mathbf{X_k}\\cdot \\mathbf{X_l} \\right)$<br><br>\n  subject to constrains:<br><br>$0 \\le \\alpha_k \\le c,$ and $\\forall k ,$ $\\displaystyle \\sum_{k=1}^{R}\\alpha_ky_k=0$<br><br>\n\n:::{.callout-note}\n\nOne of the constraint of soft margin SVM is $0 \\le \\alpha_k \\le c$ which is different for hard margin SVM constraint $\\alpha_k \\ge 0$\n\n:::\n\n# Multi-class Classification with SVMs\n\n- SVM can handle only tow-class outputs.\n- what to do for multi-class case:\n    - one vs all SVM\n        - Learn N SVMs\n        - SVM 1 learns class $1$ vs not class $1$\n        - SVM 2 learns class $2$ vs not class $2$ and so on.\n        - Then to predict the output for a new point, just predict with each SVM and fond out which one puts the prediction the furthest into the positive region.\n    - Other approaches:\n        - pair-wise SVM\n        - Tree-structured SVM\n\n# Kernel Trick  \n\n## Why do we require the Kernel Trick\n\n- We found that after solving minimization problem of dual of SVM we get following: <br>\n  $\\displaystyle \\max \\sum_{k = 1}^{R}\\alpha_k  - \\frac{1}{2}\\sum_{k=1}^{R}\\sum_{l = 1}^{R}\\alpha_k \\alpha_l Q_{kl},$ where $\\displaystyle Q_{kl} =y_ky_l\\left( \\mathbf{X_k}\\cdot \\mathbf{X_l} \\right)$<br><br>\n  subject to constrains:<br><br>$0 \\le \\alpha_k \\le c,$ and $\\forall k ,$ $\\displaystyle \\sum_{k=1}^{R}\\alpha_ky_k=0$<br><br>\n\n- But if the data can't be separated linearly we transform the data to higher dimension space using the transformation $\\phi$. So that the data can be separated using a hyper-plane in higher dimension space. In that case the above equation changes as below : <br>\n$\\displaystyle \\max \\sum_{k = 1}^{R}\\alpha_k  - \\frac{1}{2}\\sum_{k=1}^{R}\\sum_{l = 1}^{R}\\alpha_k \\alpha_l Q_{kl},$ where \n$\\displaystyle Q_{kl} =y_ky_l \\underbrace{\\left(\\mathbf{\\Phi} \\left(  \\mathbf{X}_k \\right)\\cdot \\mathbf{\\Phi}  \\left( \\mathbf{X}_l \\right)\\right)}_{\\text{Notice the term } \\Phi }$<br><br>\n  subject to constrains:<br><br>$0 \\le \\alpha_k \\le c,$ and $\\forall k ,$ $\\displaystyle \\sum_{k=1}^{R}\\alpha_ky_k=0$<br><br>\n\n- Then compute : <br>\n$\\displaystyle  \\mathbf{W}  =  \\sum_{\\text{k s.t } \\alpha_k >0 }\\alpha_k^*y_k \\mathbf{\\Phi} \\left(  \\mathbf{X}_k  \\right)$\n- Then classify with <br>\n$\\displaystyle f(\\mathbf{X},w,b)=\\mathrm{sign}\\left( \\mathbf{W}  \\cdot \\mathbf{\\Phi}(\\mathbf{X})+b \\right)$\n\n- Most important change : <br>\n$\\mathbf{X} \\rightarrow \\mathbf{\\Phi}(\\mathbf{X})$\n\n- Notice that in the term $\\displaystyle Q_{kl} =y_ky_l \\left(\\mathbf{\\Phi} \\left(  \\mathbf{X}_k \\right)\\cdot \\mathbf{\\Phi}  \\left( \\mathbf{X}_l \\right)\\right)$ we must do $\\frac{R^2}{2}$ dot products to get this matrix ready.<br><br>\nAssuming a quadratic polynomial kernel, each dot product requires $\\frac{m^2}{2}$ addition and multiplication ( where $m$ is the dimension of $X$) <br><br>\nThe whole thing costs $\\frac{R^2m^2}{4}$<br><br>\nThis is the reason we require a trick so that we need not do this large computation.\n\n## How  do we do the kernel Trick\n\nTo understand we create a data in circular fashion as shown below:\n\n- The two circle can't be separated by a line.\n- Now we transform the data to 3 dimension using below function.<br><br>\n$\\phi\\left( \\mathbf{X}\\right) =\n\\phi\\left( \n\\left( \\begin{array}{c}\n    x_1\\\\\n    x_2 \n\\end{array} \\right) \\right) = \n\\left( \\begin{array}{c}\nx_1^2 \\\\\n\\sqrt{2}x_1x_2 \\\\\nx_2^2    \n\\end{array} \\right)$\n- Python implementation of the same is shown below:\n\n\n- We can see in below pic that how data becomes sparable in  3 dimensional space.\n\n- In case of SVM we need the dot product of the transformed data, not the transformed data itself.\n\n\n- Consider two vectors $\\mathbf{a}$ and $\\mathbf{b}$, we will apply following transformation on it :<br><br>\n$\\phi\\left( \\mathbf{X}\\right) =\n\\phi\\left( \n\\left( \\begin{array}{c}\n    x_1\\\\\n    x_2 \n\\end{array} \\right) \\right) = \n\\left( \\begin{array}{c}\nx_1^2 \\\\\n\\sqrt{2}x_1x_2 \\\\\nx_2^2    \n\\end{array} \\right)$ <br><br>\nWe get below result: <br><br>\n$\\phi\\left( \\mathbf{a}\\right)^T\\phi\\left( \\mathbf{b}\\right)=\n\\left( \\begin{array}{c}\na_1^2 \\\\\n\\sqrt{2}a_1a_2 \\\\\na_2^2    \n\\end{array} \\right)^T \\cdot\n\\left( \\begin{array}{c}\nb_1^2 \\\\\n\\sqrt{2}b_1b_2 \\\\\nb_2^2    \n\\end{array} \\right)=\na_1^2 b_1^2 + 2 a_1 b_1 a_2 b_2 +a_2^2 b_2^2=\n\\left( a_1b_1+a_2b_2\\right)^2=\n\\left( \n\\left( \\begin{array}{c}\n    a_1\\\\\n    a_2 \n\\end{array} \\right)^T \\cdot\n\\left( \\begin{array}{c}\n    b_1\\\\\n    b_2 \n\\end{array} \\right)\n\\right)^2 = \n\\left( \\mathbf{a}^T \\cdot \\mathbf{b} \\right)^2$\n\n- The kernel function here is polynomial function.\n- we can see that we don't even need to use $\\phi$ we can get the result just from  $\\left( \\mathbf{a}^T \\cdot \\mathbf{b} \\right)^2$\n- So we never need to transformed the data to higher domain still we get the same benefit at the less computation cost.\n- The same is explained below:\n\n- From above figure we can see that we get exact same result in original domain without transforming data to the higher dimensional space\n- Not all functions are kernel functions.\n    - Need to be decomposable: $K(a,b)=\\phi(a)\\cdot \\phi(b)$\n- Mercer's condition : To expand kernel function $K(X,Y)$ into a dot product, i.e. $K(x,y)=\\Phi(x).\\Phi(y)$, $K(x,y)$ has to be positive [semi-definite](https://mathworld.wolfram.com/PositiveSemidefiniteMatrix.html#:~:text=A%20positive%20semidefinite%20matrix%20is,Language%20using%20PositiveSemidefiniteMatrixQ%5Bm%5D.) function, i.e., for any function $f(X)$ whose $\\displaystyle \\int f^2(x)dx$ is finite, the following inequality holds:<br>\n$$\\displaystyle \\int dx dy f(x) K(x,y) f(y) \\ge 0$$\n\n- It is not easy to select the kernel function which will work best for the given data.\n- RBF kernels are considered good in general, especially for images (and other smooth functions/data).\n- For discrete data, chi-square kernel is preferred.\n- we can also do Multiple Kernel learning.\n- If still it doesn't work we can use cross-validation to select a kernel function from some basic options.\n- Same kernel trick can also be applied to other methods including:\n    - Kernel k-NN\n    - Kernel Perceptron \n    - Kernelized Linear Regression\n    - etc.\n\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"center","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"css":["../../styles.css"],"html-math-method":"katex","output-file":"2022-09-10-CS5590-week4.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","comments":{"giscus":{"repo":"abhiyantaabhishek/IITH-Data-Science","category":"Announcements"}},"toc-location":"right","theme":"cosmo","title-block-banner":true,"author":"Abhishek Kumar Dubey","badges":true,"categories":["Machine Learning"],"date":"2022-09-10","description":"SVM, Lagrange Multipliers, KKT condition, Mercerâ€™s condition, Kernel Trick","image":"CS5590_images/Acrobat_vct8kMqVEu.png","title":"Machine Learning 4"},"extensions":{"book":{"multiFile":true}}}}}