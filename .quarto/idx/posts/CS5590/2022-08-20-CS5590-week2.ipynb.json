{"title":"Machine Learning 2","markdown":{"yaml":{"author":"Abhishek Kumar Dubey","badges":true,"categories":["Machine Learning"],"date":"2022-08-20","description":"Classification Error, ROC/PR curve, KNN","image":"CS5590_images/chrome_CNeo323E9V.png","title":"Machine Learning 2","toc":true},"headingText":"Evaluation Measures","containsRefs":false,"markdown":"\n\n\n\n- Classification\n    - It is a measure of being  right/wrong,0-1, eg:  hinge loss, cross entropy loss \n- Regression loss\n    - It is a measure if how close we are to target, eg:  MEA, MES\n- Ranking/search\n    - It is a measure of top K search\n- Clustering \n    - How well we have described the data ( not straight forward)\n\n## Is accuracy adequate\n\nAccuracy may not not be useful in cases where:\n- There is a large class skew.\n- There are differential misclassification cost, say getting a positive wrong costs more than getting a negative wrong.\n- we are most interested in a subset of high confidence predictions. \n\n## Classification Error\n\n![](CS5590_images/chrome_CNeo323E9V.png)\n\n\n:::{.callout-tip}\n\nPrecision = How many retrieved items are relevant? <br> Recall = How many relevant items are retrieved?\n\n:::\n\n:::{.callout-tip}\n\nsensitivity = Probability of positive test given a patient has a disease.<br> Specificity = Probability of a negative test given a patient is well.<br>Specificity = 1 - False Alarm\n\n:::\n\n## Utility and cost\n- Detection Cost:\n    - cost = $C_{\\mathrm{FP}} \\times \\mathrm{FP}+C_{\\mathrm{FN}} \\times \\mathrm{FN}$\n- Fmeasure\n    - $F1=\\frac{2\\times \\left(\\mathrm{Recall}\\times \\mathrm{Precision}\\right)}{\\mathrm{Recall}+\\mathrm{Precision}}$\n\n## ROC curve\n- Receiver Operative Curve\n- Plot between True positive rate on y axis and False positive rate on x axis \n- AUC : Area under the curve, higher the area, better the performance\n\nA Receiver Operating Characteristic (ROC) curve plots the TP rate vs. the FP rate as a threshold on the confidence of an instance being positive is varied : <br><br>\n![](CS5590_images/Acrobat_LfrCm8Z5SQ.png)\n<br><br>\n![](CS5590_images/MATLAB_eYMZjl3LEJ.png)\n\n# Precision Recall Curve\n- Plot between Precision on y axis and recall (TPR) on x axis. It is used for class imbalanced dataset mostly, It is also used when we can not calculate True Negative. \n\nA precision/recall curve plots the precision vs. recall (TP rate) as a threshold on the confidence of an instance being positive is varied.\n<br><br>\n![](CS5590_images/Acrobat_jfeedglqjh.png)\n\n# A nice way to define precision recall etc.\nConsider $Y$ as `true label` and $\\hat{Y}$ as `predicted label`. \n\n- precision  $=P\\left(Y=1|\\;\\hat{Y} =1\\right)$\n- Recall (TPR)   $=P\\left(\\hat{Y} =1|\\;Y=1\\right)$\n- False positive Rate (FPR) $=P\\left(\\hat{Y} =1|\\;Y=0\\right)$\n- True Negative Rate (TNR)= $=P\\left(\\hat{Y} =0|\\;Y=0\\right)$\n<br><br>\n\n\n:::{.callout-note}\n\nNotice that TPR and FPR which makes ROC curve  is conditioned on the  `true value`, and precision which is used in PR curve is conditioned on `predicted label`. This is the reason, PR curve is used for class imbalanced dataset, or where positive class is more interesting then negative class.<br> [If  question is: _\"How meaningful is a positive result from my classifier given the baseline probabilities of my problem?\"_, use a PR curve. If  question is, _\"How well can this classifier be expected to perform in general, at a variety of different baseline probabilities?\"_, go with a ROC curve.](https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves)\n\n:::\n\n# ROC curve vs PR curve\n\nConsider a dataset having 100 positive cases and 10,000 negative cases. Now consider 2 classifiers $A$ and $B$. $A$ predicts 9 as true positive, 40 as false positive whereas $B$ predicts 9 as true positive 1000 as false positive.<br>\nWe can observe that as both the classifier predicts 9 out of 10 as true positive so both has same recall value, also FPR is small(as it can be seen in below picture), so the ROC curve which is drawn between recall (TPR) and FPR, will not differentiate among the two classifier. <br>\nBut PR curve which is drawn between precision and recall, will look totally different here as both has different precision.\n\nAccuracy also is not a good measure in above case. As both the classifier has similar accuracy. \n\n# Other performance measures\n- Kullback-Leibler Diverfence : $D_{\\mathrm{KL}} \\left(P\\|Q\\right)=\\sum_i P\\left(i\\right)\\log \\frac{P\\left(i\\right)}{Q\\left(i\\right)}$\n- Gini Statistic : $2\\times \\mathrm{AUC}-1$\n- F1 score: $\\frac{2\\times \\left(\\mathrm{Recall}\\times \\mathrm{Precision}\\right)}{\\mathrm{Recall}+\\mathrm{Precision}}$\n- Akaike Information Criterion (AIC): $2k-2\\ln \\left(L\\right)$, here  $k$ is number of model parameters, L is max value of the Likelihood  function for the model\n\n# Important points\n- Randomization of data is essential so that held-aside test data can be really representative of new data. \n- Test set should never be used in any way for normalization, hyper parameter tuning etc.\n- Any preprocessing done over entire data set ( feature selection, parameter tuning, threshold selection ) must not use labels from test set. \n\n# K-Nearest Neighbors\n\n## basic idea\n- If it walks like a duck, quacks like a duck , then it's probably a duck.\n- If data points are represented well then KNN works well.\n- choosing K is important, if K is too small then it becomes sensitive to noise point. If k is too large, neighborhood may incudes points from other class.\n- Euclidean distance between two instance $d\\left(X_i ,X_j \\right)=\\sqrt{\\sum_{r=1}^n {\\left(a_r \\left(X_i \\right)-a_r{\\left(X_j \\right)} \\right)}^2 }$  here $a_i \\left(X\\right)\\;$ denotes features.\n- In case of continuous valued target function, Mean value of K nearest training examples is taken\n\n## How to determinke K\n- experiment with different value of K starting form 1 on test set to validate the error, in case of binary classification use odd number for k to avoid ties.<br>\n\nKNN is a transductive method, there is no training involved , it is refereed as Lazy learning, Learning is just storing all the training instances <br> \nSimilar Keywords: KNN, Memory Based Reasoning, Example Based Reasoning, Instance Based Learning, Case Based Reasoning, Lazy Learning<br>\nVoronoi Diagram: Decision surface formed by the training Examples for 1 nearest neighbors classifier<br> \n\n\n## Improvements\n- Distance weighted Nearest Neighbors\n- Scaling (normalization) attributes for fair computation fo distances \n- Measure \"closeness\" differently\n- Finding \"close\" example in large training set quickly , eg Efficient memory indexing using kd-tree\n\n## Pros\n- Highly effective transductive inference method for noisy training data and complex target functions.\n- Target function for a whole space may be described as a combinations of less complex local  approximations \n- Trains very fast (Lazy Learner)\n\n## Cons\n- Curse of dimensionality \n- Storage: all training example are saved in memory \n- slow at query time, can be overcome by  pre sorting and indexing training samples.\n\n## Convergence of 1-NN\n$P\\left(\\mathrm{KNNError}\\right)=2\\left(\\mathrm{Bayes}\\;\\mathrm{Optimal}\\;\\mathrm{Error}\\;\\mathrm{Rate}\\right)$<br> Probability of K NN error is at most twice the bayes optimal error, bayes optimal error is best(least) error we can get using machine learning <br>\nIt is Possible to show that: as the size of training data set approaches infinity, the one nearest neighbor classifier guarantees an error rate of no worse than twice the bayes error rate ( the minimum achievable error rate given the distribution of the data).\n\n## Density Estimation using KNN\nNon parametric Density Estimation using KNN. <br>\nnstead of fixing bin width h and counting the mumber of instances, fix the instances(neighbors) k and check bin width $\\hat{p} \\left(X\\right)=\\frac{k}{2{\\mathrm{Nd}}_k \\left(X\\right)}$ here $d_k \\left(X\\right)$ is the $k_{\\mathrm{th}}$ closest distance to to $X$ , This is also known as __Parzen density estimation__.\n\n## KNN example using sklearn\n\nImport libraries\n\nPrint shapes and class information \n\nPlot the data\n\nsplit the dataset into test set and train set\n\nFit the classifier with different values of k\n\nPlot the scores:\n\nchose the best value of K for final model.\n\n\n:::{.callout-tip}\n\nIn KNN, finding the value of k is not easy. A small value of k means that noise will have a higher influence on the result and a large value make it computationally expensive. Data scientists usually choose as an odd number if the number of classes is 2 and another simple approach to select k is set `k=sqrt(n)`. There is one more widely used method called `Elbow Method` which is also used to find the value of K, here we  plot error rate vs k value and chose k value at elbow point.\n\n:::\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"center","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"css":["../../styles.css"],"html-math-method":"katex","highlight-style":"tango","output-file":"2022-08-20-CS5590-week2.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","comments":{"giscus":{"repo":"abhiyantaabhishek/IITH-Data-Science","category":"Announcements"}},"toc-location":"right","theme":"cosmo","title-block-banner":true,"author":"Abhishek Kumar Dubey","badges":true,"categories":["Machine Learning"],"date":"2022-08-20","description":"Classification Error, ROC/PR curve, KNN","image":"CS5590_images/chrome_CNeo323E9V.png","title":"Machine Learning 2"},"extensions":{"book":{"multiFile":true}}}}}