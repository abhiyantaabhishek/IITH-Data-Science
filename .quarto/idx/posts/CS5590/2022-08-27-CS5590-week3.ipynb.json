{"title":"Machine Learning   3","markdown":{"yaml":{"author":"Abhishek Kumar Dubey","badges":true,"categories":["Machine Learning"],"date":"2022-08-27","description":"Decision Trees, Naive Bayes, Bayesian Belief network","image":"CS5590_images/Code_GehOQCy4hw.png","title":"Machine Learning   3","toc":true},"headingText":"Decision Trees","containsRefs":false,"markdown":"\n\n\n\n\n- An efficient nonparametric method.\n- A hierarchical model.\n- Divide and conquer strategy.\n- Internal decision nodes\n    - Univariate : It uses a single attribute $X_i$\n        - Numeric $X_i$:\n            - If numeric data perform Binary split:$X_i>w_m$\n        - Discrete $X_i$:\n            - For discrete data perform n- way split for n possible values\n    - Multivariate: It uses more than one attributes, $X$\n- Leaves\n    - Classification : Class labels, or proportions\n    - Regression : Numeric, r average, or local fit\n- Learning is __greedy__; find the best split recursively.\n- For node $m$, $N_m$ instances reach $m$, $N_m^i$ belong to $C_i$ <br>\n  $\\hat P(C_i|X,m)\\equiv p_m^i = \\frac{N_m^i}{N_m}$\n- Node $m$ is pure if $p_m^i$ is $0$ or $1$\n- Measure if impurity is entropy<br>\n$\\displaystyle I_m=-\\sum_{i=1}^kp_m^i \\log_2p_m^i$\n\n\ncompare probability distributions vs entropy\n\nEntropy in information theory specifies the __average (expected) amount of information derived from observing an event__ .\n\n## How to generate decision tree\n\n\n- Select a root not which divides the data best based on impurity measures.\n- If node is pure, generate a leaf and stop, otherwise split and continue recursively.\n- Impurity after split: \n    -  It is probability weighted entropy given by:\n        - $\\displaystyle I_m^{\\prime }=-\\sum_{j=1}^{n}\\frac{N_{mj}}{N_m}\\sum_{i=1}^kp_{mj}^i\\log_2p_{mj}^i$,<br>\n          here, $N_{mj}$ is $j^{th}$ branch of $N_m$ and $N_{mj}^i$ belongs to $i^{th}$ class.\n- Information gain: Expected reduction in impurity measure after split. Chose the attribute with maximum information gain.\n- Other impurity measure method - Gini impurity/index : $\\displaystyle 1- \\sum_{j=1}^cp_j^2$\n\n## Overfitting and generalization\n\n- Noisy training example or if only small number of samples are associated leaf nodes can cause overfitting.\n- Using Pruning for better generalization\n    - Pruning is the process of removing subtree.\n        - Pre-pruning: Early stopping, after a predetermined performance.\n        - Post-pruning: Grow the whole then prune the subtree which overfit on the pruning set\n    - Pre-pruning is faster, post-pruning is more accurate. \n\n## Occam's Razor Principle\nWhen multiple hypotheses can solve the problem chose the simplest one\n\n\n## Select best tree\n- Measure performance over training and separate validation data set\n- Minimum Description Length : Minimize `size(tree)+size(miscalssifications(tree))`\n\n## Rule Extraction from Trees\n- Convert tree to equivalent set of rules.(\"if else\" condition for example).\n-  Prune each rules independently of others, by removing any pre-conditions that result in improving its estimates accuracy.\n- Sort final rules into desired sequence for use.\n\n# Naive Bayes\n\n![](CS5590_images/Acrobat_BRXKLqY6Wd.png)\n\n- In Naive Bayes classifier goal is to learn function $f:X\\rightarrow y$, where $y$ is one of $k$ classes and  $X=X_1,...,X_n$: values of attributes (numeric or categorical)\n- It is a probabilistic classification \n    - most probable class given observation: $\\displaystyle \\hat{y}=\\arg \\max_y P(y|x)$\n    - Bayesian probability of a class:<br><br>\n    $\\displaystyle P\\left( Y|X \\right)=\\frac{P\\left( X|Y \\right)P\\left( Y \\right)}{\\sum_{y'}P\\left( X|Y' \\right)P\\left( Y' \\right)}$<br>\n\n## Formulation\n\n- consider a record with attributes $A_1,A_2,\\dots ,A_n$ \n- Goal is to predict class $C$\n- Specifically, we want to find the value of $C$ that maximizes $P(C|A_1,A_2,\\dots,A_n)$\n\n![](CS5590_images/Acrobat_1cXslTMZjX.png)\n\n- what is Naive about Naive Bayes?\n    - The attributes are considered independent of each other, this is Naive in Naive Bayes.\n\n- AS we assume independence among attributes $A_i$ so we can write:<br>\n  $P(A_1,A_2,\\dots ,A_n|C_j)=P(A_i|C_j)P(A_2|C_j)\\dots P(A_n|C_j)$\n- New point is classified to $C_j$ if <br>\n  $P(C_j)\\prod_{j}P(A_i|C_j)=P(C_j)P(A_i|C_j)(A_2|C_j)\\dots P(A_n|C_j)$ is __maximal__ .\n- Assume that all hypotheses (classes) are equally probable a priori, i.e., $P(C_i)=P(C_j)$ for all $i,j$\n- This is called assuming a uniform prior. It simplifies computing the posterior:<br>\n  $\\displaystyle C_{ML}=\\arg \\max_c P(A_1,A_2,\\dots A_n|C)$\n- This hypothesis is called the __maximum likelihood hypothesis__ .\n\n## Example \n\n\nGiven a data as shown in as shown in below data frame, Find if tennis will be played for a scenario  given by $X$:<br>\n$X=( \\mathrm{ Outlook = Sunny, Temperature= Cool, Humidity =High, Wind= Strong})$<br>\n\n<br><br>\nAbove table shows conditional probabilities<br>\nFor example $p(\\mathrm{outlook}=\\mathrm{sunny} \\mid \\mathrm{play Tennis} = \\mathrm{no})$ is given by row `outlook_Sunny` and column `Play_Tennis_No` of table `Outlook` and <br>\n$p(\\mathrm{Temperature}=\\mathrm{Cool} \\mid \\mathrm{play Tennis} = \\mathrm{Yes})$ is given by row `Temperature_Cool` and column `Play_Tennis_Yes` of table `Temperature` <br>\nAlso we can calculate below 2 probabilities of the two classes: <br>\n$P(\\mathrm{Play}=\\mathrm{Yes})=\\frac{9}{14}$<br>\n$P(\\mathrm{Play}=\\mathrm{No})=\\frac{5}{14}$\n\n- Form look up table: <br><br>\n    $P\\left(\\mathrm{Outlook}=\\mathrm{Sunny} \\mid \\mathrm{Play} = \\mathrm{Yes} \\right) = \\frac{2}{9}$ <br>\n    $P\\left(\\mathrm{Outlook}=\\mathrm{Sunny} \\mid \\mathrm{Play} = \\mathrm{No} \\right) = \\frac{3}{5}$ <br>\n    $P\\left(\\mathrm{Temperature}=\\mathrm{Cool} \\mid \\mathrm{Play} = \\mathrm{Yes} \\right) = \\frac{3}{9}$ <br>\n    $P\\left(\\mathrm{Temperature}=\\mathrm{Cool} \\mid \\mathrm{Play} = \\mathrm{No} \\right) = \\frac{1}{5}$<br>\n    $P\\left(\\mathrm{Humidity}=\\mathrm{High\t} \\mid \\mathrm{Play} = \\mathrm{Yes} \\right) = \\frac{3}{9}$ <br>\n    $P\\left(\\mathrm{Humidity}=\\mathrm{High\t} \\mid \\mathrm{Play} = \\mathrm{No} \\right) = \\frac{4}{5}$<br>\n    $P\\left(\\mathrm{Wind}=\\mathrm{Strong\t} \\mid \\mathrm{Play} = \\mathrm{Yes} \\right) = \\frac{3}{9}$ <br>\n    $P\\left(\\mathrm{Wind}=\\mathrm{Strong\t} \\mid \\mathrm{Play} = \\mathrm{No} \\right) = \\frac{3}{5}$<br>\n\n- MAP Rule:<br><br>\n    $P\\left(\\mathrm{Yes} \\mid X\\right)=\\\\\n    P\\left(\\mathrm{Outlook}=\\mathrm{Sunny} \\mid \\mathrm{Play} = \\mathrm{Yes} \\right) \\times \\\\\n    P\\left(\\mathrm{Temperature}=\\mathrm{Cool} \\mid \\mathrm{Play} = \\mathrm{Yes} \\right) \\times \\\\\n    P\\left(\\mathrm{Humidity}=\\mathrm{High\t} \\mid \\mathrm{Play} = \\mathrm{Yes} \\right) \\times \\\\\n    P\\left(\\mathrm{Wind}=\\mathrm{Strong\t} \\mid \\mathrm{Play} = \\mathrm{Yes} \\right) \\times \\\\\n    P(\\mathrm{Play}=\\mathrm{Yes})\\\\\n    =0.0053$<br><br>\n    $P\\left(\\mathrm{No} \\mid X\\right)= \\\\\n    P\\left(\\mathrm{Outlook}=\\mathrm{Sunny} \\mid \\mathrm{Play} = \\mathrm{No} \\right) \\times \\\\\n    P\\left(\\mathrm{Temperature}=\\mathrm{Cool} \\mid \\mathrm{Play} = \\mathrm{No} \\right) \\times \\\\\n    P\\left(\\mathrm{Humidity}=\\mathrm{High\t} \\mid \\mathrm{Play} = \\mathrm{No} \\right) \\times \\\\\n    P\\left(\\mathrm{Wind}=\\mathrm{Strong\t} \\mid \\mathrm{Play} = \\mathrm{No} \\right) \\times \\\\\n    P(\\mathrm{Play}=\\mathrm{No}) \\\\\n    = 0.0206$ <br> <br>\nSince $P(\\mathrm{Play}=\\mathrm{Yes}) < P(\\mathrm{Play}=\\mathrm{No})$ so $X$ shall be labeled to be \"No\", i.e. Given scenario $X$ tennis will not be played. \n\n## Pros and Cons\n\n- Combines prior knowledge and observed data\n- Output is not only a classification but a probability distribution over all classes\n- Robust to isolated noise points.\n- Handle missing values by ignoring instance during probability estimate calculation.\n- Robust to irrelevant attributes.\n- with each training example, the prior and the likelihood can be updated dynamically \n- Independence assumption may not hold always.\n\n## Practical Issues\n\n- Discretize the range into bins\n    - One ordinal attribute per bin\n    - Violates independence assumption\n- Two way split : (A < v) or (A < v)\n    - choose only one of the two splits as new attribute.\n- Probability density estimation:\n    - Assume attribute follows a parametrized distribution, e.g. normal distribution.\n    - Use data to estimate parameters of distribution, e.g. mean and standard deviation using maximum likelihood estimation.\n    - Once probability distribution is known, can use it to estimate the conditional probability, $P(A_i \\mid c)$\n\n## Bayesian Belief network ( Bayesian net )\n\nDescribe conditional independence among subset of variables (attributes) : combining prior knowledge about dependencies among variables with observed training data.<br>\nFor example consider below graph:<br><br>\n\n![](CS5590_images/Acrobat_Q6uR4y3nmf.png)<br><br>\n\nHere Age, Occupation and Income determine if customer will byt this product, Given that customer buys product, whether there is interest in insurance is now independent of Age, Occupation, Income.<br><br>\n$P\\left(\\mathrm{Age,Occ,Inc,Buy,Ins}\\right)=P(\\mathrm{Age})P(\\mathrm{Occ})P\\left(\\mathrm{Inc}\\right)P\\left(\\mathrm{Buy} \\mid \\mathrm{Age, Occ, Inc}\\right)P\\left(\\mathrm{Int} \\mid \\mathrm{Buy}\\right)$\n\n## Naive Bayes Classifier category \n\n-  It is an Inductive Learning.\n- It is a generative Modeling.\n- It can be Parametric or Non-parametric Models.\n- It can be online or offline Models.\n\n<br><br><br>\n$\\tiny  {\\textcolor{#808080}{\\boxed{\\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}$\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"center","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"css":["../../styles.css"],"html-math-method":"katex","highlight-style":"tango","number-sections":true,"output-file":"2022-08-27-CS5590-week3.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","comments":{"giscus":{"repo":"abhiyantaabhishek/IITH-Data-Science","category":"Announcements"}},"toc-location":"right","theme":"cosmo","title-block-banner":true,"author":"Abhishek Kumar Dubey","badges":true,"categories":["Machine Learning"],"date":"2022-08-27","description":"Decision Trees, Naive Bayes, Bayesian Belief network","image":"CS5590_images/Code_GehOQCy4hw.png","title":"Machine Learning   3"},"extensions":{"book":{"multiFile":true}}}}}