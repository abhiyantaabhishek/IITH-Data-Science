{"title":"FoML CS5590 Assignment 1","markdown":{"yaml":{"aliases":["/Quizzes and assignments - Foundations of Machine Learning/2022/08/26/CS5590-Assignment-1"],"author":"Abhishek Kumar Dubey","badges":false,"categories":["Quizzes and assignments - Foundations of Machine Learning"],"date":"2022-08-26","description":"Foundations of Machine Learning CS5590  Assignment 1","output-file":"2022-08-26-cs5590-assignment-1.html","title":"FoML CS5590 Assignment 1","toc":true},"headingText":"Q. 1.","containsRefs":false,"markdown":"\n\n\n\n\n\n-  __k-NN: (8 marks)__ In $k$-nearest neighbors $(k-NN)$, the classification is achieved by majority vote in the vicinity of data. Given $n$ points in a 2-dimensional space, consider two settings:\n    - __Setting $A$:__ imagine _two_ classes of data each of $n/2$ points, which are overlapped to some extent in the space.\n    - __Setting $B$:__ imagine _three_ classes of data each of $n/3$ points, which are overlapped to some extent in the space.<br>Now, answer the following questions:\n        - $(a)$ __(2 marks)__ Describe what happens to the training error (using all available data) when the neighbor size $k$ varies from $n$ to $1$ in Setting $A$. How does this behavior change in Setting $B$?\n        - $(b)$ __(3 marks)__ Predict and explain with a sketch how the generalization error (e.g. holding out some data for testing) would change when $k$ varies in both settings? Explain your reasoning.\n        - $(c)$ __(3 marks)__ Is it possible to build a univariate decision tree $($ with decisions at each node of the form “is $x > a$”, “is $x < b$”, “is $y > c$”, or “is $y < d$” for any real constants $a, b, c, d$ $)$ which classifies exactly similar to a $1-NN$ using the Manhattan (city block) distance? If so, give an example of a dataset setup where this can happen. If not, explain why not.\n\n## Answer of Q.1. \n\n\n- $1.(a)$ \n    - Setting A: As the value of $k$ is varied for $n$ to $1$ the error starts to decrease , becomes 'almost' constant after forming elbow like shape and then the error becomes $0$ when $k$ becomes $1$. Error may not reduce gradually, it may fluctuate depending on the noise present in the data. The training error becomes $0$ at $k=1$,This happens because for $k=1$ it tires to find the distance from itself. This can also be called as over fitting.  \n    - Setting B: The error  starts to decrease as value of $k$ is varied form $n$ to $1$ and becomes $0$ at $k=1$, the difference here is that it takes time for the error to become 'almost' constant  as value of $k$ is varied from $n$ to $1$, i.e. the elbow point appears later as compared to setting $A$ when  the value of $k$ is varied from $n$ to $1$.Here also error becomes $0$ at $k=1$ due to the same reason as in Setting $A$.\n- $1.(b)$<br> \nGeneralization error is plotted in below picture's right most column (instead of hand sketch I plotted using python code) If $k$ is too small knn becomes sensitive to noise point. If $k$ is too large, neighborhood may include points from other class.<br>\n    - Setting A: The generalization error will  decrease as the value of $k$ is varied from $n$ to $1$,  error becomes constant after forming elbow shape, but when value of $k$ is further reduced the model starts to over-fit and when $n$ becomes $1$ the generalization error increases by a significant amount, as the model highly over fits to the data at this point.<br>\n    - Setting B: In this setting also generalization error decrease as the value of $k$ is varied form $n$ to $1$. The difference among the two settings is that the elbow point appears earlier in setting $A$ and later in setting $B$ as the value of $k$ is varied from $n$ to $1$.\n\nBelow is the proof of answer given above for setting $A$ and $B$<br> \n\n- $1.(c)$ <br>\nYes it is possible to build a univariate tree as described in the question, which classifies exactly similar to $1-NN$ using Manhattan distance.<br>\nBelow is the example of the dataset:<br>\nFor simplicity we consider $2$ class classification setting, in $2$ dimensional space, Class $A$ and Class $B$ each contains just one point:<br>\n$A = {(1,3)}$<br>\n$B = {(3,3)}$<br>\nConsider value of $a=2,b=4,c=2$ and $d=4$<br>\n    - Decision Tree:<br>\n    consider point $a,b$ lies on $X$ axis and pint $d,c$ lies on $Y$ axis,Classes can be distinguished with just one question $x>a$ <br>\n        - Point $(1,3)$: here $x=1,y=3$, if we make decision using the rules, $x>a$, it is false, hence we classify it for class $A$.\n        - Point $(3,3)$: here $x=3,y=3$, if we make decision using the rules, $x>a$, it is true, $x<b$ is true,  $y>c$ is true, $y<d$ is true, hence we classify the point for class $B$\n    - $1-NN$: <br>\n    As the training dataset contains points $(1,3),(3,3)$ so any points which has $X$ value greater than $2$ will be classified to class $B$ and lesser will be classified for class $A$.\n    \nThis is explained in below code and picture plotted :\n\n# Q. 2.\n\n\n- Bayes Classifier: (6 marks)\n    - $(a)$ __(3 marks)__ Consider a classification problem with $K$ classes for which the feature vector $\\phi$ has $M$ dimensions (categorical variables) each of which can take $L$ discrete states. Let the values of the dimensions be represented by a one-hot ($1$-of-$L$) binary coding scheme. Further suppose that, conditioned on the class $C_k$, the $M$ dimensions of $\\phi$ are independent, so that the class-conditional density factorizes with respect to the feature vector components (also referred to as the naive Bayes assumption). Show that the quantities given below:<br>$$a_k=\\ln p(\\phi|C_k)p(C_k)$$ are linear function of the components  of $\\phi$.\n    - $(b)$ __(3 marks)__ You are now going to make a text classifier. To begin with, you attempt to classify documents as either _sport_ or _politics_. You decide to represent each document as a (row) vector of attributes describing the presence or absence of the following words.<br>  $X$ __= (goal,football,golf,defence,offence,wicket,office,strategy)__ <br> Training data from sport documents and from politics documents is represented below in a matrix in which each row represents the 8 attributes.<br><br> \n$\\\\\nx_{politics}=\\left\\lbrack \\begin{array}{cccccccc}\n1 & 0 & 1 & 1 & 1 & 0 & 1 & 1\\\\\n0 & 0 & 0 & 1 & 0 & 0 & 1 & 1\\\\\n1 & 0 & 0 & 1 & 1 & 0 & 1 & 0\\\\\n0 & 1 & 0 & 0 & 1 & 1 & 0 & 1\\\\\n0 & 0 & 0 & 1 & 1 & 0 & 1 & 1\\\\\n0 & 0 & 0 & 1 & 1 & 0 & 0 & 1\n\\end{array}\\right\\rbrack$ <br><br>\n$\\\\\nx_{sport} = \\left\\lbrack \\begin{array}{cccccccc}\n1 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 1 & 0 & 0 & 0 & 0 & 0\\\\\n1 & 1 & 0 & 1 & 0 & 0 & 0 & 0\\\\\n1 & 1 & 0 & 1 & 0 & 0 & 0 & 1\\\\\n1 & 1 & 0 & 1 & 1 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 1 & 0 & 1 & 0 & 0\n\\end{array}\\right\\rbrack$<br><br>\n<br> Using a maximum likelihood naive Bayes classifier, what is the probability that the document $x = (1, 0, 0, 1, 1, 1, 1, 0)$ is about politics?\n\n## Answer of Q.2.\n\n- $2.(a)$<br>\nBayes classifier has the same softmax form whenever the class-conditional densities are __any__ [exponential family density](http://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions).<br>\nGaussian distribution is just an example of exponential family density.  We can write $p(\\phi \\mid C_k)$ as below:<br>\n$p((\\phi \\mid C_k, \\eta_k)=h(\\mathbf {X})\\left\\{  \\exp{\\eta_k^T \\mathbf{X}} - \\alpha (\\eta_k) \\right\\}$<br>\n$\\displaystyle p(( C_k  \\mid \\phi, \\eta)= \\frac{\\exp \\left\\{ \\eta_k^T \\mathbf{X} -\\alpha (\\eta _k)\\right\\}}{\\sum_{j}\\exp \\left\\{ \\eta_j^T \\mathbf{X} - \\alpha (\\eta_j)\\right\\}}$<br>\n$\\displaystyle p(( C_k  \\mid \\phi, \\eta) = \\frac{e^{\\beta_k^T \\mathbf{X}}}{\\sum_je^{\\beta_k^T\\mathbf{X}}}$<br>\nWhere, $\\displaystyle  \\beta_k = \\left[ n_k;-\\alpha \\left( \\eta_k \\right) \\right]$<br>\nwe can see that the numerator term of $\\displaystyle p(( C_k  \\mid \\phi, \\eta)$ is $\\displaystyle a_k = \\ln \\;p\\left(\\phi \\mid C_k \\right)p\\left(C_K \\right)=\\ln e^{\\beta_k^T\\mathbf{X}}=\\beta_k^T \\mathbf{X}$<br>\nWhich is a liner equation hence proved. \n\n\n\n- $2.(a)$<br>\n This is another way of solving the same question $2.(a)$, here we consider probability function to be  continuous and normally distributed. <br> \n The class conditional probability is considered as multivariate Gaussian Distribution and is given by:<br>\n $\\displaystyle p\\left(\\phi \\mid C_k \\right)=\\frac{1}{\\sqrt{\\left(2\\pi\\right)^D|\\Sigma|}}\\exp \\left\\lbrace -\\frac{1}{2}{\\left(\\phi -\\mu_k \\right)}^T \\Sigma^{-1} \\left(\\phi -\\mu_k \\right)\\right\\rbrace$<br>\n Now the term $a_k$ can be written as :<br>\n $\\displaystyle a_k = \\ln \\;p\\left(\\phi \\mid C_k \\right)p\\left(C_K \\right) \\\\ \n =\\ln \\left(\\frac{1}{\\sqrt{\\left(2\\pi\\right)^D|\\Sigma|}}\\exp \\left\\lbrace -\\frac{1}{2}{\\left(\\phi -\\mu_k \\right)}^T \\Sigma^{-1} \\left(\\phi -\\mu_k \\right)\\right\\rbrace p\\left(C_K \\right)\\right)$<br>\n $\\displaystyle =\\ln \\left(\\frac{1}{\\sqrt{\\left(2\\pi\\right)^D|\\Sigma|}}\\exp \\left\\lbrace -\\frac{1}{2}{\\left(\\phi -\\mu_k \\right)}^T \\Sigma^{-1} \\left(\\phi -\\mu_k \\right)\\right\\rbrace \\right)+\\ln \\left(p\\left(C_K \\right)\\right)$<br>\n $\\displaystyle =\\ln \\left(\\exp \\left\\lbrace -\\frac{1}{2}{\\left(\\phi -\\mu_k \\right)}^T \\Sigma^{-1} \\left(\\phi -\\mu_k \\right)\\right\\rbrace \\right)+\\ln \\left(\\frac{1}{\\sqrt{\\left(2\\pi\\right)^D|\\Sigma|}}\\right)+\\ln \\left(p\\left(C_K \\right)\\right)$<br>\n$\\displaystyle =-\\frac{1}{2}{\\left(\\phi -\\mu_k \\right)}^T \\Sigma^{-1} \\left(\\phi -\\mu_k \\right)+\\ln \\left(\\frac{1}{\\sqrt{\\left(2\\pi\\right)^D|\\Sigma|}}\\right)+\\ln \\left(p\\left(C_K \\right)\\right)$<br>\n$\\displaystyle =-\\frac{1}{2} \\left( \\phi^T \\Sigma^{-1}\\phi - 2\\mu_k^T\\Sigma^{-1}\\phi +\\mu_k^T\\Sigma^{-1}\\mu_k \\right)  +\\ln \\left(\\frac{1}{\\sqrt{\\left(2\\pi\\right)^D|\\Sigma|}}\\right)+\\ln \\left(p\\left(C_K \\right)\\right)$<br>\n$\\displaystyle = -\\frac{1}{2} \\phi^T \\Sigma^{-1}\\phi + \\mu_k^T\\Sigma^{-1}\\phi -\\frac{1}{2} \\mu_k^T\\Sigma^{-1}\\mu_k   +\\ln \\left(\\frac{1}{\\sqrt{\\left(2\\pi\\right)^D|\\Sigma|}}\\right)+\\ln \\left(p\\left(C_K \\right)\\right)$<br><br>\nNow if we consider two class we can say $p(\\phi \\mid C_{k1})=p(\\phi \\mid C_{k2})$ <br>\nIn that case Due to shared covariance the quadratic term cancels out, and the rest of the term can be written as :<br><br>\n$\\displaystyle =W_k^T \\phi +w_{\\mathrm{k0}}$<br><br>\nWhere : <br>\n$\\displaystyle W_k = \\Sigma^{-1} \\mu_k$<br>\n$\\displaystyle W_{k0} = -\\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k+\\ln p(C_k)$<br><br>\n The  equation $\\displaystyle =W_k^T \\phi +w_{\\mathrm{k0}}$ is a linear equation in terms of $\\phi$ and it should satisfies the two properties of linearity, To prove it, the equation can be rewritten as :<br><br>\n $\\displaystyle \\Rightarrow f\\left(\\phi ,a_k \\right){\\;=\\;a}_k -W_k^T \\phi =w_{\\mathrm{k0}}$<br>\n    - Distributive over addition: <br>\n    $f\\left(\\phi +\\beta ,a_k +\\beta \\right){\\;=\\;a}_k +\\beta -W_k^T \\left(\\phi +\\beta \\right)={\\;\\;a}_k +\\beta -W_k^T \\phi -W_k^T \\beta =\\left({\\;\\;a}_k -W_k^T \\phi \\right)+\\left(\\beta -W_k^T \\beta \\right)=f\\left(\\phi ,a_k \\right)+f\\left(\\beta ,\\beta \\right)$<br> \n    - Homogenous of Degree one:<br>\n    $f\\left(\\beta \\phi ,\\beta a_k \\right)=\\beta a_k -\\beta W_k^T \\phi =\\beta \\;\\left(a_k -W_k^T \\phi \\right)=\\beta w_{\\mathrm{k0}} =\\beta f\\left(\\phi ,a_k \\right)$\n    \n    Hence we can say that $\\ln \\;p\\left(\\phi |C_k \\right)p\\left(C_k \\right)$ is linear in terms of $\\phi$\n\n- $2.(b)$<br>\n\n\nBelow table shows probabilities.<br> \n`class_p` denotes politics class `class_s` denotes sports class, <br>\nFor example, Row `goal_1` and column `class_p` of table `goal` denotes $P(\\mathrm{gaol}=1 \\mid \\mathrm{class}=\\mathrm{politics})$  <br> \nRow `football_0` and column `class_s\t` of table `football` denotes $P(\\mathrm{football}=0 \\mid \\mathrm{class}=\\mathrm{sports})$ \n\nWe need to find out probability of $X$ to be politics, where :<br>\n$X=\\left(\\mathrm{goal}=1,\\mathrm{football}=0,\\mathrm{golf}=0,\\mathrm{defence}=1,\\mathrm{offence}=1,\\mathrm{wicket}=1,\\mathrm{office}=1,\\mathrm{strategy}=0\\right)$<br><br>\n$P\\left(\\mathrm{ploitics}\\mid X\\right)=\\\\\nP\\left(\\mathrm{goal}=1\\mid \\mathrm{politics}\\right)\\times \nP\\left(\\mathrm{football}=0\\mid \\mathrm{politics}\\right)\\times \nP\\left(\\mathrm{golf}=0\\mid \\mathrm{politics}\\right)\\times \nP\\left(\\mathrm{defence}=1\\mid \\mathrm{politics}\\right)\\times \nP\\left(\\mathrm{offence}=1\\mid \\mathrm{politics}\\right)\\times \nP\\left(\\mathrm{wicket}=1\\mid \\mathrm{politics}\\right)\\times \nP\\left(\\mathrm{office}=1\\mid \\mathrm{politics}\\right)\\times \nP\\left(\\mathrm{strategy}=0\\mid \\mathrm{politics}\\right)\\times \nP\\left(\\mathrm{class}=\\mathrm{polotics}\\right)$\n\n\nConditional probabilities (from above table): <br>\n$P\\left(\\mathrm{goal}=1\\mid \\mathrm{politics}\\right)=\\frac{2}{6},$\n$P\\left(\\mathrm{football}=0\\mid \\mathrm{politics}\\right)=\\frac{5}{6}$<br>\n$P\\left(\\mathrm{golf}=0\\mid \\mathrm{politics}\\right)=\\frac{5}{6},$\n$P\\left(\\mathrm{defence}=1\\mid \\mathrm{politics}\\right)=\\frac{5}{6}$<br>\n$P\\left(\\mathrm{offence}=1\\mid \\mathrm{politics}\\right)=\\frac{5}{6},$\n$P\\left(\\mathrm{wicket}=1\\mid \\mathrm{politics}\\right)=\\frac{1}{6}$<br>\n$P\\left(\\mathrm{office}=1\\mid \\mathrm{politics}\\right)=\\frac{4}{6},$\n$P\\left(\\mathrm{strategy}=0\\mid \\mathrm{politics}\\right)=\\frac{1}{6}$<br>\n$P\\left(\\mathrm{class}=\\mathrm{polotics}\\right)=\\frac{1}{2}$<br>\n\nHence :<br>\n\n$P\\left(\\mathrm{ploitics}\\mid X\\right)=\\frac{2}{6}\\times \\frac{5}{6}\\times \\frac{5}{6}\\times \\frac{5}{6}\\times \\frac{5}{6}\\times \\frac{1}{6}\\times \\frac{4}{6}\\times \\frac{1}{6}\\times \\frac{1}{2}= \\frac{625}{419904}=1.4884\\times10^{-3}$\n\n\n\n# Q. 3.\n\n\n\n-   Model Selection: (__6 marks)__ Install the `pydataset` module (if you haven’t already):<br>\n    \n    ```\n    from pydataset import data\n    import pandas as pd\n    melanoma_data = data ( 'Melanoma' , show_doc=True )\n    ```\n    The _Melanoma_ dataset consists of measurements of patients with malignant melanoma (a type of cancer). For each patient, the dataset specifies if the patient died or lived at the end of the trial. Moreover, some patients died due to causes unrelated to melanoma. Your task is to do the following: \n    - $(a)$ __(1 mark)__ Remove those patients who died due to causes unrelated to Melanoma, and plot patient status vs age and patient status vs thickness - for your own understanding. \n    - $(b)$ __(4 marks)__ Split the data into 80% training and 20% test set using random stratified sampling. Now, on the 80% training data, perform 3-fold cross-validation using a classifier to predict the status of the patient (do not use the 20% held-aside test data for cross-validation; we will use it later to study generalization performance). Also, remember to use stratified sampling inside cross-validation too. You are allowed to use any existing machine learning library of your choice: scikitlearn, pandas, Weka (we recommend scikitlearn) - but you should use only the decision tree, k-NN or the naive Bayes classifier (to align with what we have covered in class so far, random forests not allowed too). Report the mean of the three quantities (accuracy, precision, recall) on 3-fold cross-validation. \n    - $(c)$ __(1 mark)__ Once you have picked the best classification model in cross-validation, train the best-performing setting on the entire 80% training data, and report performance on the held-aside test set. Report your observation on how representative the training/ validation data was w.r.t test data. \n    - Deliverables:\n        - Code \n        - Data splits (for us to verify your reported results)\n        - Brief report (PDF) with your solutions for the above questions\n\n## Answer of Q.3. \n\n- $3.(a)$ :<br>\nIf status is $3$ it means patient died due to other Cause. So we can remove this column :\n\n\n- $3.(b)$<br>\n    - The Melanoma data has following attributes:,<br>\n        - time: survival time in days\n        - status: `1` died from melanoma, `2` alive, `3` dead from other causes.\n        - sex: `1` = male, `0` = female.\n        - age: age in years.\n        - year: year of operation.\n        - thickness: tumour thickness in mm.\n        - ulcer: `1` = presence, `0` = absence.\n\nImport libraries , implement Stratified KFold Generator to be used in grid search :\n\nLoad data, separate out $20$ percent  test data, do not use it in training phase. Instantiate `GaussianNB`, `KNeighborsClassifier` and `DecisionTreeClassifier` and update parameters dictionary  for grid search :\n\nUse $80$ percent data for training and validation, use stratified sampling inside cross-validation too, perform grid search on all the models, using Stratified KFold Generator, fit the model on best parameters and evaluate it on train and val data for all the $3$ folds, do not use test data here.\n\nReport the mean of quantities on $3$-fold cross validation:\n\nPerformance of naive Bayes classifier:\n\nPerformance of KNN:\n\nPerformance of decision tree:\n\n- $3.(c)$<br>\nHere it seems from above results, KNN is performing the best, but it can be the case of over-fitting, and we may not have the good generalization. so Decision Tree Classifier is selected to train on the $80$ percent of the data. Here model is tested on held aside test data, which was not used to train or validate the model.\n\nPerformance  on held-aside Test data :\n\nObservation: The Generalization performance is close to the validation performance, hence training/validation data was a good representation of test data.\n\n- Deliverables:\n    - Code \n        - Already present with the notebook.\n    - Data splits (for us to verify your reported results)\n        - The Notebook can be run to verify the reported results.\n    - Brief report (PDF) with your solutions for the above questions\n        - Report is included inline to the code with this ipython notebook\n\n# Q. 4.\n\n\n\n-  Decision Trees: __(10 marks)__ In this question, you will use the Car Evaluation dataset, a popular dataset from the UCI Machine Learning Repository. It contains 1728 car sample information with 7 attributes, including one class feature that tells whether the car is in acceptable conditions (class labels: Unacceptable, Acceptable, VeryGood, Good). More details of the dataset description are available on https://archive.ics.uci.edu/ml/datasets/car+evaluation. You must not use the last column as an input feature when you classify the data. You can pre-process the variables into one-hot/ordinal values as you deem fit for each variable.\n    - $(a)$ (5 marks) Decision Tree Implementation: Implement your own version of the decision tree using binary univariate split, entropy and information gain.\n    - $(b)$ (2 marks) Cross-Validation: Evaluate your decision tree using 5-fold cross validation. (Divide the entire dataset into 5 parts using stratified sampling, and run this experiment; no test set is required for this question.) Report the average of the 5 folds’ accuracies. With correct implementation of both parts (decision tree and cross validation), your classification accuracy should be around 0.8.\n    - $(c)$ (3 marks) Improvement Strategies: Now, try and improve your decision tree algorithm. Some things you could do include (not exhaustive):\n        - Use Gini index instead of entropy\n        - Use multi-way split (instead of binary split)\n        - Use multivariate split (instead of univariate)\n        - Prune the tree after splitting for better generalization\n\n    Report your performance as an outcome of ANY TWO improved strategies.\n        \nDeliverables:\n- Code\n- Brief report (PDF) with: (i) Accuracy of your initial implementation; (ii) Accuracy of your improved implementation, along with your explanation of why the accuracy improved with this change.\n\n## Answer of Q.4. \n\n- $4.(a)$<br>\n    - Implementation of my own version of decision tree.\n    - Univariate tree with entropy and information gain.\n    - Also it supports gini impurity and maximum tree depth which will be used in $4(c)$\n    - For finding best split, only categorical or ordinal data is considered, as the given  data-set doesn't contain continuous feature so was not implemented.\n\n- $4.(b)$ <br>\nFirst Implement a class for stratified sampling <br>\nEvaluate decision tree on $5$- fold cross validation using `StratifiedShuffleSplit` , Report accuracy on average of $5$- fold:\n\nThe accuracy is  found to be $0.968$,the performance reported here is found to be extremely  good. \n\n- $4.(c)$<br>\n    - The decision tree is implemented in such a way so that it can be configured for max depth (pruning) or type of impurity\n    - As ANY TWO strategies was suggested to use, so Gini-index and pruning was tried .\n    - The accuracy was found to be improved to $0.976$\n\nThe Improved accuracy is $0.976$\n\nPruning and gini impurity was used  to improve the accuracy, we can see that the accuracy has been improved ! <br>\nThe reason of improved accuracy is gini score, pruning of tree did not contribute much, this can be the case when there is no or very less over-fitting. Multi-way split or multivariate-split was not tried as performance was already fond to be extremely  high, so there is very little margin of improvement.<br><br>\n\n- Deliverables:\n    - Code\n        - Already present with this Notebook\n    - Brief report (PDF) with: \n        - (i) Accuracy of your initial implementation; \n            - The Accuracy is reported inline with the code in a Markdown cell.\n        - (ii) Accuracy of your improved implementation, along with your explanation of why the accuracy improved with this change.\n            - Accuracy of improved implementation and explanation is provided inline with the code in a Markdown cell.\n            \n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"2022-08-26-cs5590-assignment-1.html","toc":true},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","theme":"cosmo","title-block-banner":true,"aliases":["/Quizzes and assignments - Foundations of Machine Learning/2022/08/26/CS5590-Assignment-1"],"author":"Abhishek Kumar Dubey","badges":false,"categories":["Quizzes and assignments - Foundations of Machine Learning"],"date":"2022-08-26","description":"Foundations of Machine Learning CS5590  Assignment 1","title":"FoML CS5590 Assignment 1"},"extensions":{"book":{"multiFile":true}}}}}