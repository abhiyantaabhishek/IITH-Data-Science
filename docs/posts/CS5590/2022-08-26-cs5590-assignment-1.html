<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Abhishek Kumar Dubey">
<meta name="dcterms.date" content="2022-08-26">
<meta name="description" content="Foundations of Machine Learning CS5590 Assignment 1">

<title>IITH-Data-Science - FoML CS5590 Assignment 1</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">IITH-Data-Science</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">About</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"><i class="bi bi-twitter" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">FoML CS5590 Assignment 1</h1>
                  <div>
        <div class="description">
          Foundations of Machine Learning CS5590 Assignment 1
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Quizzes and assignments - Foundations of Machine Learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Abhishek Kumar Dubey </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">August 26, 2022</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#q.-1." id="toc-q.-1." class="nav-link active" data-scroll-target="#q.-1.">Q. 1.</a>
  <ul class="collapse">
  <li><a href="#answer-of-q.1." id="toc-answer-of-q.1." class="nav-link" data-scroll-target="#answer-of-q.1.">Answer of Q.1.</a></li>
  </ul></li>
  <li><a href="#q.-2." id="toc-q.-2." class="nav-link" data-scroll-target="#q.-2.">Q. 2.</a>
  <ul class="collapse">
  <li><a href="#answer-of-q.2." id="toc-answer-of-q.2." class="nav-link" data-scroll-target="#answer-of-q.2.">Answer of Q.2.</a></li>
  </ul></li>
  <li><a href="#q.-3." id="toc-q.-3." class="nav-link" data-scroll-target="#q.-3.">Q. 3.</a>
  <ul class="collapse">
  <li><a href="#answer-of-q.3." id="toc-answer-of-q.3." class="nav-link" data-scroll-target="#answer-of-q.3.">Answer of Q.3.</a></li>
  </ul></li>
  <li><a href="#q.-4." id="toc-q.-4." class="nav-link" data-scroll-target="#q.-4.">Q. 4.</a>
  <ul class="collapse">
  <li><a href="#answer-of-q.4." id="toc-answer-of-q.4." class="nav-link" data-scroll-target="#answer-of-q.4.">Answer of Q.4.</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="q.-1." class="level1">
<h1>Q. 1.</h1>
<ul>
<li><strong>k-NN: (8 marks)</strong> In <span class="math inline">\(k\)</span>-nearest neighbors <span class="math inline">\((k-NN)\)</span>, the classification is achieved by majority vote in the vicinity of data. Given <span class="math inline">\(n\)</span> points in a 2-dimensional space, consider two settings:
<ul>
<li><strong>Setting <span class="math inline">\(A\)</span>:</strong> imagine <em>two</em> classes of data each of <span class="math inline">\(n/2\)</span> points, which are overlapped to some extent in the space.</li>
<li><strong>Setting <span class="math inline">\(B\)</span>:</strong> imagine <em>three</em> classes of data each of <span class="math inline">\(n/3\)</span> points, which are overlapped to some extent in the space.<br>Now, answer the following questions:
<ul>
<li><span class="math inline">\((a)\)</span> <strong>(2 marks)</strong> Describe what happens to the training error (using all available data) when the neighbor size <span class="math inline">\(k\)</span> varies from <span class="math inline">\(n\)</span> to <span class="math inline">\(1\)</span> in Setting <span class="math inline">\(A\)</span>. How does this behavior change in Setting <span class="math inline">\(B\)</span>?</li>
<li><span class="math inline">\((b)\)</span> <strong>(3 marks)</strong> Predict and explain with a sketch how the generalization error (e.g.&nbsp;holding out some data for testing) would change when <span class="math inline">\(k\)</span> varies in both settings? Explain your reasoning.</li>
<li><span class="math inline">\((c)\)</span> <strong>(3 marks)</strong> Is it possible to build a univariate decision tree <span class="math inline">\((\)</span> with decisions at each node of the form “is <span class="math inline">\(x &gt; a\)</span>”, “is <span class="math inline">\(x &lt; b\)</span>”, “is <span class="math inline">\(y &gt; c\)</span>”, or “is <span class="math inline">\(y &lt; d\)</span>” for any real constants <span class="math inline">\(a, b, c, d\)</span> <span class="math inline">\()\)</span> which classifies exactly similar to a <span class="math inline">\(1-NN\)</span> using the Manhattan (city block) distance? If so, give an example of a dataset setup where this can happen. If not, explain why not.</li>
</ul></li>
</ul></li>
</ul>
<section id="answer-of-q.1." class="level2">
<h2 class="anchored" data-anchor-id="answer-of-q.1.">Answer of Q.1.</h2>
<ul>
<li><span class="math inline">\(1.(a)\)</span>
<ul>
<li>Setting A: As the value of <span class="math inline">\(k\)</span> is varied for <span class="math inline">\(n\)</span> to <span class="math inline">\(1\)</span> the error starts to decrease , becomes ‘almost’ constant after forming elbow like shape and then the error becomes <span class="math inline">\(0\)</span> when <span class="math inline">\(k\)</span> becomes <span class="math inline">\(1\)</span>. Error may not reduce gradually, it may fluctuate depending on the noise present in the data. The training error becomes <span class="math inline">\(0\)</span> at <span class="math inline">\(k=1\)</span>,This happens because for <span class="math inline">\(k=1\)</span> it tires to find the distance from itself. This can also be called as over fitting.<br>
</li>
<li>Setting B: The error starts to decrease as value of <span class="math inline">\(k\)</span> is varied form <span class="math inline">\(n\)</span> to <span class="math inline">\(1\)</span> and becomes <span class="math inline">\(0\)</span> at <span class="math inline">\(k=1\)</span>, the difference here is that it takes time for the error to become ‘almost’ constant as value of <span class="math inline">\(k\)</span> is varied from <span class="math inline">\(n\)</span> to <span class="math inline">\(1\)</span>, i.e.&nbsp;the elbow point appears later as compared to setting <span class="math inline">\(A\)</span> when the value of <span class="math inline">\(k\)</span> is varied from <span class="math inline">\(n\)</span> to <span class="math inline">\(1\)</span>.Here also error becomes <span class="math inline">\(0\)</span> at <span class="math inline">\(k=1\)</span> due to the same reason as in Setting <span class="math inline">\(A\)</span>.</li>
</ul></li>
<li><span class="math inline">\(1.(b)\)</span><br> Generalization error is plotted in below picture’s right most column (instead of hand sketch I plotted using python code) If <span class="math inline">\(k\)</span> is too small knn becomes sensitive to noise point. If <span class="math inline">\(k\)</span> is too large, neighborhood may include points from other class.<br>
<ul>
<li>Setting A: The generalization error will decrease as the value of <span class="math inline">\(k\)</span> is varied from <span class="math inline">\(n\)</span> to <span class="math inline">\(1\)</span>, error becomes constant after forming elbow shape, but when value of <span class="math inline">\(k\)</span> is further reduced the model starts to over-fit and when <span class="math inline">\(n\)</span> becomes <span class="math inline">\(1\)</span> the generalization error increases by a significant amount, as the model highly over fits to the data at this point.<br></li>
<li>Setting B: In this setting also generalization error decrease as the value of <span class="math inline">\(k\)</span> is varied form <span class="math inline">\(n\)</span> to <span class="math inline">\(1\)</span>. The difference among the two settings is that the elbow point appears earlier in setting <span class="math inline">\(A\)</span> and later in setting <span class="math inline">\(B\)</span> as the value of <span class="math inline">\(k\)</span> is varied from <span class="math inline">\(n\)</span> to <span class="math inline">\(1\)</span>.</li>
</ul></li>
</ul>
<p>Below is the proof of answer given above for setting <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span><br></p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> metrics</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> StratifiedShuffleSplit</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_multivariate(_mean,_cov,_numData,_class):</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        np.random.seed(<span class="dv">0</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        data <span class="op">=</span> np.random.multivariate_normal(_mean,_cov,_numData)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        target <span class="op">=</span> np.ones(_numData)<span class="op">*</span>_class   </span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> data,target    </span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_dataAndTarget(numData,numClass):</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        meanA <span class="op">=</span> [<span class="dv">1</span>,<span class="dv">1</span>]</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        covA <span class="op">=</span> [[<span class="fl">0.01</span>,<span class="fl">0.001</span>],</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>                [<span class="fl">0.001</span>,<span class="fl">0.01</span>]]</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        meanB <span class="op">=</span> [<span class="fl">1.3</span>,<span class="fl">1.3</span>]</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        covB <span class="op">=</span> [[<span class="fl">0.01</span>,<span class="fl">0.001</span>],</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>                [<span class="fl">0.001</span>,<span class="fl">0.01</span>]]</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        meanC <span class="op">=</span> [<span class="fl">1.4</span>,<span class="fl">0.8</span>]</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        covC <span class="op">=</span> [[<span class="fl">0.01</span>,<span class="fl">0.001</span>],</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>                [<span class="fl">0.001</span>,<span class="fl">0.01</span>]]</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span>(numClass<span class="op">==</span><span class="dv">3</span>):</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>                dataA,targetA <span class="op">=</span>  get_multivariate(meanA,covA,numData<span class="op">//</span><span class="dv">3</span>,<span class="dv">0</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>                dataB,targetB <span class="op">=</span>  get_multivariate(meanB,covB,numData<span class="op">//</span><span class="dv">3</span>,<span class="dv">1</span>)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>                dataC,targetC <span class="op">=</span>  get_multivariate(meanC,covC,numData<span class="op">//</span><span class="dv">3</span>,<span class="dv">2</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>                data <span class="op">=</span> np.vstack([dataA,dataB,dataC])</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>                target <span class="op">=</span> np.hstack([targetA,targetB,targetC])</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> data,target</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span>(numClass<span class="op">==</span><span class="dv">2</span>):</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>                dataA,targetA <span class="op">=</span>  get_multivariate(meanA,covA,numData<span class="op">//</span><span class="dv">2</span>,<span class="dv">0</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>                dataB,targetB <span class="op">=</span>  get_multivariate(meanB,covB,numData<span class="op">//</span><span class="dv">2</span>,<span class="dv">1</span>)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>                data <span class="op">=</span> np.vstack([dataA,dataB])</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>                target <span class="op">=</span> np.hstack([targetA,targetB])</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> data,target</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>              </span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> runKNeighborsClassifier(numData,numClass):</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        data ,target <span class="op">=</span>  get_dataAndTarget(numData,numClass)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        sss <span class="op">=</span> StratifiedShuffleSplit(n_splits<span class="op">=</span><span class="dv">1</span>, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>        train_index, test_index <span class="op">=</span> <span class="bu">list</span>(sss.split(data, target))[<span class="dv">0</span>]</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>        X_train, X_test <span class="op">=</span> data[train_index], data[test_index]</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>        y_train, y_test <span class="op">=</span> target[train_index], target[test_index]        </span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>        k_range <span class="op">=</span> <span class="bu">range</span>(<span class="dv">1</span>,X_train.shape[<span class="dv">0</span>])</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>        scores_list <span class="op">=</span> []</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> k_range:</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>                knn <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span>k)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>                knn.fit(X_train,y_train)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>                y_pred<span class="op">=</span>knn.predict_proba(X_train)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>                scores_list.append(metrics.log_loss(y_train,y_pred))               </span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>        scores_list_test <span class="op">=</span> []</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> k_range:</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>                knn <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span>k)</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>                knn.fit(X_train,y_train)</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>                y_pred<span class="op">=</span>knn.predict_proba(X_test)</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>                scores_list_test.append(metrics.log_loss(y_test,y_pred))</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> scores_list,scores_list_test,data,X_train, X_test,</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>num_data<span class="op">=</span><span class="dv">1200</span></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>scores_list_2,scores_list_test_2,data_2,X_train_2, X_test_2 <span class="op">=</span>runKNeighborsClassifier(num_data,<span class="dv">2</span>)     </span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>scores_list_3,scores_list_test_3,data_3,X_train_3, X_test_3 <span class="op">=</span>runKNeighborsClassifier(num_data,<span class="dv">3</span>)   </span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">4</span>, figsize<span class="op">=</span>(<span class="dv">24</span>, <span class="dv">10</span>))</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>,<span class="dv">0</span>].scatter(data_2[:num_data<span class="op">//</span><span class="dv">2</span>,<span class="dv">0</span>],data_2[:num_data<span class="op">//</span><span class="dv">2</span>,<span class="dv">1</span>])</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>,<span class="dv">0</span>].scatter(data_2[num_data<span class="op">//</span><span class="dv">2</span>:,<span class="dv">0</span>],data_2[num_data<span class="op">//</span><span class="dv">2</span>:,<span class="dv">1</span>])<span class="op">;</span></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>,<span class="dv">0</span>].set_title(<span class="st">'Setting A, Data distribution'</span>)</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>,<span class="dv">1</span>].scatter(X_train_2[:,<span class="dv">0</span>],X_train_2[:,<span class="dv">1</span>])</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>,<span class="dv">1</span>].scatter(X_test_2[:,<span class="dv">0</span>],X_test_2[:,<span class="dv">1</span>])<span class="op">;</span></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>,<span class="dv">1</span>].set_title(<span class="st">'Setting A, Trained and test  distribution'</span>)</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>,<span class="dv">2</span>].plot(scores_list_2)</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>,<span class="dv">2</span>].set_xlabel(<span class="st">'Value of K for KNN'</span>)</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>,<span class="dv">2</span>].set_ylabel(<span class="st">'Train Error'</span>)<span class="op">;</span>  </span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>,<span class="dv">2</span>].set_title(<span class="st">'Setting A, K vs Train Error'</span>)</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>,<span class="dv">2</span>].invert_xaxis()</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>,<span class="dv">2</span>].set_xticks(np.hstack([np.arange(<span class="bu">len</span>(scores_list_2),<span class="dv">0</span>,<span class="op">-</span><span class="bu">len</span>(scores_list_2)<span class="op">//</span><span class="dv">5</span>),<span class="dv">1</span>]))</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>,<span class="dv">2</span>].set_xticklabels(np.hstack([np.arange(<span class="bu">len</span>(scores_list_2),<span class="dv">0</span>,<span class="op">-</span><span class="bu">len</span>(scores_list_2)<span class="op">//</span><span class="dv">5</span>),<span class="dv">1</span>]))</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>,<span class="dv">3</span>].plot(scores_list_test_2)</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>,<span class="dv">3</span>].set_xlabel(<span class="st">'Value of K for KNN'</span>)</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>,<span class="dv">3</span>].set_ylabel(<span class="st">'Test Error'</span>)<span class="op">;</span>  </span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>,<span class="dv">3</span>].set_title(<span class="st">'Setting A, K vs Test Error'</span>)</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>,<span class="dv">3</span>].invert_xaxis()<span class="op">;</span></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>,<span class="dv">3</span>].set_xticks( np.hstack([np.arange(<span class="bu">len</span>(scores_list_2),<span class="dv">0</span>,<span class="op">-</span><span class="bu">len</span>(scores_list_2)<span class="op">//</span><span class="dv">5</span>),<span class="dv">1</span>]))</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>,<span class="dv">3</span>].set_xticklabels(np.hstack([np.arange(<span class="bu">len</span>(scores_list_2),<span class="dv">0</span>,<span class="op">-</span><span class="bu">len</span>(scores_list_2)<span class="op">//</span><span class="dv">5</span>),<span class="dv">1</span>]))</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">0</span>].scatter(data_3[:num_data<span class="op">//</span><span class="dv">3</span>,<span class="dv">0</span>],data_3[:num_data<span class="op">//</span><span class="dv">3</span>,<span class="dv">1</span>])</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">0</span>].scatter(data_3[num_data<span class="op">//</span><span class="dv">3</span>:<span class="dv">2</span><span class="op">*</span>num_data<span class="op">//</span><span class="dv">3</span>,<span class="dv">0</span>],data_3[num_data<span class="op">//</span><span class="dv">3</span>:<span class="dv">2</span><span class="op">*</span>num_data<span class="op">//</span><span class="dv">3</span>,<span class="dv">1</span>])<span class="op">;</span></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">0</span>].scatter(data_3[<span class="dv">2</span><span class="op">*</span>num_data<span class="op">//</span><span class="dv">3</span>:,<span class="dv">0</span>],data_3[<span class="dv">2</span><span class="op">*</span>num_data<span class="op">//</span><span class="dv">3</span>:,<span class="dv">1</span>])<span class="op">;</span></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">0</span>].set_title(<span class="st">'Setting B, Data distribution'</span>)</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">1</span>].scatter(X_train_3[:,<span class="dv">0</span>],X_train_3[:,<span class="dv">1</span>])</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">1</span>].scatter(X_test_3[:,<span class="dv">0</span>],X_test_3[:,<span class="dv">1</span>])<span class="op">;</span></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">1</span>].set_title(<span class="st">'Setting B, Trained and test  distribution'</span>)</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">2</span>].plot(scores_list_3)</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">2</span>].set_xlabel(<span class="st">'Value of K for KNN'</span>)</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">2</span>].set_ylabel(<span class="st">'Train Error'</span>)<span class="op">;</span>  </span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">2</span>].set_title(<span class="st">'Setting B, K vs Train Error'</span>)</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">2</span>].invert_xaxis()</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">2</span>].set_xticks( np.hstack([np.arange(<span class="bu">len</span>(scores_list_3),<span class="dv">0</span>,<span class="op">-</span><span class="bu">len</span>(scores_list_3)<span class="op">//</span><span class="dv">5</span>),<span class="dv">1</span>]))</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">2</span>].set_xticklabels( np.hstack([np.arange(<span class="bu">len</span>(scores_list_3),<span class="dv">0</span>,<span class="op">-</span><span class="bu">len</span>(scores_list_3)<span class="op">//</span><span class="dv">5</span>),<span class="dv">1</span>]))</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">3</span>].plot(scores_list_test_3)</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">3</span>].set_xlabel(<span class="st">'Value of K for KNN'</span>)</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">3</span>].set_ylabel(<span class="st">'Test Error'</span>)<span class="op">;</span>  </span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">3</span>].set_title(<span class="st">'Setting B, K vs Test Error'</span>)</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">3</span>].invert_xaxis()<span class="op">;</span></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">3</span>].set_xticks(np.hstack([np.arange(<span class="bu">len</span>(scores_list_3),<span class="dv">0</span>,<span class="op">-</span><span class="bu">len</span>(scores_list_3)<span class="op">//</span><span class="dv">5</span>),<span class="dv">1</span>]))</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">3</span>].set_xticklabels(np.hstack([np.arange(<span class="bu">len</span>(scores_list_3),<span class="dv">0</span>,<span class="op">-</span><span class="bu">len</span>(scores_list_3)<span class="op">//</span><span class="dv">5</span>),<span class="dv">1</span>]))<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="2022-08-26-CS5590-Assignment-1_files/figure-html/cell-2-output-1.png" class="img-fluid"></p>
</div>
</div>
<ul>
<li><span class="math inline">\(1.(c)\)</span> <br> Yes it is possible to build a univariate tree as described in the question, which classifies exactly similar to <span class="math inline">\(1-NN\)</span> using Manhattan distance.<br> Below is the example of the dataset:<br> For simplicity we consider <span class="math inline">\(2\)</span> class classification setting, in <span class="math inline">\(2\)</span> dimensional space, Class <span class="math inline">\(A\)</span> and Class <span class="math inline">\(B\)</span> each contains just one point:<br> <span class="math inline">\(A = {(1,3)}\)</span><br> <span class="math inline">\(B = {(3,3)}\)</span><br> Consider value of <span class="math inline">\(a=2,b=4,c=2\)</span> and <span class="math inline">\(d=4\)</span><br>
<ul>
<li>Decision Tree:<br> consider point <span class="math inline">\(a,b\)</span> lies on <span class="math inline">\(X\)</span> axis and pint <span class="math inline">\(d,c\)</span> lies on <span class="math inline">\(Y\)</span> axis,Classes can be distinguished with just one question <span class="math inline">\(x&gt;a\)</span> <br>
<ul>
<li>Point <span class="math inline">\((1,3)\)</span>: here <span class="math inline">\(x=1,y=3\)</span>, if we make decision using the rules, <span class="math inline">\(x&gt;a\)</span>, it is false, hence we classify it for class <span class="math inline">\(A\)</span>.</li>
<li>Point <span class="math inline">\((3,3)\)</span>: here <span class="math inline">\(x=3,y=3\)</span>, if we make decision using the rules, <span class="math inline">\(x&gt;a\)</span>, it is true, <span class="math inline">\(x&lt;b\)</span> is true, <span class="math inline">\(y&gt;c\)</span> is true, <span class="math inline">\(y&lt;d\)</span> is true, hence we classify the point for class <span class="math inline">\(B\)</span></li>
</ul></li>
<li><span class="math inline">\(1-NN\)</span>: <br> As the training dataset contains points <span class="math inline">\((1,3),(3,3)\)</span> so any points which has <span class="math inline">\(X\)</span> value greater than <span class="math inline">\(2\)</span> will be classified to class <span class="math inline">\(B\)</span> and lesser will be classified for class <span class="math inline">\(A\)</span>.</li>
</ul></li>
</ul>
<p>This is explained in below code and picture plotted :</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> tree,neighbors</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.colors <span class="im">import</span> ListedColormap</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.inspection <span class="im">import</span> DecisionBoundaryDisplay <span class="co"># Use scikit-learn version 1.1.0 to use Decision Boundary,  pip install scikit-learn==1.1.0</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> tree.DecisionTreeClassifier()</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>X<span class="op">=</span>np.array([[<span class="dv">1</span>,<span class="dv">3</span>],</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>            [<span class="dv">3</span>,<span class="dv">3</span>]])</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span> [<span class="dv">0</span>,<span class="dv">1</span>]</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>className <span class="op">=</span> [<span class="st">'A'</span>,<span class="st">'B'</span>]</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> clf.fit(X, y)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">3</span>))</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>tree <span class="op">=</span> tree.plot_tree(clf,ax<span class="op">=</span>ax[<span class="dv">0</span>],filled<span class="op">=</span><span class="va">True</span>,impurity<span class="op">=</span><span class="va">False</span>,class_names<span class="op">=</span>className)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">'Decision tree '</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>knn <span class="op">=</span> neighbors.KNeighborsClassifier(n_neighbors<span class="op">=</span><span class="dv">1</span>,metric<span class="op">=</span><span class="st">'cityblock'</span>)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>knn.fit(X,y)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>xx, yy <span class="op">=</span> np.meshgrid(np.arange(<span class="dv">0</span>, <span class="dv">5</span>, <span class="fl">0.1</span>), np.arange(<span class="dv">0</span>, <span class="dv">5</span>, <span class="fl">0.01</span>))</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> knn.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>cmap_light <span class="op">=</span> ListedColormap([<span class="st">"orange"</span>, <span class="st">"cyan"</span>])</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>cmap_bold <span class="op">=</span> [<span class="st">"darkorange"</span>, <span class="st">"c"</span>]</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>DecisionBoundaryDisplay.from_estimator(knn,X,cmap<span class="op">=</span>cmap_light,ax<span class="op">=</span>ax[<span class="dv">1</span>],response_method<span class="op">=</span><span class="st">"predict"</span>,plot_method<span class="op">=</span><span class="st">"pcolormesh"</span>)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span>X[:, <span class="dv">0</span>], y<span class="op">=</span>X[:, <span class="dv">1</span>],hue<span class="op">=</span>className,palette<span class="op">=</span>cmap_bold,alpha<span class="op">=</span><span class="fl">1.0</span>,edgecolor<span class="op">=</span><span class="st">"black"</span>)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">'Decision boundey of 1 -NN'</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="2022-08-26-CS5590-Assignment-1_files/figure-html/cell-3-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="q.-2." class="level1">
<h1>Q. 2.</h1>
<ul>
<li>Bayes Classifier: (6 marks)
<ul>
<li><span class="math inline">\((a)\)</span> <strong>(3 marks)</strong> Consider a classification problem with <span class="math inline">\(K\)</span> classes for which the feature vector <span class="math inline">\(\phi\)</span> has <span class="math inline">\(M\)</span> dimensions (categorical variables) each of which can take <span class="math inline">\(L\)</span> discrete states. Let the values of the dimensions be represented by a one-hot (<span class="math inline">\(1\)</span>-of-<span class="math inline">\(L\)</span>) binary coding scheme. Further suppose that, conditioned on the class <span class="math inline">\(C_k\)</span>, the <span class="math inline">\(M\)</span> dimensions of <span class="math inline">\(\phi\)</span> are independent, so that the class-conditional density factorizes with respect to the feature vector components (also referred to as the naive Bayes assumption). Show that the quantities given below:<br><span class="math display">\[a_k=\ln p(\phi|C_k)p(C_k)\]</span> are linear function of the components of <span class="math inline">\(\phi\)</span>.</li>
<li><span class="math inline">\((b)\)</span> <strong>(3 marks)</strong> You are now going to make a text classifier. To begin with, you attempt to classify documents as either <em>sport</em> or <em>politics</em>. You decide to represent each document as a (row) vector of attributes describing the presence or absence of the following words.<br> <span class="math inline">\(X\)</span> <strong>= (goal,football,golf,defence,offence,wicket,office,strategy)</strong> <br> Training data from sport documents and from politics documents is represented below in a matrix in which each row represents the 8 attributes.<br><br> <span class="math inline">\(\\ x_{politics}=\left\lbrack \begin{array}{cccccccc} 1 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 1\\ 1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \end{array}\right\rbrack\)</span> <br><br> <span class="math inline">\(\\ x_{sport} = \left\lbrack \begin{array}{cccccccc} 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\ 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\ 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\ 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \end{array}\right\rbrack\)</span><br><br> <br> Using a maximum likelihood naive Bayes classifier, what is the probability that the document <span class="math inline">\(x = (1, 0, 0, 1, 1, 1, 1, 0)\)</span> is about politics?</li>
</ul></li>
</ul>
<section id="answer-of-q.2." class="level2">
<h2 class="anchored" data-anchor-id="answer-of-q.2.">Answer of Q.2.</h2>
<ul>
<li><p><span class="math inline">\(2.(a)\)</span><br> Bayes classifier has the same softmax form whenever the class-conditional densities are <strong>any</strong> <a href="http://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions">exponential family density</a>.<br> Gaussian distribution is just an example of exponential family density. We can write <span class="math inline">\(p(\phi \mid C_k)\)</span> as below:<br> <span class="math inline">\(p((\phi \mid C_k, \eta_k)=h(\mathbf {X})\left\{ \exp{\eta_k^T \mathbf{X}} - \alpha (\eta_k) \right\}\)</span><br> <span class="math inline">\(\displaystyle p(( C_k \mid \phi, \eta)= \frac{\exp \left\{ \eta_k^T \mathbf{X} -\alpha (\eta _k)\right\}}{\sum_{j}\exp \left\{ \eta_j^T \mathbf{X} - \alpha (\eta_j)\right\}}\)</span><br> <span class="math inline">\(\displaystyle p(( C_k \mid \phi, \eta) = \frac{e^{\beta_k^T \mathbf{X}}}{\sum_je^{\beta_k^T\mathbf{X}}}\)</span><br> Where, <span class="math inline">\(\displaystyle \beta_k = \left[ n_k;-\alpha \left( \eta_k \right) \right]\)</span><br> we can see that the numerator term of <span class="math inline">\(\displaystyle p(( C_k \mid \phi, \eta)\)</span> is <span class="math inline">\(\displaystyle a_k = \ln \;p\left(\phi \mid C_k \right)p\left(C_K \right)=\ln e^{\beta_k^T\mathbf{X}}=\beta_k^T \mathbf{X}\)</span><br> Which is a liner equation hence proved.</p></li>
<li><p><span class="math inline">\(2.(a)\)</span><br> This is another way of solving the same question <span class="math inline">\(2.(a)\)</span>, here we consider probability function to be continuous and normally distributed. <br> The class conditional probability is considered as multivariate Gaussian Distribution and is given by:<br> <span class="math inline">\(\displaystyle p\left(\phi \mid C_k \right)=\frac{1}{\sqrt{\left(2\pi\right)^D|\Sigma|}}\exp \left\lbrace -\frac{1}{2}{\left(\phi -\mu_k \right)}^T \Sigma^{-1} \left(\phi -\mu_k \right)\right\rbrace\)</span><br> Now the term <span class="math inline">\(a_k\)</span> can be written as :<br> <span class="math inline">\(\displaystyle a_k = \ln \;p\left(\phi \mid C_k \right)p\left(C_K \right) \\  =\ln \left(\frac{1}{\sqrt{\left(2\pi\right)^D|\Sigma|}}\exp \left\lbrace -\frac{1}{2}{\left(\phi -\mu_k \right)}^T \Sigma^{-1} \left(\phi -\mu_k \right)\right\rbrace p\left(C_K \right)\right)\)</span><br> <span class="math inline">\(\displaystyle =\ln \left(\frac{1}{\sqrt{\left(2\pi\right)^D|\Sigma|}}\exp \left\lbrace -\frac{1}{2}{\left(\phi -\mu_k \right)}^T \Sigma^{-1} \left(\phi -\mu_k \right)\right\rbrace \right)+\ln \left(p\left(C_K \right)\right)\)</span><br> <span class="math inline">\(\displaystyle =\ln \left(\exp \left\lbrace -\frac{1}{2}{\left(\phi -\mu_k \right)}^T \Sigma^{-1} \left(\phi -\mu_k \right)\right\rbrace \right)+\ln \left(\frac{1}{\sqrt{\left(2\pi\right)^D|\Sigma|}}\right)+\ln \left(p\left(C_K \right)\right)\)</span><br> <span class="math inline">\(\displaystyle =-\frac{1}{2}{\left(\phi -\mu_k \right)}^T \Sigma^{-1} \left(\phi -\mu_k \right)+\ln \left(\frac{1}{\sqrt{\left(2\pi\right)^D|\Sigma|}}\right)+\ln \left(p\left(C_K \right)\right)\)</span><br> <span class="math inline">\(\displaystyle =-\frac{1}{2} \left( \phi^T \Sigma^{-1}\phi - 2\mu_k^T\Sigma^{-1}\phi +\mu_k^T\Sigma^{-1}\mu_k \right) +\ln \left(\frac{1}{\sqrt{\left(2\pi\right)^D|\Sigma|}}\right)+\ln \left(p\left(C_K \right)\right)\)</span><br> <span class="math inline">\(\displaystyle = -\frac{1}{2} \phi^T \Sigma^{-1}\phi + \mu_k^T\Sigma^{-1}\phi -\frac{1}{2} \mu_k^T\Sigma^{-1}\mu_k +\ln \left(\frac{1}{\sqrt{\left(2\pi\right)^D|\Sigma|}}\right)+\ln \left(p\left(C_K \right)\right)\)</span><br><br> Now if we consider two class we can say <span class="math inline">\(p(\phi \mid C_{k1})=p(\phi \mid C_{k2})\)</span> <br> In that case Due to shared covariance the quadratic term cancels out, and the rest of the term can be written as :<br><br> <span class="math inline">\(\displaystyle =W_k^T \phi +w_{\mathrm{k0}}\)</span><br><br> Where : <br> <span class="math inline">\(\displaystyle W_k = \Sigma^{-1} \mu_k\)</span><br> <span class="math inline">\(\displaystyle W_{k0} = -\frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k+\ln p(C_k)\)</span><br><br> The equation <span class="math inline">\(\displaystyle =W_k^T \phi +w_{\mathrm{k0}}\)</span> is a linear equation in terms of <span class="math inline">\(\phi\)</span> and it should satisfies the two properties of linearity, To prove it, the equation can be rewritten as :<br><br> <span class="math inline">\(\displaystyle \Rightarrow f\left(\phi ,a_k \right){\;=\;a}_k -W_k^T \phi =w_{\mathrm{k0}}\)</span><br></p>
<ul>
<li>Distributive over addition: <br> <span class="math inline">\(f\left(\phi +\beta ,a_k +\beta \right){\;=\;a}_k +\beta -W_k^T \left(\phi +\beta \right)={\;\;a}_k +\beta -W_k^T \phi -W_k^T \beta =\left({\;\;a}_k -W_k^T \phi \right)+\left(\beta -W_k^T \beta \right)=f\left(\phi ,a_k \right)+f\left(\beta ,\beta \right)\)</span><br></li>
<li>Homogenous of Degree one:<br> <span class="math inline">\(f\left(\beta \phi ,\beta a_k \right)=\beta a_k -\beta W_k^T \phi =\beta \;\left(a_k -W_k^T \phi \right)=\beta w_{\mathrm{k0}} =\beta f\left(\phi ,a_k \right)\)</span></li>
</ul>
<p>Hence we can say that <span class="math inline">\(\ln \;p\left(\phi |C_k \right)p\left(C_k \right)\)</span> is linear in terms of <span class="math inline">\(\phi\)</span></p></li>
<li><p><span class="math inline">\(2.(b)\)</span><br></p></li>
</ul>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>columns<span class="op">=</span>[<span class="st">'goal'</span>,<span class="st">'football'</span>,<span class="st">'golf'</span>,<span class="st">'defence'</span>,<span class="st">'offence'</span>,<span class="st">'wicket'</span>,<span class="st">'office'</span>,<span class="st">'strategy'</span>,<span class="st">'class'</span>]</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>data<span class="op">=</span>[[<span class="dv">1</span> , <span class="dv">0</span> , <span class="dv">1</span> , <span class="dv">1</span> , <span class="dv">1</span> , <span class="dv">0</span> , <span class="dv">1</span> , <span class="dv">1</span> , <span class="st">'p'</span>],</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>      [<span class="dv">0</span> , <span class="dv">0</span> , <span class="dv">0</span> , <span class="dv">1</span> , <span class="dv">0</span> , <span class="dv">0</span> , <span class="dv">1</span> , <span class="dv">1</span> , <span class="st">'p'</span>],</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>      [<span class="dv">1</span> , <span class="dv">0</span> , <span class="dv">0</span> , <span class="dv">1</span> , <span class="dv">1</span> , <span class="dv">0</span> , <span class="dv">1</span> , <span class="dv">0</span> , <span class="st">'p'</span>],</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>      [<span class="dv">0</span> , <span class="dv">1</span> , <span class="dv">0</span> , <span class="dv">0</span> , <span class="dv">1</span> , <span class="dv">1</span> , <span class="dv">0</span> , <span class="dv">1</span> , <span class="st">'p'</span>],</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>      [<span class="dv">0</span> , <span class="dv">0</span> , <span class="dv">0</span> , <span class="dv">1</span> , <span class="dv">1</span> , <span class="dv">0</span> , <span class="dv">1</span> , <span class="dv">1</span> , <span class="st">'p'</span>],</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>      [<span class="dv">0</span> , <span class="dv">0</span> , <span class="dv">0</span> , <span class="dv">1</span> , <span class="dv">1</span> , <span class="dv">0</span> , <span class="dv">0</span> , <span class="dv">1</span> , <span class="st">'p'</span>],</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>      [<span class="dv">1</span> , <span class="dv">1</span> , <span class="dv">0</span> , <span class="dv">0</span> , <span class="dv">0</span> , <span class="dv">0</span> , <span class="dv">0</span> , <span class="dv">0</span> , <span class="st">'s'</span>],</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>      [<span class="dv">0</span> , <span class="dv">0</span> , <span class="dv">1</span> , <span class="dv">0</span> , <span class="dv">0</span> , <span class="dv">0</span> , <span class="dv">0</span> , <span class="dv">0</span> , <span class="st">'s'</span>],</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>      [<span class="dv">1</span> , <span class="dv">1</span> , <span class="dv">0</span> , <span class="dv">1</span> , <span class="dv">0</span> , <span class="dv">0</span> , <span class="dv">0</span> , <span class="dv">0</span> , <span class="st">'s'</span>],</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>      [<span class="dv">1</span> , <span class="dv">1</span> , <span class="dv">0</span> , <span class="dv">1</span> , <span class="dv">0</span> , <span class="dv">0</span> , <span class="dv">0</span> , <span class="dv">1</span> , <span class="st">'s'</span>],</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>      [<span class="dv">1</span> , <span class="dv">1</span> , <span class="dv">0</span> , <span class="dv">1</span> , <span class="dv">1</span> , <span class="dv">0</span> , <span class="dv">0</span> , <span class="dv">0</span> , <span class="st">'s'</span>],</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>      [<span class="dv">0</span> , <span class="dv">0</span> , <span class="dv">0</span> , <span class="dv">1</span> , <span class="dv">0</span> , <span class="dv">1</span> , <span class="dv">0</span> , <span class="dv">0</span> , <span class="st">'s'</span>]]</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(data<span class="op">=</span>data,columns<span class="op">=</span>columns)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Below table shows probabilities.<br> <code>class_p</code> denotes politics class <code>class_s</code> denotes sports class, <br> For example, Row <code>goal_1</code> and column <code>class_p</code> of table <code>goal</code> denotes <span class="math inline">\(P(\mathrm{gaol}=1 \mid \mathrm{class}=\mathrm{politics})\)</span> <br> Row <code>football_0</code> and column <code>class_s</code> of table <code>football</code> denotes <span class="math inline">\(P(\mathrm{football}=0 \mid \mathrm{class}=\mathrm{sports})\)</span></p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> display_html </span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>all_tables<span class="op">=</span><span class="st">""</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> df.columns[:<span class="op">-</span><span class="dv">1</span>]:</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    table<span class="op">=</span>pd.DataFrame()</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> c <span class="kw">in</span> df[<span class="st">'class'</span>].unique():</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> r <span class="kw">in</span> df[col].unique():</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>            n <span class="op">=</span> df.loc[(df[col]<span class="op">==</span>r) <span class="op">&amp;</span> (df[<span class="st">'class'</span>]<span class="op">==</span>c) ,[col]].count().to_numpy()[<span class="dv">0</span>]</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>            d <span class="op">=</span> df.loc[(df[<span class="st">'class'</span>]<span class="op">==</span>c) ,[col]].count().to_numpy()[<span class="dv">0</span>]</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>            table.loc[col<span class="op">+</span><span class="st">'_'</span><span class="op">+</span><span class="bu">str</span>(r),<span class="st">'class'</span><span class="op">+</span><span class="st">'_'</span><span class="op">+</span>c]<span class="op">=</span><span class="st">"</span><span class="sc">{}</span><span class="st">/</span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(n,d)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>            table.sort_index(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>            table_styler <span class="op">=</span> table.style.set_table_attributes(<span class="st">"style='display:inline'"</span>).set_caption(col)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    all_tables<span class="op">=</span>all_tables<span class="op">+</span>table_styler._repr_html_()</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>display_html (all_tables,raw<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style type="text/css">
</style>
<table id="T_9c154" style="display:inline">
  <caption>goal</caption>
  <thead>
    <tr>
      <th class="blank level0">&nbsp;</th>
      <th id="T_9c154_level0_col0" class="col_heading level0 col0">class_p</th>
      <th id="T_9c154_level0_col1" class="col_heading level0 col1">class_s</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_9c154_level0_row0" class="row_heading level0 row0">goal_0</th>
      <td id="T_9c154_row0_col0" class="data row0 col0">4/6</td>
      <td id="T_9c154_row0_col1" class="data row0 col1">2/6</td>
    </tr>
    <tr>
      <th id="T_9c154_level0_row1" class="row_heading level0 row1">goal_1</th>
      <td id="T_9c154_row1_col0" class="data row1 col0">2/6</td>
      <td id="T_9c154_row1_col1" class="data row1 col1">4/6</td>
    </tr>
  </tbody>
</table>
<style type="text/css">
</style>
<table id="T_b2507" style="display:inline">
  <caption>football</caption>
  <thead>
    <tr>
      <th class="blank level0">&nbsp;</th>
      <th id="T_b2507_level0_col0" class="col_heading level0 col0">class_p</th>
      <th id="T_b2507_level0_col1" class="col_heading level0 col1">class_s</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_b2507_level0_row0" class="row_heading level0 row0">football_0</th>
      <td id="T_b2507_row0_col0" class="data row0 col0">5/6</td>
      <td id="T_b2507_row0_col1" class="data row0 col1">2/6</td>
    </tr>
    <tr>
      <th id="T_b2507_level0_row1" class="row_heading level0 row1">football_1</th>
      <td id="T_b2507_row1_col0" class="data row1 col0">1/6</td>
      <td id="T_b2507_row1_col1" class="data row1 col1">4/6</td>
    </tr>
  </tbody>
</table>
<style type="text/css">
</style>
<table id="T_f10b7" style="display:inline">
  <caption>golf</caption>
  <thead>
    <tr>
      <th class="blank level0">&nbsp;</th>
      <th id="T_f10b7_level0_col0" class="col_heading level0 col0">class_p</th>
      <th id="T_f10b7_level0_col1" class="col_heading level0 col1">class_s</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_f10b7_level0_row0" class="row_heading level0 row0">golf_0</th>
      <td id="T_f10b7_row0_col0" class="data row0 col0">5/6</td>
      <td id="T_f10b7_row0_col1" class="data row0 col1">5/6</td>
    </tr>
    <tr>
      <th id="T_f10b7_level0_row1" class="row_heading level0 row1">golf_1</th>
      <td id="T_f10b7_row1_col0" class="data row1 col0">1/6</td>
      <td id="T_f10b7_row1_col1" class="data row1 col1">1/6</td>
    </tr>
  </tbody>
</table>
<style type="text/css">
</style>
<table id="T_da358" style="display:inline">
  <caption>defence</caption>
  <thead>
    <tr>
      <th class="blank level0">&nbsp;</th>
      <th id="T_da358_level0_col0" class="col_heading level0 col0">class_p</th>
      <th id="T_da358_level0_col1" class="col_heading level0 col1">class_s</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_da358_level0_row0" class="row_heading level0 row0">defence_0</th>
      <td id="T_da358_row0_col0" class="data row0 col0">1/6</td>
      <td id="T_da358_row0_col1" class="data row0 col1">2/6</td>
    </tr>
    <tr>
      <th id="T_da358_level0_row1" class="row_heading level0 row1">defence_1</th>
      <td id="T_da358_row1_col0" class="data row1 col0">5/6</td>
      <td id="T_da358_row1_col1" class="data row1 col1">4/6</td>
    </tr>
  </tbody>
</table>
<style type="text/css">
</style>
<table id="T_a08ed" style="display:inline">
  <caption>offence</caption>
  <thead>
    <tr>
      <th class="blank level0">&nbsp;</th>
      <th id="T_a08ed_level0_col0" class="col_heading level0 col0">class_p</th>
      <th id="T_a08ed_level0_col1" class="col_heading level0 col1">class_s</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_a08ed_level0_row0" class="row_heading level0 row0">offence_0</th>
      <td id="T_a08ed_row0_col0" class="data row0 col0">1/6</td>
      <td id="T_a08ed_row0_col1" class="data row0 col1">5/6</td>
    </tr>
    <tr>
      <th id="T_a08ed_level0_row1" class="row_heading level0 row1">offence_1</th>
      <td id="T_a08ed_row1_col0" class="data row1 col0">5/6</td>
      <td id="T_a08ed_row1_col1" class="data row1 col1">1/6</td>
    </tr>
  </tbody>
</table>
<style type="text/css">
</style>
<table id="T_3eb57" style="display:inline">
  <caption>wicket</caption>
  <thead>
    <tr>
      <th class="blank level0">&nbsp;</th>
      <th id="T_3eb57_level0_col0" class="col_heading level0 col0">class_p</th>
      <th id="T_3eb57_level0_col1" class="col_heading level0 col1">class_s</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_3eb57_level0_row0" class="row_heading level0 row0">wicket_0</th>
      <td id="T_3eb57_row0_col0" class="data row0 col0">5/6</td>
      <td id="T_3eb57_row0_col1" class="data row0 col1">5/6</td>
    </tr>
    <tr>
      <th id="T_3eb57_level0_row1" class="row_heading level0 row1">wicket_1</th>
      <td id="T_3eb57_row1_col0" class="data row1 col0">1/6</td>
      <td id="T_3eb57_row1_col1" class="data row1 col1">1/6</td>
    </tr>
  </tbody>
</table>
<style type="text/css">
</style>
<table id="T_4c988" style="display:inline">
  <caption>office</caption>
  <thead>
    <tr>
      <th class="blank level0">&nbsp;</th>
      <th id="T_4c988_level0_col0" class="col_heading level0 col0">class_p</th>
      <th id="T_4c988_level0_col1" class="col_heading level0 col1">class_s</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_4c988_level0_row0" class="row_heading level0 row0">office_0</th>
      <td id="T_4c988_row0_col0" class="data row0 col0">2/6</td>
      <td id="T_4c988_row0_col1" class="data row0 col1">6/6</td>
    </tr>
    <tr>
      <th id="T_4c988_level0_row1" class="row_heading level0 row1">office_1</th>
      <td id="T_4c988_row1_col0" class="data row1 col0">4/6</td>
      <td id="T_4c988_row1_col1" class="data row1 col1">0/6</td>
    </tr>
  </tbody>
</table>
<style type="text/css">
</style>
<table id="T_78582" style="display:inline">
  <caption>strategy</caption>
  <thead>
    <tr>
      <th class="blank level0">&nbsp;</th>
      <th id="T_78582_level0_col0" class="col_heading level0 col0">class_p</th>
      <th id="T_78582_level0_col1" class="col_heading level0 col1">class_s</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_78582_level0_row0" class="row_heading level0 row0">strategy_0</th>
      <td id="T_78582_row0_col0" class="data row0 col0">1/6</td>
      <td id="T_78582_row0_col1" class="data row0 col1">5/6</td>
    </tr>
    <tr>
      <th id="T_78582_level0_row1" class="row_heading level0 row1">strategy_1</th>
      <td id="T_78582_row1_col0" class="data row1 col0">5/6</td>
      <td id="T_78582_row1_col1" class="data row1 col1">1/6</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<p>We need to find out probability of <span class="math inline">\(X\)</span> to be politics, where :<br> <span class="math inline">\(X=\left(\mathrm{goal}=1,\mathrm{football}=0,\mathrm{golf}=0,\mathrm{defence}=1,\mathrm{offence}=1,\mathrm{wicket}=1,\mathrm{office}=1,\mathrm{strategy}=0\right)\)</span><br><br> <span class="math inline">\(P\left(\mathrm{ploitics}\mid X\right)=\\ P\left(\mathrm{goal}=1\mid \mathrm{politics}\right)\times P\left(\mathrm{football}=0\mid \mathrm{politics}\right)\times P\left(\mathrm{golf}=0\mid \mathrm{politics}\right)\times P\left(\mathrm{defence}=1\mid \mathrm{politics}\right)\times P\left(\mathrm{offence}=1\mid \mathrm{politics}\right)\times P\left(\mathrm{wicket}=1\mid \mathrm{politics}\right)\times P\left(\mathrm{office}=1\mid \mathrm{politics}\right)\times P\left(\mathrm{strategy}=0\mid \mathrm{politics}\right)\times P\left(\mathrm{class}=\mathrm{polotics}\right)\)</span></p>
<p>Conditional probabilities (from above table): <br> <span class="math inline">\(P\left(\mathrm{goal}=1\mid \mathrm{politics}\right)=\frac{2}{6},\)</span> <span class="math inline">\(P\left(\mathrm{football}=0\mid \mathrm{politics}\right)=\frac{5}{6}\)</span><br> <span class="math inline">\(P\left(\mathrm{golf}=0\mid \mathrm{politics}\right)=\frac{5}{6},\)</span> <span class="math inline">\(P\left(\mathrm{defence}=1\mid \mathrm{politics}\right)=\frac{5}{6}\)</span><br> <span class="math inline">\(P\left(\mathrm{offence}=1\mid \mathrm{politics}\right)=\frac{5}{6},\)</span> <span class="math inline">\(P\left(\mathrm{wicket}=1\mid \mathrm{politics}\right)=\frac{1}{6}\)</span><br> <span class="math inline">\(P\left(\mathrm{office}=1\mid \mathrm{politics}\right)=\frac{4}{6},\)</span> <span class="math inline">\(P\left(\mathrm{strategy}=0\mid \mathrm{politics}\right)=\frac{1}{6}\)</span><br> <span class="math inline">\(P\left(\mathrm{class}=\mathrm{polotics}\right)=\frac{1}{2}\)</span><br></p>
<p>Hence :<br></p>
<p><span class="math inline">\(P\left(\mathrm{ploitics}\mid X\right)=\frac{2}{6}\times \frac{5}{6}\times \frac{5}{6}\times \frac{5}{6}\times \frac{5}{6}\times \frac{1}{6}\times \frac{4}{6}\times \frac{1}{6}\times \frac{1}{2}= \frac{625}{419904}=1.4884\times10^{-3}\)</span></p>
</section>
</section>
<section id="q.-3." class="level1">
<h1>Q. 3.</h1>
<ul>
<li><p>Model Selection: (<strong>6 marks)</strong> Install the <code>pydataset</code> module (if you haven’t already):<br></p>
<pre><code>from pydataset import data
import pandas as pd
melanoma_data = data ( 'Melanoma' , show_doc=True )</code></pre>
<p>The <em>Melanoma</em> dataset consists of measurements of patients with malignant melanoma (a type of cancer). For each patient, the dataset specifies if the patient died or lived at the end of the trial. Moreover, some patients died due to causes unrelated to melanoma. Your task is to do the following:</p>
<ul>
<li><span class="math inline">\((a)\)</span> <strong>(1 mark)</strong> Remove those patients who died due to causes unrelated to Melanoma, and plot patient status vs age and patient status vs thickness - for your own understanding.</li>
<li><span class="math inline">\((b)\)</span> <strong>(4 marks)</strong> Split the data into 80% training and 20% test set using random stratified sampling. Now, on the 80% training data, perform 3-fold cross-validation using a classifier to predict the status of the patient (do not use the 20% held-aside test data for cross-validation; we will use it later to study generalization performance). Also, remember to use stratified sampling inside cross-validation too. You are allowed to use any existing machine learning library of your choice: scikitlearn, pandas, Weka (we recommend scikitlearn) - but you should use only the decision tree, k-NN or the naive Bayes classifier (to align with what we have covered in class so far, random forests not allowed too). Report the mean of the three quantities (accuracy, precision, recall) on 3-fold cross-validation.</li>
<li><span class="math inline">\((c)\)</span> <strong>(1 mark)</strong> Once you have picked the best classification model in cross-validation, train the best-performing setting on the entire 80% training data, and report performance on the held-aside test set. Report your observation on how representative the training/ validation data was w.r.t test data.</li>
<li>Deliverables:
<ul>
<li>Code</li>
<li>Data splits (for us to verify your reported results)</li>
<li>Brief report (PDF) with your solutions for the above questions</li>
</ul></li>
</ul></li>
</ul>
<section id="answer-of-q.3." class="level2">
<h2 class="anchored" data-anchor-id="answer-of-q.3.">Answer of Q.3.</h2>
<ul>
<li><span class="math inline">\(3.(a)\)</span> :<br> If status is <span class="math inline">\(3\)</span> it means patient died due to other Cause. So we can remove this column :</li>
</ul>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pydataset <span class="im">import</span> data</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt </span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>melanoma_data <span class="op">=</span> data(<span class="st">'Melanoma'</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>melanoma_data.drop(melanoma_data[melanoma_data[<span class="st">'status'</span>]<span class="op">==</span><span class="dv">3</span>].index,inplace<span class="op">=</span><span class="va">True</span>) <span class="co"># drop the data with status '3'</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>fig,ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>,figsize<span class="op">=</span>(<span class="dv">30</span>, <span class="dv">3</span>))</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].scatter(melanoma_data[<span class="st">'age'</span>],melanoma_data[<span class="st">'status'</span>])</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">'age'</span>,fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="st">'status'</span>,fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].scatter(melanoma_data[<span class="st">'thickness'</span>],melanoma_data[<span class="st">'status'</span>])</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">'thickness'</span>,fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="st">'status'</span>,fontsize<span class="op">=</span><span class="dv">14</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2022-08-26-CS5590-Assignment-1_files/figure-html/cell-6-output-1.png" class="img-fluid"></p>
</div>
</div>
<ul>
<li><span class="math inline">\(3.(b)\)</span><br>
<ul>
<li>The Melanoma data has following attributes:,<br>
<ul>
<li>time: survival time in days</li>
<li>status: <code>1</code> died from melanoma, <code>2</code> alive, <code>3</code> dead from other causes.</li>
<li>sex: <code>1</code> = male, <code>0</code> = female.</li>
<li>age: age in years.</li>
<li>year: year of operation.</li>
<li>thickness: tumour thickness in mm.</li>
<li>ulcer: <code>1</code> = presence, <code>0</code> = absence.</li>
</ul></li>
</ul></li>
</ul>
<p>Import libraries , implement Stratified KFold Generator to be used in grid search :</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV,StratifiedShuffleSplit</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> metrics</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.naive_bayes <span class="im">import</span>  GaussianNB</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pydataset <span class="im">import</span> data</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> StratifiedKFoldGenerator:</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_splits<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_splits <span class="op">=</span> n_splits</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.folds<span class="op">=</span>[]</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> split(<span class="va">self</span>, X, y, groups<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> train_index, test_index <span class="kw">in</span> StratifiedShuffleSplit(n_splits<span class="op">=</span><span class="va">self</span>.n_splits,test_size<span class="op">=</span><span class="fl">0.2</span>,random_state<span class="op">=</span><span class="dv">0</span>).split(X,y):</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.folds.append([train_index, test_index])</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>            <span class="cf">yield</span> train_index, test_index</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> getStratifiedKFold(<span class="va">self</span>):</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.folds</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_n_splits(<span class="va">self</span>, X, y, groups<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.n_splits   </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Load data, separate out <span class="math inline">\(20\)</span> percent test data, do not use it in training phase. Instantiate <code>GaussianNB</code>, <code>KNeighborsClassifier</code> and <code>DecisionTreeClassifier</code> and update parameters dictionary for grid search :</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>melanoma_data <span class="op">=</span> data(<span class="st">'Melanoma'</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>melanoma_data.drop(melanoma_data[melanoma_data[<span class="st">'status'</span>]<span class="op">==</span><span class="dv">3</span>].index,inplace<span class="op">=</span><span class="va">True</span>) <span class="co"># drop the data with status '3'</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> melanoma_data[[<span class="st">'time'</span>,<span class="st">'status'</span>,<span class="st">'sex'</span>,<span class="st">'age'</span>,<span class="st">'year'</span>,<span class="st">'thickness'</span>]]</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> melanoma_data[<span class="st">'ulcer'</span>]</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>sss1 <span class="op">=</span> StratifiedShuffleSplit(n_splits<span class="op">=</span><span class="dv">1</span>, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>train_val_index, test_index <span class="op">=</span> <span class="bu">list</span>(sss1.split(X, y))[<span class="dv">0</span>]</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>X_train_val, X_test <span class="op">=</span> X.iloc[train_val_index], X.iloc[test_index]</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>y_train_val, y_test <span class="op">=</span> y.iloc[train_val_index], y.iloc[test_index] </span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>classifiers <span class="op">=</span> {}</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>classifiers.update({<span class="st">"GaussianNB"</span>: GaussianNB()})</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>classifiers.update({<span class="st">"KNeighborsClassifier"</span>: KNeighborsClassifier()})</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>classifiers.update({<span class="st">"DecisionTreeClassifier"</span>: DecisionTreeClassifier()})</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>parameters <span class="op">=</span> {}</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>parameters.update({<span class="st">"GaussianNB"</span>: { </span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>                            <span class="st">"classifier__var_smoothing"</span>: [<span class="fl">1e-9</span>, <span class="fl">1e-8</span>,<span class="fl">1e-7</span>, <span class="fl">1e-6</span>, <span class="fl">1e-5</span>]</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>                             }})</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>parameters.update({<span class="st">"KNeighborsClassifier"</span>: { </span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>                            <span class="st">"classifier__n_neighbors"</span>: <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">1</span>,<span class="dv">31</span>)),</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>                            <span class="st">"classifier__p"</span>: [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>],</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>                            <span class="st">"classifier__leaf_size"</span>: [<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">20</span>, <span class="dv">25</span>, <span class="dv">30</span>, <span class="dv">35</span>, <span class="dv">40</span>, <span class="dv">45</span>, <span class="dv">50</span>],</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>                            <span class="st">"classifier__n_jobs"</span>: [<span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>                            <span class="st">"classifier__weights"</span>: [<span class="st">'uniform'</span>,<span class="st">'distance'</span>]</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>                             }})</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>parameters.update({<span class="st">"DecisionTreeClassifier"</span>: { </span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>                            <span class="st">"classifier__criterion"</span> :[<span class="st">"gini"</span>, <span class="st">"entropy"</span>],</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>                            <span class="st">"classifier__splitter"</span>: [<span class="st">"best"</span>, <span class="st">"random"</span>],</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>                            <span class="st">"classifier__class_weight"</span>: [<span class="va">None</span>, <span class="st">"balanced"</span>],</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>                            <span class="st">"classifier__max_depth"</span> : [<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">8</span>],</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>                            <span class="st">"classifier__min_samples_split"</span>: [<span class="fl">0.005</span>, <span class="fl">0.01</span>, <span class="fl">0.05</span>, <span class="fl">0.10</span>],</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>                            <span class="st">"classifier__min_samples_leaf"</span>: [<span class="fl">0.005</span>, <span class="fl">0.01</span>, <span class="fl">0.05</span>, <span class="fl">0.10</span>],</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>                            <span class="st">"classifier__max_features"</span>:[<span class="dv">6</span>,<span class="st">"sqrt"</span>, <span class="st">"log2"</span>]</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>                             }})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Use <span class="math inline">\(80\)</span> percent data for training and validation, use stratified sampling inside cross-validation too, perform grid search on all the models, using Stratified KFold Generator, fit the model on best parameters and evaluate it on train and val data for all the <span class="math inline">\(3\)</span> folds, do not use test data here.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> {}</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>models <span class="op">=</span> {}</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>Stratified_Split <span class="op">=</span> StratifiedKFoldGenerator(<span class="dv">3</span>) <span class="co"># create the object of stratified  sampling </span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> classifier_label, classifier <span class="kw">in</span> classifiers.items():</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    scaler <span class="op">=</span> StandardScaler() <span class="co"># for data normalization </span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    steps <span class="op">=</span> [(<span class="st">"scaler"</span>, scaler), (<span class="st">"classifier"</span>, classifier)]</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    pipeline <span class="op">=</span> Pipeline(steps <span class="op">=</span> steps) </span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    param_grid <span class="op">=</span> parameters[classifier_label]</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    gscv <span class="op">=</span> GridSearchCV(pipeline, param_grid, cv <span class="op">=</span> Stratified_Split,  n_jobs<span class="op">=</span> <span class="op">-</span><span class="dv">1</span>, verbose <span class="op">=</span> <span class="dv">0</span>, scoring <span class="op">=</span> <span class="st">"roc_auc"</span>) <span class="co"># perform grid search to find the best parameters</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    gscv.fit(X_train_val, np.ravel(y_train_val))   <span class="co"># Train the model, using grid search</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    best_params <span class="op">=</span> gscv.best_params_</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    train_accuracy<span class="op">=</span>[]</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    train_precision<span class="op">=</span>[]</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    train_recall<span class="op">=</span>[]</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    train_auc <span class="op">=</span> []</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>    val_accuracy<span class="op">=</span>[]</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    val_precision<span class="op">=</span>[]</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    val_recall<span class="op">=</span>[]</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    val_auc<span class="op">=</span>[]</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    result_df <span class="op">=</span> pd.DataFrame(data<span class="op">=</span>[],columns<span class="op">=</span>[<span class="st">'accuracy'</span>,<span class="st">'precision'</span>,<span class="st">'recall'</span>,<span class="st">'AUC'</span>],index<span class="op">=</span>[<span class="st">'train'</span>,<span class="st">'val'</span>])</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> train_index,val_index <span class="kw">in</span> Stratified_Split.getStratifiedKFold():  <span class="co"># Just for finding performance parameters, on the same fold which model was trained on.</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>             X_train,X_val <span class="op">=</span>  X_train_val.iloc[train_index],X_train_val.iloc[val_index]   <span class="co"># get the same fold index on which model was trained.</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>             y_train,y_val <span class="op">=</span>  y_train_val.iloc[train_index],y_train_val.iloc[val_index]  </span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Calculate  the train performance </span></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>             y_pred <span class="op">=</span> gscv.predict_proba(X_train)[:,<span class="dv">1</span>] </span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>             train_auc.append(metrics.roc_auc_score(y_train, y_pred))</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>             y_pred <span class="op">=</span> (y_pred<span class="op">&gt;</span><span class="fl">0.5</span>).astype(np.int0)</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>             train_accuracy.append(metrics.accuracy_score(y_train,y_pred))</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>             train_precision.append(metrics.precision_score(y_train,y_pred))</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>             train_recall.append(metrics.recall_score(y_train,y_pred))</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Calculate  the Val performance </span></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>             y_pred <span class="op">=</span> gscv.predict_proba(X_val)[:,<span class="dv">1</span>] </span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>             val_auc.append(metrics.roc_auc_score(y_val, y_pred))</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>             y_pred <span class="op">=</span> (y_pred<span class="op">&gt;</span><span class="fl">0.5</span>).astype(np.int0)</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>             val_accuracy.append(metrics.accuracy_score(y_val,y_pred))</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>             val_precision.append(metrics.precision_score(y_val,y_pred))</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>             val_recall.append(metrics.recall_score(y_val,y_pred))</span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Find mean of all the 3 fold performance and store in panda data frame</span></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> {<span class="st">"Classifier"</span>: gscv,<span class="st">"Best Parameters"</span>: best_params}</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>    result_df.loc[[<span class="st">'train'</span>],[<span class="st">'accuracy'</span>]]<span class="op">=</span>np.array(train_accuracy).mean()</span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>    result_df.loc[[<span class="st">'train'</span>],[<span class="st">'precision'</span>]]<span class="op">=</span>np.array(train_precision).mean()</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>    result_df.loc[[<span class="st">'train'</span>],[<span class="st">'recall'</span>]]<span class="op">=</span>np.array(train_recall).mean()</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>    result_df.loc[[<span class="st">'train'</span>],[<span class="st">'AUC'</span>]]<span class="op">=</span>np.array(train_auc).mean()</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>    result_df.loc[[<span class="st">'val'</span>],[<span class="st">'accuracy'</span>]]<span class="op">=</span>np.array(val_accuracy).mean()</span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>    result_df.loc[[<span class="st">'val'</span>],[<span class="st">'precision'</span>]]<span class="op">=</span>np.array(val_precision).mean()</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>    result_df.loc[[<span class="st">'val'</span>],[<span class="st">'recall'</span>]]<span class="op">=</span>np.array(val_recall).mean()</span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>    result_df.loc[[<span class="st">'val'</span>],[<span class="st">'AUC'</span>]]<span class="op">=</span>np.array(val_auc).mean()</span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a>    results.update({classifier_label: result_df})</span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a>    models.update({classifier_label: model})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Report the mean of quantities on <span class="math inline">\(3\)</span>-fold cross validation:</p>
<p>Performance of naive Bayes classifier:</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>results[<span class="st">'GaussianNB'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>accuracy</th>
      <th>precision</th>
      <th>recall</th>
      <th>AUC</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>train</th>
      <td>0.716253</td>
      <td>0.705731</td>
      <td>0.603774</td>
      <td>0.788938</td>
    </tr>
    <tr>
      <th>val</th>
      <td>0.688172</td>
      <td>0.628788</td>
      <td>0.615385</td>
      <td>0.806268</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>Performance of KNN:</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>results[<span class="st">'KNeighborsClassifier'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>accuracy</th>
      <th>precision</th>
      <th>recall</th>
      <th>AUC</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>train</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>val</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>Performance of decision tree:</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>results[<span class="st">'DecisionTreeClassifier'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>accuracy</th>
      <th>precision</th>
      <th>recall</th>
      <th>AUC</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>train</th>
      <td>0.785124</td>
      <td>0.768235</td>
      <td>0.72956</td>
      <td>0.879671</td>
    </tr>
    <tr>
      <th>val</th>
      <td>0.774194</td>
      <td>0.737179</td>
      <td>0.717949</td>
      <td>0.869658</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<ul>
<li><span class="math inline">\(3.(c)\)</span><br> Here it seems from above results, KNN is performing the best, but it can be the case of over-fitting, and we may not have the good generalization. so Decision Tree Classifier is selected to train on the <span class="math inline">\(80\)</span> percent of the data. Here model is tested on held aside test data, which was not used to train or validate the model.</li>
</ul>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>selected <span class="op">=</span> <span class="st">'DecisionTreeClassifier'</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>classifier<span class="op">=</span> classifiers[selected]</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>tuned_params <span class="op">=</span> {item[<span class="dv">12</span>:]: models[selected][<span class="st">'Best Parameters'</span>][item] <span class="cf">for</span> item <span class="kw">in</span> models[selected][<span class="st">'Best Parameters'</span>]}</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>classifier.set_params(<span class="op">**</span>tuned_params)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>classifier.fit(X_train_val,y_train_val)<span class="op">;</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>test_accuracy<span class="op">=</span>[]</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>test_precision<span class="op">=</span>[]</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>test_recall<span class="op">=</span>[]</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> classifier.predict_proba(X_test)[:,<span class="dv">1</span>] </span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> (y_pred<span class="op">&gt;</span><span class="fl">0.5</span>).astype(np.int0)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>test_accuracy.append(metrics.accuracy_score(y_test,y_pred))</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>test_precision.append(metrics.precision_score(y_test,y_pred))</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>test_recall.append(metrics.recall_score(y_test,y_pred))    </span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>test_rest_df <span class="op">=</span> pd.DataFrame(data<span class="op">=</span>[[np.array(test_accuracy).mean(),np.array(test_precision).mean(),np.array(test_recall).mean()]],columns<span class="op">=</span>[<span class="st">'accuracy'</span>,<span class="st">'precision'</span>,<span class="st">'recall'</span>],index<span class="op">=</span>[<span class="st">'test'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Performance on held-aside Test data :</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>test_rest_df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>accuracy</th>
      <th>precision</th>
      <th>recall</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>test</th>
      <td>0.74359</td>
      <td>0.705882</td>
      <td>0.705882</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>Observation: The Generalization performance is close to the validation performance, hence training/validation data was a good representation of test data.</p>
<ul>
<li>Deliverables:
<ul>
<li>Code
<ul>
<li>Already present with the notebook.</li>
</ul></li>
<li>Data splits (for us to verify your reported results)
<ul>
<li>The Notebook can be run to verify the reported results.</li>
</ul></li>
<li>Brief report (PDF) with your solutions for the above questions
<ul>
<li>Report is included inline to the code with this ipython notebook</li>
</ul></li>
</ul></li>
</ul>
</section>
</section>
<section id="q.-4." class="level1">
<h1>Q. 4.</h1>
<ul>
<li>Decision Trees: <strong>(10 marks)</strong> In this question, you will use the Car Evaluation dataset, a popular dataset from the UCI Machine Learning Repository. It contains 1728 car sample information with 7 attributes, including one class feature that tells whether the car is in acceptable conditions (class labels: Unacceptable, Acceptable, VeryGood, Good). More details of the dataset description are available on https://archive.ics.uci.edu/ml/datasets/car+evaluation. You must not use the last column as an input feature when you classify the data. You can pre-process the variables into one-hot/ordinal values as you deem fit for each variable.
<ul>
<li><span class="math inline">\((a)\)</span> (5 marks) Decision Tree Implementation: Implement your own version of the decision tree using binary univariate split, entropy and information gain.</li>
<li><span class="math inline">\((b)\)</span> (2 marks) Cross-Validation: Evaluate your decision tree using 5-fold cross validation. (Divide the entire dataset into 5 parts using stratified sampling, and run this experiment; no test set is required for this question.) Report the average of the 5 folds’ accuracies. With correct implementation of both parts (decision tree and cross validation), your classification accuracy should be around 0.8.</li>
<li><span class="math inline">\((c)\)</span> (3 marks) Improvement Strategies: Now, try and improve your decision tree algorithm. Some things you could do include (not exhaustive):
<ul>
<li>Use Gini index instead of entropy</li>
<li>Use multi-way split (instead of binary split)</li>
<li>Use multivariate split (instead of univariate)</li>
<li>Prune the tree after splitting for better generalization</li>
</ul></li>
</ul>
Report your performance as an outcome of ANY TWO improved strategies.</li>
</ul>
<p>Deliverables: - Code - Brief report (PDF) with: (i) Accuracy of your initial implementation; (ii) Accuracy of your improved implementation, along with your explanation of why the accuracy improved with this change.</p>
<section id="answer-of-q.4." class="level2">
<h2 class="anchored" data-anchor-id="answer-of-q.4.">Answer of Q.4.</h2>
<ul>
<li><span class="math inline">\(4.(a)\)</span><br>
<ul>
<li>Implementation of my own version of decision tree.</li>
<li>Univariate tree with entropy and information gain.</li>
<li>Also it supports gini impurity and maximum tree depth which will be used in <span class="math inline">\(4(c)\)</span></li>
<li>For finding best split, only categorical or ordinal data is considered, as the given data-set doesn’t contain continuous feature so was not implemented.</li>
</ul></li>
</ul>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span>  accuracy_score</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Class for leaf node </span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Leaf_Node:</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, df):</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classification <span class="op">=</span> df[<span class="st">'class'</span>].value_counts().to_dict() <span class="co"># stores classification in the format of dictionary</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Decision Node class</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Question_Node:</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>                 best_split_column,</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>                 best_split_value,</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>                 true_branch,</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>                 false_branch):</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.best_split_column <span class="op">=</span> best_split_column</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.best_split_value<span class="op">=</span>best_split_value</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.true_branch <span class="op">=</span> true_branch</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.false_branch <span class="op">=</span> false_branch   </span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Main class for Decision Tree, It contains all the functionality, it can be configured for max_depth, and type of impurity to be used.  </span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DecisionTreeClassifier:</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,max_depth<span class="op">=</span><span class="va">None</span>,impurity<span class="op">=</span><span class="st">'entropy'</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tree<span class="op">=</span><span class="va">None</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_depth<span class="op">=</span>max_depth</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> impurity <span class="op">==</span> <span class="st">'gini'</span>:</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.impurity<span class="op">=</span><span class="va">self</span>.gini</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> impurity <span class="op">==</span> <span class="st">'entropy'</span>:</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.impurity<span class="op">=</span><span class="va">self</span>.entropy</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.impurity<span class="op">=</span><span class="va">None</span></span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> partition(<span class="va">self</span>,df, split_column, split_value):</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>        true_df <span class="op">=</span>  df[df[split_column]<span class="op">==</span>split_value] <span class="co"># as dataset contains only categorical data so only == is used</span></span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>        false_df <span class="op">=</span> df[df[split_column]<span class="op">!=</span>split_value]</span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> true_df, false_df </span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> gini(<span class="va">self</span>,df):</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>        counts <span class="op">=</span> df[<span class="st">'class'</span>].value_counts().to_dict()</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>        impurity <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> lbl <span class="kw">in</span> counts:</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>            prob_of_lbl <span class="op">=</span> counts[lbl] <span class="op">/</span> <span class="bu">float</span>(<span class="bu">len</span>(df))</span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>            impurity <span class="op">-=</span> prob_of_lbl<span class="op">**</span><span class="dv">2</span></span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> impurity</span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> entropy(<span class="va">self</span>,df): </span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a>        counts <span class="op">=</span> df[<span class="st">'class'</span>].value_counts().to_list()</span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a>        probabilities <span class="op">=</span> counts <span class="op">/</span> np.<span class="bu">sum</span>(counts)</span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a>        entropy <span class="op">=</span> np.<span class="bu">sum</span>(probabilities <span class="op">*</span> (<span class="op">-</span>np.log2(probabilities)))</span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> entropy    </span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> information_gain(<span class="va">self</span>,left, right, current_uncertainty):</span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> <span class="bu">float</span>(<span class="bu">len</span>(left)) <span class="op">/</span> (<span class="bu">len</span>(left) <span class="op">+</span> <span class="bu">len</span>(right))</span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> current_uncertainty <span class="op">-</span> p <span class="op">*</span> <span class="va">self</span>.impurity(left) <span class="op">-</span> (<span class="dv">1</span> <span class="op">-</span> p) <span class="op">*</span> <span class="va">self</span>.impurity(right)</span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> find_best_split(<span class="va">self</span>,df):</span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a>        best_gain <span class="op">=</span> <span class="dv">0</span>  <span class="co"># keep track of the best information gain</span></span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true" tabindex="-1"></a>        best_split_column <span class="op">=</span><span class="va">None</span></span>
<span id="cb15-52"><a href="#cb15-52" aria-hidden="true" tabindex="-1"></a>        best_split_value <span class="op">=</span><span class="va">None</span></span>
<span id="cb15-53"><a href="#cb15-53" aria-hidden="true" tabindex="-1"></a>        current_uncertainty <span class="op">=</span> <span class="va">self</span>.impurity(df) <span class="co"># current uncertainty, used to find information gain </span></span>
<span id="cb15-54"><a href="#cb15-54" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> df.columns[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb15-55"><a href="#cb15-55" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> col <span class="kw">in</span> features:  <span class="co"># for each feature</span></span>
<span id="cb15-56"><a href="#cb15-56" aria-hidden="true" tabindex="-1"></a>            values <span class="op">=</span> df[col].unique() <span class="co"># As dataset contains only categorical data so unique value is enough no need to find average </span></span>
<span id="cb15-57"><a href="#cb15-57" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> val <span class="kw">in</span> values:  <span class="co"># for each unique value in a feature  </span></span>
<span id="cb15-58"><a href="#cb15-58" aria-hidden="true" tabindex="-1"></a>                true_df, false_df <span class="op">=</span> <span class="va">self</span>.partition(df, col, val) <span class="co"># partition based on value</span></span>
<span id="cb15-59"><a href="#cb15-59" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="bu">len</span>(true_df) <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> <span class="bu">len</span>(false_df) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb15-60"><a href="#cb15-60" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">continue</span></span>
<span id="cb15-61"><a href="#cb15-61" aria-hidden="true" tabindex="-1"></a>                gain <span class="op">=</span> <span class="va">self</span>.information_gain(true_df, false_df, current_uncertainty) <span class="co"># find information gain based on type of impurity configured</span></span>
<span id="cb15-62"><a href="#cb15-62" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> gain <span class="op">&gt;=</span> best_gain:</span>
<span id="cb15-63"><a href="#cb15-63" aria-hidden="true" tabindex="-1"></a>                    best_gain, best_split_column,best_split_value <span class="op">=</span> gain,col, val <span class="co"># store the best gain, column and value</span></span>
<span id="cb15-64"><a href="#cb15-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> best_gain, best_split_column,best_split_value</span>
<span id="cb15-65"><a href="#cb15-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-66"><a href="#cb15-66" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Lear the tree recursively </span></span>
<span id="cb15-67"><a href="#cb15-67" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit_recursive(<span class="va">self</span>,df,level<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb15-68"><a href="#cb15-68" aria-hidden="true" tabindex="-1"></a>        best_gain, best_split_column,best_split_value <span class="op">=</span> <span class="va">self</span>.find_best_split(df)</span>
<span id="cb15-69"><a href="#cb15-69" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Base condition , either leaf node reached or max depth reached</span></span>
<span id="cb15-70"><a href="#cb15-70" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> best_gain <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> (<span class="va">self</span>.max_depth<span class="op">!=</span><span class="va">None</span> <span class="kw">and</span> <span class="va">self</span>.max_depth<span class="op">==</span>level):</span>
<span id="cb15-71"><a href="#cb15-71" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> Leaf_Node(df)</span>
<span id="cb15-72"><a href="#cb15-72" aria-hidden="true" tabindex="-1"></a>        level<span class="op">+=</span><span class="dv">1</span></span>
<span id="cb15-73"><a href="#cb15-73" aria-hidden="true" tabindex="-1"></a>        true_df, false_df <span class="op">=</span> <span class="va">self</span>.partition(df,  best_split_column,best_split_value) <span class="co"># partition on best split</span></span>
<span id="cb15-74"><a href="#cb15-74" aria-hidden="true" tabindex="-1"></a>        true_branch <span class="op">=</span> <span class="va">self</span>.fit_recursive(true_df,level)  <span class="co"># learn true branch </span></span>
<span id="cb15-75"><a href="#cb15-75" aria-hidden="true" tabindex="-1"></a>        false_branch <span class="op">=</span> <span class="va">self</span>.fit_recursive(false_df,level) <span class="co"># Learn False branch</span></span>
<span id="cb15-76"><a href="#cb15-76" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> Question_Node(best_split_column, best_split_value, true_branch, false_branch)</span>
<span id="cb15-77"><a href="#cb15-77" aria-hidden="true" tabindex="-1"></a>    <span class="co"># to be called by user, kind of same interface as sk learn provide </span></span>
<span id="cb15-78"><a href="#cb15-78" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>,df,level<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb15-79"><a href="#cb15-79" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tree<span class="op">=</span><span class="va">self</span>.fit_recursive(df,level)</span>
<span id="cb15-80"><a href="#cb15-80" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_recursive(<span class="va">self</span>,df, node):</span>
<span id="cb15-81"><a href="#cb15-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(node, Leaf_Node): <span class="co"># If leaf return </span></span>
<span id="cb15-82"><a href="#cb15-82" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> node.classification</span>
<span id="cb15-83"><a href="#cb15-83" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (df[node.best_split_column]<span class="op">==</span>node.best_split_value).to_numpy()[<span class="dv">0</span>]:</span>
<span id="cb15-84"><a href="#cb15-84" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.predict_recursive(df, node.true_branch)  <span class="co"># Recursively check for true branch </span></span>
<span id="cb15-85"><a href="#cb15-85" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb15-86"><a href="#cb15-86" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.predict_recursive(df, node.false_branch) <span class="co"># Recursively check for false branch </span></span>
<span id="cb15-87"><a href="#cb15-87" aria-hidden="true" tabindex="-1"></a>    <span class="co"># to be called by user, kind of same interface as sk learn provide </span></span>
<span id="cb15-88"><a href="#cb15-88" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>,df):</span>
<span id="cb15-89"><a href="#cb15-89" aria-hidden="true" tabindex="-1"></a>        pred_y<span class="op">=</span>[]</span>
<span id="cb15-90"><a href="#cb15-90" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(df)): <span class="co"># predict for each row in data frame one by one</span></span>
<span id="cb15-91"><a href="#cb15-91" aria-hidden="true" tabindex="-1"></a>            pred <span class="op">=</span> <span class="va">self</span>.predict_recursive(df.iloc[[i]], <span class="va">self</span>.tree)</span>
<span id="cb15-92"><a href="#cb15-92" aria-hidden="true" tabindex="-1"></a>            pred_y.append(<span class="bu">list</span>(pred.keys())[<span class="dv">0</span>])</span>
<span id="cb15-93"><a href="#cb15-93" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pred_y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><span class="math inline">\(4.(b)\)</span> <br> First Implement a class for stratified sampling <br> Evaluate decision tree on <span class="math inline">\(5\)</span>- fold cross validation using <code>StratifiedShuffleSplit</code> , Report accuracy on average of <span class="math inline">\(5\)</span>- fold:</li>
</ul>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> StratifyAndShuffleAndSplit():</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_splits<span class="op">=</span><span class="dv">10</span>, val_size<span class="op">=</span><span class="va">None</span>, random_state<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_splits <span class="op">=</span> n_splits</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.val_size <span class="op">=</span> val_size</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.random_state <span class="op">=</span> random_state</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> split(<span class="va">self</span>, X, y<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> train_index, val_index <span class="kw">in</span> <span class="va">self</span>.indices_generator(X, y):</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">yield</span> train_index, val_index  </span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> indices_generator(<span class="va">self</span>, X, y):</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>        n_samples <span class="op">=</span>  <span class="bu">len</span>(X) </span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>        n_val <span class="op">=</span> np.ceil(<span class="va">self</span>.val_size <span class="op">*</span> n_samples)  </span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>        n_train <span class="op">=</span> n_samples <span class="op">-</span> n_val </span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>        n_train, n_val <span class="op">=</span> <span class="bu">int</span>(n_train), <span class="bu">int</span>(n_val)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>        classes, y_indices <span class="op">=</span> np.unique(y, return_inverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>        n_classes <span class="op">=</span> classes.shape[<span class="dv">0</span>]</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>        class_counts <span class="op">=</span> np.bincount(y_indices)</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>        class_indices <span class="op">=</span> np.split(np.argsort(y_indices, kind<span class="op">=</span><span class="st">"mergesort"</span>), np.cumsum(class_counts)[:<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>        rng <span class="op">=</span> np.random.RandomState(<span class="va">self</span>.random_state) </span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_splits):</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>            n_i <span class="op">=</span>np.int64(np.floor(class_counts<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span><span class="va">self</span>.val_size)))</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>            t_i <span class="op">=</span> class_counts <span class="op">-</span> n_i</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>            train_index <span class="op">=</span> []</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>            val_index <span class="op">=</span> []</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_classes):</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>                permutation <span class="op">=</span> rng.permutation(class_counts[i])</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>                perm_indices_class_i <span class="op">=</span> class_indices[i].take(permutation, mode<span class="op">=</span><span class="st">"clip"</span>)</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>                train_index.extend(perm_indices_class_i[: n_i[i]])</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>                val_index.extend(perm_indices_class_i[n_i[i] : n_i[i] <span class="op">+</span> t_i[i]])</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>            train_index <span class="op">=</span> rng.permutation(train_index)</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>            val_index <span class="op">=</span> rng.permutation(val_index)</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>            <span class="cf">yield</span> train_index, val_index</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>column_names <span class="op">=</span> [<span class="st">'buying'</span>,<span class="st">'maint'</span>,<span class="st">'doors'</span>,<span class="st">'persons'</span>,<span class="st">'lug_boot'</span>,<span class="st">'safety'</span>,<span class="st">'class'</span>]</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>car_data <span class="op">=</span> pd.read_csv(<span class="st">'https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data'</span>,names<span class="op">=</span>column_names,header<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>mapping <span class="op">=</span>{<span class="st">'buying'</span>:{<span class="st">'vhigh'</span>:<span class="dv">3</span>,<span class="st">'high'</span>:<span class="dv">2</span>,<span class="st">'med'</span>:<span class="dv">1</span>,<span class="st">'low'</span>:<span class="dv">0</span>},</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>          <span class="st">'maint'</span>:{<span class="st">'vhigh'</span>:<span class="dv">3</span>,<span class="st">'high'</span>:<span class="dv">2</span>,<span class="st">'med'</span>:<span class="dv">1</span>,<span class="st">'low'</span>:<span class="dv">0</span>},</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>          <span class="st">'doors'</span>:{<span class="st">'5more'</span>:<span class="dv">3</span>,<span class="st">'4'</span>:<span class="dv">2</span>,<span class="st">'3'</span>:<span class="dv">1</span>,<span class="st">'2'</span>:<span class="dv">0</span>},</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>          <span class="st">'persons'</span>:{<span class="st">'more'</span>:<span class="dv">2</span>,<span class="st">'4'</span>:<span class="dv">1</span>,<span class="st">'2'</span>:<span class="dv">0</span>},</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>          <span class="st">'lug_boot'</span>:{<span class="st">'big'</span>:<span class="dv">2</span>,<span class="st">'med'</span>:<span class="dv">1</span>,<span class="st">'small'</span>:<span class="dv">0</span>},</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>          <span class="st">'safety'</span>:{<span class="st">'high'</span>:<span class="dv">2</span>,<span class="st">'med'</span>:<span class="dv">1</span>,<span class="st">'low'</span>:<span class="dv">0</span>},</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>          <span class="st">'class'</span>:{<span class="st">'vgood'</span>:<span class="dv">3</span>,<span class="st">'good'</span>:<span class="dv">2</span>,<span class="st">'acc'</span>:<span class="dv">1</span>,<span class="st">'unacc'</span>:<span class="dv">0</span>}}</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> key, sdict <span class="kw">in</span> mapping.items():</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>    condition<span class="op">=</span>[]</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>    choice <span class="op">=</span> []</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> skey,sval <span class="kw">in</span> sdict.items():</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>        condition.append(car_data[key].eq(skey))</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>        choice.append(sval)</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>    car_data[key]<span class="op">=</span>np.select(condition,choice)</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> car_data[[<span class="st">'buying'</span>,<span class="st">'maint'</span>,<span class="st">'doors'</span>,<span class="st">'persons'</span>,<span class="st">'lug_boot'</span>,<span class="st">'safety'</span>]]</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span> car_data[[<span class="st">'class'</span>]]</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>accuracy<span class="op">=</span>[]</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>sss1 <span class="op">=</span> StratifyAndShuffleAndSplit(n_splits<span class="op">=</span><span class="dv">5</span>, val_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">1976</span>)</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_index, val_index <span class="kw">in</span> sss1.split(X, y):</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>    X_train, X_val <span class="op">=</span> X.iloc[train_index], X.iloc[val_index]</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>    y_train, y_val <span class="op">=</span> y.iloc[train_index], y.iloc[val_index] </span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>    train <span class="op">=</span>pd.concat([X_train,y_train],axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>    val <span class="op">=</span>pd.concat([X_val,y_val],axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> DecisionTreeClassifier(impurity<span class="op">=</span><span class="st">'entropy'</span>)</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>    model.fit(train)</span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>    pred_y <span class="op">=</span> model.predict(val)</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>    accuracy.append(accuracy_score(val[<span class="st">'class'</span>],pred_y))</span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Mean of 5-fold accuracy:'</span>,np.mean(accuracy))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Mean of 5-fold accuracy: 0.9676300578034682</code></pre>
</div>
</div>
<p>The accuracy is found to be <span class="math inline">\(0.968\)</span>,the performance reported here is found to be extremely good.</p>
<ul>
<li><span class="math inline">\(4.(c)\)</span><br>
<ul>
<li>The decision tree is implemented in such a way so that it can be configured for max depth (pruning) or type of impurity</li>
<li>As ANY TWO strategies was suggested to use, so Gini-index and pruning was tried .</li>
<li>The accuracy was found to be improved to <span class="math inline">\(0.976\)</span></li>
</ul></li>
</ul>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>accuracy<span class="op">=</span>[]</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Make sure exact same data was used here as in 4.b. by using same  random state</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>sss1 <span class="op">=</span> StratifyAndShuffleAndSplit(n_splits<span class="op">=</span><span class="dv">5</span>, val_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">1976</span>) </span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_index, val_index <span class="kw">in</span> sss1.split(X, y):</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    X_train, X_val <span class="op">=</span> X.iloc[train_index], X.iloc[val_index]</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    y_train, y_val <span class="op">=</span> y.iloc[train_index], y.iloc[val_index] </span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    train <span class="op">=</span>pd.concat([X_train,y_train],axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    val <span class="op">=</span>pd.concat([X_val,y_val],axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> DecisionTreeClassifier(impurity<span class="op">=</span><span class="st">'gini'</span>,max_depth<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>    model.fit(train)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>    pred_y <span class="op">=</span> model.predict(val)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    accuracy.append(accuracy_score(val[<span class="st">'class'</span>],pred_y))</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Mean of 5-fold accuracy with gini impurity:'</span>,np.mean(accuracy))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Mean of 5-fold accuracy with gini impurity: 0.9763005780346822</code></pre>
</div>
</div>
<p>The Improved accuracy is <span class="math inline">\(0.976\)</span></p>
<p>Pruning and gini impurity was used to improve the accuracy, we can see that the accuracy has been improved ! <br> The reason of improved accuracy is gini score, pruning of tree did not contribute much, this can be the case when there is no or very less over-fitting. Multi-way split or multivariate-split was not tried as performance was already fond to be extremely high, so there is very little margin of improvement.<br><br></p>
<ul>
<li>Deliverables:
<ul>
<li>Code
<ul>
<li>Already present with this Notebook</li>
</ul></li>
<li>Brief report (PDF) with:
<ul>
<li><ol type="i">
<li>Accuracy of your initial implementation;</li>
</ol>
<ul>
<li>The Accuracy is reported inline with the code in a Markdown cell.</li>
</ul></li>
<li><ol start="2" type="i">
<li>Accuracy of your improved implementation, along with your explanation of why the accuracy improved with this change.</li>
</ol>
<ul>
<li>Accuracy of improved implementation and explanation is provided inline with the code in a Markdown cell.</li>
</ul></li>
</ul></li>
</ul></li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>