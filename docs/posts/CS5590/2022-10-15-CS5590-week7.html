<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Abhishek Kumar Dubey">
<meta name="dcterms.date" content="2022-10-15">
<meta name="description" content="Classifier System Design,Introduction to Learning Theory">

<title>IITH-Data-Science - Machine Learning 7</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../posts/CS5590/2022-10-08-CS5590-week6.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-5ZQX02R26E"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-5ZQX02R26E', { 'anonymize_ip': true});
</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="IITH-Data-Science - Machine Learning 7">
<meta property="og:description" content="Classifier System Design,Introduction to Learning Theory">
<meta property="og:image" content="CS5590_images/Acrobat_DOQ241iPvG.png">
<meta property="og:site-name" content="IITH-Data-Science">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">IITH-Data-Science</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">Home</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">About</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/abhiyantaabhishek"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/abhishek-kumar-dubey-585a86179/"><i class="bi bi-linkedin" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-reader-toggle nav-link" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Machine Learning 7</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title d-none d-lg-block">Machine Learning 7</h1>
                  <div>
        <div class="description">
          Classifier System Design,Introduction to Learning Theory
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Machine Learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Abhishek Kumar Dubey </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 15, 2022</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">CS6660</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">Probability Theory</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/CS6660/2022-08-06-CS6660-week1.html" class="sidebar-item-text sidebar-link">Probability Theory 1</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/CS6660/2022-08-13-CS6660-week2.html" class="sidebar-item-text sidebar-link">Probability Theory 2</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/CS6660/2022-09-17-CS6660-week4_1.html" class="sidebar-item-text sidebar-link">Probability Theory 3</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/CS6660/2022-09-24-CS6660-week5.html" class="sidebar-item-text sidebar-link">Probability Theory 4</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/CS6660/2022-10-15-CS6660-week7.html" class="sidebar-item-text sidebar-link">Probability Theory 5</a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">Linear Algebra</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/CS6660/2022-09-03-CS6660-week3.html" class="sidebar-item-text sidebar-link">Linear Algebra 1</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/CS6660/2022-09-17-CS6660-week4_2.html" class="sidebar-item-text sidebar-link">Linear Algebra 2</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/CS6660/2022-10-01-CS6660-week6.html" class="sidebar-item-text sidebar-link">Linear Algebra 3</a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">CS5590</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">Machine Learning</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth2 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/CS5590/2022-08-06-CS5590-week1.html" class="sidebar-item-text sidebar-link">Machine Learning 1</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/CS5590/2022-08-20-CS5590-week2.html" class="sidebar-item-text sidebar-link">Machine Learning 2</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/CS5590/2022-08-27-CS5590-week3.html" class="sidebar-item-text sidebar-link">Machine Learning 3</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/CS5590/2022-09-10-CS5590-week4.html" class="sidebar-item-text sidebar-link">Machine Learning 4</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/CS5590/2022-09-24-CS5590-week5.html" class="sidebar-item-text sidebar-link">Machine Learning 5</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/CS5590/2022-10-08-CS5590-week6.html" class="sidebar-item-text sidebar-link">Machine Learning 6</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/CS5590/2022-10-15-CS5590-week7.html" class="sidebar-item-text sidebar-link active">Machine Learning 7</a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#pre-processing" id="toc-pre-processing" class="nav-link active" data-scroll-target="#pre-processing"><span class="toc-section-number">1</span>  Pre-Processing</a></li>
  <li><a href="#feature-extraction-from-data" id="toc-feature-extraction-from-data" class="nav-link" data-scroll-target="#feature-extraction-from-data"><span class="toc-section-number">2</span>  Feature Extraction from Data</a></li>
  <li><a href="#challenges" id="toc-challenges" class="nav-link" data-scroll-target="#challenges"><span class="toc-section-number">3</span>  Challenges</a></li>
  <li><a href="#class-imbalance" id="toc-class-imbalance" class="nav-link" data-scroll-target="#class-imbalance"><span class="toc-section-number">4</span>  Class Imbalance</a>
  <ul class="collapse">
  <li><a href="#solution" id="toc-solution" class="nav-link" data-scroll-target="#solution"><span class="toc-section-number">4.1</span>  Solution</a></li>
  </ul></li>
  <li><a href="#smote" id="toc-smote" class="nav-link" data-scroll-target="#smote"><span class="toc-section-number">5</span>  SMOTE</a></li>
  <li><a href="#using-large-datasets" id="toc-using-large-datasets" class="nav-link" data-scroll-target="#using-large-datasets"><span class="toc-section-number">6</span>  Using Large Datasets</a></li>
  <li><a href="#generalization-error" id="toc-generalization-error" class="nav-link" data-scroll-target="#generalization-error"><span class="toc-section-number">7</span>  Generalization Error</a>
  <ul class="collapse">
  <li><a href="#bias-variance-tradeoff" id="toc-bias-variance-tradeoff" class="nav-link" data-scroll-target="#bias-variance-tradeoff"><span class="toc-section-number">7.1</span>  Bias variance tradeoff</a></li>
  </ul></li>
  <li><a href="#measuring-bias-and-variance" id="toc-measuring-bias-and-variance" class="nav-link" data-scroll-target="#measuring-bias-and-variance"><span class="toc-section-number">8</span>  Measuring Bias and Variance</a></li>
  <li><a href="#some-inferences" id="toc-some-inferences" class="nav-link" data-scroll-target="#some-inferences"><span class="toc-section-number">9</span>  Some Inferences</a></li>
  <li><a href="#regularizers" id="toc-regularizers" class="nav-link" data-scroll-target="#regularizers"><span class="toc-section-number">10</span>  Regularizers</a></li>
  <li><a href="#model-based-machine-learning" id="toc-model-based-machine-learning" class="nav-link" data-scroll-target="#model-based-machine-learning"><span class="toc-section-number">11</span>  Model-based Machine Learning</a>
  <ul class="collapse">
  <li><a href="#regularization-in-model-based-ml" id="toc-regularization-in-model-based-ml" class="nav-link" data-scroll-target="#regularization-in-model-based-ml"><span class="toc-section-number">11.1</span>  Regularization in Model based ML</a></li>
  </ul></li>
  <li><a href="#introduction-to-learning-theory" id="toc-introduction-to-learning-theory" class="nav-link" data-scroll-target="#introduction-to-learning-theory"><span class="toc-section-number">12</span>  Introduction to Learning Theory</a>
  <ul class="collapse">
  <li><a href="#optimality-of-bayes-decision-rule" id="toc-optimality-of-bayes-decision-rule" class="nav-link" data-scroll-target="#optimality-of-bayes-decision-rule"><span class="toc-section-number">12.1</span>  Optimality of Bayes Decision Rule</a></li>
  </ul></li>
  <li><a href="#towards-formalizing-learning" id="toc-towards-formalizing-learning" class="nav-link" data-scroll-target="#towards-formalizing-learning"><span class="toc-section-number">13</span>  Towards formalizing ‘learning’</a>
  <ul class="collapse">
  <li><a href="#the-basic-process-of-learning" id="toc-the-basic-process-of-learning" class="nav-link" data-scroll-target="#the-basic-process-of-learning"><span class="toc-section-number">13.1</span>  The basic process of learning</a></li>
  <li><a href="#a-statistical-machinery-for-learning" id="toc-a-statistical-machinery-for-learning" class="nav-link" data-scroll-target="#a-statistical-machinery-for-learning"><span class="toc-section-number">13.2</span>  A statistical machinery for learning</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="pre-processing" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="pre-processing"><span class="header-section-number">1</span> Pre-Processing</h2>
<ul>
<li>Numeric
<ul>
<li>Zero mean, unit variance: <span class="math inline">x’= (x-\mu)/ \sigma</span></li>
<li>In interval <span class="math inline">[0,1]: x'=(x -\min)/(\max - \min)</span></li>
</ul></li>
<li>Categorical
<ul>
<li>Encoded as number in such a way that there is no sense of ordering, for e.g.&nbsp;if there are 3 classes apple, orange and banana, and encoded as <span class="math inline">1,2,3</span> respectively, it appears as apple comes first than orange, which is not correct. So the correct way to encode is one hot encoding.</li>
<li>Also here only equality testing is meaningful.</li>
</ul></li>
<li>Ordinal
<ul>
<li>Encoded as numbers to preserve ordering</li>
<li><span class="math inline">\le, \ge</span> operations meaningful</li>
</ul></li>
</ul>
</section>
<section id="feature-extraction-from-data" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="feature-extraction-from-data"><span class="header-section-number">2</span> Feature Extraction from Data</h2>
<ul>
<li>Images
<ul>
<li>Pixel values, Segment and extract features, Handcrafted features: HOG, SIFT,etc</li>
<li>Deep learned features!</li>
</ul></li>
<li>Text
<ul>
<li>Bag of words, Ngrams</li>
<li>Deep learned features!</li>
</ul></li>
<li>Speech
<ul>
<li>Mel Frequency Cepstral Coefficients (MFCCs), Other frequency based features</li>
<li>Deep learned features!</li>
</ul></li>
<li>Time varying sensor Data
<ul>
<li>Statistical and moment based features (mean, variance) etc</li>
</ul></li>
</ul>
</section>
<section id="challenges" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="challenges"><span class="header-section-number">3</span> Challenges</h2>
<ul>
<li>Structured input/Structured output
<ul>
<li>One fix: Attribute = root-to-leaf paths</li>
</ul></li>
<li>Missing data
<ul>
<li>Fix: Fill in the value, Introduce special label, remove instance, remove attribute, Use classifiers that can handle missing values</li>
</ul></li>
<li>Outliers
<ul>
<li>Fix: Remove, Threshold, Visualize!</li>
</ul></li>
<li>Data assumptions
<ul>
<li>Generated how? Sources?</li>
<li>Smooth? Linear? Noise?</li>
</ul></li>
</ul>
</section>
<section id="class-imbalance" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="class-imbalance"><span class="header-section-number">4</span> Class Imbalance</h2>
<p>Almost all classifiers attempt to reduce global quantities such as the error rate, not taking the data distribution into consideration.</p>
<p>As a result, examples from the overwhelming class are well classified whereas examples from the minority class tend to be misclassified.</p>
<ul>
<li>Are all classifiers sensitive to class imbalance?
<ul>
<li>Decision Tree:Very sensitive to class imbalances. This is because the algorithm works globally, not paying attention to specific data points.</li>
<li>Multi Layer perceptrons (MLPs): are less prone to the class imbalance problem. This is because of their flexibility: their solution gets adjusted by each data point in a bottom up manner as well as by the overall data set in a top down manner.</li>
<li>Support Vector Machines (SVMs) SVMs are even less prone to the class imbalance problem than MLPs because they are only concerned with a few support vectors, the data points located close to the boundaries.</li>
</ul></li>
</ul>
<section id="solution" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="solution"><span class="header-section-number">4.1</span> Solution</h3>
<ul>
<li>Collect more data!</li>
<li>Change your performance metric:
<ul>
<li>Confusion Matrix, Precision Recall, F1 score, etc.</li>
</ul></li>
<li>Resample dataset</li>
<li>Generate synthetic samples</li>
<li>Try penalized models</li>
<li>Try a different perspective anomaly/change detection <br><br></li>
<li>At the data Level: Re Sampling
<ul>
<li>Oversampling (Random or Directed)</li>
<li>Under-sampling (Random or Directed), (not good for model performance)</li>
<li>Active Sampling</li>
</ul></li>
<li>At the Algorithmic Level:
<ul>
<li>Adjusting the Costs</li>
<li>Adjusting the decision threshold / probabilistic estimate at the tree leaf <br><br></li>
</ul></li>
<li>Under-sampling (random and directed) is not effective and can even hurt performance.</li>
<li>Random oversampling helps quite dramatically. Directed oversampling makes a bit of a difference by helping slightly more.</li>
<li>Cost adjusting is about as effective as Directed oversampling. Generally, however, it is found to be slightly more useful.</li>
</ul>
</section>
</section>
<section id="smote" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="smote"><span class="header-section-number">5</span> SMOTE</h2>
<p>SMOTE = Synthetic Minority Oversampling Technique</p>
<ul>
<li>For each minority example <span class="math inline">k</span>, compute nearest minority class examples <span class="math inline">(i,j,l,n,m)</span></li>
<li>Synthetically generate event <span class="math inline">k_1</span> such that <span class="math inline">k_1</span> lies between <span class="math inline">k</span> and <span class="math inline">i</span></li>
<li>Randomly chose an example out of <span class="math inline">5</span> closest points.</li>
</ul>
</section>
<section id="using-large-datasets" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="using-large-datasets"><span class="header-section-number">6</span> Using Large Datasets</h2>
<ul>
<li>At large data scales, the performance of different algorithms converge such that performance differences virtually disappear.</li>
<li>Given a large enough data set, the algorithm you’d want to use is the one that is computationally less expensive.</li>
<li>It’s only at smaller data scales that the performance differences between algorithms matter.</li>
<li>CPUs vs GPUs
<ul>
<li>Deep learning has greatly benefited from GPUs</li>
</ul></li>
<li>Map Reduce/ Hadoop , Apache Spark, Vowpal Wabbit frameworks
<ul>
<li>Many learning algorithms amenable to partitioning of computations</li>
</ul></li>
</ul>
</section>
<section id="generalization-error" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="generalization-error"><span class="header-section-number">7</span> Generalization Error</h2>
<ul>
<li><p>Components of generalization error</p></li>
<li><p>Bias: how much the average model over all training sets differ from the true model?</p>
<ul>
<li>Error due to inaccurate assumptions/simplifications made by the model</li>
</ul></li>
<li><p>Variance: how much models estimated from different training sets differ from each other</p></li>
<li><p>MSE in terms of bias and variance<br><br> <span class="math display">\color{blue} \text{MSE}=\color{red} \underbrace{\text{Bias}^2}_{\text{error due to incorrect assumption}} + \color{green} \underbrace{\text{Variance}}_{\text{error due to variance in training}} + \color{purple} \underbrace{\text{Noise}}_{\text{Unavoidable error}} \tag{1}</span> <br><br></p>
<p>Suppose the ultimate true function is <span class="math inline">f</span>, the one which we ideally want to learn. Our target is <span class="math inline">t</span>. Relation between <span class="math inline">t</span> and <span class="math inline">f</span> is as follows <span class="math display">t=f+\epsilon</span> where <span class="math inline">\epsilon</span> is noise and it’s expected value is considered to be zero i.e.&nbsp;<span class="math inline">\mathbf{E}[\epsilon]=0</span> <br> We consider <span class="math inline">y_i</span> to be predicted output by a Neural Network then MSE is given as <span class="math display">\mathrm{MSE}=\frac{1}{N}\sum_{i=1}^n {\left(t_i -y_i \right)}^2</span> Find expectation of MSE <span class="math display">\begin{align*}{}
\mathit{\mathbf{E}}\left\lbrack \mathrm{MSE}\right\rbrack &amp;=\mathit{\mathbf{E}}\left\lbrack \frac{1}{N}\sum_{i=1}^n {\left(t_i -y_i \right)}^2 \right\rbrack \\
&amp;=\frac{1}{N}\sum_{i=1}^n \mathit{\mathbf{E}}\left\lbrack {\left(t_i -y_i \right)}^2 \right\rbrack
\end{align*}</span> Now we examine <span class="math inline">\mathit{\mathbf{E}}\left\lbrack {\left(t_i -y_i \right)}^2 \right\rbrack</span> <span class="math display">\begin{align*}{}
\mathit{\mathbf{E}}\left\lbrack {\left(t_i -y_i \right)}^2 \right\rbrack &amp;=\mathit{\mathbf{E}}\left\lbrack {\left(\left(t_i -f_i \right)+\left(f_i -y_i \right)\right)}^2 \right\rbrack \\
&amp;=\mathit{\mathbf{E}}\left\lbrack {\left(t_i -f_i \right)}^2 +{\left(f_i -y_i \right)}^2 -2\left(t_i -f_i \right)\left(f_i -y_i \right)\right\rbrack \\
&amp;=\mathit{\mathbf{E}}\left\lbrack {\left(t_i -f_i \right)}^2 \right\rbrack +\mathit{\mathbf{E}}\left\lbrack {\left(f_i -y_i \right)}^2 \right\rbrack -\mathit{\mathbf{E}}\left\lbrack 2\left(t_i -f_i \right)\left(f_i -y_i \right)\right\rbrack \\
&amp;=\mathit{\mathbf{E}}\left\lbrack {\left(t_i -f_i \right)}^2 \right\rbrack +\mathit{\mathbf{E}}\left\lbrack {\left(f_i -y_i \right)}^2 \right\rbrack -2\left(\mathit{\mathbf{E}}\left\lbrack t_i f_i \right\rbrack -\mathit{\mathbf{E}}\left\lbrack t_i y_i \right\rbrack -\mathit{\mathbf{E}}\left\lbrack f_i^2 \right\rbrack +\mathit{\mathbf{E}}\left\lbrack f_i y_i \right\rbrack \right)\\
&amp;=\mathit{\mathbf{E}}\left\lbrack {\left(t_i -f_i \right)}^2 \right\rbrack +\mathit{\mathbf{E}}\left\lbrack {\left(f_i -y_i \right)}^2 \right\rbrack -2\left(f_i^2 -\mathit{\mathbf{E}}\left\lbrack f_i y_i \right\rbrack -f_i^2 +\mathit{\mathbf{E}}\left\lbrack f_i y_i \right\rbrack \right)\\
&amp;=\mathit{\mathbf{E}}\left\lbrack {\left(t_i -f_i \right)}^2 \right\rbrack +\mathit{\mathbf{E}}\left\lbrack {\left(f_i -y_i \right)}^2 \right\rbrack
\end{align*}</span> Above we used the fact that</p>
<ul>
<li><span class="math inline">\mathit{\mathbf{E}}\left\lbrack t_i f_i \right\rbrack =f_i^2 \;\mathrm{Since}\;f\;\mathrm{is}\;\mathrm{deterministic}\;\mathrm{and}\;\mathit{\mathbf{E}}\left\lbrack t_i \right\rbrack =f_i</span></li>
<li><span class="math inline">:\mathit{\mathbf{E}}\left\lbrack f_i^2 \right\rbrack =f^2 \;\mathrm{Since}\;f\;\mathrm{is}\;\mathrm{deterministic}</span></li>
<li><span class="math inline">:\mathit{\mathbf{E}}\left\lbrack t_i y_i \right\rbrack =\mathit{\mathbf{E}}\left\lbrack \left(f_i +\epsilon \right)y_i \right\rbrack =\mathit{\mathbf{E}}\left\lbrack f_i y_i \right\rbrack +\mathit{\mathbf{E}}\left\lbrack \epsilon y_i \right\rbrack =\mathit{\mathbf{E}}\left\lbrack f_i y_i \right\rbrack +0=\mathit{\mathbf{E}}\left\lbrack f_i y_i \right\rbrack</span>, Here <span class="math inline">\mathit{\mathbf{E}}\left\lbrack \epsilon y_i \right\rbrack</span> is zero because the noise in the infinite test set over which we take the expectation is probabilistically independent of the NN prediction</li>
</ul>
<p>So we got: <span class="math display">\mathit{\mathbf{E}}\left\lbrack {\left(t_i -y_i \right)}^2 \right\rbrack =\mathit{\mathbf{E}}\left\lbrack {\left(t_i -f_i \right)}^2 \right\rbrack +\mathit{\mathbf{E}}\left\lbrack {\left(f_i -y_i \right)}^2 \right\rbrack \tag{2}</span> <em>Thus the MSE can be decomposed in expectation into the variance of the noise and the MSE between the true function and the predicted values</em></p>
<p>We can apply same trick on last term of equation <span class="math inline">(2)</span></p>
<p><span class="math display">\begin{align*}{}
\mathit{\mathbf{E}}\left\lbrack {\left(f_i -y_i \right)}^2 \right\rbrack &amp;=\mathit{\mathbf{E}}\left\lbrack {\left(\left(f_i -\mathit{\mathbf{E}}\left\lbrack y_i \right\rbrack \right)+\left(\mathit{\mathbf{E}}\left\lbrack y_i \right\rbrack -y_i \right)\right)}^2 \right\rbrack \\
&amp;=\mathit{\mathbf{E}}\left\lbrack {\left(f_i -\mathit{\mathbf{E}}\left\lbrack y_i \right\rbrack \right)}^2 +{\left(\mathit{\mathbf{E}}\left\lbrack y_i \right\rbrack -y_i \right)}^2 -2\left(f_i -\mathit{\mathbf{E}}\left\lbrack y_i \right\rbrack \right)\left(\mathit{\mathbf{E}}\left\lbrack y_i \right\rbrack -y_i \right)\right\rbrack \\
&amp;=\mathit{\mathbf{E}}\left\lbrack {\left(f_i -\mathit{\mathbf{E}}\left\lbrack y_i \right\rbrack \right)}^2 \right\rbrack +\mathit{\mathbf{E}}\left\lbrack {\left(\mathit{\mathbf{E}}\left\lbrack y_i \right\rbrack -y_i \right)}^2 \right\rbrack -\mathit{\mathbf{E}}\left\lbrack 2\left(f_i -\mathit{\mathbf{E}}\left\lbrack y_i \right\rbrack \right)\left(\mathit{\mathbf{E}}\left\lbrack y_i \right\rbrack -y_i \right)\right\rbrack \\
&amp;=\mathit{\mathbf{E}}\left\lbrack {\left(f_i -\mathit{\mathbf{E}}\left\lbrack y_i \right\rbrack \right)}^2 \right\rbrack +\mathit{\mathbf{E}}\left\lbrack {\left(\mathit{\mathbf{E}}\left\lbrack y_i \right\rbrack -y_i \right)}^2 \right\rbrack -2\left(\mathit{\mathbf{E}}\left\lbrack f_i \times \mathit{\mathbf{E}}\left\lbrack y_i \right\rbrack \right\rbrack -\mathit{\mathbf{E}}\left\lbrack f_i y_i \right\rbrack -\mathit{\mathbf{E}}\left\lbrack {\left(\mathit{\mathbf{E}}\left\lbrack y_i \right\rbrack \right)}^2 \right\rbrack +\mathit{\mathbf{E}}\left\lbrack \mathit{\mathbf{E}}\left\lbrack y_i \right\rbrack \times y_i \right\rbrack \right)\\
&amp;=\mathit{\mathbf{E}}\left\lbrack {\left(f_i -\mathit{\mathbf{E}}\left\lbrack y_i \right\rbrack \right)}^2 \right\rbrack +\mathit{\mathbf{E}}\left\lbrack {\left(\mathit{\mathbf{E}}\left\lbrack y_i \right\rbrack -y_i \right)}^2 \right\rbrack -2\left(\mathit{\mathbf{E}}\left\lbrack f_i \right\rbrack \mathit{\mathbf{E}}\left\lbrack y_i \right\rbrack -\mathit{\mathbf{E}}\left\lbrack f_i \right\rbrack \mathit{\mathbf{E}}\left\lbrack y_i \right\rbrack -{\left(\mathit{\mathbf{E}}\left\lbrack y_i \right\rbrack \right)}^2 +{\left(\mathit{\mathbf{E}}\left\lbrack y_i \right\rbrack \right)}^2 \right)\\
&amp;=\mathit{\mathbf{E}}\left\lbrack {\left(f_i -\mathit{\mathbf{E}}\left\lbrack y_i \right\rbrack \right)}^2 \right\rbrack +\mathit{\mathbf{E}}\left\lbrack {\left(\mathit{\mathbf{E}}\left\lbrack y_i \right\rbrack -y_i \right)}^2 \right\rbrack \\
\mathit{\mathbf{E}}\left\lbrack {\left(f_i -y_i \right)}^2 \right\rbrack &amp;={\mathrm{bias}}^2 +\mathrm{Var}\left(y_i \right)
\end{align*}</span></p>
<p>Now we can write equation <span class="math inline">(2)</span> as <span class="math display">\mathit{\mathbf{E}}\left\lbrack {\left(t_i -y_i \right)}^2 \right\rbrack =\mathrm{Var}\left(\mathrm{Noise}\right)+{\mathrm{bias}}^2 +\mathrm{Var}\left(y_i \right)</span> Hence proved!</p></li>
</ul>
<section id="bias-variance-tradeoff" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="bias-variance-tradeoff"><span class="header-section-number">7.1</span> Bias variance tradeoff</h3>
<p>From equation <span class="math inline">(1)</span> we can see that when MSE is constant and if we try to reduce the variance Bias has to increase and vice versa.</p>
<ul>
<li>Models with too few parameters are inaccurate because of a <strong>large bias</strong> bias (not enough flexibility).</li>
<li>Models with too many parameters are inaccurate because of a <strong>large variance</strong> (too much sensitivity to the sample).</li>
<li><strong>Underfitting</strong>: Model is too “simple” to represent all the relevant class characteristics
<ul>
<li>High bias and low variance</li>
<li>High training error and high test error</li>
</ul></li>
<li><strong>Overfitting</strong>: Model is too “complex” and fits irrelevant characteristics (noise) in the data
<ul>
<li>Low bias and high variance</li>
<li>Low training error and high test error</li>
</ul></li>
</ul>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>In case of classification, variance dominates bias. Very roughly, this is because we only need to make a discrete decision rather than get an exact value.</p>
</div>
</div>
</section>
</section>
<section id="measuring-bias-and-variance" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="measuring-bias-and-variance"><span class="header-section-number">8</span> Measuring Bias and Variance</h2>
<ul>
<li>Create multiple training set using bootstrap replicates.</li>
<li>Apply learning algorithm on each replicates to obtain hypothesis.</li>
<li>compute predicted value for each hypothesis on the data which did not appear on the bootstrap replicate the hypothesis was trained on.</li>
<li>compute the average prediction</li>
<li>Estimate bias</li>
<li>Estimate variance.</li>
<li>Assume noise is 0
<ul>
<li>If we have multiple data points with the same <span class="math inline">x</span> value, then we can estimate the noise not generally available in machine learning</li>
</ul></li>
</ul>
</section>
<section id="some-inferences" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="some-inferences"><span class="header-section-number">9</span> Some Inferences</h2>
<ul>
<li>How to reduce variance of classifier
<ul>
<li>Choose a simpler classifier</li>
<li>Regularize the parameters</li>
<li>Get more training data</li>
</ul></li>
<li>Training Error and Cross Validation
<ul>
<li>Suppose we use the training error to estimate the difference between the true model prediction and the learned model prediction.</li>
<li>The training error is <em>downward biased</em>: on average it <em>underestimates</em> the generalization error.</li>
<li>Cross validation is nearly unbiased; it <em>slightly</em> <em>overestimates</em> the generalization error.</li>
</ul></li>
</ul>
</section>
<section id="regularizers" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="regularizers"><span class="header-section-number">10</span> Regularizers</h2>
<ul>
<li>KNN
<ul>
<li>Choose higher <span class="math inline">k</span></li>
</ul></li>
<li>Decision Trees
<ul>
<li>Pruning</li>
</ul></li>
<li>Naïve Bayes
<ul>
<li>Parametric models automatically act as regularizers</li>
</ul></li>
<li>SVMs
<ul>
<li>Control <span class="math inline">c</span> value</li>
</ul></li>
</ul>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>If the wight value is high the model is likely to over fit, so we want weight to be small. Consider a dataset in <span class="math inline">2</span>-D, Hiving large variance across <span class="math inline">y</span>-axis, now consider a model which overfits to these data, to do so <span class="math inline">W</span> has to be larger because for very small change in <span class="math inline">x,\;y</span> will have to change by very high amount (due to such data distribution) which can be achieved only if <span class="math inline">W</span> is large. It is shown in the below pic.</p>
<p><img src="CS5590_images/mspaint_AEgund0GVg.png" class="img-fluid"></p></li>
</ul>
</div>
</div>
</section>
<section id="model-based-machine-learning" class="level2" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="model-based-machine-learning"><span class="header-section-number">11</span> Model-based Machine Learning</h2>
<ul>
<li>pick a model</li>
<li>pick a criteria to optimize (aka objective function)</li>
<li>develop a learning algorithm (aka Find <span class="math inline">W</span> and <span class="math inline">b</span> that minimizes the loss)</li>
<li>Generally, we don’t want huge weights
<ul>
<li>If weights are large, a small change in a feature can result in a large change in the prediction</li>
<li>Also, can give too much weight to any one feature</li>
</ul></li>
</ul>
<section id="regularization-in-model-based-ml" class="level3" data-number="11.1">
<h3 data-number="11.1" class="anchored" data-anchor-id="regularization-in-model-based-ml"><span class="header-section-number">11.1</span> Regularization in Model based ML</h3>
<ul>
<li>A regularizer is an additional criteria to the loss function to make sure that we don’t overfit</li>
<li>It’s called a regularizer since it tries to keep the parameters more normal/regular</li>
<li>It is a bias (inductive bias) on the model that forces the learning to prefer certain types (smaller) of weights over others (larger). <span class="math display">\argmin_{w,b} \sum_{i=1}^n \mathrm{loss}\left(y,y^{\prime } \right)+\lambda \times \boxed{\mathrm{regulizer}\left(w,b\right)}</span></li>
<li>Type of norm regularizer
<ul>
<li>1-norm ( sum of weights ) <span class="math display">r(w,b)=\sum_{w_j} \left\lvert w_j \right\rvert </span></li>
<li>2-norm ( sum of squared weights ) <span class="math display">r(w,b)=\sum_{w_j} \sqrt{\left\lvert w_j \right\rvert^2} </span><br>
</li>
<li><span class="math inline">p</span>-norm ( sum of squared weights ) <span class="math display">r(w,b)=\sum_{w_j} \sqrt[p]{\left\lvert w_j \right\rvert^p}=\left\lVert w\right\rVert ^p</span></li>
</ul>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Smaller values of <span class="math inline">p, (p &lt; 2)</span> encourage sparser vectors<br></p></li>
<li><p>Larger values of <span class="math inline">p</span> discourage large weights more.</p></li>
<li><p>All <span class="math inline">p</span> norms penalize larger weights.</p></li>
<li><p><span class="math inline">p &lt; 2</span> tends to create sparse i.e.&nbsp;lots of <span class="math inline">0</span> weights</p>
<p><img src="CS5590_images/Acrobat_DOQ241iPvG.png" class="img-fluid"></p></li>
</ul>
</div>
</div></li>
<li>L1 is popular because it tends to result in sparse solutions (i.e.&nbsp;lots of zero weights)</li>
<li>However, it is not differentiable, so it only works for gradient descent solvers</li>
<li>L2 is also popular because for some loss functions, it can be solved directly (no gradient descent required, though often iterative solvers still)</li>
<li>Lp is less popular since they don’t tend to shrink the weights enough</li>
</ul>
</section>
</section>
<section id="introduction-to-learning-theory" class="level2" data-number="12">
<h2 data-number="12" class="anchored" data-anchor-id="introduction-to-learning-theory"><span class="header-section-number">12</span> Introduction to Learning Theory</h2>
<section id="optimality-of-bayes-decision-rule" class="level3" data-number="12.1">
<h3 data-number="12.1" class="anchored" data-anchor-id="optimality-of-bayes-decision-rule"><span class="header-section-number">12.1</span> Optimality of Bayes Decision Rule</h3>
<ul>
<li>Let <span class="math inline">X</span> be a random variable over a space <span class="math inline">\Omega</span></li>
<li>Two category decision problem:<br> <span class="math inline">H_1:X \in \omega_1</span><br> <span class="math inline">H_2:X \in \omega_2</span><br></li>
<li>optimal decision is <br>
<ul>
<li>Chose <span class="math inline">H_1</span> when <span class="math inline">p(x \mid \omega_1)p(\omega_1)\ge p(x \mid \omega_2)p(\omega_2)</span></li>
<li>Chose <span class="math inline">H_2</span> when <span class="math inline">p(x \mid \omega_1)p(\omega_1) &lt; p(x \mid \omega_2)p(\omega_2)</span></li>
</ul></li>
<li>Consider the partition of reason <span class="math inline">\Omega</span> as shown below, <span class="math inline">\mathcal{R}_1,\mathcal{R}_2,\Omega_1,\Omega_2</span> are partitions of <span class="math inline">\Omega</span>. Every thing outside of <span class="math inline">\mathcal{R}_1</span> is <span class="math inline">\mathcal{R}_2</span> and everything outside of <span class="math inline">\Omega_1</span> is <span class="math inline">\Omega_2</span><br><br> <img src="CS5590_images/ApplicationFrameHost_r01wS0Cy4S.png" class="img-fluid"> <img src="CS5590_images/ApplicationFrameHost_bShObAKBRQ.png" class="img-fluid"> <img src="CS5590_images/ApplicationFrameHost_lGXEWFz8BX.png" class="img-fluid"> <img src="CS5590_images/ApplicationFrameHost_NxoXTrbauD.png" class="img-fluid"></li>
</ul>
<p>Consider arbitrary decision rule: - partition <span class="math inline">\Omega</span> into two disjoint regions: <span class="math inline">\mathcal{R}_1</span> and <span class="math inline">\mathcal{R}_2</span><br> choose <span class="math inline">H_1</span> if <span class="math inline">X \in \mathcal{R}_1</span><br> choose <span class="math inline">H_2</span> if <span class="math inline">X \in \mathcal{R}_2</span><br></p>
<p>Consider bayesian decision rule:</p>
<ul>
<li>partition <span class="math inline">\Omega</span> into two disjoint regions: <span class="math inline">\Omega_1</span> and <span class="math inline">\Omega_2</span><br> choose <span class="math inline">H_1</span> if <span class="math inline">X \in \Omega_1</span><br> choose <span class="math inline">H_2</span> if <span class="math inline">X \in \Omega_2</span><br></li>
<li>where <br> <span class="math inline">\Omega_1 = \{x \in \Omega : p(x \mid \omega_1)p(\omega_1)\ge p(x \mid \omega_2)p(\omega_2)\}</span><br> <span class="math inline">\Omega_2 = \{x \in \Omega : p(x \mid \omega_1)p(\omega_1)&lt; p(x \mid \omega_2)p(\omega_2)\}</span></li>
</ul>
<p>Now,<br> For the arbitrary decision rule:</p>
<p><span class="math display">\begin{align*}
p(\text{error})&amp;=p(X\in \mathcal{R}_1,\omega_2)+p(X\in \mathcal{R}_2,\omega_1) \\
&amp;= p(X\in \mathcal{R}_1\mid \omega_2)p(\omega_2)+p(X\in \mathcal{R}_2\mid \omega_1)p(\omega_1) \\
&amp;= \int_{\mathcal{R}_1} p(x\mid \omega_2)p(\omega_2)dx + \int_{\mathcal{R}_2} p(x\mid \omega_1)p(\omega_1)dx \\
\end{align*}</span></p>
<p>Similarly we can write for Bayesian decision rule:</p>
<p><span class="math display">\begin{align*}
p(\text{error}_\text{Bayes})&amp;=p(X\in \Omega_1,\omega_2)+p(X\in \Omega_2,\omega_1) \\
&amp;= p(X\in \Omega_1\mid \omega_2)p(\omega_2)+p(X\in \Omega_2\mid \omega_1)p(\omega_1) \\
&amp;= \int_{\Omega_1} p(x\mid \omega_2)p(\omega_2)dx + \int_{\Omega_2} p(x\mid \omega_1)p(\omega_1)dx \\
\end{align*}</span></p>
<p>Let <span class="math display">\Delta(\text{error})=p(\text{error})-p(\text{error}_\text{Bayes}) \tag{2}</span> <br> If we can prove that <span class="math inline">\Delta(\text{error})</span> is always positive, it means <span class="math inline">p(\text{error}_\text{Bayes})</span> is the optimal error (least error we can get).</p>
<ul>
<li><p>With reference of the above figure we can write:<br> <span class="math inline">\mathcal{R}_1=(\mathcal{R}_1\cap \Omega_1)\cup (\mathcal{R}_1 \cap \Omega_2)</span><br> <span class="math inline">\mathcal{R}_2=(\mathcal{R}_2\cap \Omega_1)\cup (\mathcal{R}_2 \cap \Omega_2)</span><br></p>
<p><span class="math inline">\Omega_1=( \Omega_1 \cap \mathcal{R}_1)\cup (\Omega_1 \cap \mathcal{R}_2)</span><br> <span class="math inline">\Omega_2=( \Omega_2 \cap \mathcal{R}_1)\cup (\Omega_2 \cap \mathcal{R}_2)</span><br><br>
</p></li>
<li><p>From above points we can write:<br> <span class="math display">\mathcal{R}_1-\Omega_1 = (\mathcal{R}_1 \cap \Omega_2)-(\Omega_1 \cap \mathcal{R}_2) \tag{3}</span> <span class="math display">\mathcal{R}_2-\Omega_2=(\mathcal{R}_2\cap \Omega_1)-( \Omega_2 \cap \mathcal{R}_1) \tag{4}</span></p></li>
</ul>
<p>substituting values in equation <span class="math inline">(2)</span></p>
<p><span class="math display">\begin{align*}
\Delta(\text{error})&amp;=\int_{\mathcal{R}_1} p(x\mid \omega_2)p(\omega_2)dx + \int_{\mathcal{R}_2} p(x\mid \omega_1)p(\omega_1)dx - \int_{\Omega_1} p(x\mid \omega_2)p(\omega_2)dx - \int_{\Omega_2} p(x\mid \omega_1)p(\omega_1)dx \\
&amp;=p(\omega_2)\left [\int_{\mathcal{R}_1} p(x\mid \omega_2)dx- \int_{\Omega_1} p(x\mid \omega_2)dx \right ]  + p(\omega_1)\left [\int_{\mathcal{R}_2} p(x\mid \omega_1)dx  - \int_{\Omega_2} p(x\mid \omega_1)dx \right] \\
&amp;=p(\omega_2)\left [\int_{ (\mathcal{R}_1 \cap \Omega_2)} p(x\mid \omega_2)dx- \int_{(\Omega_1 \cap \mathcal{R}_2)} p(x\mid \omega_2)dx \right ]  + p(\omega_1)\left [\int_{(\mathcal{R}_2\cap \Omega_1)} p(x\mid \omega_1)dx  - \int_{( \Omega_2 \cap \mathcal{R}_1)} p(x\mid \omega_1)dx \right] \;\;\text{Using eq } (3) \text{ and } (4)\\
&amp;=\left\lbrack \int_{({\mathcal{R}}_1 \cap \Omega_2 )} p(x\mid \omega_2 )p(\omega_2 )dx-\int_{({\mathcal{R}}_1 \cap \Omega_2 )} p(x\mid \omega_1 )p(\omega_1 )dx\right\rbrack +\left\lbrack \int_{(\Omega_1 \cap {\mathcal{R}}_2 )} p(x\mid \omega_1 )p(\omega_1 )dx-\int_{(\Omega_1 \cap {\mathcal{R}}_2 )} p(x\mid \omega_2 )p(\omega_2 )dx\right\rbrack\\
&amp;=\underbrace{\int_{({\mathcal{R}}_1 \cap \Omega_2 )} \left\lbrack p(x\mid \omega_2 )p(\omega_2 )-p(x\mid \omega_1 )p(\omega_1 )\right\rbrack dx}_{\ge0}  +  \underbrace{\int_{(\Omega_1 \cap {\mathcal{R}}_2 )} \left\lbrack p(x\mid \omega_1 )p(\omega_1 )-p(x\mid \omega_2 )p(\omega_2 )\right\rbrack dx}_{\ge0} \\
\Delta(\text{error})&amp;\ge 0
\end{align*}</span></p>
<p>In above two terms are zero due to below reason <span class="math display">\begin{align*}
  \Omega_1 = \{x \in \Omega : p(x \mid \omega_1)p(\omega_1)\ge p(x \mid \omega_2)p(\omega_2)\} \Rightarrow \int_{(\Omega_1 \cap {\mathcal{R}}_2 )} \left\lbrack p(x\mid \omega_1 )p(\omega_1 )-p(x\mid \omega_2 )p(\omega_2 )\right\rbrack dx \ge 0 \\
  \Omega_2 = \{x \in \Omega : p(x \mid \omega_1)p(\omega_1)&lt; p(x \mid \omega_2)p(\omega_2)\} \Rightarrow \int_{({\mathcal{R}}_1 \cap \Omega_2 )} \left\lbrack p(x\mid \omega_2 )p(\omega_2 )-p(x\mid \omega_1 )p(\omega_1 )\right\rbrack dx \ge 0
  \end{align*}</span></p>
<p><span class="math display">\begin{align*}
  &amp;&amp;\Delta(\text{error})&amp;\ge0\\
  &amp;\Rightarrow &amp;p(\text{error})-p(\text{error}_\text{Bayes}) &amp;\ge 0 \\
  &amp;\Rightarrow &amp;p(\text{error}) &amp;\ge p(\text{error}_\text{Bayes})
  \end{align*}</span></p>
</section>
</section>
<section id="towards-formalizing-learning" class="level2" data-number="13">
<h2 data-number="13" class="anchored" data-anchor-id="towards-formalizing-learning"><span class="header-section-number">13</span> Towards formalizing ‘learning’</h2>
<section id="the-basic-process-of-learning" class="level3" data-number="13.1">
<h3 data-number="13.1" class="anchored" data-anchor-id="the-basic-process-of-learning"><span class="header-section-number">13.1</span> The basic process of learning</h3>
<ul>
<li>Observe a phenomenon</li>
<li>Construct a model from observations</li>
<li>Use that model to make decisions/predictions</li>
</ul>
</section>
<section id="a-statistical-machinery-for-learning" class="level3" data-number="13.2">
<h3 data-number="13.2" class="anchored" data-anchor-id="a-statistical-machinery-for-learning"><span class="header-section-number">13.2</span> A statistical machinery for learning</h3>
<p>Phenomenon of interest:</p>
<ul>
<li>Input space: <span class="math inline">X</span>; Output space: <span class="math inline">Y</span></li>
<li>There is an unknown distribution <span class="math inline">D</span> over <span class="math inline">(X,Y)</span></li>
<li>The learner observes <span class="math inline">m</span> examples <span class="math inline">(x_1 ,y_1,\dots, x_m ,y_m )</span> drawn from <span class="math inline">D</span></li>
</ul>
<p>Construct a model:</p>
<ul>
<li>Let <span class="math inline">F</span> be a collection of models, where each <span class="math inline">f: X \rightarrow Y</span> predicts <span class="math inline">y</span> given <span class="math inline">x</span></li>
<li>From <span class="math inline">m</span> observations, select a model <span class="math inline">f_m</span> in <span class="math inline">F</span> which predicts well.</li>
<li>Generalization error of <span class="math inline">f</span>: <span class="math display">\mathrm{err}(f):=\mathbb{P}_{(x,y)\sim D}\left[f(x)\ne y\right]</span></li>
<li>We can say that we have learned a phenomenon if <span class="math display">\mathrm{err}(f_m)-\mathrm{err}(f^*)\le \epsilon \quad f^*:=\argmin_{f \in F}\mathrm{err}(f)</span> for any tolerance level <span class="math inline">\epsilon</span> of our choice.</li>
</ul>
<p><br><br><br> <span class="math inline">\tiny {\textcolor{#808080}{\boxed{\text{Reference: Dr. Vineeth, IIT Hyderabad }}}}</span></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="abhiyantaabhishek/IITH-Data-Science" data-repo-id="R_kgDOILoB8A" data-category="Announcements" data-category-id="DIC_kwDOILoB8M4CSJcL" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../posts/CS5590/2022-10-08-CS5590-week6.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Machine Learning 6</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Copyright 2022, Abhishek Kumar Dubey</div>   
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/abhiyantaabhishek">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/abhishek-kumar-dubey-585a86179/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>



</body></html>