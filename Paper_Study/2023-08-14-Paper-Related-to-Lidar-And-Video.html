<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Abhishek Kumar Dubey">
<meta name="dcterms.date" content="2023-08-14">
<meta name="description" content="3D Object Detection for Self-Driving Cars Using Video and LiDAR, An Ablation Study">

<title>3D Object Detection for Self-Driving Cars Using Video and LiDAR, An Ablation Study</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../Paper_Study/2023-09-10-Paper-Related-to-Lidar-And-Video.html" rel="next">
<link href="../Paper_Study/index.html" rel="prev">
<link href="../logo.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-05fe91a66cf75bbbb8c9664867fe5124.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-28bcfe2f18316f36a05318c8cae61386.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-5ZQX02R26E"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-5ZQX02R26E', { 'anonymize_ip': true});
</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
<meta property="og:title" content="3D Object Detection for Self-Driving Cars Using Video and LiDAR, An Ablation Study">
<meta property="og:description" content="3D Object Detection for Self-Driving Cars Using Video and LiDAR, An Ablation Study">
<meta property="og:image" content="http://localhost:4200/Paper_Study/images/2023-09-07_01-17.png">
<meta property="og:image:height" content="628">
<meta property="og:image:width" content="1015">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../logo.png" alt="" class="navbar-logo">
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text"><i class="fa-solid fa-house" aria-label="house"></i> Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../Data_Science_Notes/index.html"> 
<span class="menu-text"><i class="fa-solid fa-book-open-reader" aria-label="book-open-reader"></i> Data Science Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="../Paper_Study/index.html" aria-current="page"> 
<span class="menu-text"><i class="fa-solid fa-user-ninja" aria-label="user-ninja"></i> Paper Study</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../Data_Science_Projects/index.html"> 
<span class="menu-text"><i class="fa-solid fa-file-powerpoint" aria-label="file-powerpoint"></i> Data Science Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text"><i class="fa-solid fa-address-card" aria-label="address-card"></i> About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/abhiyantaabhishek"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/abhishek-kumar-dubey-585a86179/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../Paper_Study/index.html">Paper Study</a></li><li class="breadcrumb-item"><a href="../Paper_Study/2023-08-14-Paper-Related-to-Lidar-And-Video.html">3D Object Detection for Self-Driving Cars Using Video and LiDAR, An Ablation Study</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../Paper_Study/index.html">Paper Study</a></li><li class="breadcrumb-item"><a href="../Paper_Study/2023-08-14-Paper-Related-to-Lidar-And-Video.html">3D Object Detection for Self-Driving Cars Using Video and LiDAR, An Ablation Study</a></li></ol></nav>
      <h1 class="title">3D Object Detection for Self-Driving Cars Using Video and LiDAR, An Ablation Study</h1>
                  <div>
        <div class="description">
          3D Object Detection for Self-Driving Cars Using Video and LiDAR, An Ablation Study
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">LiDAR, Video</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Abhishek Kumar Dubey </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">August 14, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../Paper_Study/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Paper Study</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Paper_Study/2023-08-14-Paper-Related-to-Lidar-And-Video.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">3D Object Detection for Self-Driving Cars Using Video and LiDAR, An Ablation Study</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Paper_Study/2023-09-10-Paper-Related-to-Lidar-And-Video.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Transformersin3DPointClouds, ASurvey</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#related-work" id="toc-related-work" class="nav-link active" data-scroll-target="#related-work"><span class="header-section-number">1</span> Related work</a></li>
  <li><a href="#analysis-of-the-role-of-each-sensor-in-the-3d-object-detection-task" id="toc-analysis-of-the-role-of-each-sensor-in-the-3d-object-detection-task" class="nav-link" data-scroll-target="#analysis-of-the-role-of-each-sensor-in-the-3d-object-detection-task"><span class="header-section-number">2</span> Analysis of the Role of Each Sensor in the 3D Object Detection Task</a></li>
  <li><a href="#characteristics-of-the-neural-network-architecture-used" id="toc-characteristics-of-the-neural-network-architecture-used" class="nav-link" data-scroll-target="#characteristics-of-the-neural-network-architecture-used"><span class="header-section-number">3</span> Characteristics of the Neural Network Architecture Used</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<div class="callout callout-style-default callout-caution callout-titled" title="Citation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Citation
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="citation" data-cites="DBLP:journals/sensors/SalmaneVKMDCSV23">Salmane et al. (<a href="#ref-DBLP:journals/sensors/SalmaneVKMDCSV23" role="doc-biblioref">2023</a>)</span></p>
<p><a href="https://www.mdpi.com/1424-8220/23/6/3223">Paper link</a></p>
</div>
</div>
<section id="related-work" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="related-work"><span class="header-section-number">1</span> Related work</h2>
<ul>
<li><p>A 3D bounding box is represented by seven parameters <span class="citation" data-cites="DBLP:conf/cvpr/QiLWSG18">(<a href="#ref-DBLP:conf/cvpr/QiLWSG18" role="doc-biblioref">Charles R. Qi et al. 2018</a>)</span>: <span class="math inline">(x,y,z,h,w,l,\theta)</span> These parameters consist of the object’s central coordinates <span class="math inline">(x,y,z)</span>, its dimensions (height, width, and length), and its orientation angle <span class="math inline">\theta</span>.</p></li>
<li><p><span class="citation" data-cites="DBLP:conf/cvpr/Chen0SJ20">Y. Chen et al. (<a href="#ref-DBLP:conf/cvpr/Chen0SJ20" role="doc-biblioref">2020</a>)</span> introduced a one-stage stereo-based 3D detection system that simultaneously estimates depth and identifies 3D objects using an end-to-end learning approach. They assert that their method surpasses earlier stereo-based 3D detectors and even rivals certain LiDAR-based techniques on the KITTI 3D object detection leaderboard.</p></li>
<li><p><span class="citation" data-cites="DBLP:conf/iros/LiKW20">Li, Ku, and Waslander (<a href="#ref-DBLP:conf/iros/LiKW20" role="doc-biblioref">2020</a>)</span>, addressed the issue of significant variability in depth estimation accuracy when using a video sensor. They introduce CG-Stereo, a confidence-guided stereo 3D object detection system. This pipeline employs distinct decoders for foreground and background pixels during the depth estimation process and utilizes confidence estimation from the depth estimation network as a form of soft attention in the 3D object detection. The authors contend that their method surpasses all leading stereo-based 3D detectors on the KITTI benchmark.</p></li>
<li><p>Another compelling approach found in scholarly work involves integrating LiDAR with a stereo camera. Such techniques leverage LiDAR’s ability to enhance the visual data captured by the camera by introducing concepts of dimension and distance pertaining to various objects within the environment. Specifically, the method outlined by <span class="citation" data-cites="DBLP:conf/cvpr/WangCGHCW19">Wang et al. (<a href="#ref-DBLP:conf/cvpr/WangCGHCW19" role="doc-biblioref">2019</a>)</span> capitalizes on the capability to recreate a 3D setting from stereo camera images. This allows for the derivation of a depth map from the stereo camera data, which is then augmented with measurements from the LiDAR sensor, such as height, width, length, and orientation angle.</p></li>
<li><p>SLS fusion, sparse LiDAR and stereo fusion network <span class="citation" data-cites="DBLP:journals/corr/abs-2103-03977">(<a href="#ref-DBLP:journals/corr/abs-2103-03977" role="doc-biblioref">Mai et al. 2021</a>)</span> which is based on DeepLiDAR <span class="citation" data-cites="DBLP:conf/cvpr/QiuCZZLZP19">(<a href="#ref-DBLP:conf/cvpr/QiuCZZLZP19" role="doc-biblioref">Qiu et al. 2019</a>)</span> and the pseudo-LiDAR pipeline <span class="citation" data-cites="DBLP:conf/cvpr/WangCGHCW19">(<a href="#ref-DBLP:conf/cvpr/WangCGHCW19" role="doc-biblioref">Wang et al. 2019</a>)</span> used information coming from four beam LiDAR and a stereo camera. Fusion improved the depth estimation. resulting in better dense depth map, inturn improving the performance.</p></li>
<li><p>3D object detection methods are classified according to the type of input data: camera-based, LiDAR-based, and fusion based 3D object detection.</p></li>
<li><p>For 3D object detection, LiDAR-based methods are usually classified into four categories: view-based, voxel-based, point-based, and hybrid point-voxel-based detection.</p></li>
<li><p>it is difficult to distinguish whether it is a car or a bush based on point cloud data alone, while this can be handled more easily by looking at the image data. This is why methods based on data fusion have been developed exploiting the advantages of both sensors</p></li>
<li><p>There are three main fusion methods: early fusion, where the raw data are fused at the data level or feature level to form a tensor data of numerous channels; late fusion, where the fusion takes place at the decision level; and deep fusion, where fusion is carefully constructed to combine the advantages of both early and late fusion systems.</p></li>
<li><p>Frustum-PointNet <span class="citation" data-cites="DBLP:conf/cvpr/QiLWSG18">(<a href="#ref-DBLP:conf/cvpr/QiLWSG18" role="doc-biblioref">Charles R. Qi et al. 2018</a>)</span> is composed of three phases: 3D frustum proposal , 3D instance segmentation, and 3D bounding box estimation.</p>
<ul>
<li>The first phase of this procedure is to produce 2D region proposals. By extruding the matching 2D region proposal under a 3D projection, a 3D frustum proposal is generated.</li>
<li>The instance segmentation stage feeds the frustum proposal point cloud to the PointNet segmentation network <span class="citation" data-cites="DBLP:conf/cvpr/QiSMG17">(<a href="#ref-DBLP:conf/cvpr/QiSMG17" role="doc-biblioref">Charles Ruizhongtai Qi et al. 2017</a>)</span>, which classifies each point and determines if it is linked with the discovered item.</li>
<li>In the last stage, all positively classified points are loaded into a new PointNet that estimates 3D bounding box parameters.</li>
</ul></li>
<li><p><span class="citation" data-cites="DBLP:conf/cvpr/ChenMWLX17">X. Chen et al. (<a href="#ref-DBLP:conf/cvpr/ChenMWLX17" role="doc-biblioref">2017</a>)</span> introduced MV3D, where the LiDAR point cloud is projected onto both a 2D top view and a 2D front view, from which feature maps are extracted using two separate CNN. The LiDAR top-view feature map is passed to an RPN (Region Proposal Network) to output proposal 3D bounding boxes. Each 3D proposal is projected onto the feature maps of all three views and a fixed-size feature vector is extracted for each view using pooling. The three feature vectors are then fused in a region-based fusion network, which finally outputs class scores and regresses 3D bounding box residuals.</p></li>
</ul>
</section>
<section id="analysis-of-the-role-of-each-sensor-in-the-3d-object-detection-task" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="analysis-of-the-role-of-each-sensor-in-the-3d-object-detection-task"><span class="header-section-number">2</span> Analysis of the Role of Each Sensor in the 3D Object Detection Task</h2>
<p>SLS–Fusion is a fusion method for LiDAR and stereo cameras based on a deep neural network for the detection of 3D objects, is shown in the below picture <a href="#fig-1" class="quarto-xref">Figure&nbsp;1</a>.</p>
<div id="fig-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/2023-09-07_01-17.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Overall structure of the SLS–Fusion neural network
</figcaption>
</figure>
</div>
<ul>
<li>Firstly, an encoder–decoder based on a ResNet network is designed to extract and fuse left/right features from stereo camera images and project the LiDAR depth map.</li>
<li>Secondly, the decoder network constructs a left and right depth map of optimized features through a depth cost volume model to predict the corrected depth</li>
<li>After the expected dense depth map is obtained, a pseudo-point cloud is generated using calibrated cameras.</li>
<li>Finally, a LiDAR-based method for detecting 3D objects PointRCNN <span class="citation" data-cites="DBLP:conf/cvpr/ShiWL19">(<a href="#ref-DBLP:conf/cvpr/ShiWL19" role="doc-biblioref">Shi, Wang, and Li 2019</a>)</span> is applied to the predicted pseudo-point cloud.</li>
</ul>
</section>
<section id="characteristics-of-the-neural-network-architecture-used" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="characteristics-of-the-neural-network-architecture-used"><span class="header-section-number">3</span> Characteristics of the Neural Network Architecture Used</h2>
<ul>
<li><p>The main component of the SLS–Fusion neural network, used to fuse or separate LiDAR and stereo camera features (for an ablation study), is the encoder–decoder component (see <a href="#fig-1" class="quarto-xref">Figure&nbsp;1</a> and <a href="#fig-2" class="quarto-xref">Figure&nbsp;2</a>)</p>
<div id="fig-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/2023-09-07_02-37.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: SLS–Fusion encoder–decoder architecture: The residual neural network blocks (ResNet blocks) within the encoder are used to extract features from the LiDAR and stereo inputs. The fusion process inside the decoder is accomplished through the use of addition and up-projection operators
</figcaption>
</figure>
</div></li>
<li><p>As shown in <a href="#fig-2" class="quarto-xref">Figure&nbsp;2</a>, both the stereo camera and LiDAR encoders are composed of a series of residual blocks from the neural network ResNet, followed by step-down convolution to reduce the feature resolution of the input. ResNet is a group of residual neural network blocks and each residual block is a stack of layers placed in such a way that the output of one layer is taken and added to another deeper layer within the block, as shown in <a href="#fig-3" class="quarto-xref">Figure&nbsp;3</a></p>
<div id="fig-3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/2023-09-07_02-47.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: The structure of Stereo and LiDAR residual blocks inside the encoder/decoder of the SLS– Fusion model. A stack of layers is grouped into blocks for stereo and LiDAR networks, conducted by a step-down convolution direction and followed by a set of fusion blocks
</figcaption>
</figure>
</div></li>
<li><p>The main advantage of ResNet is its ability to prevent the accuracy from saturating and degrading rapidly during the training of deeper neural networks (networks with more than 20 layers). This advantage helps in choosing a network to be as deep as needed for the problem at hand.</p></li>
<li><p>The network of the decoder consists of adding the functions of both LiDAR and stereo encoders, then up-projecting the result to progressively increase the resolution of the features and generate a dense depth map as a decoder output.</p></li>
<li><p>Because the sparse input of LiDAR is heavily linked to the depth decoder output, features related to the LiDAR sensor should contribute more to the decoder than features related to the stereo sensor.</p></li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-DBLP:conf/cvpr/ChenMWLX17" class="csl-entry" role="listitem">
Chen, Xiaozhi, Huimin Ma, Ji Wan, Bo Li, and Tian Xia. 2017. <span>“Multi-View 3D Object Detection Network for Autonomous Driving.”</span> In <em>2017 <span>IEEE</span> Conference on Computer Vision and Pattern Recognition, <span>CVPR</span> 2017, Honolulu, HI, USA, July 21-26, 2017</em>, 6526–34. <span>IEEE</span> Computer Society. <a href="https://doi.org/10.1109/CVPR.2017.691">https://doi.org/10.1109/CVPR.2017.691</a>.
</div>
<div id="ref-DBLP:conf/cvpr/Chen0SJ20" class="csl-entry" role="listitem">
Chen, Yilun, Shu Liu, Xiaoyong Shen, and Jiaya Jia. 2020. <span>“<span>DSGN:</span> Deep Stereo Geometry Network for 3D Object Detection.”</span> In <em>2020 <span>IEEE/CVF</span> Conference on Computer Vision and Pattern Recognition, <span>CVPR</span> 2020, Seattle, WA, USA, June 13-19, 2020</em>, 12533–42. Computer Vision Foundation / <span>IEEE</span>. <a href="https://doi.org/10.1109/CVPR42600.2020.01255">https://doi.org/10.1109/CVPR42600.2020.01255</a>.
</div>
<div id="ref-DBLP:conf/iros/LiKW20" class="csl-entry" role="listitem">
Li, Chengyao, Jason Ku, and Steven L. Waslander. 2020. <span>“Confidence Guided Stereo 3D Object Detection with Split Depth Estimation.”</span> In <em><span>IEEE/RSJ</span> International Conference on Intelligent Robots and Systems, <span>IROS</span> 2020, Las Vegas, NV, USA, October 24, 2020 - January 24, 2021</em>, 5776–83. <span>IEEE</span>. <a href="https://doi.org/10.1109/IROS45743.2020.9341188">https://doi.org/10.1109/IROS45743.2020.9341188</a>.
</div>
<div id="ref-DBLP:journals/corr/abs-2103-03977" class="csl-entry" role="listitem">
Mai, Nguyen Anh Minh, Pierre Duthon, Louahdi Khoudour, Alain Crouzil, and Sergio A. Velastin. 2021. <span>“Sparse LiDAR and Stereo Fusion (SLS-Fusion) for Depth Estimationand 3D Object Detection.”</span> <em>CoRR</em> abs/2103.03977. <a href="https://arxiv.org/abs/2103.03977">https://arxiv.org/abs/2103.03977</a>.
</div>
<div id="ref-DBLP:conf/cvpr/QiLWSG18" class="csl-entry" role="listitem">
Qi, Charles R., Wei Liu, Chenxia Wu, Hao Su, and Leonidas J. Guibas. 2018. <span>“Frustum PointNets for 3D Object Detection from <span>RGB-D</span> Data.”</span> In <em>2018 <span>IEEE</span> Conference on Computer Vision and Pattern Recognition, <span>CVPR</span> 2018, Salt Lake City, UT, USA, June 18-22, 2018</em>, 918–27. Computer Vision Foundation / <span>IEEE</span> Computer Society. <a href="https://doi.org/10.1109/CVPR.2018.00102">https://doi.org/10.1109/CVPR.2018.00102</a>.
</div>
<div id="ref-DBLP:conf/cvpr/QiSMG17" class="csl-entry" role="listitem">
Qi, Charles Ruizhongtai, Hao Su, Kaichun Mo, and Leonidas J. Guibas. 2017. <span>“PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation.”</span> In <em>2017 <span>IEEE</span> Conference on Computer Vision and Pattern Recognition, <span>CVPR</span> 2017, Honolulu, HI, USA, July 21-26, 2017</em>, 77–85. <span>IEEE</span> Computer Society. <a href="https://doi.org/10.1109/CVPR.2017.16">https://doi.org/10.1109/CVPR.2017.16</a>.
</div>
<div id="ref-DBLP:conf/cvpr/QiuCZZLZP19" class="csl-entry" role="listitem">
Qiu, Jiaxiong, Zhaopeng Cui, Yinda Zhang, Xingdi Zhang, Shuaicheng Liu, Bing Zeng, and Marc Pollefeys. 2019. <span>“DeepLiDAR: Deep Surface Normal Guided Depth Prediction for Outdoor Scene from Sparse LiDAR Data and Single Color Image.”</span> In <em><span>IEEE</span> Conference on Computer Vision and Pattern Recognition, <span>CVPR</span> 2019, Long Beach, CA, USA, June 16-20, 2019</em>, 3313–22. Computer Vision Foundation / <span>IEEE</span>. <a href="https://doi.org/10.1109/CVPR.2019.00343">https://doi.org/10.1109/CVPR.2019.00343</a>.
</div>
<div id="ref-DBLP:journals/sensors/SalmaneVKMDCSV23" class="csl-entry" role="listitem">
Salmane, Houssam, Josué Manuel Rivera Velázquez, Louahdi Khoudour, Nguyen Anh Minh Mai, Pierre Duthon, Alain Crouzil, Guillaume Saint-Pierre, and Sergio A. Velastin. 2023. <span>“3D Object Detection for Self-Driving Cars Using Video and LiDAR: An Ablation Study.”</span> <em>Sensors</em> 23 (6): 3223. <a href="https://doi.org/10.3390/s23063223">https://doi.org/10.3390/s23063223</a>.
</div>
<div id="ref-DBLP:conf/cvpr/ShiWL19" class="csl-entry" role="listitem">
Shi, Shaoshuai, Xiaogang Wang, and Hongsheng Li. 2019. <span>“PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud.”</span> In <em><span>IEEE</span> Conference on Computer Vision and Pattern Recognition, <span>CVPR</span> 2019, Long Beach, CA, USA, June 16-20, 2019</em>, 770–79. Computer Vision Foundation / <span>IEEE</span>. <a href="https://doi.org/10.1109/CVPR.2019.00086">https://doi.org/10.1109/CVPR.2019.00086</a>.
</div>
<div id="ref-DBLP:conf/cvpr/WangCGHCW19" class="csl-entry" role="listitem">
Wang, Yan, Wei-Lun Chao, Divyansh Garg, Bharath Hariharan, Mark E. Campbell, and Kilian Q. Weinberger. 2019. <span>“Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving.”</span> In <em><span>IEEE</span> Conference on Computer Vision and Pattern Recognition, <span>CVPR</span> 2019, Long Beach, CA, USA, June 16-20, 2019</em>, 8445–53. Computer Vision Foundation / <span>IEEE</span>. <a href="https://doi.org/10.1109/CVPR.2019.00864">https://doi.org/10.1109/CVPR.2019.00864</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("http:\/\/localhost:4200\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="abhiyantaabhishek/IITH-Data-Science" data-repo-id="R_kgDOILoB8A" data-category="Announcements" data-category-id="DIC_kwDOILoB8M4CSJcL" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../Paper_Study/index.html" class="pagination-link" aria-label="Paper Study">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Paper Study</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../Paper_Study/2023-09-10-Paper-Related-to-Lidar-And-Video.html" class="pagination-link" aria-label="Transformersin3DPointClouds, ASurvey">
        <span class="nav-page-text">Transformersin3DPointClouds, ASurvey</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2022, Abhishek Kumar Dubey</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/abhiyantaabhishek">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/abhishek-kumar-dubey-585a86179/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>