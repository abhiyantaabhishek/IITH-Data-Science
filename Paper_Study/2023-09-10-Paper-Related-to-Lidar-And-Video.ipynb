{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "author: Abhishek Kumar Dubey\n",
        "badges: True\n",
        "categories:\n",
        "- LiDAR, Video\n",
        "date: '2023-09-10'\n",
        "description: Transformersin3DPointClouds, ASurvey\n",
        "title: Transformersin3DPointClouds, ASurvey\n",
        "toc: true\n",
        "bibliography: references.bib\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-caution title=\"Citation\"}\n",
        "@DBLP:journals/corr/abs-2205-07417\n",
        "\n",
        "[Paper link](https://www.mdpi.com/1424-8220/23/6/3223)\n",
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notation \n",
        "\n",
        "- Lidar points $P=\\{p_1,p_2,\\dots,p_N\\} \\in \\mathbb R^{N \\times D}$ \n",
        "- Embedded feature map $X\\in \\mathbb R^{N \\times C}$\n",
        "- Learnable weight matrices for query $W_Q \\in \\mathbb R^{C \\times C_Q},$ for key $W_K \\in \\mathbb R^{C \\times C_K},$ and for value $W_V\\in \\mathbb R^{C \\times C}$, typically $C_K=C_Q$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- For  3D point cloud applications, decoders can be specifically designed (i.e. not be a pure Transformer) for dense prediction  tasks such as part segmentation and semantic segmentation  in 3D point cloud analysis\n",
        "- Researchers in 3D computer vision often adopt PointNet++ [@DBLP:conf/nips/QiYSG17] or convolutional backbones with Transformer blocks incorporated therein.\n",
        "- Firstly, for the input embedding, pints $P$ is projected to a high-dimension feature space which can facilitate subsequent learning. This can be achieved by using a Multi-Layer Perception (MLP) or other feature extraction backbone net- works like PointNet [@DBLP:conf/cvpr/QiSMG17]\n",
        "- The positional encoding is used to either capture the geometrical information, or the relative ordering of input tokens/points if relevant.\n",
        "- Note that the Transformer is order-agnostic without above step, which is not an issue for point clouds, since they are naturally unordered. Nonetheless, the frequency-based positional encoding can be used by mapping spatial coordinates with sine and cosine functions [@DBLP:conf/nips/VaswaniSPUJGKP17]\n",
        "- The core component of the Transformer encoder  is the self-attention mechanism. If a sine/cosine based positional encoding is used, it is typically added to the embedded feature map $X$\n",
        "- $$\\begin{cases}\n",
        "    \\text{Query}(Q) &=XW_Q \\\\\n",
        "    \\text{Key}(K) &=XW_K \\tag{1}\\\\\n",
        "    \\text{Value}(V) &=XW_V \\\\\n",
        "  \\end{cases}$$\n",
        "- So now attention can be formulated as shown below:\n",
        "  $$\\text{attention map}=\\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{C_K}} \\right)\\tag{2}$$\n",
        "  The size of attention map is $N \\times N$, it measures the similarity of any tow input points and called similarity matrix.\n",
        "- Next the attention map and matrix $\\text{Value}(V)$ are multiplied to generate the new feature map $F$ fo the same size as $X$\n",
        "- Each feature vector in $F$ is obtained by computing a weighted sum of all input features. It is therefore able to establish connections with all input features.\n",
        "- ::: {.callout-note }\n",
        "When inputs are global, this feature $F$  allows for the Transformer to easily learn global features. Therefore, compared with convolutional neural networks (CNNs), Transformers are better at long-range dependency modeling.\n",
        "::: \n",
        "- Compared to MLP\n",
        "    - Attention map in the Transformer is dynamic depending on the input during the inference, which is more adaptive than MLPs with fixed weight matrices.\n",
        "    - self-attention mechanism is permutation-equivariant, while for MLPs, the order of input and output is encoded in the weight matrix\n",
        "- A normalization layer is placed before and after the feed-forward layer, performing standardization and normalization on feature maps.\n",
        "    - LayerNormalization : commonly used in NLP\n",
        "    - BatchNormalization : commonly used in CV like 2D or 3D data processing\n",
        "- A feed-forward layer is added to enhance the representation of attention features.Generally, it consists of two fully-connection layers with a RELU function\n",
        "- Finally, a skip connection is used between the input and output of the self-attention module as shown in @fig-1\n",
        "\n",
        "  ![Illustration of the Transformer encoder architecture](images/2023-09-10_05-08.png){#fig-1}\n",
        "\n",
        "- so we saw that there are below 6 components which are common in transformer:\n",
        "    1. Input (word) embedding\n",
        "    2. Positional encoding \n",
        "    3. Self-attention\n",
        "    4. Normalization\n",
        "    5. Feed-Forward operation\n",
        "    6. Skip connection \n",
        "\n",
        "- But there are some early 3-D transformers which doesn't have all the 6 components "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
