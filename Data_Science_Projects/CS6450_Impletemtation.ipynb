{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "author: Abhishek Kumar  Dubey <br> cs22mds15010\n",
    "badges: True\n",
    "categories:\n",
    "- 3D vehicle Detection\n",
    "date: '2023-03-12'\n",
    "description: IMPLEMENTATION \n",
    "title: Point cloud based 3d object detection \n",
    "title-slide-attributes:\n",
    "    data-background-image: images/FirstPage_background.png \n",
    "    data-background-size: contain\n",
    "    data-background-opacity: \"0.6\"\n",
    "subtitle: Y. Zhou and O. Tuzel,Voxelnet, IEEE Conference on CVPR, pp. 4490-4499, 2018.\n",
    "comments: false\n",
    "format:\n",
    "  revealjs: \n",
    "    theme: sky\n",
    "    reference-location: document\n",
    "    incremental: false \n",
    "    smaller: true\n",
    "    scrollable: true\n",
    "    transition: slide\n",
    "    slide-number: true\n",
    "    history: false\n",
    "    chalkboard: \n",
    "      buttons: true\n",
    "    preview-links: auto\n",
    "    logo: images/logo.png\n",
    "    css: styles.css\n",
    "    footer: IIT Hyderabad\n",
    "resources:\n",
    "  - demo.pdf\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VoxelNet Introduction \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"40%\"}\n",
    "-  <p align=\"justify\">it is a  generic 3D detection framework that simultaneously learns a discriminative feature representation from point clouds and predicts accurate 3D bounding boxes </p> \n",
    "- <p align=\"justify\">VoxelNet directly operates on the raw point cloud.</p>\n",
    ":::\n",
    "\n",
    "::: {.column width=\"60%\"}\n",
    "![](images/Acrobat_I9iR951esL.png)\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VoxelNet Architecture \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"70%\"}\n",
    "![](images/Acrobat_ZOIOLjvhTa.png)\n",
    ":::\n",
    "\n",
    "::: {.column width=\"30%\"}\n",
    "The VoxelNet consists of three functional blocks\n",
    "\n",
    "1. Feature learning network\n",
    "2. Convolutional middle layers\n",
    "3. Region proposal network\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Learning Network \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"60%\"}\n",
    "### Voxel Partition\n",
    "\n",
    "- Suppose the point cloud encompasses 3D space with range $D, H, W$ along the $Z, Y, X$ axes respectively.\n",
    "- The resulting 3D voxel grid is of size $D'=D/v_D, H'=H/v_H,  W'=W/v_W$ where $v_D,v_H,v_W$ defines the voxel size accordingly \n",
    "- Point density in LiDAR point clouds is variable, resulting in varying numbers of points within each voxel.\n",
    ":::\n",
    "\n",
    "::: {.column width=\"40%\"}\n",
    "![](images/Acrobat_ffA1dvD6zy.png)\n",
    ":::\n",
    "\n",
    "::::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "\n",
    "Feature Learning Network cont ... \n",
    "\n",
    "### Random Sampling\n",
    "- A high-definition LiDAR point cloud typically has around 100k points.\n",
    "- Directly processing all points can overload computing resources and create biased detection due to variable point density.\n",
    "- To avoid this, a fixed number of points (T) is randomly sampled from voxels that contain more than T points.\n",
    "- This sampling strategy has two purposes:\n",
    "    - computational savings and\n",
    "    - decreases the imbalance of points between the voxels which reduces the sampling bias, and adds more variation to training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "\n",
    "Feature Learning Network cont ... \n",
    "\n",
    "### Stacked Voxel Feature Encoding\n",
    "\n",
    "- let ${\\bf V}\\,=\\,\\{{\\bf p}_i\\,=\\,[x_{i},y_{i},z_{i},r_{i}]^{T}\\,\\in\\,\\mathbb{R}^{4}\\}_{i=1...t}$ as a non-empty voxel containing $t <T$ LiDAR points.\n",
    "- Here ${\\bf p}_i$ contains $XYZ$ coordinates for the $i^{\\text{th}}$ point and $r_i$ is the received reflectance.\n",
    "- Now find ${\\mathrm{\\bf V}}_{\\mathrm{in}}\\,=\\,\\{\\;\\mathrm{\\hat {\\bf p}_i}\\,=\\,[x_{i},y_{i},z_{i},r_{i},x_{i}-v_{x},y_{i}-v_{y},z_{i}-v_{z}]^{T}\\in\\mathbb{R}^{7}\\}_{i=1\\ldots t}.$ Here $(v_{x},v_{y},v_{z})$ is the local mean as the centroid of all the points in ${\\bf V}$\n",
    "- Next, each $\\hat {\\bf p}_i$ is transformed through the fully connected network (FCN) into a feature space.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "\n",
    "Feature Learning Network. VFE cont ... \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"40%\"}\n",
    "- Information from point feature ${\\bf f}_{i}\\ \\in\\ \\mathbb{R}^{m}$ is aggregated __to encode the shape of the surface contained within the voxel__\n",
    "- The FCN is composed of a linear layer, a batch normalization (BN) layer, and a rectified linear unit (ReLU) layer\n",
    "\n",
    ":::\n",
    "\n",
    "::: {.column width=\"60%\"}\n",
    "![](images/Acrobat_HibvuJQPZU.png)\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "- Elementwise MaxPooling across all $\\bf f_i$ is used to __get the locally aggregated feature__ $\\hat{\\mathbf{f}}\\ \\in\\ \\mathbb{R}^{m}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "\n",
    "Feature Learning Network. VFE cont ... \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"80%\"}\n",
    "- Now form the form the point-wise concatenated feature as $\\mathbf{f}_{i}^{o u t}\\,=\\,\\left[\\mathbf{f}_{i}^{T},{\\tilde{\\mathbf{f}}}^{T}\\right]^{T}\\,\\in\\,\\mathbb{R}^{2m}.$\n",
    "- Finally we get ${\\bf V_{\\ o u t}}\\ =\\ \\{{\\bf f}_{i}^{o u t}\\}_{i...t}.$\n",
    "- The output feature combines point-wise and locally aggregated features.\n",
    "- Stacking VFE layers allows encoding of point interactions within a voxel, __enabling the final feature representation to learn descriptive shape information__.\n",
    ":::\n",
    "\n",
    "::: {.column width=\"20%\"}\n",
    "![](images/chrome_Tb0CmOU9L4.png)\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "\n",
    "- Voxel features, uniquely associated with their spatial coordinates, are obtained by __processing only non-empty voxels__, resulting in a sparse 4D tensor representation.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Convolutional Middle Layers \n",
    "\n",
    "- Each convolutional middle layer applies 3D convolution, BN layer, and ReLU layer sequentially.\n",
    "- The convolutional middle layers aggregate voxel-wise features within a __progressively expanding receptive field, adding more context to the shape description__\n",
    "- Depending on the task this layer' details can vary.\n",
    "\n",
    "<br><br>\n",
    "![](images/chrome_jVqU0WaxpR.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Region Proposal Network \n",
    " \n",
    " - RPN is combined with  the feature learning network and convolutional middle layers to form an __end-to-end trainable pipeline__.\n",
    "   ![](images/Acrobat_gayrjAQFrA.png)\n",
    "- The first layer of each block downsamples the feature map by half. via convolution with a stride of 2\n",
    "- Followed by a sequence of convolutions of stride 1, BN and ReLU\n",
    "- The output of each block is upsampled to a fixed size and concatenated to construct a high-resolution feature map."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Loss Function \n",
    "\n",
    "- 3D ground truth box is parameterized as $(x_{c}^{g},y_{c}^{g},z_{c}^{g},l^{g},w^{g},h^{g},\\theta^{g}),$ where $x_{c}^{g},y_{c}^{g},z_{c}^{g}$ represent the center location, $l^{g},w^{g},h^{g}$ are length, width, height of the box,${\\theta}^{g}$ is the yaw rotation.\n",
    "- To retrieve the ground truth box from a matching positive anchor parameterized as $(x_{c}^{a},y_{c}^{a},z_{c}^{a},l^{a},w^{a},h^{a},\\theta^{a})$ a vector $\\mathbf{u}^{*}\\in\\mathbb{R}^{7}$ containing below 7 elements are calculated as below:\n",
    "  $$\\begin{align*}\n",
    "      &\\Delta x={\\frac{x_{c}^{g}-x_{c}^{a}}{d^{a}}},\\Delta y={\\frac{y_{c}^{g}-y_{c}^{a}}{d^{a}}},\\Delta z={\\frac{z_{c}^{g}-z_{c}^{a}}{h^{a}}},\\\\\n",
    "      &\\Delta l=\\log(\\frac{l^{g}}{l^{a}}),\\Delta w=\\log(\\frac{w^{g}}{w^{a}}),\\Delta h=\\log(\\frac{h^{g}}{h^{a}}), \\tag{1} \\\\\n",
    "      &\\Delta\\theta=\\theta^{g}-\\theta^{a}\n",
    "  \\end{align*}$$\n",
    "  Where  $\\;d^{a}\\,=\\,\\sqrt{(l^{a})^{2}+(w^{a})^{2}}$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   \n",
    "\n",
    "Loss Function cont ...\n",
    "\n",
    "- Now we find loss as shown below \n",
    "  $$\\begin{align*}\n",
    "     L&=\\alpha{\\frac{1}{N_{\\mathrm{pos}}}}\\sum_{i}L_{\\mathrm{cls}}(p_{i}^{\\mathrm{pos}},1)\\\\ \n",
    "     &+\\beta{\\frac{1}{N_{\\mathrm{neg}}}}\\sum_{j}L_{\\mathrm{cls}}(p_{j}^{\\mathrm{neg}},0)\\\\ \n",
    "     &+\\frac{1}{N_{\\mathrm{pos}}}\\sum_{i}L_{\\mathrm{reg}}({\\bf u}_{i},{\\bf u}_{i}^{*}) \\tag{2}\n",
    "  \\end{align*}$$\n",
    "- where $p_{i}^{\\mathrm{pos}}$ and $p_{j}^{\\mathrm{neg}}$ represent the softmax output for positive anchor $a_{i}^{\\mathrm{pos}}$ and negative anchor $a_{j}^{\\mathrm{neg}}$ respectively,\n",
    "- $\\mathbf{u}_{i}\\ \\in\\ \\mathbb{R}^{7}$ and $\\mathbf{u}_{i}^{*}\\ \\in\\ \\mathbb{R}^{7}$ are the regression output and ground truth for positive anchor $a_{i}^{\\mathrm{pos}}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Details  \n",
    "\n",
    "### Car Detection\n",
    "\n",
    "- Point clouds within the range of $[-3,1]\\times[-40,40]\\times[0,70.4]$ meters along $X,Y,Z$ respectively is considered. \n",
    "- $v_{D}\\,=\\,0.4,v_{H}\\,=\\,0.2,v_{W}\\,=\\,0.2$\n",
    "- $D^{\\prime}\\;=\\;10,\\;H^{\\prime}\\;=\\;400,\\;W^{\\prime}\\;=\\;352$\n",
    "- The maximum number of randomly sampled points in each non-empty voxel $T\\,=\\,35$\n",
    "- For $\\mathrm{VFE}-1(7, 32)$ and and $\\mathrm{VFE}-2(32, 128)$ was used.\n",
    "- The final $\\mathrm{FCN}$  maps $\\mathrm{VFE}$ output to $\\mathbb{R}^{128}$\n",
    "- Thus the feature learning net generates a sparse tensor of shape $128\\times10\\times400\\times352$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    " \n",
    " Network Details, Car Detection cont ...\n",
    "\n",
    " ![](images/Acrobat_9Ud4aJ0aiX.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    " \n",
    " Network Details, Car Detection cont ...\n",
    " \n",
    "- To aggregate voxel-wise features, three convolution middle layers was employed sequentially as $\\mathrm{Conv3D}(128, 64, 3,(2,1,1), (1,1,1)),$ $\\mathrm{Conv3D}(64, 64, 3, (1,1,1), (0,1,1)),$ and $\\mathrm{Conv3D}(64, 64, 3, (2,1,1), (1,1,1)),$\n",
    "- which yields a 4D tensor of size $64\\times2\\times400\\times352.$\n",
    "- After reshaping, size becomes $128\\times400\\times352.$ and is input to RPN.\n",
    "- Only one anchor size was used $l^{a}\\,=\\,3.9.\\,w^{a}\\,=\\,1.6.\\,h^{a}\\,=\\,1.56$ meters centered at $z_{c}^{a}\\,=\\,-1.0$ meters with two rotations, $0^\\circ$ and $90^\\circ$ \n",
    "- Positive anchor: if it has the highest Intersection over Union (IoU) with a ground truth or its IoU with ground truth is above 0.6 (in bird’s eye view).\n",
    "- Positive anchor: if the IoU between it and all ground truth boxes is less than 0.45.\n",
    "- Set $\\alpha=1.5$ and $\\beta=1$ in equation $(2)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training  \n",
    "\n",
    " \n",
    " - A similar network with different parameters, was used for Pedestrian and Cyclist Detection\n",
    "    - 2 anchors, smaller anchors, small stride to capture fine details etc.\n",
    "- During training, stochastic gradient descent (SGD) with learning rate 0.01 was used for the first 150 epochs and decreased the learning rate to 0.001 for the last 10 epochs.\n",
    "- Data Augmentation \n",
    "    - perturbation independently to each ground truth 3D bounding box together with those LiDAR points within the box.\n",
    "    - Global scaling."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments  \n",
    " \n",
    "- VoxelNet evaluated on KITTI 3D object detection benchmark\n",
    "- 7,481 training images/point clouds and 7,518 test images/point clouds\n",
    "- Three categories: Car, Pedestrian, and Cyclist\n",
    "- Detection outcomes evaluated based on three difficulty levels: easy, moderate, and hard\n",
    "- Evaluation protocol based on object size, occlusion state, and truncation level\n",
    "- 3,712 data samples for training and 3,769 for validation\n",
    "- Hand-crafted baseline (HC-baseline) was implemented and trained to evaluate the importance of end-to-end learning.\n",
    "- HC-baseline uses the bird’s eye view features described in Multi-view 3d object detection network for autonomous driving^[X. Chen, H. Ma, J. Wan, B. Li, and T. Xia. Multi-view 3d object detection network for autonomous driving. In IEEE CVPR, 2017.] which are computed at 0.1 m resolution.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Novelty  \n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "- We do not need to predict separately  for positive and negative.\n",
    "- Extend  VoxelNet for joint LiDAR and image-based end-to-end 3D detection.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Novelty \n",
    "\n",
    "- Previous time frames can be taken into account improve the detection. \n",
    "  ![](images/chrome_44h3Hukd9d.gif)\n",
    "- VFE can be improved with the help of transformer positional encoding. \n",
    "- One universal network instead of two.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORK\n",
    "\n",
    "- [MVX-Net: Multimodal VoxelNet for 3D Object Detection](https://arxiv.org/pdf/1904.01649.pdf)\n",
    "- [ImVoxelNet: Image to Voxels Projection for Monocular and Multi-View General-Purpose 3D Object Detection](https://arxiv.org/pdf/2106.01178.pdf)\n",
    "- [MVX-Net: Multimodal VoxelNet for 3D Object Detection](https://arxiv.org/pdf/1904.01649.pdf)\n",
    "- [Group-Free 3D Object Detection via Transformers](https://arxiv.org/pdf/2104.00678.pdf)\n",
    "- [End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point Clouds (Dynamic Voxelization)](https://arxiv.org/pdf/1910.06528.pdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$\\mathcal{THANKS!}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0e5e69b8442e8f020791fad6bfcef3777f0e89e0f1a2517b6b628b2eaf0fe66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
