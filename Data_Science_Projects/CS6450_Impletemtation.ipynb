{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "author: Abhishek Kumar  Dubey <br> cs22mds15010\n",
    "badges: True\n",
    "image: images/007402.png\n",
    "categories:\n",
    "- 3D vehicle Detection\n",
    "date: '2023-04-29'\n",
    "description: IMPLEMENTATION \n",
    "title: Point cloud based 3d object detection \n",
    "title-slide-attributes:\n",
    "    data-background-image: images/FirstPage_background.png \n",
    "    data-background-size: contain\n",
    "    data-background-opacity: \"0.6\"\n",
    "subtitle: Y. Zhou and O. Tuzel,Voxelnet, IEEE Conference on CVPR, pp. 4490-4499, 2018.\n",
    "comments: false\n",
    "format:\n",
    "  revealjs: \n",
    "    theme: mySkyTheme.scss\n",
    "    reference-location: document\n",
    "    incremental: false \n",
    "    smaller: true\n",
    "    scrollable: true\n",
    "    transition: slide\n",
    "    slide-number: true\n",
    "    history: false\n",
    "    chalkboard: \n",
    "      buttons: true\n",
    "    preview-links: auto\n",
    "    logo: images/logo.png\n",
    "    css: styles.css\n",
    "    footer: IIT Hyderabad\n",
    "resources:\n",
    "  - demo.pdf\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Presentation  summray : \n",
    "\n",
    "### VoxelNet Architecture \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"70%\"}\n",
    "![](images/Acrobat_ZOIOLjvhTa.png)\n",
    ":::\n",
    "\n",
    "::: {.column width=\"30%\"}\n",
    "The VoxelNet consists of three functional blocks\n",
    "\n",
    "1. Feature learning network\n",
    "2. Convolutional middle layers\n",
    "3. Region proposal network\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Presentation  summray :  \n",
    "### Feature Learning Network \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"60%\"}\n",
    "### Voxel Partition\n",
    "\n",
    "- The  3D voxel grid is of size $D'=D/v_D, H'=H/v_H,  W'=W/v_W$ where $v_D,v_H,v_W$ defines the voxel size accordingly , where $D, H, W$  are point clouds across $Z, Y, X$ axes respectively.\n",
    "- Point density in LiDAR point clouds is variable, resulting in varying numbers of points within each voxel.\n",
    ":::\n",
    "\n",
    "::: {.column width=\"40%\"}\n",
    "![](images/Acrobat_ffA1dvD6zy.png)\n",
    ":::\n",
    "\n",
    "::::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Presentation  summray : \n",
    "### Convolutional Middle Layers \n",
    "\n",
    "- The convolutional middle layers aggregate voxel-wise features within a __progressively expanding receptive field, adding more context to the shape description__\n",
    "- Depending on the task this layer' details can vary.\n",
    "\n",
    "<br><br>\n",
    "![](images/chrome_jVqU0WaxpR.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Presentation  summray : \n",
    "### Region Proposal Network \n",
    " \n",
    " - RPN is combined with  the feature learning network and convolutional middle layers to form an __end-to-end trainable pipeline__.<br>\n",
    "   ![](images/Acrobat_gayrjAQFrA.png)<br>\n",
    "- The first layer of each block downsamples the feature map by half.\n",
    "- The output of each block is upsampled to a fixed size and concatenated to construct a high-resolution feature map."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Presentation  summray :  \n",
    "### Loss Function \n",
    "\n",
    "- Consider ground truth  $(x_{c}^{g},y_{c}^{g},z_{c}^{g},l^{g},w^{g},h^{g},\\theta^{g}),$ where $x_{c}^{g},y_{c}^{g},z_{c}^{g}$ represent the center location, $l^{g},w^{g},h^{g}$ are length, width, height of the box,${\\theta}^{g}$ is the yaw rotation.Consider Anchor   $(x_{c}^{a},y_{c}^{a},z_{c}^{a},l^{a},w^{a},h^{a},\\theta^{a})$ A vector $\\mathbf{u}^{*}\\in\\mathbb{R}^{7}$ containing below 7 elements are calculated as below:\n",
    "  $$\\begin{align*}\n",
    "      &\\Delta x={\\frac{x_{c}^{g}-x_{c}^{a}}{d^{a}}},\\Delta y={\\frac{y_{c}^{g}-y_{c}^{a}}{d^{a}}},\\Delta z={\\frac{z_{c}^{g}-z_{c}^{a}}{h^{a}}},\\\\\n",
    "      &\\Delta l=\\log(\\frac{l^{g}}{l^{a}}),\\Delta w=\\log(\\frac{w^{g}}{w^{a}}),\\Delta h=\\log(\\frac{h^{g}}{h^{a}}), \\tag{1} \\\\\n",
    "      &\\Delta\\theta=\\theta^{g}-\\theta^{a}\n",
    "  \\end{align*}$$\n",
    "  Where  $\\;d^{a}\\,=\\,\\sqrt{(l^{a})^{2}+(w^{a})^{2}}$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   \n",
    "\n",
    "Loss Function cont ...\n",
    "\n",
    "- Now we find loss as shown below \n",
    "  $$\\begin{align*}\n",
    "     L&=\\alpha{\\frac{1}{N_{\\mathrm{pos}}}}\\sum_{i}L_{\\mathrm{cls}}(p_{i}^{\\mathrm{pos}},1)\\\\ \n",
    "     &+\\beta{\\frac{1}{N_{\\mathrm{neg}}}}\\sum_{j}L_{\\mathrm{cls}}(p_{j}^{\\mathrm{neg}},0)\\\\ \n",
    "     &+\\frac{1}{N_{\\mathrm{pos}}}\\sum_{i}L_{\\mathrm{reg}}({\\bf u}_{i},{\\bf u}_{i}^{*}) \\tag{2}\n",
    "  \\end{align*}$$\n",
    "- where $p_{i}^{\\mathrm{pos}}$ and $p_{j}^{\\mathrm{neg}}$ represent the softmax output for positive anchor $a_{i}^{\\mathrm{pos}}$ and negative anchor $a_{j}^{\\mathrm{neg}}$ respectively,\n",
    "- $\\mathbf{u}_{i}\\ \\in\\ \\mathbb{R}^{7}$ and $\\mathbf{u}_{i}^{*}\\ \\in\\ \\mathbb{R}^{7}$ are the regression output and ground truth for positive anchor $a_{i}^{\\mathrm{pos}}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation : \n",
    "### Network Details  \n",
    "\n",
    "\n",
    "- $v_{D}\\,=\\,0.4,v_{H}\\,=\\,0.2,v_{W}\\,=\\,0.2$\n",
    "- $D^{\\prime}\\;=\\;10,\\;H^{\\prime}\\;=\\;400,\\;W^{\\prime}\\;=\\;352$\n",
    "- The maximum number of randomly sampled points in each non-empty voxel $T\\,=\\,35$\n",
    "- For $\\mathrm{VFE}-1(7, 32)$ and and $\\mathrm{VFE}-2(32, 128)$ was used.\n",
    "- The final $\\mathrm{FCN}$  maps $\\mathrm{VFE}$ output to $\\mathbb{R}^{128}$\n",
    "- Thus the feature learning net generates a sparse tensor of shape $128\\times10\\times400\\times352$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    " \n",
    " Network Details, Car Detection cont ...\n",
    "\n",
    " ![](images/Acrobat_9Ud4aJ0aiX.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation : \n",
    "\n",
    "### Data set details\n",
    "\n",
    "- KITTI 3D object detection dataset is used,which contains 7,481 training images/point clouds and 7,518 test images/point clouds\n",
    "- The data has three categories : Car, Pedestrian, and Cyclist, for each class data is divided in the three difficulty levels : easy, moderate, and hard\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"40%\"}\n",
    "Training Dataset<br>\n",
    "length : 3712\n",
    "\n",
    "\n",
    "\n",
    "| category   | number |\n",
    "|------------|--------|\n",
    "| Pedestrian | 2207   |\n",
    "| Cyclist    | 734    |\n",
    "| Car        | 14357  |\n",
    "\n",
    "\n",
    ":::\n",
    "\n",
    "::: {.column width=\"60%\"}\n",
    "Validation  Dataset<br>\n",
    "length: 3769\n",
    "\n",
    "\n",
    "| category   | number |\n",
    "|------------|--------|\n",
    "| Pedestrian | 2280   |\n",
    "| Cyclist    | 893    |\n",
    "| Car        | 14385  |\n",
    "\n",
    "\n",
    ":::\n",
    "\n",
    "::::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation : \n",
    "\n",
    "### Training  \n",
    "\n",
    " - Above model was implemented using the below environment:\n",
    "   - PyTorch: 1.7.0\n",
    "   - CUDA Runtime 10.2\n",
    "   - Python: 3.7.16\n",
    "   - CuDNN 7.6.5\n",
    "   - TorchVision: 0.8.1\n",
    "   - MMEngine: 0.7.2\n",
    "   - MMDetection3D: 1.1.0rc3\n",
    " - A similar network with different parameters, was used for Pedestrian and Cyclist Detection\n",
    "    - 2 anchors, smaller anchors, small stride to capture fine details etc.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation : \n",
    "\n",
    "### Training  \n",
    "\n",
    "- The training was done with the exact same hyper parameter as suggested by the authors: \n",
    "    - During training, stochastic gradient descent (SGD) with learning rate 0.01 was used for the first 150 epochs and decreased the learning rate to 0.001 for the last 10 epochs.\n",
    "- Data Augmentation \n",
    "    - perturbation independently to each ground truth 3D bounding box together with those LiDAR points within the box.\n",
    "    - Global scaling."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation : Evaluation\n",
    "\n",
    " \n",
    "- Author follows official KITTI evaluation protocol, which is the most appropriate method to capture object detection performance.\n",
    "- As we can't find True negative in case of object detection so we do not report confusion matrix.\n",
    "- For the same reason we can't find ROC curve, Hence for object detection evaluation we go for Precision-Recall curve, the curve doesn't involve True negative so it is possible to compute.\n",
    "- We first generate prediction using model and find class label then find IOU, precision, recall\n",
    "- Find the area under precision recall curve. which is known as the  Average precision.\n",
    "- There are many method to find the area under this PR curve, for eg. AP11, AP40. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation : Results \n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"50%\"}\n",
    "\n",
    "From Implementation\n",
    "\n",
    "|  Car   | Easy  | Moderate | Hard  | |\n",
    "|-----|-------|----------|-------|-|\n",
    "| BEV | 87.50 | 83.80    | 77.37 | |\n",
    "| 3D  | 80.96 | 64.45    | 61.75 | |\n",
    "\n",
    "\n",
    "|  Ped   | Easy  | Moderate | Hard  | |\n",
    "|-----|-------|----------|-------|-|\n",
    "| BEV | 66.94 | 60.16    | 57.79 | |\n",
    "| 3D  | 56.36 | 54.44    | 49.56 | |\n",
    "\n",
    "\n",
    "|  Cyc   | Easy  | Moderate | Hard  | |\n",
    "|-----|-------|----------|-------|-|\n",
    "| BEV | 73.34 | 51.23    | 51.24 | |\n",
    "| 3D  | 66.23 | 46.32    | 46.23 | |\n",
    "\n",
    ":::\n",
    "\n",
    "::: {.column width=\"50%\"}\n",
    "\n",
    "From Paper\n",
    "\n",
    "| |  Car   | Easy  | Moderate | Hard  |\n",
    "|-|-----|-------|----------|-------|\n",
    "| | BEV | 89.60 | 84.81    | 78.57 |\n",
    "| | 3D  | 81.97 | 65.46    | 62.85 |\n",
    "\n",
    "\n",
    "| |  Ped   | Easy  | Moderate | Hard  |\n",
    "|-|-----|-------|----------|-------|\n",
    "| | BEV | 65.95 | 61.05    | 56.98 |\n",
    "| | 3D  | 57.86 | 53.42    | 48.87 |\n",
    "\n",
    "\n",
    "| |   Cyc  | Easy  | Moderate | Hard  |\n",
    "|-|-----|-------|----------|-------|\n",
    "| | BEV | 74.41 | 52.18    | 50.49 |\n",
    "| | 3D  | 67.17 | 47.65    | 45.11 |\n",
    "\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Novelty  \n",
    "\n",
    "Following Novelty were proposed:\n",
    "\n",
    "- We do not need to predict separately  for positive and negative.\n",
    "- VFE can be improved with the help of another encoder ( eg. transformer ) \n",
    "- One universal network instead of two.\n",
    "- Extend  VoxelNet for joint LiDAR, Radar and image-based end-to-end 3D detection.\n",
    "- Previous time frames can be taken into account improve the detection. \n",
    "\n",
    "Among all these, the below 3 was implemented, others can be implemented \n",
    "\n",
    "- We do not need to predict separately  for positive and negative.\n",
    "- One universal network instead of two.\n",
    "- VFE can be improved with the help of another encoder ( point pillar ) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Novelty  \n",
    "\n",
    "\n",
    "- The Original voxel net has two separate head for predicting Negative and Positive class classification score, \n",
    "- It was changed to  have just one output for classification head  and cross entropy loss was used.\n",
    "- Finally I changed the cross entropy loss to focal loss^[Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollár], While doing hyper parameter tuning, I observed that the cyclist class was not performing well, as we can notice this class has only 734 sample in the training data, which is much lesser compared to other class, \n",
    "- It was found that for gamma = 2.0 and alpha = 0.25 the model performs the best.\n",
    "  $$FL(p_t)=-\\alpha(1-p_t)^\\gamma\\log (p_t)$$\n",
    "  Here $p_t$ is the target class, $\\gamma=2.0$ and $\\alpha=0.25$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Novelty \n",
    "\n",
    "- It was found that instead of training two network one for car, and another for pedestrian and cyclist, it is possible to train just one network if we carefully adjust stride in the network.\n",
    "- Following Point pillars network^[alex, sourabh, holger, lubing, jiong.yang, oscar], I adjusted the stride of the network to 2 for back bone layer, and anchor size as well, and found that the the model was performing good on just one common network architecture. \n",
    "- It can also be noticed that yolo has 1000s of class and just one network for the classes. Yolo does it by careful selection of anchor box.\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Novelty \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"40%\"}\n",
    "\n",
    "- For the voxel encoder part we again follow point pillars network\n",
    "- We borrow only encoder form point pillars not the whole network.\n",
    "- point pillars uses SSD as feature extractor we use RPN as suggested by voxelnet.\n",
    "- Point pillar encodes voxel very differently and more efficiently as compared to voxelnet\n",
    "\n",
    ":::\n",
    "\n",
    "::: {.column width=\"60%\"}\n",
    "\n",
    "![](images/chrome_W2rYqD4tqm.png) \n",
    "\n",
    "::: {style=\"font-size: 50%;\"}\n",
    "\n",
    "Courtesy: [medium](https://becominghuman.ai/pointpillars-3d-point-clouds-bounding-box-detection-and-tracking-pointnet-pointnet-lasernet-67e26116de5a)\n",
    ":::\n",
    "\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"70%\"}\n",
    "![](images/Acrobat_8nIjIYkFMb.png)\n",
    ":::\n",
    "\n",
    "::: {.column width=\"30%\"}\n",
    "- consider $l$ a point in the point cloud with coordinates of $x,y,z$ and reflectance $r$.\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "\n",
    "- Now discretize the point cloud into evenly spaced $x,y$ plane creating a set of pillars $\\mathcal P$, $|\\mathcal P|=B$\n",
    "- There is no hyper parameter in $z$ dimension\n",
    "- Now find $x_c, y_c, z_c, x_p,y_p$ where the $c$ subscript denotes\n",
    "distance to the arithmetic mean of all points in the pillar and\n",
    "the $p$ subscript denotes the offset from the pillar $x, y$ center.\n",
    "- Now the lidar point is 9 dimensional $D=9$ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Point pillar network mentioned $D=9$  but in reality while implementing we need to include reflectance as well so it is actually $10$.\n",
    "- Next, we use a simplified version of PointNet to generate a $(C, P,N)$ sized tensor.\n",
    "- This is followed by a max operation over the channels to create an output tensor of size $(C, P)$\n",
    "  ![](images/chrome_JcI6AbcymW.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "- The point piller is just a simple version of point net \n",
    "\n",
    "  ```python\n",
    "  PillarFeatureNet(\n",
    "    (pfn_layers): ModuleList(\n",
    "      (0): PFNLayer(\n",
    "        (norm): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "        (linear): Linear(in_features=10, out_features=64, bias=False)\n",
    "      )\n",
    "    )\n",
    "  )\n",
    "  ```\n",
    "- After encoding we get 64 dimensional feature which we feed to the back bone, as the encoding shape is different the back bone was modified to accommodate this change.\n",
    "- Our Back bone has 3 layers same as the voxelnet but we used 2D backbone due the nature of the encoded feature.\n",
    "- Once encoded, the features are scattered back to the original pillar locations to create a pseudo-image of size $(C,H,W)$ where $H$ and $W$ indicate the height and width of the canvas.\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Detail of back bone\n",
    "\n",
    "![](images/POWERPNT_yjVxjKIFZ4.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "- Block 1\n",
    "```python\n",
    "    (0): Sequential(\n",
    "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "      (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (2): ReLU(inplace=True)\n",
    "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (4): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (5): ReLU(inplace=True)\n",
    "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (7): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (8): ReLU(inplace=True)\n",
    "      (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (10): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (11): ReLU(inplace=True)\n",
    "    )\n",
    "```\n",
    "- Block 2\n",
    "```python\n",
    "    (1): Sequential(\n",
    "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "      (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (2): ReLU(inplace=True)\n",
    "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (4): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (5): ReLU(inplace=True)\n",
    "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (7): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (8): ReLU(inplace=True)\n",
    "      (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (10): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (11): ReLU(inplace=True)\n",
    "      (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (13): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (14): ReLU(inplace=True)\n",
    "      (15): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (16): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (17): ReLU(inplace=True)\n",
    "    )\n",
    "```\n",
    "- Block 3\n",
    "```python\n",
    "    (2): Sequential(\n",
    "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "      (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (2): ReLU(inplace=True)\n",
    "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (4): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (5): ReLU(inplace=True)\n",
    "      (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (7): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (8): ReLU(inplace=True)\n",
    "      (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (10): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (11): ReLU(inplace=True)\n",
    "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (13): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (14): ReLU(inplace=True)\n",
    "      (15): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (16): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (17): ReLU(inplace=True)\n",
    "    )\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Detail of back bone\n",
    "\n",
    "- For our specific implementation the image size at each block was as follows:\n",
    "    - Block 1\n",
    "      `in: [1, 64, 496, 432], out: [1, 64, 248, 216]`\n",
    "    - Block 2\n",
    "      `in: [1, 64, 248, 216], out: [1, 128, 124, 108]`\n",
    "    - Block 3\n",
    "      `in: [1, 128, 124, 108],out: [1, 256, 62, 54]`\n",
    "- All these 3 block are fed to neck. The neck is very similar to Voxel net :\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation \n",
    "\n",
    "\n",
    "- Neck concatenate all the 3 and produced a tensor with shape [1, 384, 248, 216] and this was fed to the loss calculation.\n",
    "- we find focal loss, for class prediction, after class head ( class head similar to voxel net)\n",
    "- we find bounding box loss after the class head."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation \n",
    "\n",
    "### Neck \n",
    "\n",
    "```python\n",
    "    (0): Sequential(\n",
    "      (0): ConvTranspose2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (2): ReLU(inplace=True)\n",
    "    )\n",
    "    (1): Sequential(\n",
    "      (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
    "      (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (2): ReLU(inplace=True)\n",
    "    )\n",
    "    (2): Sequential(\n",
    "      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(4, 4), bias=False)\n",
    "      (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (2): ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments \n",
    "\n",
    "![](images/Code_mZ4BlvEcw3.png)\n",
    "\n",
    "## Experiments \n",
    "\n",
    "![](images/Code_bzpSd5S747.png)\n",
    "\n",
    "## Experiments \n",
    "\n",
    "![](images/Code_3jguzGHXJl.png)\n",
    "\n",
    "## Experiments \n",
    "\n",
    "![](images/Code_VRuh7ylhGX.png)\n",
    "\n",
    "## Experiments \n",
    "\n",
    "![](images/Code_yVGFxInDz1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results : Quantitative  \n",
    "\n",
    "![](images/EXCEL_0r5dQBNuZJ.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results : Qualitative\n",
    "\n",
    "![](images/001733.png)\n",
    "![](images/007065.png)\n",
    "![](images/001666.png)\n",
    "![](images/007402.png)\n",
    "![](images/002079.png)\n",
    "![](images/007067.png)\n",
    "![](images/007403.png)\n",
    "![](images/001817.png)\n",
    "![](images/002058.png)\n",
    "![](images/001931.png)\n",
    "![](images/002424.png)\n",
    "![](images/001774.png)\n",
    "![](images/001979.png)\n",
    "![](images/001740.png)\n",
    "![](images/002035.png)\n",
    "![](images/002153.png)\n",
    "![](images/007232.png)\n",
    "![](images/002232.png)\n",
    "![](images/002348.png)\n",
    "![](images/002362.png)\n",
    "![](images/002418.png)\n",
    "![](images/002612.png)\n",
    "![](images/003677.png)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referenced papers\n",
    "\n",
    "- [MVX-Net: Multimodal VoxelNet for 3D Object Detection](https://arxiv.org/pdf/1904.01649.pdf)^[MVX-Net: Multimodal VoxelNet for 3D Object Detection Vishwanath A. Sindagi, Yin Zhou, Oncel Tuzel \tarXiv:1904.01649 ]\n",
    "- [ImVoxelNet: Image to Voxels Projection for Monocular and Multi-View General-Purpose 3D Object Detection](https://arxiv.org/pdf/2106.01178.pdf)^[ImVoxelNet: Image to Voxels Projection for Monocular and Multi-View General-Purpose 3D Object Detection Danila Rukhovich, Anna Vorontsova, Anton Konushin \tarXiv:2106.01178]\n",
    "- [MVX-Net: Multimodal VoxelNet for 3D Object Detection](https://arxiv.org/pdf/1904.01649.pdf)^[MVX-Net: Multimodal VoxelNet for 3D Object Detection Vishwanath A. Sindagi, Yin Zhou, Oncel Tuzel \tarXiv:1904.01649 ]\n",
    "- [Group-Free 3D Object Detection via Transformers](https://arxiv.org/pdf/2104.00678.pdf)^[Group-Free 3D Object Detection via Transformers  Ze Liu, Zheng Zhang, Yue Cao, Han Hu, Xin Tong \tarXiv:2104.00678]\n",
    "- [End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point Clouds (Dynamic Voxelization)](https://arxiv.org/pdf/1910.06528.pdf)^[End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point Clouds Yin Zhou, Pei Sun, Yu Zhang, Dragomir Anguelov, Jiyang Gao, Tom Ouyang, James Guo, Jiquan Ngiam, Vijay Vasudevan \tarXiv:1910.06528]\n",
    "- [PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation](https://arxiv.org/abs/1612.00593)^[PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation Charles R. Qi, Hao Su, Kaichun Mo, Leonidas J. Guibas \tarXiv:1612.00593]\n",
    "- [PointPillars: Fast Encoders for Object Detection from Point Clouds](https://arxiv.org/abs/1812.05784)^[PointPillars: Fast Encoders for Object Detection from Point Clouds Alex H. Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, Oscar Beijbom \tarXiv:1812.05784]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- In this work we show that the performance of voxelnet can be improved by designing an improved version of voxel encoder. \n",
    "- The performance of the network highly depends on the input data encoding.\n",
    "- we also show that the one classification head is sufficient, and only one network is sufficient. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$\\mathcal{THANKS!}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0e5e69b8442e8f020791fad6bfcef3777f0e89e0f1a2517b6b628b2eaf0fe66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
