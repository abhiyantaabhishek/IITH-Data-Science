{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "author: Abhishek Kumar  Dubey <br> cs22mds15010\n",
    "badges: True\n",
    "image: images/arch_pic1_a_1.png\n",
    "categories:\n",
    "- 3D vehicle Detection\n",
    "date: '2023-11-18'\n",
    "description: IMPLEMENTATION\n",
    "title: Point Cloud  Attention  based 3D Object Detection\n",
    "title-slide-attributes:\n",
    "    data-background-image: images/FirstPage_background_white.png \n",
    "    data-background-size: contain\n",
    "    data-background-opacity: \"0.6\"\n",
    "subtitle: Point attention network for semantic segmentation of 3D point clouds Feng et al. (2020), published in -  Pattern Recognit. 107, 107446 (2020), CoRR abs/1909.12663 (2019)\n",
    "comments: false\n",
    "format:\n",
    "  revealjs: \n",
    "    theme: mySkyTheme.scss\n",
    "    reference-location: document\n",
    "    incremental: false \n",
    "    smaller: true\n",
    "    scrollable: true\n",
    "    transition: slide\n",
    "    slide-number: true\n",
    "    history: false\n",
    "    chalkboard: \n",
    "      buttons: true\n",
    "    preview-links: auto\n",
    "    logo: images/logo.png\n",
    "    css: styles.css\n",
    "    footer: IIT Hyderabad\n",
    "resources:\n",
    "  - demo.pdf\n",
    "bibliography: references.bib\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"60%\"}\n",
    "The motivations are:\n",
    "\n",
    "- <p align=\"justify\">Safety:Improve safety on the road.</p>\n",
    "- <p align=\"justify\">Traffic Management: Manage traffic flow by identifying areas with high congestion.</p>\n",
    "- <p align=\"justify\">Improved Navigation: Better path planning, trajectory calculation.</p>\n",
    "- <p align=\"justify\">Use LiDAR and image: LiDAR and Images works best when combined together.</p>\n",
    ":::\n",
    "\n",
    "::: {.column width=\"40%\"}\n",
    "![](images/Acrobat_Jy4Az0Sjhm.png)\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "- <p align=\"justify\">Flexibility:can be used in robotics, and augmented reality as well.</p>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"40%\"}\n",
    "- <p align=\"justify\">Develop a solution to locate 3D box in point cloud.</p>\n",
    "- <p align=\"justify\">Encode the point cloud efficiently.</p>\n",
    ":::\n",
    "\n",
    "::: {.column width=\"60%\"}\n",
    "![](images/Acrobat_UClLO5CYIH.png)\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "- <p align=\"justify\">Use transformer directly on the lidar point cloud without voxelizing.</p>\n",
    "- <p align=\"justify\">Develop an accurate, efficient, and robust model that can generalize to new environments and tasks.</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"60%\"}\n",
    "![](images/Acrobat_YOwpXCawvB.png)\n",
    ":::\n",
    "\n",
    "::: {.column width=\"40%\"}\n",
    "- <p align=\"justify\">LiDAR point clouds are inherently sparse.</p>\n",
    "- <p align=\"justify\">TLiDAR point cloud density varies due to sensor range, scanning pattern, and object-sensor pose..\n",
    "- Occlusion issues\n",
    "- Algorithm design challenges\n",
    "- 3D detection hurdles\n",
    "\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existing Methods and Limitations \n",
    "\n",
    "- PointNet [@DBLP:conf/cvpr/QiSMG17]:\n",
    "  - Achieves permutation invariance via symmetric functions.\n",
    "  - Lacks efficient capture of local structures.\n",
    "\n",
    "- VoxelNet [@VoxelNet_2018]:\n",
    "  - Exclusively employs LiDAR data.\n",
    "  - Grid-based Voxelization can sacrifice details, especially at low resolutions.\n",
    "\n",
    "- Point Pillar [@PointPillars_2019]:\n",
    "  - Encodes LiDAR points as pillars, Limits the local resolution.\n",
    "\n",
    "- Pseudo LiDAR [@DBLP:conf/cvpr/WangCGHCW19]:\n",
    "  - Converts depth images to pseudo LiDAR.\n",
    "  - Claimed to suffer from overfitting, as per [@DBLP:conf/iccv/ParkAG0G21]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed approach\n",
    "\n",
    "- 3D Transformer Types:\n",
    "  - Global [@DBLP:conf/cvpr/YuTR00L22]\n",
    "  - Local [@DBLP:conf/cvpr/PanXSLH21]\n",
    "  - Point-wise [@DBLP:journals/cvm/GuoCLMMH21]\n",
    "  - Channel-wise [@DBLP:conf/accv/QiuAB22]\n",
    "- Point-bert Strategy:\n",
    "  - Bert-style pre-training for 3D global [@DBLP:conf/cvpr/YuTR00L22]\n",
    "  - Boosts pure transformer performance but overlooks local features\n",
    "- Global transformers excel in classification; for localization, both local and global features are vital.\n",
    "- Our approach draws from the aforementioned studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "- Lidar points $P=\\{p_1,p_2,\\dots,p_N\\} \\in \\mathbb R^{N \\times D}$ \n",
    "- Embedded feature map $X\\in \\mathbb R^{N \\times C}$\n",
    "- Learnable weight matrices for query $W_Q \\in \\mathbb R^{C \\times C_Q},$ for key $W_K \\in \\mathbb R^{C \\times C_K},$ and for value $W_V\\in \\mathbb R^{C \\times C}$, typically $C_K=C_Q$ \n",
    "- A Typical Transformer used as an encoder, it has 6 components in general\n",
    "\n",
    "   ![Transformer Encoder Architecture, courtesy:  @DBLP:journals/corr/abs-2205-07417](images/2023-09-13_15-16.png){#fig-1}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulation \n",
    "\n",
    "$$\\begin{cases}\n",
    "    \\text{Query}(Q) &=XW_Q \\\\\n",
    "    \\text{Key}(K) &=XW_K \\tag{1}\\\\\n",
    "    \\text{Value}(V) &=XW_V \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "- Query, Key and value are the core part of transformer\n",
    "- When we multiply Query with Key it generates attention map\n",
    "- In the simplest form, if the weights are for key and Query are all 1 it is just a correlation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulation \n",
    "\n",
    "\n",
    "So now attention can be formulated as shown below (Point wise transformer):\n",
    "$$\\text{attention map}=\\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{C_K}} \\right)\\tag{2}$$\n",
    "Channel wise attention:\n",
    "$$\\text{attention map}=\\text{Softmax}\\left(\\frac{Q^TK}{\\sqrt{C_K}} \\right)\\tag{3}$$\n",
    "\n",
    "- Pointwise transformer: spatial relationship\n",
    "- Channelwise transformer: contextual relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point Cloud sparsity example\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"60%\"}\n",
    "![A typical  point colud](images/2023-09-13_17-45.png){#fig-pcloud}\n",
    ":::\n",
    "\n",
    "::: {.column width=\"40%\"}\n",
    " ![Corresponding image of point cloud in @fig-pcloud](images/2023-09-13_17-54.png){#fig-image}\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "\n",
    "- we can see in figure @fig-pcloud how sparse these data are,But in @fig-image for  the same point cloud, the image is well represented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point Cloud sparsity example 2\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"40%\"}\n",
    "![Another example](images/2023-09-13_19-19.png){#fig-pcloud2}\n",
    ":::\n",
    "\n",
    "::: {.column width=\"60%\"}\n",
    "![Corrosponding image of @fig-pcloud2](images/2023-09-13_19-25.png){#fig-immage2}\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "\n",
    "- we can see the here again that in @fig-pcloud2, for far object data is less, but in @fig-immage2, there is good representation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology \n",
    "\n",
    "\n",
    "![Proposed Architecture](images/arch_pic1_a_1.png){#fig-fullArch}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention  Encoder \n",
    "\n",
    "![Attention based Encoder](images/GlobalFeatures.drawio_1.png){#fig-attEncd}\n",
    "\n",
    "\n",
    "- we used transformer to encode point cloud directly so that we can get long range attention map\n",
    "- we used not only point based attention but also channel based attention as well, so that out network can have special attention as well as contextual attention. \n",
    "- we take inspiration from @DBLP:journals/pr/FengZLGM20, __And  as a novelty we add channel wise and point wise attention together, and concat it, also we use FPS,__ our method is completely different from @DBLP:journals/pr/FengZLGM20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backbone\n",
    "\n",
    "![Modified backbone](images/Second.png){#fig-backbone}\n",
    "\n",
    "- we use a modified version of SECOND [@Second] architecture.\n",
    "- we are using only 2D version of it, we do not process data in in way as SECOND [@Second]\n",
    "- The features from 3 different layers goes in parallel to FPN[@FPN]\n",
    "- We use FPN [@FPN] as it is, so we are not showing FPN architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "- The loss is defined as the combination of localization loss, classification loss and directional loss. \n",
    "- For classification we will use focal loss as there is class imbalance issue.\n",
    "- For localization we will use smooth L1 loss, as used in point Pillar [@PointPillars_2019], VoxelNet [@VoxelNet_2018] etc.\n",
    "- The directional loss is simply a cross entropy loss\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathcal  L = \\frac{1}{N}(\\beta_{\\text{loc}}\\mathcal L_{\\text{loc}} + \\beta_{\\text{cls}}\\mathcal L_{\\text{cls}} + \\beta_{\\text{dir}}\\mathcal L_{\\text{dir}})\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment and Result\n",
    "\n",
    "- The Kitti dataset is used for training.\n",
    "- There are 3 class namely car, pedestrian, cyclist, one network shall be trained for all the 3 classes\n",
    "- Adam optimizer was  tried followed by SGD and other, and was be selected based on validation set performance, the same goes for learning rate and other hyperparameter \n",
    "- The $\\gamma,\\beta$ parameter of the learning rate will be selected based on the experiment.\n",
    "- The loss weightage are chosen as per Point Pillar [@PointPillars_2019] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"50%\"}\n",
    "- Training Dataset length =  3712\n",
    "\n",
    "    - Class Distribution \n",
    "\n",
    "        | **category** | **number** |\n",
    "        |--------------|------------|\n",
    "        | Pedestrian   | 2207       |\n",
    "        | Cyclist      | 734        |\n",
    "        | Car          | 14357      |\n",
    ":::\n",
    "::: {.column width=\"50%\"}\n",
    "- Training Dataset length =  3769\n",
    "\n",
    "    - Class Distribution \n",
    "\n",
    "        | **category** | **number** |\n",
    "        |--------------|------------|\n",
    "        | Pedestrian   | 2280       |\n",
    "        | Cyclist      | 893        |\n",
    "        | Car          | 14385      |\n",
    ":::\n",
    "::::\n",
    "\n",
    "\n",
    "- we can see the imbalance in the dataset, hence we are using focal loss for classification \n",
    "- The Evaluation will be done based on AP11 and AP40 as suggested by the  KITTI Benchmark [@DBLP:conf/cvpr/GeigerLU12]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation : \n",
    "\n",
    "- Dual Attention Encoder (__Novelty__)\n",
    "```python\n",
    "    VoxelDualAttentionEncoder(\n",
    "    (linear): Linear(in_features=10, out_features=16, bias=True)\n",
    "    (layers): ModuleList(\n",
    "        (0): TransformerBlock(\n",
    "        (attention): MultiHeadSelfAttention(\n",
    "            (values): Linear(in_features=8, out_features=8, bias=False)\n",
    "            (keys): Linear(in_features=8, out_features=8, bias=False)\n",
    "            (queries): Linear(in_features=8, out_features=8, bias=False)\n",
    "            (fc_out): Linear(in_features=16, out_features=16, bias=True)\n",
    "            (pointWiseFeatureTransform): Conv2d(2, 16, kernel_size=(2, 2), stride=(2, 2))\n",
    "            (channelWiseFeatureTransform): ConvTranspose2d(32, 16, kernel_size=(8, 8), stride=(8, 8))\n",
    "            (combinedFeatureTrnasform): ConvTranspose2d(16, 2, kernel_size=(2, 2), stride=(2, 2))\n",
    "        )\n",
    "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
    "        (dropout): Dropout(p=0.3, inplace=False)\n",
    "        )\n",
    "    )\n",
    "    (dropout): Dropout(p=0.3, inplace=False)\n",
    "    (fc_out): Linear(in_features=16, out_features=64, bias=True)\n",
    "    )\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "- Block 1\n",
    "```python\n",
    "    (0): Sequential(\n",
    "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "      (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (2): ReLU(inplace=True)\n",
    "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (4): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (5): ReLU(inplace=True)\n",
    "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (7): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (8): ReLU(inplace=True)\n",
    "      (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (10): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (11): ReLU(inplace=True)\n",
    "    )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "\n",
    "- Block 2\n",
    "```python\n",
    "    (1): Sequential(\n",
    "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "      (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (2): ReLU(inplace=True)\n",
    "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (4): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (5): ReLU(inplace=True)\n",
    "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (7): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (8): ReLU(inplace=True)\n",
    "      (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (10): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (11): ReLU(inplace=True)\n",
    "      (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (13): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (14): ReLU(inplace=True)\n",
    "      (15): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (16): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (17): ReLU(inplace=True)\n",
    "    )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "- Block 3\n",
    "```python\n",
    "    (2): Sequential(\n",
    "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "      (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (2): ReLU(inplace=True)\n",
    "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (4): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (5): ReLU(inplace=True)\n",
    "      (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (7): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (8): ReLU(inplace=True)\n",
    "      (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (10): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (11): ReLU(inplace=True)\n",
    "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (13): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (14): ReLU(inplace=True)\n",
    "      (15): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "      (16): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (17): ReLU(inplace=True)\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation \n",
    "\n",
    "### Neck \n",
    "\n",
    "```python\n",
    "    (0): Sequential(\n",
    "      (0): ConvTranspose2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "      (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (2): ReLU(inplace=True)\n",
    "    )\n",
    "    (1): Sequential(\n",
    "      (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
    "      (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (2): ReLU(inplace=True)\n",
    "    )\n",
    "    (2): Sequential(\n",
    "      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(4, 4), bias=False)\n",
    "      (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
    "      (2): ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation : \n",
    "\n",
    "### Training  \n",
    "\n",
    "- The training was done with the grid based hyper parameter tuning: \n",
    "    - During training, stochastic gradient descent (SGD) with learning rate 0.01 was used for the first 150 epochs and decreased the learning rate to 0.001 for the last 10 epochs.\n",
    "- Data Augmentation \n",
    "    - perturbation independently to each ground truth 3D bounding box together with those LiDAR points within the box.\n",
    "    - Global scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation : Evaluation\n",
    "\n",
    " \n",
    "- We follows official KITTI evaluation protocol, which is the most appropriate method to capture object detection performance.\n",
    "- As we can't find True negative in case of object detection so we do not report confusion matrix.\n",
    "- For the same reason we can't find ROC curve, Hence for object detection evaluation we go for Precision-Recall curve, the curve doesn't involve True negative so it is possible to compute.\n",
    "- We first generate prediction using model and find class label then find IOU, precision, recall\n",
    "- Find the area under precision recall curve. which is known as the  Average precision.\n",
    "- There are many method to find the area under this PR curve, for eg. AP11, AP40. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses \n",
    "\n",
    "![](images/loss_compare_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Results :Quantitative\n",
    "\n",
    "![](images/metrices_compare.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Results :Qualitative \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"65%\"}\n",
    "![](images/images_000004.png)\n",
    ":::\n",
    "\n",
    "::: {.column width=\"35%\"}\n",
    "![](images/lidar_000004.png)\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"65%\"}\n",
    "![](images/images_000005.png)\n",
    ":::\n",
    "\n",
    "::: {.column width=\"35%\"}\n",
    "![](images/lidar_000005.png)\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"65%\"}\n",
    "![](images/images_000058.png)\n",
    ":::\n",
    "\n",
    "::: {.column width=\"35%\"}\n",
    "![](images/lidar_000058.png)\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"65%\"}\n",
    "![](images/images_000059.png)\n",
    ":::\n",
    "\n",
    "::: {.column width=\"35%\"}\n",
    "![](images/lidar_000059.png)\n",
    ":::\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"65%\"}\n",
    "![](images/images_000062.png)\n",
    ":::\n",
    "\n",
    "::: {.column width=\"35%\"}\n",
    "![](images/lidar_000062.png)\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"65%\"}\n",
    "![](images/images_000098.png)\n",
    ":::\n",
    "\n",
    "::: {.column width=\"35%\"}\n",
    "![](images/lidar_000098.png)\n",
    ":::\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"65%\"}\n",
    "![](images/images_000191.png)\n",
    ":::\n",
    "\n",
    "::: {.column width=\"35%\"}\n",
    "![](images/lidar_000191.png)\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"65%\"}\n",
    "![](images/images_000211.png)\n",
    ":::\n",
    "\n",
    "::: {.column width=\"35%\"}\n",
    "![](images/lidar_000211.png)\n",
    ":::\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future work 1 \n",
    "\n",
    "![Local Features](images/LocalFeatures.png){#fig-local}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future work 1 \n",
    "\n",
    "![OverAll Architecture](images/arch_pic1.png){#fig-strechArch}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future  work 1 \n",
    "\n",
    "\n",
    "- we can also get local features, using patch based network\n",
    "- Point Bert [@DBLP:conf/cvpr/YuTR00L22] used a pre-trained network on point cloud, but they tokenized the point cloud and the performed positional encoding.\n",
    "- But we can do it more efficiently, the point cloud already has position information as it's coordinate, so if we do not tokenize it we can utilize the coordinate as positional encoding feature.\n",
    "- __Novelty__: Use patch based attention encoder to get local feature, use the coordinate location as positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future work  2 \n",
    "\n",
    "![Proposed Architecture](images/overAll.drawio.png){#fig-arch}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future work  2 \n",
    "\n",
    "\n",
    "- @fig-arch shows the over all architecture, we extract global and local  features from image and 3D point cloud, these features are extracted from a transformer based encoder, having point and channel wise attention.\n",
    "- these features are then fused together with a cross attention mechanism as explained in CAT-Det [@DBLP:conf/cvpr/ZhangC022]  \n",
    "- __Novelty__ : Use feature based multi modality  fusion of channel wise attention and point wise attention, for cross attention use both global and local attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0e5e69b8442e8f020791fad6bfcef3777f0e89e0f1a2517b6b628b2eaf0fe66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
