{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "author: Abhishek Kumar  Dubey <br> cs22mds15010\n",
    "badges: True\n",
    "categories:\n",
    "- 3D vehicle Detection\n",
    "date: '2023-03-12'\n",
    "description: VoxelNet- End-to-End Learning\n",
    "title: Point cloud based 3d object detection\n",
    "title-slide-attributes:\n",
    "    data-background-image: images/heading.png \n",
    "    data-background-size: contain\n",
    "    data-background-opacity: \"0.6\"\n",
    "subtitle: Y. Zhou and O. Tuzel,Voxelnet, IEEE Conference on CVPR, pp. 4490-4499, 2018.\n",
    "comments: false\n",
    "format:\n",
    "  revealjs: \n",
    "    theme: sky\n",
    "    reference-location: document\n",
    "    incremental: false \n",
    "    smaller: true\n",
    "    scrollable: true\n",
    "    transition: slide\n",
    "    slide-number: true\n",
    "    chalkboard: \n",
    "      buttons: true\n",
    "    preview-links: auto\n",
    "    logo: images/logo.png\n",
    "    css: styles.css\n",
    "    footer: IIT Hyderabad\n",
    "resources:\n",
    "  - demo.pdf\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"60%\"}\n",
    "The motivations are:\n",
    "\n",
    "- <p align=\"justify\">Safety:Improve safety on the road.</p>\n",
    "- <p align=\"justify\">Traffic Management: Manage traffic flow by identifying areas with high congestion.</p>\n",
    "- <p align=\"justify\">Improved Navigation: Better path planning, trajectory calculation.</p>\n",
    "- <p align=\"justify\">Use LiDAR : Compared to image based detection, LiDAR provides reliable depth information that can be used to accurately localize objects and characterize their shapes.</p>\n",
    ":::\n",
    "\n",
    "::: {.column width=\"40%\"}\n",
    "![](images/Acrobat_Jy4Az0Sjhm.png)\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "- <p align=\"justify\">Flexibility:can be used robotics, and augmented reality as well.</p>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"40%\"}\n",
    "- <p align=\"justify\">Develop a solution that can accurately locate a object in 3D space using LiDAR point Cloud.</p>\n",
    "- <p align=\"justify\">The solution must not use any hand-crafted features or prior assumptions, which can limit accuracy and generalization.</p>\n",
    ":::\n",
    "\n",
    "::: {.column width=\"60%\"}\n",
    "![](images/Acrobat_UClLO5CYIH.png)\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "- <p align=\"justify\">Directly process raw LiDAR point clouds.</p>\n",
    "- <p align=\"justify\">Develop an accurate, efficient, and robust model that can generalize to new environments and tasks.</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"40%\"}\n",
    "![](images/Acrobat_YOwpXCawvB.png)\n",
    ":::\n",
    "\n",
    "::: {.column width=\"60%\"}\n",
    "- <p align=\"justify\">LiDAR point clouds are inherently sparse.</p>\n",
    "- <p align=\"justify\">The point density of LiDAR point clouds can vary significantly depending on factors such as the sensor's effective range, scanning pattern, and the relative pose of the object and the sensor.\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "- <p align=\"justify\">Occlusion can also result in missing or incomplete point cloud data, further exacerbating the sparsity of the data.</p>\n",
    "- <p align=\"justify\">Variations in the point density of LiDAR point clouds can make it difficult to design effective feature extraction and object detection algorithms.</p>\n",
    "- <p align=\"justify\">Addressing the sparsity and variability of LiDAR point clouds is a key challenge in developing accurate and robust 3D object detection models.</p>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existing methods and Limitations \n",
    "\n",
    "-  <p align=\"justify\">Use pseudo Lidar instead of original Lidar and create the point cloud from the RGB image.However, these kind of method does not perform well, Because extracting point cloud from 2D images are not efficient. There are no depth information present in the 2D image.^[Yan, Wang., Wei-Lun, Chao., Divyansh, Garg., Bharath, Hariharan., Mark, Campbell., Kilian, Q., Weinberger. (2019). Pseudo-LiDAR From Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving.  8445-8453. doi: 10.1109/CVPR.2019.00864]</p>\n",
    "- <p align=\"justify\">Some methods for 3D object detection from LiDAR point clouds converts the point cloud data into a 2D image-like format, and then use techniques typically used for analyzing images to identify and classify objects.^[C. Premebida, J. Carreira, J. Batista, and U. Nunes. Pedestrian detection combining RGB and dense LIDAR data. In IROS, pages 0â€“1. IEEE, Sep 2014. 1, 2] ^[A. Gonzalez, G. Villalonga, J. Xu, D. Vazquez, J. Amores, and A. Lopez. Multiview random forest of local experts combining rgb and lidar data for pedestrian detection. In IEEE Intelligent Vehicles Symposium (IV), 2015. 1, 2]</p>\n",
    "- <p align=\"justify\">Another method for 3D object detection from LiDAR point clouds involves converting the point cloud data into a 3D grid of small cubes (called voxels), and then manually designing features to represent the objects within each voxel.^[A. Gonzalez, G. Villalonga, J. Xu, D. Vazquez, J. Amores, and A. Lopez. Multiview random forest of local experts combining rgb and lidar data for pedestrian detection. In IEEE Intelligent Vehicles Symposium (IV), 2015. 1, 2M. Engelcke, D. Rao, D. Z.Wang, C. H. Tong, and I. Posner. Vote3deep: Fast object detection in 3d point clouds using efficient convolutional neural networks. In 2017 IEEE International Conference on Robotics and Automation (ICRA),]</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VoxelNet Introduction \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"40%\"}\n",
    "-  <p align=\"justify\">it is a  generic 3D detection framework that simultaneously learns a discriminative feature representation from point clouds and predicts accurate 3D bounding boxes </p> \n",
    "- <p align=\"justify\">VoxelNet directly operates on the raw point cloud.</p>\n",
    ":::\n",
    "\n",
    "::: {.column width=\"60%\"}\n",
    "![](images/Acrobat_I9iR951esL.png)\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VoxelNet Architecture \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"70%\"}\n",
    "![](images/Acrobat_ZOIOLjvhTa.png)\n",
    ":::\n",
    "\n",
    "::: {.column width=\"30%\"}\n",
    "The VoxelNet consists of three functional blocks\n",
    "\n",
    "1. Feature learning network\n",
    "2. Convolutional middle layers\n",
    "3. Region proposal network\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Learning Network \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"60%\"}\n",
    "### Voxel Partition\n",
    "\n",
    "- Suppose the point cloud encompasses 3D space with range $D, H, W$ along the $Z, Y, X$ axes respectively.\n",
    "- The resulting 3D voxel grid is of size $D'=D/v_D, H'=H/v_H,  W'=W/v_W$ where $v_D,v_H,v_W$ defines the voxel size accordingly \n",
    "- Point density in LiDAR point clouds is variable, resulting in varying numbers of points within each voxel.\n",
    ":::\n",
    "\n",
    "::: {.column width=\"40%\"}\n",
    "![](images/Acrobat_ffA1dvD6zy.png)\n",
    ":::\n",
    "\n",
    "::::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "\n",
    "Feature Learning Network cont ... \n",
    "\n",
    "### Random Sampling\n",
    "- A high-definition LiDAR point cloud typically has around 100k points.\n",
    "- Directly processing all points can overload computing resources and create biased detection due to variable point density.\n",
    "- To avoid this, a fixed number of points (T) is randomly sampled from voxels that contain more than T points.\n",
    "- This sampling strategy has two purposes:\n",
    "    - computational savings and\n",
    "    - decreases the imbalance of points between the voxels which reduces the sampling bias, and adds more variation to training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "\n",
    "Feature Learning Network cont ... \n",
    "\n",
    "### Stacked Voxel Feature Encoding\n",
    "\n",
    "- let ${\\bf V}\\,=\\,\\{{\\bf p}_i\\,=\\,[x_{i},y_{i},z_{i},r_{i}]^{T}\\,\\in\\,\\mathbb{R}^{4}\\}_{i=1...t}$ as a non-empty voxel containing $t <T$ LiDAR points.\n",
    "- Here ${\\bf p}_i$ contains $XYZ$ coordinates for the $i^{\\text{th}}$ point and $r_i$ is the received reflectance.\n",
    "- Now find ${\\mathrm{\\bf V}}_{\\mathrm{in}}\\,=\\,\\{\\;\\mathrm{\\hat {\\bf p}_i}\\,=\\,[x_{i},y_{i},z_{i},r_{i},x_{i}-v_{x},y_{i}-v_{y},z_{i}-v_{z}]^{T}\\in\\mathbb{R}^{7}\\}_{i=1\\ldots t}.$ Here $(v_{x},v_{y},v_{z})$ is the local mean as the centroid of all the points in ${\\bf V}$\n",
    "- Next, each $\\hat {\\bf p}_i$ is transformed through the fully connected network (FCN) into a feature space.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "\n",
    "Feature Learning Network. VFE cont ... \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"40%\"}\n",
    "- Information from point feature ${\\bf f}_{i}\\ \\in\\ \\mathbb{R}^{m}$ is aggregated __to encode the shape of the surface contained within the voxel__\n",
    "- The FCN is composed of a linear layer, a batch normalization (BN) layer, and a rectified linear unit (ReLU) layer\n",
    "\n",
    ":::\n",
    "\n",
    "::: {.column width=\"60%\"}\n",
    "![](images/Acrobat_HibvuJQPZU.png)\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "- Elementwise MaxPooling across all $\\bf f_i$ is used to __get the locally aggregated feature__ $\\hat{\\mathbf{f}}\\ \\in\\ \\mathbb{R}^{m}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    "\n",
    "Feature Learning Network. VFE cont ... \n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"80%\"}\n",
    "- Now form the form the point-wise concatenated feature as $\\mathbf{f}_{i}^{o u t}\\,=\\,\\left[\\mathbf{f}_{i}^{T},{\\tilde{\\mathbf{f}}}^{T}\\right]^{T}\\,\\in\\,\\mathbb{R}^{2m}.$\n",
    "- Finally we get ${\\bf V_{\\ o u t}}\\ =\\ \\{{\\bf f}_{i}^{o u t}\\}_{i...t}.$\n",
    "- The output feature combines point-wise and locally aggregated features.\n",
    "- Stacking VFE layers allows encoding of point interactions within a voxel, __enabling the final feature representation to learn descriptive shape information__.\n",
    ":::\n",
    "\n",
    "::: {.column width=\"20%\"}\n",
    "![](images/chrome_Tb0CmOU9L4.png)\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "\n",
    "- Voxel features, uniquely associated with their spatial coordinates, are obtained by __processing only non-empty voxels__, resulting in a sparse 4D tensor representation.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Convolutional Middle Layers \n",
    "\n",
    "- Each convolutional middle layer applies 3D convolution, BN layer, and ReLU layer sequentially.\n",
    "- The convolutional middle layers aggregate voxel-wise features within a __progressively expanding receptive field, adding more context to the shape description__\n",
    "- Depending on the task this layer' details can vary.\n",
    "\n",
    "<br><br>\n",
    "![](images/chrome_jVqU0WaxpR.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Region Proposal Network \n",
    " \n",
    " - RPN is combined with  the feature learning network and convolutional middle layers to form an __end-to-end trainable pipeline__.\n",
    "   ![](images/Acrobat_gayrjAQFrA.png)\n",
    "- The first layer of each block downsamples the feature map by half. via convolution with a stride of 2\n",
    "- Followed by a sequence of convolutions of stride 1, BN and ReLU\n",
    "- The output of each block is upsampled to a fixed size and concatenated to construct a high-resolution feature map."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Loss Function \n",
    "\n",
    "- 3D ground truth box is parameterized as $(x_{c}^{g},y_{c}^{g},z_{c}^{g},l^{g},w^{g},h^{g},\\theta^{g}),$ where $x_{c}^{g},y_{c}^{g},z_{c}^{g}$ represent the center location, $l^{g},w^{g},h^{g}$ are length, width, height of the box,${\\theta}^{g}$ is the yaw rotation.\n",
    "- To retrieve the ground truth box from a matching positive anchor parameterized as $(x_{c}^{a},y_{c}^{a},z_{c}^{a},l^{a},w^{a},h^{a},\\theta^{a})$ a vector $\\mathbf{u}^{*}\\in\\mathbb{R}^{7}$ containing below 7 elements are calculated as below:\n",
    "  $$\\begin{align*}\n",
    "      &\\Delta x={\\frac{x_{c}^{g}-x_{c}^{a}}{d^{a}}},\\Delta y={\\frac{y_{c}^{g}-y_{c}^{a}}{d^{a}}},\\Delta z={\\frac{z_{c}^{g}-z_{c}^{a}}{h^{a}}},\\\\\n",
    "      &\\Delta l=\\log(\\frac{l^{g}}{l^{a}}),\\Delta w=\\log(\\frac{w^{g}}{w^{a}}),\\Delta h=\\log(\\frac{h^{g}}{h^{a}}), \\tag{1} \\\\\n",
    "      &\\Delta\\theta=\\theta^{g}-\\theta^{a}\n",
    "  \\end{align*}$$\n",
    "  Where  $\\;d^{a}\\,=\\,\\sqrt{(l^{a})^{2}+(w^{a})^{2}}$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   \n",
    "\n",
    "Loss Function cont ...\n",
    "\n",
    "- Now we find loss as shown below \n",
    "  $$\\begin{align*}\n",
    "     L&=\\alpha{\\frac{1}{N_{\\mathrm{pos}}}}\\sum_{i}L_{\\mathrm{cls}}(p_{i}^{\\mathrm{pos}},1)\\\\ \n",
    "     &+\\beta{\\frac{1}{N_{\\mathrm{neg}}}}\\sum_{j}L_{\\mathrm{cls}}(p_{j}^{\\mathrm{neg}},0)\\\\ \n",
    "     &+\\frac{1}{N_{\\mathrm{pos}}}\\sum_{i}L_{\\mathrm{reg}}({\\bf u}_{i},{\\bf u}_{i}^{*}) \\tag{2}\n",
    "  \\end{align*}$$\n",
    "- where $p_{i}^{\\mathrm{pos}}$ and $p_{j}^{\\mathrm{neg}}$ represent the softmax output for positive anchor $a_{i}^{\\mathrm{pos}}$ and negative anchor $a_{j}^{\\mathrm{neg}}$ respectively,\n",
    "- $\\mathbf{u}_{i}\\ \\in\\ \\mathbb{R}^{7}$ and $\\mathbf{u}_{i}^{*}\\ \\in\\ \\mathbb{R}^{7}$ are the regression output and ground truth for positive anchor $a_{i}^{\\mathrm{pos}}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Details  \n",
    "\n",
    "### Car Detection\n",
    "\n",
    "- Point clouds within the range of $[-3,1]\\times[-40,40]\\times[0,70.4]$ meters along $X,Y,Z$ respectively is considered. \n",
    "- $v_{D}\\,=\\,0.4,v_{H}\\,=\\,0.2,v_{W}\\,=\\,0.2$\n",
    "- $D^{\\prime}\\;=\\;10,\\;H^{\\prime}\\;=\\;400,\\;W^{\\prime}\\;=\\;352$\n",
    "- The maximum number of randomly sampled points in each non-empty voxel $T\\,=\\,35$\n",
    "- For $\\mathrm{VFE}-1(7, 32)$ and and $\\mathrm{VFE}-2(32, 128)$ was used.\n",
    "- The final $\\mathrm{FCN}$  maps $\\mathrm{VFE}$ output to $\\mathbb{R}^{128}$\n",
    "- Thus the feature learning net generates a sparse tensor of shape $128\\times10\\times400\\times352$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  \n",
    " \n",
    " Network Details, Car Detection cont ...\n",
    " \n",
    "- To aggregate voxel-wise features, three convolution middle layers was employed sequentially as $\\mathrm{Conv3D}(128, 64, 3,(2,1,1), (1,1,1)),$ $\\mathrm{Conv3D}(64, 64, 3, (1,1,1), (0,1,1)),$ and $\\mathrm{Conv3D}(64, 64, 3, (2,1,1), (1,1,1)),$\n",
    "- which yields a 4D tensor of size $64\\times2\\times400\\times352.$\n",
    "- After reshaping, size becomes $128\\times400\\times352.$ and is input to RPN.\n",
    "- Only one anchor size was used $l^{a}\\,=\\,3.9.\\,w^{a}\\,=\\,1.6.\\,h^{a}\\,=\\,1.56$ meters centered at $z_{c}^{a}\\,=\\,-1.0$ meters with two rotations, $0^\\circ$ and $90^\\circ$ \n",
    "- Positive anchor: if it has the highest Intersection over Union (IoU) with a ground truth or its IoU with ground truth is above 0.6 (in birdâ€™s eye view).\n",
    "- Positive anchor: if the IoU between it and all ground truth boxes is less than 0.45.\n",
    "- Set $\\alpha=1.5$ and $\\beta=1$ in equation $(2)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training  \n",
    "\n",
    " \n",
    " - A similar network with different parameters, was used for Pedestrian and Cyclist Detection\n",
    "    - 2 anchors, smaller anchors, small stride to capture fine details etc.\n",
    "- During training, stochastic gradient descent (SGD) with learning rate 0.01 was used for the first 150 epochs and decreased the learning rate to 0.001 for the last 10 epochs.\n",
    "- Data Augmentation \n",
    "    - perturbation independently to each ground truth 3D bounding box together with those LiDAR points within the box.\n",
    "    - Global scaling."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments  \n",
    " \n",
    "- VoxelNet evaluated on KITTI 3D object detection benchmark\n",
    "- 7,481 training images/point clouds and 7,518 test images/point clouds\n",
    "- Three categories: Car, Pedestrian, and Cyclist\n",
    "- Detection outcomes evaluated based on three difficulty levels: easy, moderate, and hard\n",
    "- Evaluation protocol based on object size, occlusion state, and truncation level\n",
    "- 3,712 data samples for training and 3,769 for validation\n",
    "- Hand-crafted baseline (HC-baseline) was implemented and trained to evaluate the importance of end-to-end learning.\n",
    "- HC-baseline uses the birdâ€™s eye view features described in Multi-view 3d object detection network for autonomous driving^[X. Chen, H. Ma, J. Wan, B. Li, and T. Xia. Multi-view 3d object detection network for autonomous driving. In IEEE CVPR, 2017.] which are computed at 0.1 m resolution.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative results.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Performance comparison in 3D detection: average precision (in %) on KITTI validation set \n",
    "  ![](images/Acrobat_FtLnL5LR5N.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Qualitative results for car\n",
    "\n",
    "<br>\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"50%\"}\n",
    "![](images/Acrobat_DslR4apvOA.png)\n",
    "![](images/Acrobat_33FtFdyehe.png)\n",
    ":::\n",
    "\n",
    "::: {.column width=\"50%\"}\n",
    "![](images/Acrobat_6H3QmfEkNv.png)\n",
    "![](images/Acrobat_sTmPlMHP48.png)\n",
    ":::\n",
    "\n",
    "::::\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Qualitative results for pedestrian\n",
    "\n",
    "<br>\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"50%\"}\n",
    "![](images/Acrobat_9Tjq4xFei9.png)\n",
    "![](images/Acrobat_DgRwmDIplZ.png)\n",
    ":::\n",
    "\n",
    "::: {.column width=\"50%\"}\n",
    "![](images/Acrobat_6JoySlaJbb.png)\n",
    "![](images/Acrobat_Dtk61MhKCI.png)\n",
    ":::\n",
    "\n",
    "::::\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Qualitative results for cyclist\n",
    "\n",
    "<br>\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"50%\"}\n",
    "![](images/Acrobat_WoOn9jcs4T.png)\n",
    "![](images/Acrobat_HZMsnkVH36.png)\n",
    ":::\n",
    "\n",
    "::: {.column width=\"50%\"}\n",
    "![](images/Acrobat_14gpVJ4qbh.png)\n",
    "![](images/Acrobat_FTiptHGBZ0.png)\n",
    ":::\n",
    "\n",
    "::::\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Conclusion   \n",
    "\n",
    "- Existing LiDAR-based 3D detection methods rely on hand-crafted features\n",
    "- VoxelNet proposes an end-to-end trainable deep architecture for point cloud-based 3D detection\n",
    "- VoxelNet can operate directly on sparse 3D points and efficiently capture 3D shape information\n",
    "- VoxelNet outperforms state-of-the-art LiDAR-based 3D detection methods on the KITTI car detection task\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Novelty  \n",
    "\n",
    "#### Short term achievable  \n",
    "- Extend  VoxelNet for joint LiDAR and image-based end-to-end 3D detection.\n",
    "- We do not need to predict separately  for positive and negative.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Requires much involved research\n",
    "\n",
    "- Previous time frames can be taken into account improve the detection. \n",
    "- Voxel Feature Encoding can be improved with the help of transformer positional encoding. \n",
    "- One universal network instead of two.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$\\mathcal{THANKS!}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0e5e69b8442e8f020791fad6bfcef3777f0e89e0f1a2517b6b628b2eaf0fe66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
