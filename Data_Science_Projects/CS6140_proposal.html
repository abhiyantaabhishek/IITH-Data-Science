<!DOCTYPE html>
<html lang="en"><head>
<link href="../logo.png" rel="icon" type="image/png">
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.39">

  <meta name="author" content="Abhishek Kumar Dubey   cs22mds15010">
  <meta name="dcterms.date" content="2023-09-16">
  <title>Point Cloud Attention based 3D Object Detection</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-211b605157fec2d62feaced7e933c580.css">
  <link rel="stylesheet" href="styles.css">
  <script async="" src="https://www.googletagmanager.com/gtag/js?id=G-5ZQX02R26E"></script>

  <script type="text/javascript">

  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-5ZQX02R26E', { 'anonymize_ip': true});
  </script>
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
<meta property="og:title" content="Point Cloud Attention based 3D Object Detection">
<meta property="og:description" content="PROPOSAL">
<meta property="og:image" content="http://localhost:4200/Data_Science_Projects/images/arch_pic1_a_1.png">
<meta property="og:image:height" content="437">
<meta property="og:image:width" content="837">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="images/FirstPage_background_white.png" data-background-opacity="0.6" data-background-size="contain" class="quarto-title-block center">
  <h1 class="title">Point Cloud Attention based 3D Object Detection</h1>
  <p class="subtitle">Point attention network for semantic segmentation of 3D point clouds Feng et al.&nbsp;(2020), published in - Pattern Recognit. 107, 107446 (2020), CoRR abs/1909.12663 (2019)</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Abhishek Kumar Dubey <br> cs22mds15010 
</div>
</div>
</div>

  <p class="date">2023-09-16</p>
</section>
<section id="motivation" class="slide level2">
<h2>Motivation</h2>
<div class="columns">
<div class="column" style="width:60%;">
<p>The motivations are:</p>
<ul>
<li><p align="justify">
Safety:Improve safety on the road.
</p></li>
<li><p align="justify">
Traffic Management: Manage traffic flow by identifying areas with high congestion.
</p></li>
<li><p align="justify">
Improved Navigation: Better path planning, trajectory calculation.
</p></li>
<li><p align="justify">
Use LiDAR and image: LiDAR and Images works best when combined together.
</p></li>
</ul>
</div><div class="column" style="width:40%;">
<p><img data-src="images/Acrobat_Jy4Az0Sjhm.png"></p>
</div></div>
<ul>
<li><p align="justify">
Flexibility:can be used in robotics, and augmented reality as well.
</p></li>
</ul>
</section>
<section id="problem-statement" class="slide level2">
<h2>Problem statement</h2>
<div class="columns">
<div class="column" style="width:40%;">
<ul>
<li><p align="justify">
Develop a solution to locate 3D box in point cloud.
</p></li>
<li><p align="justify">
Encode the point cloud efficiently.
</p></li>
</ul>
</div><div class="column" style="width:60%;">
<p><img data-src="images/Acrobat_UClLO5CYIH.png"></p>
</div></div>
<ul>
<li><p align="justify">
Use transformer directly on the lidar point cloud without voxelizing.
</p></li>
<li><p align="justify">
Develop an accurate, efficient, and robust model that can generalize to new environments and tasks.
</p></li>
</ul>
</section>
<section id="challenges" class="slide level2">
<h2>Challenges</h2>
<div class="columns">
<div class="column" style="width:60%;">
<p><img data-src="images/Acrobat_YOwpXCawvB.png"></p>
</div><div class="column" style="width:40%;">
<ul>
<li><p align="justify">
LiDAR point clouds are inherently sparse.
</p></li>
<li><p align="justify">
TLiDAR point cloud density varies due to sensor range, scanning pattern, and object-sensor pose..</p></li>
<li>Occlusion issues</li>
<li>Algorithm design challenges</li>
<li>3D detection hurdles</li>
</ul>
</div></div>
</section>
<section id="existing-methods-and-limitations" class="slide level2">
<h2>Existing Methods and Limitations</h2>
<ul>
<li>PointNet <span class="citation" data-cites="DBLP:conf/cvpr/QiSMG17">(<a href="#/references" role="doc-biblioref" onclick="">Qi et al. 2017</a>)</span>:
<ul>
<li>Achieves permutation invariance via symmetric functions.</li>
<li>Lacks efficient capture of local structures.</li>
</ul></li>
<li>VoxelNet <span class="citation" data-cites="VoxelNet_2018">(<a href="#/references" role="doc-biblioref" onclick="">Zhou and Tuzel 2018</a>)</span>:
<ul>
<li>Exclusively employs LiDAR data.</li>
<li>Grid-based Voxelization can sacrifice details, especially at low resolutions.</li>
</ul></li>
<li>Point Pillar <span class="citation" data-cites="PointPillars_2019">(<a href="#/references" role="doc-biblioref" onclick="">Lang et al. 2019</a>)</span>:
<ul>
<li>Encodes LiDAR points as pillars, Limits the local resolution.</li>
</ul></li>
<li>Pseudo LiDAR <span class="citation" data-cites="DBLP:conf/cvpr/WangCGHCW19">(<a href="#/references" role="doc-biblioref" onclick="">Wang et al. 2019</a>)</span>:
<ul>
<li>Converts depth images to pseudo LiDAR.</li>
<li>Claimed to suffer from overfitting, as per <span class="citation" data-cites="DBLP:conf/iccv/ParkAG0G21">(<a href="#/references" role="doc-biblioref" onclick="">Park et al. 2021</a>)</span>.</li>
</ul></li>
</ul>
</section>
<section id="proposed-approach" class="slide level2">
<h2>Proposed approach</h2>
<ul>
<li>3D Transformer Types:
<ul>
<li>Global <span class="citation" data-cites="DBLP:conf/cvpr/YuTR00L22">(<a href="#/references" role="doc-biblioref" onclick="">Yu et al. 2022</a>)</span></li>
<li>Local <span class="citation" data-cites="DBLP:conf/cvpr/PanXSLH21">(<a href="#/references" role="doc-biblioref" onclick="">Pan et al. 2021</a>)</span></li>
<li>Point-wise <span class="citation" data-cites="DBLP:journals/cvm/GuoCLMMH21">(<a href="#/references" role="doc-biblioref" onclick="">Guo et al. 2021</a>)</span></li>
<li>Channel-wise <span class="citation" data-cites="DBLP:conf/accv/QiuAB22">(<a href="#/references" role="doc-biblioref" onclick="">Qiu, Anwar, and Barnes 2022</a>)</span></li>
</ul></li>
<li>Point-bert Strategy:
<ul>
<li>Bert-style pre-training for 3D global <span class="citation" data-cites="DBLP:conf/cvpr/YuTR00L22">(<a href="#/references" role="doc-biblioref" onclick="">Yu et al. 2022</a>)</span></li>
<li>Boosts pure transformer performance but overlooks local features</li>
</ul></li>
<li>Global transformers excel in classification; for localization, both local and global features are vital.</li>
<li>Our approach draws from the aforementioned studies.</li>
</ul>
</section>
<section id="notation" class="slide level2">
<h2>Notation</h2>
<ul>
<li><p>Lidar points <span class="math inline">\(P=\{p_1,p_2,\dots,p_N\} \in \mathbb R^{N \times D}\)</span></p></li>
<li><p>Embedded feature map <span class="math inline">\(X\in \mathbb R^{N \times C}\)</span></p></li>
<li><p>Learnable weight matrices for query <span class="math inline">\(W_Q \in \mathbb R^{C \times C_Q},\)</span> for key <span class="math inline">\(W_K \in \mathbb R^{C \times C_K},\)</span> and for value <span class="math inline">\(W_V\in \mathbb R^{C \times C}\)</span>, typically <span class="math inline">\(C_K=C_Q\)</span></p></li>
<li><p>A Typical Transformer used as an encoder, it has 6 components in general</p>
<div id="fig-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img data-src="images/2023-09-13_15-16.png">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Transformer Encoder Architecture, courtesy: <span class="citation" data-cites="DBLP:journals/corr/abs-2205-07417">Lu et al. (<a href="#/references" role="doc-biblioref" onclick="">2022</a>)</span>
</figcaption>
</figure>
</div></li>
</ul>
</section>
<section id="formulation" class="slide level2">
<h2>Formulation</h2>
<p><span class="math display">\[\begin{cases}
    \text{Query}(Q) &amp;=XW_Q \\
    \text{Key}(K) &amp;=XW_K \tag{1}\\
    \text{Value}(V) &amp;=XW_V \\
\end{cases}\]</span></p>
<ul>
<li>Query, Key and value are the core part of transformer</li>
<li>When we multiply Query with Key it generates attention map</li>
<li>In the simplest form, if the weights are for key and Query are all 1 it is just a correlation.</li>
</ul>
</section>
<section id="formulation-1" class="slide level2">
<h2>Formulation</h2>
<p>So now attention can be formulated as shown below (Point wise transformer): <span class="math display">\[\text{attention map}=\text{Softmax}\left(\frac{QK^T}{\sqrt{C_K}} \right)\tag{2}\]</span> Channel wise attention: <span class="math display">\[\text{attention map}=\text{Softmax}\left(\frac{Q^TK}{\sqrt{C_K}} \right)\tag{3}\]</span></p>
<ul>
<li>Pointwise transformer: spatial relationship</li>
<li>Channelwise transformer: contextual relationship.</li>
</ul>
</section>
<section id="point-cloud-sparsity-example" class="slide level2">
<h2>Point Cloud sparsity example</h2>
<div class="columns">
<div class="column" style="width:60%;">
<div id="fig-pcloud" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-pcloud-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img data-src="images/2023-09-13_17-45.png">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pcloud-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: A typical point colud
</figcaption>
</figure>
</div>
</div><div class="column" style="width:40%;">
<div id="fig-image" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-image-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img data-src="images/2023-09-13_17-54.png">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-image-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Corresponding image of point cloud in <a href="#/fig-pcloud" class="quarto-xref">Figure&nbsp;2</a>
</figcaption>
</figure>
</div>
</div></div>
<ul>
<li>we can see in figure <a href="#/fig-pcloud" class="quarto-xref">Figure&nbsp;2</a> how sparse these data are,But in <a href="#/fig-image" class="quarto-xref">Figure&nbsp;3</a> for the same point cloud, the image is well represented</li>
</ul>
</section>
<section id="point-cloud-sparsity-example-2" class="slide level2">
<h2>Point Cloud sparsity example 2</h2>
<div class="columns">
<div class="column" style="width:40%;">
<div id="fig-pcloud2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-pcloud2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img data-src="images/2023-09-13_19-19.png">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pcloud2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Another example
</figcaption>
</figure>
</div>
</div><div class="column" style="width:60%;">
<div id="fig-immage2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-immage2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img data-src="images/2023-09-13_19-25.png">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-immage2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Corrosponding image of <a href="#/fig-pcloud2" class="quarto-xref">Figure&nbsp;4</a>
</figcaption>
</figure>
</div>
</div></div>
<ul>
<li>we can see the here again that in <a href="#/fig-pcloud2" class="quarto-xref">Figure&nbsp;4</a>, for far object data is less, but in <a href="#/fig-immage2" class="quarto-xref">Figure&nbsp;5</a>, there is good representation</li>
</ul>
</section>
<section id="methodology" class="slide level2">
<h2>Methodology</h2>

<img data-src="images/arch_pic1_a_1.png" class="r-stretch quarto-figure-center" id="fig-fullArch"><p class="caption">
Figure&nbsp;6: Proposed Architecture
</p></section>
<section id="attention-encoder" class="slide level2">
<h2>Attention Encoder</h2>

<img data-src="images/GlobalFeatures.png" class="r-stretch quarto-figure-center" id="fig-attEncd"><p class="caption">
Figure&nbsp;7: Attention based Encoder
</p><ul>
<li>we used transformer to encode point cloud directly so that we can get long range attention map</li>
<li>we used not only point based attention but also channel based attention as well, so that out network can have special attention as well as contextual attention.</li>
<li>we take inspiration from <span class="citation" data-cites="DBLP:journals/pr/FengZLGM20">Feng et al. (<a href="#/references" role="doc-biblioref" onclick="">2020</a>)</span>, <strong>And as a novelty we add channel wise and point wise attention together, and concat it, also we use FPS,</strong> our method is completely different from <span class="citation" data-cites="DBLP:journals/pr/FengZLGM20">Feng et al. (<a href="#/references" role="doc-biblioref" onclick="">2020</a>)</span></li>
</ul>
</section>
<section id="backbone" class="slide level2">
<h2>Backbone</h2>

<img data-src="images/Second.png" class="r-stretch quarto-figure-center" id="fig-backbone"><p class="caption">
Figure&nbsp;8: Modified backbone
</p><ul>
<li>we use a modified version of SECOND <span class="citation" data-cites="Second">(<a href="#/references" role="doc-biblioref" onclick="">Yan, Mao, and Li 2018</a>)</span> architecture.</li>
<li>we are using only 2D version of it, we do not process data in in way as SECOND <span class="citation" data-cites="Second">(<a href="#/references" role="doc-biblioref" onclick="">Yan, Mao, and Li 2018</a>)</span></li>
<li>The features from 3 different layers goes in parallel to FPN<span class="citation" data-cites="FPN">(<a href="#/references" role="doc-biblioref" onclick="">Lin et al. 2017</a>)</span></li>
<li>We use FPN <span class="citation" data-cites="FPN">(<a href="#/references" role="doc-biblioref" onclick="">Lin et al. 2017</a>)</span> as it is, so we are not showing FPN architecture.</li>
</ul>
</section>
<section id="loss-function" class="slide level2">
<h2>Loss function</h2>
<ul>
<li>The loss is defined as the combination of localization loss, classification loss and directional loss.</li>
<li>For classification we will use focal loss as there is class imbalance issue.</li>
<li>For localization we will use smooth L1 loss, as used in point Pillar <span class="citation" data-cites="PointPillars_2019">(<a href="#/references" role="doc-biblioref" onclick="">Lang et al. 2019</a>)</span>, VoxelNet <span class="citation" data-cites="VoxelNet_2018">(<a href="#/references" role="doc-biblioref" onclick="">Zhou and Tuzel 2018</a>)</span> etc.</li>
<li>The directional loss is simply a cross entropy loss</li>
</ul>
<p><span class="math display">\[\begin{align*}
\mathcal  L = \frac{1}{N}(\beta_{\text{loc}}\mathcal L_{\text{loc}} + \beta_{\text{cls}}\mathcal L_{\text{cls}} + \beta_{\text{dir}}\mathcal L_{\text{dir}})
\end{align*}\]</span></p>
</section>
<section id="experiment-and-result" class="slide level2">
<h2>Experiment and Result</h2>
<ul>
<li>The Kitti dataset shall be used for training.</li>
<li>There are 3 class namely car, pedestrian, cyclist, one network shall be trained for all the 3 classes</li>
<li>Adam optimizer shall be tried followed by SGD and other, and will be selected based on validation set performance, the same goes for learning rate and other hyperparameter</li>
<li>The <span class="math inline">\(\gamma,\beta\)</span> parameter of the learning rate will be selected based on the experiment.</li>
<li>The loss weightage are chosen as per Point Pillar <span class="citation" data-cites="PointPillars_2019">(<a href="#/references" role="doc-biblioref" onclick="">Lang et al. 2019</a>)</span> to start with, and will be changed based on validation set performance.</li>
</ul>
</section>
<section id="dataset" class="slide level2">
<h2>Dataset</h2>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p>Training Dataset length = 3712</p>
<ul>
<li><p>Class Distribution</p>
<table class="caption-top">
<thead>
<tr class="header">
<th><strong>category</strong></th>
<th><strong>number</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Pedestrian</td>
<td>2207</td>
</tr>
<tr class="even">
<td>Cyclist</td>
<td>734</td>
</tr>
<tr class="odd">
<td>Car</td>
<td>14357</td>
</tr>
</tbody>
</table></li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<ul>
<li><p>Training Dataset length = 3769</p>
<ul>
<li><p>Class Distribution</p>
<table class="caption-top">
<thead>
<tr class="header">
<th><strong>category</strong></th>
<th><strong>number</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Pedestrian</td>
<td>2280</td>
</tr>
<tr class="even">
<td>Cyclist</td>
<td>893</td>
</tr>
<tr class="odd">
<td>Car</td>
<td>14385</td>
</tr>
</tbody>
</table></li>
</ul></li>
</ul>
</div></div>
<ul>
<li>we can see the imbalance in the dataset, hence we are using focal loss for classification</li>
<li>The Evaluation will be done based on AP11 and AP40 as suggested by the KITTI Benchmark <span class="citation" data-cites="DBLP:conf/cvpr/GeigerLU12">(<a href="#/references" role="doc-biblioref" onclick="">Geiger, Lenz, and Urtasun 2012</a>)</span></li>
</ul>
</section>
<section id="future-work-1" class="slide level2">
<h2>Future work 1</h2>

<img data-src="images/LocalFeatures.png" class="r-stretch quarto-figure-center" id="fig-local"><p class="caption">
Figure&nbsp;9: Local Features
</p></section>
<section id="future-work-1-1" class="slide level2">
<h2>Future work 1</h2>

<img data-src="images/arch_pic1.png" class="r-stretch quarto-figure-center" id="fig-strechArch"><p class="caption">
Figure&nbsp;10: OverAll Architecture
</p></section>
<section id="future-work-1-2" class="slide level2">
<h2>Future work 1</h2>
<ul>
<li>we can also get local features, using patch based network</li>
<li>Point Bert <span class="citation" data-cites="DBLP:conf/cvpr/YuTR00L22">(<a href="#/references" role="doc-biblioref" onclick="">Yu et al. 2022</a>)</span> used a pre-trained network on point cloud, but they tokenized the point cloud and the performed positional encoding.</li>
<li>But we can do it more efficiently, the point cloud already has position information as it’s coordinate, so if we do not tokenize it we can utilize the coordinate as positional encoding feature.</li>
<li><strong>Novelty</strong>: Use patch based attention encoder to get local feature, use the coordinate location as positional encoding</li>
</ul>
</section>
<section id="future-work-2" class="slide level2">
<h2>Future work 2</h2>

<img data-src="images/overAll.drawio.png" class="r-stretch quarto-figure-center" id="fig-arch"><p class="caption">
Figure&nbsp;11: Proposed Architecture
</p></section>
<section id="future-work-2-1" class="slide level2">
<h2>Future work 2</h2>
<ul>
<li><a href="#/fig-arch" class="quarto-xref">Figure&nbsp;11</a> shows the over all architecture, we extract global and local features from image and 3D point cloud, these features are extracted from a transformer based encoder, having point and channel wise attention.</li>
<li>these features are then fused together with a cross attention mechanism as explained in CAT-Det <span class="citation" data-cites="DBLP:conf/cvpr/ZhangC022">(<a href="#/references" role="doc-biblioref" onclick="">Zhang, Chen, and Huang 2022</a>)</span><br>
</li>
<li><strong>Novelty</strong> : Use feature based multi modality fusion of channel wise attention and point wise attention, for cross attention use both global and local attention.</li>
</ul>
</section>
<section id="references" class="slide level2 smaller scrollable">
<h2>References</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-DBLP:journals/pr/FengZLGM20" class="csl-entry" role="listitem">
Feng, Mingtao, Liang Zhang, Xuefei Lin, Syed Zulqarnain Gilani, and Ajmal Mian. 2020. <span>“Point Attention Network for Semantic Segmentation of 3D Point Clouds.”</span> <em>Pattern Recognit.</em> 107: 107446. <a href="https://doi.org/10.1016/j.patcog.2020.107446">https://doi.org/10.1016/j.patcog.2020.107446</a>.
</div>
<div id="ref-DBLP:conf/cvpr/GeigerLU12" class="csl-entry" role="listitem">
Geiger, Andreas, Philip Lenz, and Raquel Urtasun. 2012. <span>“Are We Ready for Autonomous Driving? The <span>KITTI</span> Vision Benchmark Suite.”</span> In <em>2012 <span>IEEE</span> Conference on Computer Vision and Pattern Recognition, Providence, RI, USA, June 16-21, 2012</em>, 3354–61. <span>IEEE</span> Computer Society. <a href="https://doi.org/10.1109/CVPR.2012.6248074">https://doi.org/10.1109/CVPR.2012.6248074</a>.
</div>
<div id="ref-DBLP:journals/cvm/GuoCLMMH21" class="csl-entry" role="listitem">
Guo, Meng-Hao, Junxiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R. Martin, and Shi-Min Hu. 2021. <span>“<span>PCT:</span> Point Cloud Transformer.”</span> <em>Comput. Vis. Media</em> 7 (2): 187–99. <a href="https://doi.org/10.1007/s41095-021-0229-5">https://doi.org/10.1007/s41095-021-0229-5</a>.
</div>
<div id="ref-PointPillars_2019" class="csl-entry" role="listitem">
Lang, Alex H., Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. 2019. <span>“PointPillars: Fast Encoders for Object Detection from Point Clouds.”</span> In <em><span>IEEE</span> Conference on Computer Vision and Pattern Recognition, <span>CVPR</span> 2019, Long Beach, CA, USA, June 16-20, 2019</em>, 12697–705. Computer Vision Foundation / <span>IEEE</span>. <a href="https://doi.org/10.1109/CVPR.2019.01298">https://doi.org/10.1109/CVPR.2019.01298</a>.
</div>
<div id="ref-FPN" class="csl-entry" role="listitem">
Lin, Tsung-Yi, Piotr Dollár, Ross B. Girshick, Kaiming He, Bharath Hariharan, and Serge J. Belongie. 2017. <span>“Feature Pyramid Networks for Object Detection.”</span> In <em>2017 <span>IEEE</span> Conference on Computer Vision and Pattern Recognition, <span>CVPR</span> 2017, Honolulu, HI, USA, July 21-26, 2017</em>, 936–44. <span>IEEE</span> Computer Society. <a href="https://doi.org/10.1109/CVPR.2017.106">https://doi.org/10.1109/CVPR.2017.106</a>.
</div>
<div id="ref-DBLP:journals/corr/abs-2205-07417" class="csl-entry" role="listitem">
Lu, Dening, Qian Xie, Mingqiang Wei, Linlin Xu, and Jonathan Li. 2022. <span>“Transformers in 3D Point Clouds: <span>A</span> Survey.”</span> <em>CoRR</em> abs/2205.07417. <a href="https://doi.org/10.48550/arXiv.2205.07417">https://doi.org/10.48550/arXiv.2205.07417</a>.
</div>
<div id="ref-DBLP:conf/cvpr/PanXSLH21" class="csl-entry" role="listitem">
Pan, Xuran, Zhuofan Xia, Shiji Song, Li Erran Li, and Gao Huang. 2021. <span>“3D Object Detection with Pointformer.”</span> In <em><span>IEEE</span> Conference on Computer Vision and Pattern Recognition, <span>CVPR</span> 2021, Virtual, June 19-25, 2021</em>, 7463–72. Computer Vision Foundation / <span>IEEE</span>. <a href="https://doi.org/10.1109/CVPR46437.2021.00738">https://doi.org/10.1109/CVPR46437.2021.00738</a>.
</div>
<div id="ref-DBLP:conf/iccv/ParkAG0G21" class="csl-entry" role="listitem">
Park, Dennis, Rares Ambrus, Vitor Guizilini, Jie Li, and Adrien Gaidon. 2021. <span>“Is Pseudo-Lidar Needed for Monocular 3D Object Detection?”</span> In <em>2021 <span>IEEE/CVF</span> International Conference on Computer Vision, <span>ICCV</span> 2021, Montreal, QC, Canada, October 10-17, 2021</em>, 3122–32. <span>IEEE</span>. <a href="https://doi.org/10.1109/ICCV48922.2021.00313">https://doi.org/10.1109/ICCV48922.2021.00313</a>.
</div>
<div id="ref-DBLP:conf/cvpr/QiSMG17" class="csl-entry" role="listitem">
Qi, Charles Ruizhongtai, Hao Su, Kaichun Mo, and Leonidas J. Guibas. 2017. <span>“PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation.”</span> In <em>2017 <span>IEEE</span> Conference on Computer Vision and Pattern Recognition, <span>CVPR</span> 2017, Honolulu, HI, USA, July 21-26, 2017</em>, 77–85. <span>IEEE</span> Computer Society. <a href="https://doi.org/10.1109/CVPR.2017.16">https://doi.org/10.1109/CVPR.2017.16</a>.
</div>
<div id="ref-DBLP:conf/accv/QiuAB22" class="csl-entry" role="listitem">
Qiu, Shi, Saeed Anwar, and Nick Barnes. 2022. <span>“PU-Transformer: Point Cloud Upsampling Transformer.”</span> In <em>Computer Vision - <span>ACCV</span> 2022 - 16th Asian Conference on Computer Vision, Macao, China, December 4-8, 2022, Proceedings, Part <span>I</span></em>, edited by Lei Wang, Juergen Gall, Tat-Jun Chin, Imari Sato, and Rama Chellappa, 13841:326–43. Lecture Notes in Computer Science. Springer. <a href="https://doi.org/10.1007/978-3-031-26319-4\_20">https://doi.org/10.1007/978-3-031-26319-4\_20</a>.
</div>
<div id="ref-DBLP:conf/cvpr/WangCGHCW19" class="csl-entry" role="listitem">
Wang, Yan, Wei-Lun Chao, Divyansh Garg, Bharath Hariharan, Mark E. Campbell, and Kilian Q. Weinberger. 2019. <span>“Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving.”</span> In <em><span>IEEE</span> Conference on Computer Vision and Pattern Recognition, <span>CVPR</span> 2019, Long Beach, CA, USA, June 16-20, 2019</em>, 8445–53. Computer Vision Foundation / <span>IEEE</span>. <a href="https://doi.org/10.1109/CVPR.2019.00864">https://doi.org/10.1109/CVPR.2019.00864</a>.
</div>
<div id="ref-Second" class="csl-entry" role="listitem">
Yan, Yan, Yuxing Mao, and Bo Li. 2018. <span>“<span>SECOND:</span> Sparsely Embedded Convolutional Detection.”</span> <em>Sensors</em> 18 (10): 3337. <a href="https://doi.org/10.3390/s18103337">https://doi.org/10.3390/s18103337</a>.
</div>
<div id="ref-DBLP:conf/cvpr/YuTR00L22" class="csl-entry" role="listitem">
Yu, Xumin, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, and Jiwen Lu. 2022. <span>“Point-BERT: Pre-Training 3D Point Cloud Transformers with Masked Point Modeling.”</span> In <em><span>IEEE/CVF</span> Conference on Computer Vision and Pattern Recognition, <span>CVPR</span> 2022, New Orleans, LA, USA, June 18-24, 2022</em>, 19291–300. <span>IEEE</span>. <a href="https://doi.org/10.1109/CVPR52688.2022.01871">https://doi.org/10.1109/CVPR52688.2022.01871</a>.
</div>
<div id="ref-DBLP:conf/cvpr/ZhangC022" class="csl-entry" role="listitem">
Zhang, Yanan, Jiaxin Chen, and Di Huang. 2022. <span>“CAT-Det: Contrastively Augmented Transformer for Multimodal 3D Object Detection.”</span> In <em><span>IEEE/CVF</span> Conference on Computer Vision and Pattern Recognition, <span>CVPR</span> 2022, New Orleans, LA, USA, June 18-24, 2022</em>, 898–907. <span>IEEE</span>. <a href="https://doi.org/10.1109/CVPR52688.2022.00098">https://doi.org/10.1109/CVPR52688.2022.00098</a>.
</div>
<div id="ref-VoxelNet_2018" class="csl-entry" role="listitem">
Zhou, Yin, and Oncel Tuzel. 2018. <span>“VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection.”</span> In <em>2018 <span>IEEE</span> Conference on Computer Vision and Pattern Recognition, <span>CVPR</span> 2018, Salt Lake City, UT, USA, June 18-22, 2018</em>, 4490–99. Computer Vision Foundation / <span>IEEE</span> Computer Society. <a href="https://doi.org/10.1109/CVPR.2018.00472">https://doi.org/10.1109/CVPR.2018.00472</a>.
</div>
</div>
</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="images/logo.png" class="slide-logo"></p>
<div class="footer footer-default">
<p>IIT Hyderabad</p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': true,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const icon = "";
      const anchorJS = new window.AnchorJS();
      anchorJS.options = {
        placement: 'right',
        icon: icon
      };
      anchorJS.add('.anchored');
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp("http:\/\/localhost:4200\/");
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>