<!DOCTYPE html>
<html lang="en"><head>
<link href="../logo.png" rel="icon" type="image/png">
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.39">

  <meta name="author" content="Abhishek Kumar Dubey   cs22mds15010">
  <meta name="dcterms.date" content="2023-03-12">
  <title>Point cloud based 3d object detection</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-3e1f16ca148d8119ab1c7fbdb2a1aed0.css">
  <link rel="stylesheet" href="styles.css">
  <script async="" src="https://www.googletagmanager.com/gtag/js?id=G-5ZQX02R26E"></script>

  <script type="text/javascript">

  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-5ZQX02R26E', { 'anonymize_ip': true});
  </script>
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
<meta property="og:title" content="Point cloud based 3d object detection">
<meta property="og:description" content="PROPOSAL">
<meta property="og:image" content="http://localhost:4200/Data_Science_Projects/images/Acrobat_UClLO5CYIH.png">
<meta property="og:image:height" content="268">
<meta property="og:image:width" content="487">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="images/FirstPage_background.png" data-background-opacity="0.6" data-background-size="contain" class="quarto-title-block center">
  <h1 class="title">Point cloud based 3d object detection</h1>
  <p class="subtitle">Y. Zhou and O. Tuzel,Voxelnet, IEEE Conference on CVPR, pp.&nbsp;4490-4499, 2018.</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Abhishek Kumar Dubey <br> cs22mds15010 
</div>
</div>
</div>

  <p class="date">2023-03-12</p>
</section>
<section id="motivation" class="slide level2">
<h2>Motivation</h2>
<div class="columns">
<div class="column" style="width:60%;">
<p>The motivations are:</p>
<ul>
<li><p align="justify">
Safety:Improve safety on the road.
</p></li>
<li><p align="justify">
Traffic Management: Manage traffic flow by identifying areas with high congestion.
</p></li>
<li><p align="justify">
Improved Navigation: Better path planning, trajectory calculation.
</p></li>
<li><p align="justify">
Use LiDAR : Compared to image based detection, LiDAR provides reliable depth information that can be used to accurately localize objects and characterize their shapes.
</p></li>
</ul>
</div><div class="column" style="width:40%;">
<p><img data-src="images/Acrobat_Jy4Az0Sjhm.png"></p>
</div></div>
<ul>
<li><p align="justify">
Flexibility:can be used robotics, and augmented reality as well.
</p></li>
</ul>
</section>
<section id="problem-statement" class="slide level2">
<h2>Problem Statement</h2>
<div class="columns">
<div class="column" style="width:40%;">
<ul>
<li><p align="justify">
Develop a solution that can accurately locate a object in 3D space using LiDAR point Cloud.
</p></li>
<li><p align="justify">
The solution must not use any hand-crafted features or prior assumptions, which can limit accuracy and generalization.
</p></li>
</ul>
</div><div class="column" style="width:60%;">
<p><img data-src="images/Acrobat_UClLO5CYIH.png"></p>
</div></div>
<ul>
<li><p align="justify">
Directly process raw LiDAR point clouds.
</p></li>
<li><p align="justify">
Develop an accurate, efficient, and robust model that can generalize to new environments and tasks.
</p></li>
</ul>
</section>
<section id="challenges" class="slide level2">
<h2>Challenges</h2>
<div class="columns">
<div class="column" style="width:40%;">
<p><img data-src="images/Acrobat_YOwpXCawvB.png"></p>
</div><div class="column" style="width:60%;">
<ul>
<li><p align="justify">
LiDAR point clouds are inherently sparse.
</p></li>
<li><p align="justify">
The point density of LiDAR point clouds can vary significantly depending on factors such as the sensorâ€™s effective range, scanning pattern, and the relative pose of the object and the sensor.</p></li>
</ul>
</div></div>
<ul>
<li><p align="justify">
Occlusion can also result in missing or incomplete point cloud data, further exacerbating the sparsity of the data.
</p></li>
<li><p align="justify">
Variations in the point density of LiDAR point clouds can make it difficult to design effective feature extraction and object detection algorithms.
</p></li>
<li><p align="justify">
Addressing the sparsity and variability of LiDAR point clouds is a key challenge in developing accurate and robust 3D object detection models.
</p></li>
</ul>
</section>
<section id="existing-methods-and-limitations" class="slide level2">
<h2>Existing methods and Limitations</h2>
<ul>
<li><p align="justify">
Use pseudo Lidar instead of original Lidar and create the point cloud from the RGB image.However, these kind of method does not perform well, Because extracting point cloud from 2D images are not efficient. There are no depth information present in the 2D image.<a href="#/footnotes" class="footnote-ref" id="fnref1" role="doc-noteref" data-footnote-href="#/fn1" onclick=""><sup>1</sup></a>
</p></li>
<li><p align="justify">
Some methods for 3D object detection from LiDAR point clouds converts the point cloud data into a 2D image-like format, and then use techniques typically used for analyzing images to identify and classify objects.<a href="#/footnotes" class="footnote-ref" id="fnref2" role="doc-noteref" data-footnote-href="#/fn2" onclick=""><sup>2</sup></a> <a href="#/footnotes" class="footnote-ref" id="fnref3" role="doc-noteref" data-footnote-href="#/fn3" onclick=""><sup>3</sup></a>
</p></li>
<li><p align="justify">
Another method for 3D object detection from LiDAR point clouds involves converting the point cloud data into a 3D grid of small cubes (called voxels), and then manually designing features to represent the objects within each voxel.<a href="#/footnotes" class="footnote-ref" id="fnref4" role="doc-noteref" data-footnote-href="#/fn4" onclick=""><sup>4</sup></a>
</p></li>
</ul>
<aside></aside></section>
<section id="voxelnet-introduction" class="slide level2">
<h2>VoxelNet Introduction</h2>
<div class="columns">
<div class="column" style="width:40%;">
<ul>
<li><p align="justify">
it is a generic 3D detection framework that simultaneously learns a discriminative feature representation from point clouds and predicts accurate 3D bounding boxes
</p></li>
<li><p align="justify">
VoxelNet directly operates on the raw point cloud.
</p></li>
</ul>
</div><div class="column" style="width:60%;">
<p><img data-src="images/Acrobat_I9iR951esL.png"></p>
</div></div>
</section>
<section id="voxelnet-architecture" class="slide level2 scrollable">
<h2>VoxelNet Architecture</h2>
<div class="columns">
<div class="column" style="width:70%;">
<p><img data-src="images/Acrobat_ZOIOLjvhTa.png"></p>
</div><div class="column" style="width:30%;">
<p>The VoxelNet consists of three functional blocks</p>
<ol type="1">
<li>Feature learning network</li>
<li>Convolutional middle layers</li>
<li>Region proposal network</li>
</ol>
</div></div>
</section>
<section id="feature-learning-network" class="slide level2">
<h2>Feature Learning Network</h2>
<div class="columns">
<div class="column" style="width:60%;">
<h3 id="voxel-partition" class="anchored">Voxel Partition</h3>
<ul>
<li>Suppose the point cloud encompasses 3D space with range <span class="math inline">\(D, H, W\)</span> along the <span class="math inline">\(Z, Y, X\)</span> axes respectively.</li>
<li>The resulting 3D voxel grid is of size <span class="math inline">\(D'=D/v_D, H'=H/v_H,  W'=W/v_W\)</span> where <span class="math inline">\(v_D,v_H,v_W\)</span> defines the voxel size accordingly</li>
<li>Point density in LiDAR point clouds is variable, resulting in varying numbers of points within each voxel.</li>
</ul>
</div><div class="column" style="width:40%;">
<p><img data-src="images/Acrobat_ffA1dvD6zy.png"></p>
</div></div>
</section>
<section id="section" class="slide level2">
<h2></h2>
<p>Feature Learning Network cont â€¦</p>
<h3 id="random-sampling" class="anchored">Random Sampling</h3>
<ul>
<li>A high-definition LiDAR point cloud typically has around 100k points.</li>
<li>Directly processing all points can overload computing resources and create biased detection due to variable point density.</li>
<li>To avoid this, a fixed number of points (T) is randomly sampled from voxels that contain more than T points.</li>
<li>This sampling strategy has two purposes:
<ul>
<li>computational savings and</li>
<li>decreases the imbalance of points between the voxels which reduces the sampling bias, and adds more variation to training.</li>
</ul></li>
</ul>
</section>
<section id="section-1" class="slide level2">
<h2></h2>
<p>Feature Learning Network cont â€¦</p>
<h3 id="stacked-voxel-feature-encoding" class="anchored">Stacked Voxel Feature Encoding</h3>
<ul>
<li>let <span class="math inline">\({\bf V}\,=\,\{{\bf p}_i\,=\,[x_{i},y_{i},z_{i},r_{i}]^{T}\,\in\,\mathbb{R}^{4}\}_{i=1...t}\)</span> as a non-empty voxel containing <span class="math inline">\(t &lt;T\)</span> LiDAR points.</li>
<li>Here <span class="math inline">\({\bf p}_i\)</span> contains <span class="math inline">\(XYZ\)</span> coordinates for the <span class="math inline">\(i^{\text{th}}\)</span> point and <span class="math inline">\(r_i\)</span> is the received reflectance.</li>
<li>Now find <span class="math inline">\({\mathrm{\bf V}}_{\mathrm{in}}\,=\,\{\;\mathrm{\hat {\bf p}_i}\,=\,[x_{i},y_{i},z_{i},r_{i},x_{i}-v_{x},y_{i}-v_{y},z_{i}-v_{z}]^{T}\in\mathbb{R}^{7}\}_{i=1\ldots t}.\)</span> Here <span class="math inline">\((v_{x},v_{y},v_{z})\)</span> is the local mean as the centroid of all the points in <span class="math inline">\({\bf V}\)</span></li>
<li>Next, each <span class="math inline">\(\hat {\bf p}_i\)</span> is transformed through the fully connected network (FCN) into a feature space.</li>
</ul>
</section>
<section id="section-2" class="slide level2">
<h2></h2>
<p>Feature Learning Network. VFE cont â€¦</p>
<div class="columns">
<div class="column" style="width:40%;">
<ul>
<li>Information from point feature <span class="math inline">\({\bf f}_{i}\ \in\ \mathbb{R}^{m}\)</span> is aggregated <strong>to encode the shape of the surface contained within the voxel</strong></li>
<li>The FCN is composed of a linear layer, a batch normalization (BN) layer, and a rectified linear unit (ReLU) layer</li>
</ul>
</div><div class="column" style="width:60%;">
<p><img data-src="images/Acrobat_HibvuJQPZU.png"></p>
</div></div>
<ul>
<li>Elementwise MaxPooling across all <span class="math inline">\(\bf f_i\)</span> is used to <strong>get the locally aggregated feature</strong> <span class="math inline">\(\hat{\mathbf{f}}\ \in\ \mathbb{R}^{m}\)</span></li>
</ul>
</section>
<section id="section-3" class="slide level2">
<h2></h2>
<p>Feature Learning Network. VFE cont â€¦</p>
<div class="columns">
<div class="column" style="width:80%;">
<ul>
<li>Now form the form the point-wise concatenated feature as <span class="math inline">\(\mathbf{f}_{i}^{o u t}\,=\,\left[\mathbf{f}_{i}^{T},{\tilde{\mathbf{f}}}^{T}\right]^{T}\,\in\,\mathbb{R}^{2m}.\)</span></li>
<li>Finally we get <span class="math inline">\({\bf V_{\ o u t}}\ =\ \{{\bf f}_{i}^{o u t}\}_{i...t}.\)</span></li>
<li>The output feature combines point-wise and locally aggregated features.</li>
<li>Stacking VFE layers allows encoding of point interactions within a voxel, <strong>enabling the final feature representation to learn descriptive shape information</strong>.</li>
</ul>
</div><div class="column" style="width:20%;">
<p><img data-src="images/chrome_Tb0CmOU9L4.png"></p>
</div></div>
<ul>
<li>Voxel features, uniquely associated with their spatial coordinates, are obtained by <strong>processing only non-empty voxels</strong>, resulting in a sparse 4D tensor representation.</li>
</ul>
</section>
<section id="convolutional-middle-layers" class="slide level2">
<h2>Convolutional Middle Layers</h2>
<ul>
<li>Each convolutional middle layer applies 3D convolution, BN layer, and ReLU layer sequentially.</li>
<li>The convolutional middle layers aggregate voxel-wise features within a <strong>progressively expanding receptive field, adding more context to the shape description</strong></li>
<li>Depending on the task this layerâ€™ details can vary.</li>
</ul>
<p><br><br> <img data-src="images/chrome_jVqU0WaxpR.png"></p>
</section>
<section id="region-proposal-network" class="slide level2">
<h2>Region Proposal Network</h2>
<ul>
<li>RPN is combined with the feature learning network and convolutional middle layers to form an <strong>end-to-end trainable pipeline</strong>. <img data-src="images/Acrobat_gayrjAQFrA.png"></li>
<li>The first layer of each block downsamples the feature map by half. via convolution with a stride of 2</li>
<li>Followed by a sequence of convolutions of stride 1, BN and ReLU</li>
<li>The output of each block is upsampled to a fixed size and concatenated to construct a high-resolution feature map.</li>
</ul>
</section>
<section id="loss-function" class="slide level2">
<h2>Loss Function</h2>
<ul>
<li>3D ground truth box is parameterized as <span class="math inline">\((x_{c}^{g},y_{c}^{g},z_{c}^{g},l^{g},w^{g},h^{g},\theta^{g}),\)</span> where <span class="math inline">\(x_{c}^{g},y_{c}^{g},z_{c}^{g}\)</span> represent the center location, <span class="math inline">\(l^{g},w^{g},h^{g}\)</span> are length, width, height of the box,<span class="math inline">\({\theta}^{g}\)</span> is the yaw rotation.</li>
<li>To retrieve the ground truth box from a matching positive anchor parameterized as <span class="math inline">\((x_{c}^{a},y_{c}^{a},z_{c}^{a},l^{a},w^{a},h^{a},\theta^{a})\)</span> a vector <span class="math inline">\(\mathbf{u}^{*}\in\mathbb{R}^{7}\)</span> containing below 7 elements are calculated as below: <span class="math display">\[\begin{align*}
    &amp;\Delta x={\frac{x_{c}^{g}-x_{c}^{a}}{d^{a}}},\Delta y={\frac{y_{c}^{g}-y_{c}^{a}}{d^{a}}},\Delta z={\frac{z_{c}^{g}-z_{c}^{a}}{h^{a}}},\\
    &amp;\Delta l=\log(\frac{l^{g}}{l^{a}}),\Delta w=\log(\frac{w^{g}}{w^{a}}),\Delta h=\log(\frac{h^{g}}{h^{a}}), \tag{1} \\
    &amp;\Delta\theta=\theta^{g}-\theta^{a}
\end{align*}\]</span> Where <span class="math inline">\(\;d^{a}\,=\,\sqrt{(l^{a})^{2}+(w^{a})^{2}}\)</span></li>
</ul>
</section>
<section id="section-4" class="slide level2">
<h2></h2>
<p>Loss Function cont â€¦</p>
<ul>
<li>Now we find loss as shown below <span class="math display">\[\begin{align*}
   L&amp;=\alpha{\frac{1}{N_{\mathrm{pos}}}}\sum_{i}L_{\mathrm{cls}}(p_{i}^{\mathrm{pos}},1)\\
   &amp;+\beta{\frac{1}{N_{\mathrm{neg}}}}\sum_{j}L_{\mathrm{cls}}(p_{j}^{\mathrm{neg}},0)\\
   &amp;+\frac{1}{N_{\mathrm{pos}}}\sum_{i}L_{\mathrm{reg}}({\bf u}_{i},{\bf u}_{i}^{*}) \tag{2}
\end{align*}\]</span></li>
<li>where <span class="math inline">\(p_{i}^{\mathrm{pos}}\)</span> and <span class="math inline">\(p_{j}^{\mathrm{neg}}\)</span> represent the softmax output for positive anchor <span class="math inline">\(a_{i}^{\mathrm{pos}}\)</span> and negative anchor <span class="math inline">\(a_{j}^{\mathrm{neg}}\)</span> respectively,</li>
<li><span class="math inline">\(\mathbf{u}_{i}\ \in\ \mathbb{R}^{7}\)</span> and <span class="math inline">\(\mathbf{u}_{i}^{*}\ \in\ \mathbb{R}^{7}\)</span> are the regression output and ground truth for positive anchor <span class="math inline">\(a_{i}^{\mathrm{pos}}\)</span></li>
</ul>
</section>
<section id="network-details" class="slide level2">
<h2>Network Details</h2>
<h3 id="car-detection" class="anchored">Car Detection</h3>
<ul>
<li>Point clouds within the range of <span class="math inline">\([-3,1]\times[-40,40]\times[0,70.4]\)</span> meters along <span class="math inline">\(X,Y,Z\)</span> respectively is considered.</li>
<li><span class="math inline">\(v_{D}\,=\,0.4,v_{H}\,=\,0.2,v_{W}\,=\,0.2\)</span></li>
<li><span class="math inline">\(D^{\prime}\;=\;10,\;H^{\prime}\;=\;400,\;W^{\prime}\;=\;352\)</span></li>
<li>The maximum number of randomly sampled points in each non-empty voxel <span class="math inline">\(T\,=\,35\)</span></li>
<li>For <span class="math inline">\(\mathrm{VFE}-1(7, 32)\)</span> and and <span class="math inline">\(\mathrm{VFE}-2(32, 128)\)</span> was used.</li>
<li>The final <span class="math inline">\(\mathrm{FCN}\)</span> maps <span class="math inline">\(\mathrm{VFE}\)</span> output to <span class="math inline">\(\mathbb{R}^{128}\)</span></li>
<li>Thus the feature learning net generates a sparse tensor of shape <span class="math inline">\(128\times10\times400\times352\)</span></li>
</ul>
</section>
<section id="section-5" class="slide level2">
<h2></h2>
<p>Network Details, Car Detection cont â€¦</p>

<img data-src="images/Acrobat_9Ud4aJ0aiX.png" class="r-stretch"></section>
<section id="section-6" class="slide level2">
<h2></h2>
<p>Network Details, Car Detection cont â€¦</p>
<ul>
<li>To aggregate voxel-wise features, three convolution middle layers was employed sequentially as <span class="math inline">\(\mathrm{Conv3D}(128, 64, 3,(2,1,1), (1,1,1)),\)</span> <span class="math inline">\(\mathrm{Conv3D}(64, 64, 3, (1,1,1), (0,1,1)),\)</span> and <span class="math inline">\(\mathrm{Conv3D}(64, 64, 3, (2,1,1), (1,1,1)),\)</span></li>
<li>which yields a 4D tensor of size <span class="math inline">\(64\times2\times400\times352.\)</span></li>
<li>After reshaping, size becomes <span class="math inline">\(128\times400\times352.\)</span> and is input to RPN.</li>
<li>Only one anchor size was used <span class="math inline">\(l^{a}\,=\,3.9.\,w^{a}\,=\,1.6.\,h^{a}\,=\,1.56\)</span> meters centered at <span class="math inline">\(z_{c}^{a}\,=\,-1.0\)</span> meters with two rotations, <span class="math inline">\(0^\circ\)</span> and <span class="math inline">\(90^\circ\)</span></li>
<li>Positive anchor: if it has the highest Intersection over Union (IoU) with a ground truth or its IoU with ground truth is above 0.6 (in birdâ€™s eye view).</li>
<li>Positive anchor: if the IoU between it and all ground truth boxes is less than 0.45.</li>
<li>Set <span class="math inline">\(\alpha=1.5\)</span> and <span class="math inline">\(\beta=1\)</span> in equation <span class="math inline">\((2)\)</span></li>
</ul>
</section>
<section id="training" class="slide level2">
<h2>Training</h2>
<ul>
<li>A similar network with different parameters, was used for Pedestrian and Cyclist Detection
<ul>
<li>2 anchors, smaller anchors, small stride to capture fine details etc.</li>
</ul></li>
<li>During training, stochastic gradient descent (SGD) with learning rate 0.01 was used for the first 150 epochs and decreased the learning rate to 0.001 for the last 10 epochs.</li>
<li>Data Augmentation
<ul>
<li>perturbation independently to each ground truth 3D bounding box together with those LiDAR points within the box.</li>
<li>Global scaling.</li>
</ul></li>
</ul>
</section>
<section id="experiments" class="slide level2">
<h2>Experiments</h2>
<ul>
<li>VoxelNet evaluated on KITTI 3D object detection benchmark</li>
<li>7,481 training images/point clouds and 7,518 test images/point clouds</li>
<li>Three categories: Car, Pedestrian, and Cyclist</li>
<li>Detection outcomes evaluated based on three difficulty levels: easy, moderate, and hard</li>
<li>Evaluation protocol based on object size, occlusion state, and truncation level</li>
<li>3,712 data samples for training and 3,769 for validation</li>
<li>Hand-crafted baseline (HC-baseline) was implemented and trained to evaluate the importance of end-to-end learning.</li>
<li>HC-baseline uses the birdâ€™s eye view features described in Multi-view 3d object detection network for autonomous driving<a href="#/footnotes" class="footnote-ref" id="fnref5" role="doc-noteref" data-footnote-href="#/fn5" onclick=""><sup>5</sup></a> which are computed at 0.1 m resolution.</li>
</ul>
<aside></aside></section>
<section id="quantitative-results." class="slide level2">
<h2>Quantitative results.</h2>
<p><br></p>
<ul>
<li>Performance comparison in 3D detection: average precision (in %) on KITTI validation set <img data-src="images/Acrobat_FtLnL5LR5N.png"></li>
</ul>
</section>
<section id="qualitative-results-for-car" class="slide level2">
<h2>Qualitative results for car</h2>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="images/Acrobat_DslR4apvOA.png"> <img data-src="images/Acrobat_33FtFdyehe.png"></p>
</div><div class="column" style="width:50%;">
<p><img data-src="images/Acrobat_6H3QmfEkNv.png"> <img data-src="images/Acrobat_sTmPlMHP48.png"></p>
</div></div>
</section>
<section id="qualitative-results-for-pedestrian" class="slide level2">
<h2>Qualitative results for pedestrian</h2>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="images/Acrobat_9Tjq4xFei9.png"> <img data-src="images/Acrobat_DgRwmDIplZ.png"></p>
</div><div class="column" style="width:50%;">
<p><img data-src="images/Acrobat_6JoySlaJbb.png"> <img data-src="images/Acrobat_Dtk61MhKCI.png"></p>
</div></div>
</section>
<section id="qualitative-results-for-cyclist" class="slide level2">
<h2>Qualitative results for cyclist</h2>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="images/Acrobat_WoOn9jcs4T.png"> <img data-src="images/Acrobat_HZMsnkVH36.png"></p>
</div><div class="column" style="width:50%;">
<p><img data-src="images/Acrobat_14gpVJ4qbh.png"> <img data-src="images/Acrobat_FTiptHGBZ0.png"></p>
</div></div>
</section>
<section id="conclusion" class="slide level2">
<h2>Conclusion</h2>
<ul>
<li>Existing LiDAR-based 3D detection methods rely on hand-crafted features</li>
<li>VoxelNet proposes an end-to-end trainable deep architecture for point cloud-based 3D detection</li>
<li>VoxelNet can operate directly on sparse 3D points and efficiently capture 3D shape information</li>
<li>VoxelNet outperforms state-of-the-art LiDAR-based 3D detection methods on the KITTI car detection task</li>
</ul>
</section>
<section id="novelty" class="slide level2">
<h2>Novelty</h2>
<p><br><br></p>
<ul>
<li>We do not need to predict separately for positive and negative.</li>
<li>Extend VoxelNet for joint LiDAR and image-based end-to-end 3D detection.</li>
</ul>
</section>
<section id="novelty-1" class="slide level2">
<h2>Novelty</h2>
<ul>
<li>Previous time frames can be taken into account improve the detection. <img data-src="images/chrome_44h3Hukd9d.gif"></li>
<li>VFE can be improved with the help of transformer positional encoding.</li>
<li>One universal network instead of two.</li>
</ul>
</section>
<section id="mathcalthanks" class="title-slide slide level1 center">
<h1><span class="math display">\[\mathcal{THANKS!}\]</span></h1>


</section>

<section id="footnotes" class="footnotes footnotes-end-of-document smaller scrollable" role="doc-endnotes"><h2>Footnotes</h2>

<ol>
<li id="fn1"><p>Yan, Wang., Wei-Lun, Chao., Divyansh, Garg., Bharath, Hariharan., Mark, Campbell., Kilian, Q., Weinberger. (2019). Pseudo-LiDAR From Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving. 8445-8453. doi: 10.1109/CVPR.2019.00864</p></li>
<li id="fn2"><p>C. Premebida, J. Carreira, J. Batista, and U. Nunes. Pedestrian detection combining RGB and dense LIDAR data. In IROS, pages 0â€“1. IEEE, Sep 2014. 1, 2</p></li>
<li id="fn3"><p>A. Gonzalez, G. Villalonga, J. Xu, D. Vazquez, J. Amores, and A. Lopez. Multiview random forest of local experts combining rgb and lidar data for pedestrian detection. In IEEE Intelligent Vehicles Symposium (IV), 2015. 1, 2</p></li>
<li id="fn4"><p>A. Gonzalez, G. Villalonga, J. Xu, D. Vazquez, J. Amores, and A. Lopez. Multiview random forest of local experts combining rgb and lidar data for pedestrian detection. In IEEE Intelligent Vehicles Symposium (IV), 2015. 1, 2M. Engelcke, D. Rao, D. Z.Wang, C. H. Tong, and I. Posner. Vote3deep: Fast object detection in 3d point clouds using efficient convolutional neural networks. In 2017 IEEE International Conference on Robotics and Automation (ICRA),</p></li>
<li id="fn5"><p>X. Chen, H. Ma, J. Wan, B. Li, and T. Xia. Multi-view 3d object detection network for autonomous driving. In IEEE CVPR, 2017.</p></li>
</ol>
</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="images/logo.png" class="slide-logo"></p>
<div class="footer footer-default">
<p>IIT Hyderabad</p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': true,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const icon = "î§‹";
      const anchorJS = new window.AnchorJS();
      anchorJS.options = {
        placement: 'right',
        icon: icon
      };
      anchorJS.add('.anchored');
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp("http:\/\/localhost:4200\/");
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>